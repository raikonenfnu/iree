// -----// IR Dump After mlir::iree_compiler::IREE::HAL::AssignTargetDevicesPass (iree-hal-assign-target-devices) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() {
    %c64 = arith.constant 64 : index
    %c8192 = arith.constant 8192 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f16
    %0 = util.unfoldable_constant dense<1.000000e+00> : tensor<1x4096xf16>
    %1 = util.unfoldable_constant dense<1.000000e+00> : tensor<32000x4096xf16>
    %2 = tensor.empty() : tensor<1x32000xf16>
    %3 = linalg.fill ins(%cst : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    check.expect_eq_const(%4, dense<4.096000e+03> : tensor<1x32000xf16>) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After AutoInputConversionPipeline (iree-auto-input-conversion) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() {
    %c64 = arith.constant 64 : index
    %c8192 = arith.constant 8192 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f16
    %0 = util.unfoldable_constant dense<1.000000e+00> : tensor<1x4096xf16>
    %1 = util.unfoldable_constant dense<1.000000e+00> : tensor<32000x4096xf16>
    %2 = tensor.empty() : tensor<1x32000xf16>
    %3 = linalg.fill ins(%cst : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    check.expect_eq_const(%4, dense<4.096000e+03> : tensor<1x32000xf16>) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After IREEImportPublic (iree-import-public) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() {
    %c64 = arith.constant 64 : index
    %c8192 = arith.constant 8192 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f16
    %0 = util.unfoldable_constant dense<1.000000e+00> : tensor<1x4096xf16>
    %1 = util.unfoldable_constant dense<1.000000e+00> : tensor<32000x4096xf16>
    %2 = tensor.empty() : tensor<1x32000xf16>
    %3 = linalg.fill ins(%cst : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    check.expect_eq_const(%4, dense<4.096000e+03> : tensor<1x32000xf16>) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After ImportMLProgram (iree-import-ml-program) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() {
    %c64 = arith.constant 64 : index
    %c8192 = arith.constant 8192 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f16
    %0 = util.unfoldable_constant dense<1.000000e+00> : tensor<1x4096xf16>
    %1 = util.unfoldable_constant dense<1.000000e+00> : tensor<32000x4096xf16>
    %2 = tensor.empty() : tensor<1x32000xf16>
    %3 = linalg.fill ins(%cst : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    check.expect_eq_const(%4, dense<4.096000e+03> : tensor<1x32000xf16>) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After SanitizeModuleNames (iree-sanitize-module-names) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() {
    %c64 = arith.constant 64 : index
    %c8192 = arith.constant 8192 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f16
    %0 = util.unfoldable_constant dense<1.000000e+00> : tensor<1x4096xf16>
    %1 = util.unfoldable_constant dense<1.000000e+00> : tensor<32000x4096xf16>
    %2 = tensor.empty() : tensor<1x32000xf16>
    %3 = linalg.fill ins(%cst : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    check.expect_eq_const(%4, dense<4.096000e+03> : tensor<1x32000xf16>) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::ABI::ConvertStreamableOpsPass (iree-abi-convert-streamable-ops) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() {
    %c64 = arith.constant 64 : index
    %c8192 = arith.constant 8192 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f16
    %0 = util.unfoldable_constant dense<1.000000e+00> : tensor<1x4096xf16>
    %1 = util.unfoldable_constant dense<1.000000e+00> : tensor<32000x4096xf16>
    %2 = tensor.empty() : tensor<1x32000xf16>
    %3 = linalg.fill ins(%cst : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    check.expect_eq_const(%4, dense<4.096000e+03> : tensor<1x32000xf16>) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::ABI::WrapEntryPointsPass (iree-abi-wrap-entry-points) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  func.func private @_main() {
    %c64 = arith.constant 64 : index
    %c8192 = arith.constant 8192 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f16
    %0 = util.unfoldable_constant dense<1.000000e+00> : tensor<1x4096xf16>
    %1 = util.unfoldable_constant dense<1.000000e+00> : tensor<32000x4096xf16>
    %2 = tensor.empty() : tensor<1x32000xf16>
    %3 = linalg.fill ins(%cst : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    check.expect_eq_const(%4, dense<4.096000e+03> : tensor<1x32000xf16>) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %cst_2 = arith.constant 0.000000e+00 : f16
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = tensor.empty() : tensor<1x32000xf16>
  %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%4, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After Inliner (inline) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  func.func private @_main() {
    %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
    %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
    %cst_2 = arith.constant 0.000000e+00 : f16
    %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
    %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
    %2 = tensor.empty() : tensor<1x32000xf16>
    %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    check.expect_eq(%4, %cst) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %cst_2 = arith.constant 0.000000e+00 : f16
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = tensor.empty() : tensor<1x32000xf16>
  %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%4, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %cst_2 = arith.constant 0.000000e+00 : f16
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = tensor.empty() : tensor<1x32000xf16>
  %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%4, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  func.func private @_main() {
    %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
    %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
    %cst_2 = arith.constant 0.000000e+00 : f16
    %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
    %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
    %2 = tensor.empty() : tensor<1x32000xf16>
    %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    check.expect_eq(%4, %cst) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After DemoteF64ToF32 (iree-util-demote-f64-to-f32) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  func.func private @_main() {
    %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
    %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
    %cst_2 = arith.constant 0.000000e+00 : f16
    %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
    %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
    %2 = tensor.empty() : tensor<1x32000xf16>
    %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    check.expect_eq(%4, %cst) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After RemoveZeroExtentTensors (iree-global-opt-remove-zero-extent-tensors) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After RemoveZeroExtentTensors (iree-global-opt-remove-zero-extent-tensors) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %cst_2 = arith.constant 0.000000e+00 : f16
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = tensor.empty() : tensor<1x32000xf16>
  %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%4, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After DetachElementwiseFromNamedOps (iree-global-opt-detach-elementwise-from-named-ops) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After LinalgNamedOpConversion (linalg-named-op-conversion) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After DetachElementwiseFromNamedOps (iree-global-opt-detach-elementwise-from-named-ops) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %cst_2 = arith.constant 0.000000e+00 : f16
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = tensor.empty() : tensor<1x32000xf16>
  %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%4, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After Convert1X1FilterConv2DToMatmul (iree-global-opt-convert-1x1-filter-conv2d-to-matmul) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After LinalgNamedOpConversion (linalg-named-op-conversion) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %cst_2 = arith.constant 0.000000e+00 : f16
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = tensor.empty() : tensor<1x32000xf16>
  %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%4, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After LiftGenericToTransposeBatchMatmul (iree-global-opt-lift-generic-to-tranpose-batch-matmul) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After Convert1X1FilterConv2DToMatmul (iree-global-opt-convert-1x1-filter-conv2d-to-matmul) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %cst_2 = arith.constant 0.000000e+00 : f16
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = tensor.empty() : tensor<1x32000xf16>
  %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%4, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After LiftGenericToTransposeBatchMatmul (iree-global-opt-lift-generic-to-tranpose-batch-matmul) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %cst_2 = arith.constant 0.000000e+00 : f16
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = tensor.empty() : tensor<1x32000xf16>
  %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%4, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After EraseUnusedLinalgOperands (iree-global-opt-erase-unused-linalg-operands) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  func.func private @_main() {
    %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
    %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
    %cst_2 = arith.constant 0.000000e+00 : f16
    %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
    %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
    %2 = tensor.empty() : tensor<1x32000xf16>
    %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    check.expect_eq(%4, %cst) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After ExpandTensorShapes (iree-flow-expand-tensor-shapes) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  func.func private @_main() {
    %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
    %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
    %cst_2 = arith.constant 0.000000e+00 : f16
    %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
    %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
    %2 = tensor.empty() : tensor<1x32000xf16>
    %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    check.expect_eq(%4, %cst) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After ConvertElementwiseToLinalg (convert-elementwise-to-linalg) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After GeneralizeLinalgNamedOps (iree-flow-generalize-linalg-named-ops) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After ConvertElementwiseToLinalg (convert-elementwise-to-linalg) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %cst_2 = arith.constant 0.000000e+00 : f16
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = tensor.empty() : tensor<1x32000xf16>
  %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%4, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After RaiseSpecialOps (iree-flow-raise-special-ops) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After GeneralizeLinalgNamedOps (iree-flow-generalize-linalg-named-ops) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %cst_2 = arith.constant 0.000000e+00 : f16
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = tensor.empty() : tensor<1x32000xf16>
  %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%4, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After FoldUnitExtentDims (iree-flow-fold-unit-extent-dims) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After RaiseSpecialOps (iree-flow-raise-special-ops) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %cst_2 = arith.constant 0.000000e+00 : f16
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = tensor.empty() : tensor<1x32000xf16>
  %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%4, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After FuseDequantizationMatmul (iree-global-opt-fuse-dequantization-matmul) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After FoldUnitExtentDims (iree-flow-fold-unit-extent-dims) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %cst_2 = arith.constant 0.000000e+00 : f16
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = tensor.empty() : tensor<1x32000xf16>
  %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%4, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After FuseDequantizationMatmul (iree-global-opt-fuse-dequantization-matmul) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %cst_2 = arith.constant 0.000000e+00 : f16
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = tensor.empty() : tensor<1x32000xf16>
  %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%4, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %cst_2 = arith.constant 0.000000e+00 : f16
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = tensor.empty() : tensor<1x32000xf16>
  %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%4, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %cst_2 = arith.constant 0.000000e+00 : f16
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = tensor.empty() : tensor<1x32000xf16>
  %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%4, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %cst_2 = arith.constant 0.000000e+00 : f16
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = tensor.empty() : tensor<1x32000xf16>
  %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%4, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan], iree.fixedpoint.iteration = 0 : index} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  func.func private @_main() {
    %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
    %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
    %cst_2 = arith.constant 0.000000e+00 : f16
    %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
    %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
    %2 = tensor.empty() : tensor<1x32000xf16>
    %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    check.expect_eq(%4, %cst) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan], iree.fixedpoint.iteration = 0 : index} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  func.func private @_main() {
    %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
    %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
    %cst_2 = arith.constant 0.000000e+00 : f16
    %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
    %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
    %2 = tensor.empty() : tensor<1x32000xf16>
    %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    check.expect_eq(%4, %cst) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan], iree.fixedpoint.iteration = 0 : index} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  func.func private @_main() {
    %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
    %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
    %cst_2 = arith.constant 0.000000e+00 : f16
    %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
    %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
    %2 = tensor.empty() : tensor<1x32000xf16>
    %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    check.expect_eq(%4, %cst) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan], iree.fixedpoint.iteration = 0 : index} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  func.func private @_main() {
    %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
    %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
    %cst_2 = arith.constant 0.000000e+00 : f16
    %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
    %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
    %2 = tensor.empty() : tensor<1x32000xf16>
    %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    check.expect_eq(%4, %cst) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan], iree.fixedpoint.iteration = 0 : index} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  func.func private @_main() {
    %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
    %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
    %cst_2 = arith.constant 0.000000e+00 : f16
    %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
    %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
    %2 = tensor.empty() : tensor<1x32000xf16>
    %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    check.expect_eq(%4, %cst) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After HoistIntoGlobals (iree-util-hoist-into-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan], iree.fixedpoint.iteration = 0 : index} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  func.func private @_main() {
    %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
    %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
    %cst_2 = arith.constant 0.000000e+00 : f16
    %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
    %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
    %2 = tensor.empty() : tensor<1x32000xf16>
    %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    check.expect_eq(%4, %cst) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After JitGlobals (iree-consteval-jit-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan], iree.fixedpoint.iteration = 0 : index} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  func.func private @_main() {
    %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
    %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
    %cst_2 = arith.constant 0.000000e+00 : f16
    %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
    %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
    %2 = tensor.empty() : tensor<1x32000xf16>
    %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    check.expect_eq(%4, %cst) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %cst_2 = arith.constant 0.000000e+00 : f16
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = tensor.empty() : tensor<1x32000xf16>
  %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%4, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %cst_2 = arith.constant 0.000000e+00 : f16
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = tensor.empty() : tensor<1x32000xf16>
  %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%4, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After FixedPointIterator (iree-util-fixed-point-iterator) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  func.func private @_main() {
    %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
    %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
    %cst_2 = arith.constant 0.000000e+00 : f16
    %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
    %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
    %2 = tensor.empty() : tensor<1x32000xf16>
    %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    check.expect_eq(%4, %cst) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After VerifyInputLegality (iree-verify-input-legality) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  func.func private @_main() {
    %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
    %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
    %cst_2 = arith.constant 0.000000e+00 : f16
    %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
    %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
    %2 = tensor.empty() : tensor<1x32000xf16>
    %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    check.expect_eq(%4, %cst) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After TensorPadToTensorInsertSlice (iree-flow-tensor-pad-to-tensor-insert-slice) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  func.func private @_main() {
    %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
    %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
    %cst_2 = arith.constant 0.000000e+00 : f16
    %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
    %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
    %2 = tensor.empty() : tensor<1x32000xf16>
    %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    check.expect_eq(%4, %cst) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After RaiseSpecialOps (iree-flow-raise-special-ops) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After RaiseSpecialOps (iree-flow-raise-special-ops) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %cst_2 = arith.constant 0.000000e+00 : f16
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = tensor.empty() : tensor<1x32000xf16>
  %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%4, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After InterchangeGenericOps (iree-flow-interchange-generic-ops) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After InterchangeGenericOps (iree-flow-interchange-generic-ops) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %cst_2 = arith.constant 0.000000e+00 : f16
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = tensor.empty() : tensor<1x32000xf16>
  %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%4, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After CollapseDims (iree-flow-collapse-dims) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After CollapseDims (iree-flow-collapse-dims) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %cst_2 = arith.constant 0.000000e+00 : f16
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = tensor.empty() : tensor<1x32000xf16>
  %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%4, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After ResolveShapedTypeResultDims (resolve-shaped-type-result-dims) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After ResolveShapedTypeResultDims (resolve-shaped-type-result-dims) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %cst_2 = arith.constant 0.000000e+00 : f16
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = tensor.empty() : tensor<1x32000xf16>
  %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%4, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %cst_2 = arith.constant 0.000000e+00 : f16
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = tensor.empty() : tensor<1x32000xf16>
  %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%4, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After FusionOfTensorOps (iree-flow-fusion-of-tensor-ops) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %cst_2 = arith.constant 0.000000e+00 : f16
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = tensor.empty() : tensor<1x32000xf16>
  %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%4, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After SplitReduction (iree-flow-split-reduction-ops) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After FusionOfTensorOps (iree-flow-fusion-of-tensor-ops) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %cst_2 = arith.constant 0.000000e+00 : f16
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = tensor.empty() : tensor<1x32000xf16>
  %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%4, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After InterchangeGenericOps (iree-flow-interchange-generic-ops) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After FormScalarDispatches (iree-flow-form-scalar-dispatches) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %cst_2 = arith.constant 0.000000e+00 : f16
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = tensor.empty() : tensor<1x32000xf16>
  %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%4, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After FormDispatchRegions (iree-flow-form-dispatch-regions) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After CollapseDimensions (iree-flow-collapse-dimensions) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After CloneProducersIntoDispatchRegions (iree-flow-clone-producers-into-dispatch-regions) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %cst_2 = arith.constant 0.000000e+00 : f16
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = tensor.empty() : tensor<1x32000xf16>
  %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%4, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After FormDispatchWorkgroups (iree-flow-form-dispatch-workgroups) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After CaptureDispatchDynamicDims (iree-flow-capture-dispatch-dynamic-dims) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After SplitReduction (iree-flow-split-reduction-ops) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %cst_2 = arith.constant 0.000000e+00 : f16
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = tensor.empty() : tensor<1x32000xf16>
  %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%4, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After InterchangeGenericOps (iree-flow-interchange-generic-ops) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %cst_2 = arith.constant 0.000000e+00 : f16
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = tensor.empty() : tensor<1x32000xf16>
  %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%4, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After InitializeEmptyTensors (iree-flow-initialize-empty-tensors) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After FormScalarDispatches (iree-flow-form-scalar-dispatches) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %cst_2 = arith.constant 0.000000e+00 : f16
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = tensor.empty() : tensor<1x32000xf16>
  %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%4, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After FormDispatchRegions (iree-flow-form-dispatch-regions) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %cst_2 = arith.constant 0.000000e+00 : f16
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = tensor.empty() : tensor<1x32000xf16>
  %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  %4 = flow.dispatch.region -> (tensor<1x32000xf16>) {
    %5 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    flow.return %5 : tensor<1x32000xf16>
  }
  check.expect_eq(%4, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After CollapseDimensions (iree-flow-collapse-dimensions) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %cst_2 = arith.constant 0.000000e+00 : f16
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = tensor.empty() : tensor<1x32000xf16>
  %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  %4 = flow.dispatch.region -> (tensor<1x32000xf16>) {
    %5 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    flow.return %5 : tensor<1x32000xf16>
  }
  check.expect_eq(%4, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After CloneProducersIntoDispatchRegions (iree-flow-clone-producers-into-dispatch-regions) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %cst_2 = arith.constant 0.000000e+00 : f16
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = tensor.empty() : tensor<1x32000xf16>
  %3 = linalg.fill ins(%cst_2 : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  %4 = flow.dispatch.region -> (tensor<1x32000xf16>) {
    %5 = tensor.empty() : tensor<1x32000xf16>
    %cst_3 = arith.constant 0.000000e+00 : f16
    %6 = linalg.fill ins(%cst_3 : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    %7 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    flow.return %7 : tensor<1x32000xf16>
  }
  check.expect_eq(%4, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After FormDispatchWorkgroups (iree-flow-form-dispatch-workgroups) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = flow.dispatch.workgroups(%0, %1) : (tensor<1x4096xf16>, tensor<32000x4096xf16>) -> tensor<1x32000xf16> =
      (%arg0: !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>, %arg1: !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>) {
    %cst_2 = arith.constant 0.000000e+00 : f16
    %3 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
    %4 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
    %5 = tensor.empty() : tensor<1x32000xf16>
    %6 = linalg.fill ins(%cst_2 : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    flow.dispatch.tensor.store %7, %arg2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  check.expect_eq(%2, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After CaptureDispatchDynamicDims (iree-flow-capture-dispatch-dynamic-dims) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = flow.dispatch.workgroups(%0, %1) : (tensor<1x4096xf16>, tensor<32000x4096xf16>) -> tensor<1x32000xf16> =
      (%arg0: !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>, %arg1: !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>) {
    %cst_2 = arith.constant 0.000000e+00 : f16
    %3 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
    %4 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
    %5 = tensor.empty() : tensor<1x32000xf16>
    %6 = linalg.fill ins(%cst_2 : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    flow.dispatch.tensor.store %7, %arg2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  check.expect_eq(%2, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = flow.dispatch.workgroups(%0, %1) : (tensor<1x4096xf16>, tensor<32000x4096xf16>) -> tensor<1x32000xf16> =
      (%arg0: !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>, %arg1: !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>) {
    %cst_2 = arith.constant 0.000000e+00 : f16
    %3 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
    %4 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
    %5 = tensor.empty() : tensor<1x32000xf16>
    %6 = linalg.fill ins(%cst_2 : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    flow.dispatch.tensor.store %7, %arg2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  check.expect_eq(%2, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = flow.dispatch.workgroups(%0, %1) : (tensor<1x4096xf16>, tensor<32000x4096xf16>) -> tensor<1x32000xf16> =
      (%arg0: !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>, %arg1: !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>) {
    %cst_2 = arith.constant 0.000000e+00 : f16
    %3 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
    %4 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
    %5 = tensor.empty() : tensor<1x32000xf16>
    %6 = linalg.fill ins(%cst_2 : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    flow.dispatch.tensor.store %7, %arg2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  check.expect_eq(%2, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After InitializeEmptyTensors (iree-flow-initialize-empty-tensors) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = flow.dispatch.workgroups(%0, %1) : (tensor<1x4096xf16>, tensor<32000x4096xf16>) -> tensor<1x32000xf16> =
      (%arg0: !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>, %arg1: !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>) {
    %cst_2 = arith.constant 0.000000e+00 : f16
    %3 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
    %4 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
    %5 = tensor.empty() : tensor<1x32000xf16>
    %6 = linalg.fill ins(%cst_2 : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    flow.dispatch.tensor.store %7, %arg2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  check.expect_eq(%2, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After OutlineDispatchRegions (iree-flow-outline-dispatch-regions) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  flow.executable private @_main_dispatch_0 {
    flow.executable.export public @_main_dispatch_0 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>, %arg1: !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>) {
        %cst = arith.constant 0.000000e+00 : f16
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %2 = tensor.empty() : tensor<1x32000xf16>
        %3 = linalg.fill ins(%cst : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %4, %arg2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
    %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
    %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
    %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
    %2 = flow.dispatch @_main_dispatch_0::@_main_dispatch_0(%0, %1) : (tensor<1x4096xf16>, tensor<32000x4096xf16>) -> tensor<1x32000xf16>
    check.expect_eq(%2, %cst) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After AnnotateDispatches (iree-flow-annotate-dispatches) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  flow.executable private @_main_dispatch_0 {
    flow.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>, %arg1: !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>) {
        %cst = arith.constant 0.000000e+00 : f16
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %2 = tensor.empty() : tensor<1x32000xf16>
        %3 = linalg.fill ins(%cst : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %4, %arg2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
    %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
    %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
    %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
    %2 = flow.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%0, %1) : (tensor<1x4096xf16>, tensor<32000x4096xf16>) -> tensor<1x32000xf16>
    check.expect_eq(%2, %cst) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After StripDebugOps (iree-util-strip-debug-ops) //----- //
flow.executable private @_main_dispatch_0 {
  flow.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>, %arg1: !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>) {
      %cst = arith.constant 0.000000e+00 : f16
      %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
      %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
      %2 = tensor.empty() : tensor<1x32000xf16>
      %3 = linalg.fill ins(%cst : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
      %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
      flow.dispatch.tensor.store %4, %arg2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
      return
    }
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = flow.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%0, %1) : (tensor<1x4096xf16>, tensor<32000x4096xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%2, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After DeduplicateExecutables (iree-flow-deduplicate-executables) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  flow.executable private @_main_dispatch_0 {
    flow.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>, %arg1: !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>) {
        %cst = arith.constant 0.000000e+00 : f16
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %2 = tensor.empty() : tensor<1x32000xf16>
        %3 = linalg.fill ins(%cst : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %4, %arg2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
    %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
    %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
    %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
    %2 = flow.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%0, %1) : (tensor<1x4096xf16>, tensor<32000x4096xf16>) -> tensor<1x32000xf16>
    check.expect_eq(%2, %cst) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After CleanupTensorShapes (iree-flow-cleanup-tensor-shapes) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After CleanupTensorShapes (iree-flow-cleanup-tensor-shapes) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = flow.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%0, %1) : (tensor<1x4096xf16>, tensor<32000x4096xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%2, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
flow.executable private @_main_dispatch_0 {
  flow.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>, %arg1: !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>) {
      %cst = arith.constant 0.000000e+00 : f16
      %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
      %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
      %2 = tensor.empty() : tensor<1x32000xf16>
      %3 = linalg.fill ins(%cst : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
      %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
      flow.dispatch.tensor.store %4, %arg2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
      return
    }
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After CSE (cse) //----- //
flow.executable private @_main_dispatch_0 {
  flow.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>, %arg1: !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>) {
      %cst = arith.constant 0.000000e+00 : f16
      %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
      %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
      %2 = tensor.empty() : tensor<1x32000xf16>
      %3 = linalg.fill ins(%cst : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
      %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
      flow.dispatch.tensor.store %4, %arg2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
      return
    }
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = flow.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%0, %1) : (tensor<1x4096xf16>, tensor<32000x4096xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%2, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = flow.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%0, %1) : (tensor<1x4096xf16>, tensor<32000x4096xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%2, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  flow.executable private @_main_dispatch_0 {
    flow.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>, %arg1: !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>) {
        %cst = arith.constant 0.000000e+00 : f16
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %2 = tensor.empty() : tensor<1x32000xf16>
        %3 = linalg.fill ins(%cst : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %4, %arg2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
    %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
    %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
    %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
    %2 = flow.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%0, %1) : (tensor<1x4096xf16>, tensor<32000x4096xf16>) -> tensor<1x32000xf16>
    check.expect_eq(%2, %cst) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After VerifyInput (iree-stream-verify-input) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  flow.executable private @_main_dispatch_0 {
    flow.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>, %arg1: !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>) {
        %cst = arith.constant 0.000000e+00 : f16
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %2 = tensor.empty() : tensor<1x32000xf16>
        %3 = linalg.fill ins(%cst : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %4, %arg2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
    %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
    %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
    %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
    %2 = flow.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%0, %1) : (tensor<1x4096xf16>, tensor<32000x4096xf16>) -> tensor<1x32000xf16>
    check.expect_eq(%2, %cst) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = flow.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%0, %1) : (tensor<1x4096xf16>, tensor<32000x4096xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%2, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = flow.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%0, %1) : (tensor<1x4096xf16>, tensor<32000x4096xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%2, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = flow.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%0, %1) : (tensor<1x4096xf16>, tensor<32000x4096xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%2, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  flow.executable private @_main_dispatch_0 {
    flow.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>, %arg1: !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>) {
        %cst = arith.constant 0.000000e+00 : f16
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %2 = tensor.empty() : tensor<1x32000xf16>
        %3 = linalg.fill ins(%cst : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %4, %arg2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
    %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
    %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
    %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
    %2 = flow.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%0, %1) : (tensor<1x4096xf16>, tensor<32000x4096xf16>) -> tensor<1x32000xf16>
    check.expect_eq(%2, %cst) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  flow.executable private @_main_dispatch_0 {
    flow.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>, %arg1: !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>) {
        %cst = arith.constant 0.000000e+00 : f16
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %2 = tensor.empty() : tensor<1x32000xf16>
        %3 = linalg.fill ins(%cst : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %4, %arg2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
    %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
    %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
    %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
    %2 = flow.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%0, %1) : (tensor<1x4096xf16>, tensor<32000x4096xf16>) -> tensor<1x32000xf16>
    check.expect_eq(%2, %cst) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  flow.executable private @_main_dispatch_0 {
    flow.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>, %arg1: !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>) {
        %cst = arith.constant 0.000000e+00 : f16
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %2 = tensor.empty() : tensor<1x32000xf16>
        %3 = linalg.fill ins(%cst : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %4, %arg2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
    %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
    %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
    %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
    %2 = flow.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%0, %1) : (tensor<1x4096xf16>, tensor<32000x4096xf16>) -> tensor<1x32000xf16>
    check.expect_eq(%2, %cst) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  flow.executable private @_main_dispatch_0 {
    flow.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>, %arg1: !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>) {
        %cst = arith.constant 0.000000e+00 : f16
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %2 = tensor.empty() : tensor<1x32000xf16>
        %3 = linalg.fill ins(%cst : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %4, %arg2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
    %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
    %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
    %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
    %2 = flow.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%0, %1) : (tensor<1x4096xf16>, tensor<32000x4096xf16>) -> tensor<1x32000xf16>
    check.expect_eq(%2, %cst) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After OutlineConstants (iree-util-outline-constants) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  flow.executable private @_main_dispatch_0 {
    flow.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>, %arg1: !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>) {
        %cst = arith.constant 0.000000e+00 : f16
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %2 = tensor.empty() : tensor<1x32000xf16>
        %3 = linalg.fill ins(%cst : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %4, %arg2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
    %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
    %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
    %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
    %2 = flow.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%0, %1) : (tensor<1x4096xf16>, tensor<32000x4096xf16>) -> tensor<1x32000xf16>
    check.expect_eq(%2, %cst) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = flow.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%0, %1) : (tensor<1x4096xf16>, tensor<32000x4096xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%2, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = flow.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%0, %1) : (tensor<1x4096xf16>, tensor<32000x4096xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%2, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func private @_main() {
  %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
  %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
  %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
  %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
  %2 = flow.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%0, %1) : (tensor<1x4096xf16>, tensor<32000x4096xf16>) -> tensor<1x32000xf16>
  check.expect_eq(%2, %cst) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  flow.executable private @_main_dispatch_0 {
    flow.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>, %arg1: !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>) {
        %cst = arith.constant 0.000000e+00 : f16
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %2 = tensor.empty() : tensor<1x32000xf16>
        %3 = linalg.fill ins(%cst : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %4, %arg2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
    %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
    %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
    %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
    %2 = flow.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%0, %1) : (tensor<1x4096xf16>, tensor<32000x4096xf16>) -> tensor<1x32000xf16>
    check.expect_eq(%2, %cst) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  flow.executable private @_main_dispatch_0 {
    flow.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>, %arg1: !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>) {
        %cst = arith.constant 0.000000e+00 : f16
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %2 = tensor.empty() : tensor<1x32000xf16>
        %3 = linalg.fill ins(%cst : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %4, %arg2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
    %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
    %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
    %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
    %2 = flow.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%0, %1) : (tensor<1x4096xf16>, tensor<32000x4096xf16>) -> tensor<1x32000xf16>
    check.expect_eq(%2, %cst) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  flow.executable private @_main_dispatch_0 {
    flow.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>, %arg1: !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>) {
        %cst = arith.constant 0.000000e+00 : f16
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %2 = tensor.empty() : tensor<1x32000xf16>
        %3 = linalg.fill ins(%cst : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %4, %arg2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
    %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
    %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
    %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
    %2 = flow.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%0, %1) : (tensor<1x4096xf16>, tensor<32000x4096xf16>) -> tensor<1x32000xf16>
    check.expect_eq(%2, %cst) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  flow.executable private @_main_dispatch_0 {
    flow.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>, %arg1: !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>) {
        %cst = arith.constant 0.000000e+00 : f16
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %2 = tensor.empty() : tensor<1x32000xf16>
        %3 = linalg.fill ins(%cst : f16) outs(%2 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %4 = linalg.matmul_transpose_b ins(%0, %1 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%3 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %4, %arg2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %cst = arith.constant dense<4.096000e+03> : tensor<1x32000xf16>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<32000x4096xf16>
    %cst_1 = arith.constant dense<1.000000e+00> : tensor<1x4096xf16>
    %0 = util.optimization_barrier %cst_1 : tensor<1x4096xf16>
    %1 = util.optimization_barrier %cst_0 : tensor<32000x4096xf16>
    %2 = flow.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%0, %1) : (tensor<1x4096xf16>, tensor<32000x4096xf16>) -> tensor<1x32000xf16>
    check.expect_eq(%2, %cst) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After ConvertToStream (iree-stream-conversion) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %cst = arith.constant 0.000000e+00 : f16
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %cst = stream.tensor.constant : tensor<1x32000xf16> in !stream.resource<constant> = dense<4.096000e+03> : tensor<1x32000xf16>
    %0 = stream.resource.size %cst : !stream.resource<constant>
    %1 = stream.async.transfer %cst : !stream.resource<constant>{%0} -> !stream.resource<*>{%0}
    %cst_0 = stream.tensor.constant : tensor<32000x4096xf16> in !stream.resource<constant> = dense<1.000000e+00> : tensor<32000x4096xf16>
    %2 = stream.resource.size %cst_0 : !stream.resource<constant>
    %3 = stream.async.transfer %cst_0 : !stream.resource<constant>{%2} -> !stream.resource<*>{%2}
    %cst_1 = stream.tensor.constant : tensor<1x4096xf16> in !stream.resource<constant> = dense<1.000000e+00> : tensor<1x4096xf16>
    %4 = stream.resource.size %cst_1 : !stream.resource<constant>
    %5 = stream.async.transfer %cst_1 : !stream.resource<constant>{%4} -> !stream.resource<*>{%4}
    %6 = util.optimization_barrier %5 : !stream.resource<*>
    %7 = util.optimization_barrier %3 : !stream.resource<*>
    %c0 = arith.constant 0 : index
    %8 = stream.resource.size %6 : !stream.resource<*>
    %9 = stream.resource.size %7 : !stream.resource<*>
    %10 = stream.tensor.sizeof tensor<1x32000xf16> : index
    %11 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%6[%c0 to %8 for %8], %7[%c0 to %9 for %9]) : (!stream.resource<*>{%8}, !stream.resource<*>{%9}) -> !stream.resource<*>{%10}
    %12 = stream.async.transfer %11 : !stream.resource<*>{%10} -> !stream.resource<external>{%10}
    %13 = stream.tensor.export %12 : tensor<1x32000xf16> in !stream.resource<external>{%10} -> tensor<1x32000xf16>
    %14 = stream.async.transfer %1 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
    %15 = stream.tensor.export %14 : tensor<1x32000xf16> in !stream.resource<external>{%0} -> tensor<1x32000xf16>
    check.expect_eq(%13, %15) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After VerifyLoweringToTensors (iree-stream-verify-lowering-to-tensors) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %cst = arith.constant 0.000000e+00 : f16
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %cst = stream.tensor.constant : tensor<1x32000xf16> in !stream.resource<constant> = dense<4.096000e+03> : tensor<1x32000xf16>
    %0 = stream.resource.size %cst : !stream.resource<constant>
    %1 = stream.async.transfer %cst : !stream.resource<constant>{%0} -> !stream.resource<*>{%0}
    %cst_0 = stream.tensor.constant : tensor<32000x4096xf16> in !stream.resource<constant> = dense<1.000000e+00> : tensor<32000x4096xf16>
    %2 = stream.resource.size %cst_0 : !stream.resource<constant>
    %3 = stream.async.transfer %cst_0 : !stream.resource<constant>{%2} -> !stream.resource<*>{%2}
    %cst_1 = stream.tensor.constant : tensor<1x4096xf16> in !stream.resource<constant> = dense<1.000000e+00> : tensor<1x4096xf16>
    %4 = stream.resource.size %cst_1 : !stream.resource<constant>
    %5 = stream.async.transfer %cst_1 : !stream.resource<constant>{%4} -> !stream.resource<*>{%4}
    %6 = util.optimization_barrier %5 : !stream.resource<*>
    %7 = util.optimization_barrier %3 : !stream.resource<*>
    %c0 = arith.constant 0 : index
    %8 = stream.resource.size %6 : !stream.resource<*>
    %9 = stream.resource.size %7 : !stream.resource<*>
    %10 = stream.tensor.sizeof tensor<1x32000xf16> : index
    %11 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%6[%c0 to %8 for %8], %7[%c0 to %9 for %9]) : (!stream.resource<*>{%8}, !stream.resource<*>{%9}) -> !stream.resource<*>{%10}
    %12 = stream.async.transfer %11 : !stream.resource<*>{%10} -> !stream.resource<external>{%10}
    %13 = stream.tensor.export %12 : tensor<1x32000xf16> in !stream.resource<external>{%10} -> tensor<1x32000xf16>
    %14 = stream.async.transfer %1 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
    %15 = stream.tensor.export %14 : tensor<1x32000xf16> in !stream.resource<external>{%0} -> tensor<1x32000xf16>
    check.expect_eq(%13, %15) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func private @_main() {
  %cst = arith.constant 1.000000e+00 : f16
  %cst_0 = arith.constant 4.096000e+03 : f16
  %c0 = arith.constant 0 : index
  %0 = stream.tensor.sizeof tensor<1x32000xf16> : index
  %1 = stream.tensor.splat %cst_0 : f16 -> tensor<1x32000xf16> in !stream.resource<*>{%0}
  %2 = stream.tensor.sizeof tensor<32000x4096xf16> : index
  %3 = stream.tensor.splat %cst : f16 -> tensor<32000x4096xf16> in !stream.resource<*>{%2}
  %4 = stream.tensor.sizeof tensor<1x4096xf16> : index
  %5 = stream.tensor.splat %cst : f16 -> tensor<1x4096xf16> in !stream.resource<*>{%4}
  %6 = util.optimization_barrier %5 : !stream.resource<*>
  %7 = util.optimization_barrier %3 : !stream.resource<*>
  %8 = stream.resource.size %6 : !stream.resource<*>
  %9 = stream.resource.size %7 : !stream.resource<*>
  %10 = stream.tensor.sizeof tensor<1x32000xf16> : index
  %11 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%6[%c0 to %8 for %8], %7[%c0 to %9 for %9]) : (!stream.resource<*>{%8}, !stream.resource<*>{%9}) -> !stream.resource<*>{%10}
  %12 = stream.async.transfer %11 : !stream.resource<*>{%10} -> !stream.resource<external>{%10}
  %13 = stream.tensor.export %12 : tensor<1x32000xf16> in !stream.resource<external>{%10} -> tensor<1x32000xf16>
  %14 = stream.async.transfer %1 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
  %15 = stream.tensor.export %14 : tensor<1x32000xf16> in !stream.resource<external>{%0} -> tensor<1x32000xf16>
  check.expect_eq(%13, %15) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func private @_main() {
  %cst = arith.constant 1.000000e+00 : f16
  %cst_0 = arith.constant 4.096000e+03 : f16
  %c0 = arith.constant 0 : index
  %0 = stream.tensor.sizeof tensor<1x32000xf16> : index
  %1 = stream.tensor.splat %cst_0 : f16 -> tensor<1x32000xf16> in !stream.resource<*>{%0}
  %2 = stream.tensor.sizeof tensor<32000x4096xf16> : index
  %3 = stream.tensor.splat %cst : f16 -> tensor<32000x4096xf16> in !stream.resource<*>{%2}
  %4 = stream.tensor.sizeof tensor<1x4096xf16> : index
  %5 = stream.tensor.splat %cst : f16 -> tensor<1x4096xf16> in !stream.resource<*>{%4}
  %6 = util.optimization_barrier %5 : !stream.resource<*>
  %7 = util.optimization_barrier %3 : !stream.resource<*>
  %8 = stream.resource.size %6 : !stream.resource<*>
  %9 = stream.resource.size %7 : !stream.resource<*>
  %10 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%6[%c0 to %8 for %8], %7[%c0 to %9 for %9]) : (!stream.resource<*>{%8}, !stream.resource<*>{%9}) -> !stream.resource<*>{%0}
  %11 = stream.async.transfer %10 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
  %12 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%0} -> tensor<1x32000xf16>
  %13 = stream.async.transfer %1 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
  %14 = stream.tensor.export %13 : tensor<1x32000xf16> in !stream.resource<external>{%0} -> tensor<1x32000xf16>
  check.expect_eq(%12, %14) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func private @_main() {
  %cst = arith.constant 1.000000e+00 : f16
  %cst_0 = arith.constant 4.096000e+03 : f16
  %c0 = arith.constant 0 : index
  %0 = stream.tensor.sizeof tensor<1x32000xf16> : index
  %1 = stream.tensor.splat %cst_0 : f16 -> tensor<1x32000xf16> in !stream.resource<*>{%0}
  %2 = stream.tensor.sizeof tensor<32000x4096xf16> : index
  %3 = stream.tensor.splat %cst : f16 -> tensor<32000x4096xf16> in !stream.resource<*>{%2}
  %4 = stream.tensor.sizeof tensor<1x4096xf16> : index
  %5 = stream.tensor.splat %cst : f16 -> tensor<1x4096xf16> in !stream.resource<*>{%4}
  %6 = util.optimization_barrier %5 : !stream.resource<*>
  %7 = util.optimization_barrier %3 : !stream.resource<*>
  %8 = stream.resource.size %6 : !stream.resource<*>
  %9 = stream.resource.size %7 : !stream.resource<*>
  %10 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%6[%c0 to %8 for %8], %7[%c0 to %9 for %9]) : (!stream.resource<*>{%8}, !stream.resource<*>{%9}) -> !stream.resource<*>{%0}
  %11 = stream.async.transfer %10 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
  %12 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%0} -> tensor<1x32000xf16>
  %13 = stream.async.transfer %1 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
  %14 = stream.tensor.export %13 : tensor<1x32000xf16> in !stream.resource<external>{%0} -> tensor<1x32000xf16>
  check.expect_eq(%12, %14) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %cst = arith.constant 1.000000e+00 : f16
    %cst_0 = arith.constant 4.096000e+03 : f16
    %c0 = arith.constant 0 : index
    %0 = stream.tensor.sizeof tensor<1x32000xf16> : index
    %1 = stream.tensor.splat %cst_0 : f16 -> tensor<1x32000xf16> in !stream.resource<*>{%0}
    %2 = stream.tensor.sizeof tensor<32000x4096xf16> : index
    %3 = stream.tensor.splat %cst : f16 -> tensor<32000x4096xf16> in !stream.resource<*>{%2}
    %4 = stream.tensor.sizeof tensor<1x4096xf16> : index
    %5 = stream.tensor.splat %cst : f16 -> tensor<1x4096xf16> in !stream.resource<*>{%4}
    %6 = util.optimization_barrier %5 : !stream.resource<*>
    %7 = util.optimization_barrier %3 : !stream.resource<*>
    %8 = stream.resource.size %6 : !stream.resource<*>
    %9 = stream.resource.size %7 : !stream.resource<*>
    %10 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%6[%c0 to %8 for %8], %7[%c0 to %9 for %9]) : (!stream.resource<*>{%8}, !stream.resource<*>{%9}) -> !stream.resource<*>{%0}
    %11 = stream.async.transfer %10 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
    %12 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%0} -> tensor<1x32000xf16>
    %13 = stream.async.transfer %1 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
    %14 = stream.tensor.export %13 : tensor<1x32000xf16> in !stream.resource<external>{%0} -> tensor<1x32000xf16>
    check.expect_eq(%12, %14) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %cst = arith.constant 1.000000e+00 : f16
    %cst_0 = arith.constant 4.096000e+03 : f16
    %c0 = arith.constant 0 : index
    %0 = stream.tensor.sizeof tensor<1x32000xf16> : index
    %1 = stream.tensor.splat %cst_0 : f16 -> tensor<1x32000xf16> in !stream.resource<*>{%0}
    %2 = stream.tensor.sizeof tensor<32000x4096xf16> : index
    %3 = stream.tensor.splat %cst : f16 -> tensor<32000x4096xf16> in !stream.resource<*>{%2}
    %4 = stream.tensor.sizeof tensor<1x4096xf16> : index
    %5 = stream.tensor.splat %cst : f16 -> tensor<1x4096xf16> in !stream.resource<*>{%4}
    %6 = util.optimization_barrier %5 : !stream.resource<*>
    %7 = util.optimization_barrier %3 : !stream.resource<*>
    %8 = stream.resource.size %6 : !stream.resource<*>
    %9 = stream.resource.size %7 : !stream.resource<*>
    %10 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%6[%c0 to %8 for %8], %7[%c0 to %9 for %9]) : (!stream.resource<*>{%8}, !stream.resource<*>{%9}) -> !stream.resource<*>{%0}
    %11 = stream.async.transfer %10 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
    %12 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%0} -> tensor<1x32000xf16>
    %13 = stream.async.transfer %1 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
    %14 = stream.tensor.export %13 : tensor<1x32000xf16> in !stream.resource<external>{%0} -> tensor<1x32000xf16>
    check.expect_eq(%12, %14) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %cst = arith.constant 1.000000e+00 : f16
    %cst_0 = arith.constant 4.096000e+03 : f16
    %c0 = arith.constant 0 : index
    %0 = stream.tensor.sizeof tensor<1x32000xf16> : index
    %1 = stream.tensor.splat %cst_0 : f16 -> tensor<1x32000xf16> in !stream.resource<*>{%0}
    %2 = stream.tensor.sizeof tensor<32000x4096xf16> : index
    %3 = stream.tensor.splat %cst : f16 -> tensor<32000x4096xf16> in !stream.resource<*>{%2}
    %4 = stream.tensor.sizeof tensor<1x4096xf16> : index
    %5 = stream.tensor.splat %cst : f16 -> tensor<1x4096xf16> in !stream.resource<*>{%4}
    %6 = util.optimization_barrier %5 : !stream.resource<*>
    %7 = util.optimization_barrier %3 : !stream.resource<*>
    %8 = stream.resource.size %6 : !stream.resource<*>
    %9 = stream.resource.size %7 : !stream.resource<*>
    %10 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%6[%c0 to %8 for %8], %7[%c0 to %9 for %9]) : (!stream.resource<*>{%8}, !stream.resource<*>{%9}) -> !stream.resource<*>{%0}
    %11 = stream.async.transfer %10 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
    %12 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%0} -> tensor<1x32000xf16>
    %13 = stream.async.transfer %1 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
    %14 = stream.tensor.export %13 : tensor<1x32000xf16> in !stream.resource<external>{%0} -> tensor<1x32000xf16>
    check.expect_eq(%12, %14) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %cst = arith.constant 1.000000e+00 : f16
    %cst_0 = arith.constant 4.096000e+03 : f16
    %c0 = arith.constant 0 : index
    %0 = stream.tensor.sizeof tensor<1x32000xf16> : index
    %1 = stream.tensor.splat %cst_0 : f16 -> tensor<1x32000xf16> in !stream.resource<*>{%0}
    %2 = stream.tensor.sizeof tensor<32000x4096xf16> : index
    %3 = stream.tensor.splat %cst : f16 -> tensor<32000x4096xf16> in !stream.resource<*>{%2}
    %4 = stream.tensor.sizeof tensor<1x4096xf16> : index
    %5 = stream.tensor.splat %cst : f16 -> tensor<1x4096xf16> in !stream.resource<*>{%4}
    %6 = util.optimization_barrier %5 : !stream.resource<*>
    %7 = util.optimization_barrier %3 : !stream.resource<*>
    %8 = stream.resource.size %6 : !stream.resource<*>
    %9 = stream.resource.size %7 : !stream.resource<*>
    %10 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%6[%c0 to %8 for %8], %7[%c0 to %9 for %9]) : (!stream.resource<*>{%8}, !stream.resource<*>{%9}) -> !stream.resource<*>{%0}
    %11 = stream.async.transfer %10 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
    %12 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%0} -> tensor<1x32000xf16>
    %13 = stream.async.transfer %1 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
    %14 = stream.tensor.export %13 : tensor<1x32000xf16> in !stream.resource<external>{%0} -> tensor<1x32000xf16>
    check.expect_eq(%12, %14) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After CombineInitializers (iree-util-combine-initializers) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %cst = arith.constant 1.000000e+00 : f16
    %cst_0 = arith.constant 4.096000e+03 : f16
    %c0 = arith.constant 0 : index
    %0 = stream.tensor.sizeof tensor<1x32000xf16> : index
    %1 = stream.tensor.splat %cst_0 : f16 -> tensor<1x32000xf16> in !stream.resource<*>{%0}
    %2 = stream.tensor.sizeof tensor<32000x4096xf16> : index
    %3 = stream.tensor.splat %cst : f16 -> tensor<32000x4096xf16> in !stream.resource<*>{%2}
    %4 = stream.tensor.sizeof tensor<1x4096xf16> : index
    %5 = stream.tensor.splat %cst : f16 -> tensor<1x4096xf16> in !stream.resource<*>{%4}
    %6 = util.optimization_barrier %5 : !stream.resource<*>
    %7 = util.optimization_barrier %3 : !stream.resource<*>
    %8 = stream.resource.size %6 : !stream.resource<*>
    %9 = stream.resource.size %7 : !stream.resource<*>
    %10 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%6[%c0 to %8 for %8], %7[%c0 to %9 for %9]) : (!stream.resource<*>{%8}, !stream.resource<*>{%9}) -> !stream.resource<*>{%0}
    %11 = stream.async.transfer %10 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
    %12 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%0} -> tensor<1x32000xf16>
    %13 = stream.async.transfer %1 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
    %14 = stream.tensor.export %13 : tensor<1x32000xf16> in !stream.resource<external>{%0} -> tensor<1x32000xf16>
    check.expect_eq(%12, %14) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After EncodeHostTensors (iree-stream-encode-host-tensors) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After EncodeDeviceTensors (iree-stream-encode-device-tensors) //----- //
stream.executable private @_main_dispatch_0 {
  stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    stream.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
      %cst = arith.constant 0.000000e+00 : f16
      %c0 = arith.constant 0 : index
      %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
      %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
      %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
      %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
      %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
      %5 = tensor.empty() : tensor<1x32000xf16>
      %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
      %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
      flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
      return
    }
  }
}

// -----// IR Dump After EncodeHostTensors (iree-stream-encode-host-tensors) //----- //
func.func private @_main() {
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %c0 = arith.constant 0 : index
  %0 = stream.async.splat %c27648_i16 : i16 -> !stream.resource<*>{%c64000}
  %1 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<*>{%c262144000}
  %2 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<*>{%c8192}
  %3 = util.optimization_barrier %2 : !stream.resource<*>
  %4 = util.optimization_barrier %1 : !stream.resource<*>
  %5 = stream.resource.size %3 : !stream.resource<*>
  %6 = stream.resource.size %4 : !stream.resource<*>
  %7 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%3[%c0 to %5 for %5], %4[%c0 to %6 for %6]) : (!stream.resource<*>{%5}, !stream.resource<*>{%6}) -> !stream.resource<*>{%c64000}
  %8 = stream.async.transfer %7 : !stream.resource<*>{%c64000} -> !stream.resource<external>{%c64000}
  %9 = stream.tensor.export %8 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  %10 = stream.async.transfer %0 : !stream.resource<*>{%c64000} -> !stream.resource<external>{%c64000}
  %11 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  check.expect_eq(%9, %11) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func private @_main() {
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %c0 = arith.constant 0 : index
  %0 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<*>{%c8192}
  %1 = util.optimization_barrier %0 : !stream.resource<*>
  %2 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<*>{%c262144000}
  %3 = util.optimization_barrier %2 : !stream.resource<*>
  %4 = stream.resource.size %1 : !stream.resource<*>
  %5 = stream.resource.size %3 : !stream.resource<*>
  %6 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%1[%c0 to %4 for %4], %3[%c0 to %5 for %5]) : (!stream.resource<*>{%4}, !stream.resource<*>{%5}) -> !stream.resource<*>{%c64000}
  %7 = stream.async.transfer %6 : !stream.resource<*>{%c64000} -> !stream.resource<external>{%c64000}
  %8 = stream.tensor.export %7 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  %9 = stream.async.splat %c27648_i16 : i16 -> !stream.resource<*>{%c64000}
  %10 = stream.async.transfer %9 : !stream.resource<*>{%c64000} -> !stream.resource<external>{%c64000}
  %11 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  check.expect_eq(%8, %11) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func private @_main() {
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %c0 = arith.constant 0 : index
  %0 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<*>{%c8192}
  %1 = util.optimization_barrier %0 : !stream.resource<*>
  %2 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<*>{%c262144000}
  %3 = util.optimization_barrier %2 : !stream.resource<*>
  %4 = stream.resource.size %1 : !stream.resource<*>
  %5 = stream.resource.size %3 : !stream.resource<*>
  %6 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%1[%c0 to %4 for %4], %3[%c0 to %5 for %5]) : (!stream.resource<*>{%4}, !stream.resource<*>{%5}) -> !stream.resource<*>{%c64000}
  %7 = stream.async.transfer %6 : !stream.resource<*>{%c64000} -> !stream.resource<external>{%c64000}
  %8 = stream.tensor.export %7 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  %9 = stream.async.splat %c27648_i16 : i16 -> !stream.resource<*>{%c64000}
  %10 = stream.async.transfer %9 : !stream.resource<*>{%c64000} -> !stream.resource<external>{%c64000}
  %11 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  check.expect_eq(%8, %11) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func private @_main() {
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %c0 = arith.constant 0 : index
  %0 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<*>{%c8192}
  %1 = util.optimization_barrier %0 : !stream.resource<*>
  %2 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<*>{%c262144000}
  %3 = util.optimization_barrier %2 : !stream.resource<*>
  %4 = stream.resource.size %1 : !stream.resource<*>
  %5 = stream.resource.size %3 : !stream.resource<*>
  %6 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%1[%c0 to %4 for %4], %3[%c0 to %5 for %5]) : (!stream.resource<*>{%4}, !stream.resource<*>{%5}) -> !stream.resource<*>{%c64000}
  %7 = stream.async.transfer %6 : !stream.resource<*>{%c64000} -> !stream.resource<external>{%c64000}
  %8 = stream.tensor.export %7 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  %9 = stream.async.splat %c27648_i16 : i16 -> !stream.resource<*>{%c64000}
  %10 = stream.async.transfer %9 : !stream.resource<*>{%c64000} -> !stream.resource<external>{%c64000}
  %11 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  check.expect_eq(%8, %11) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %c0 = arith.constant 0 : index
    %0 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<*>{%c8192}
    %1 = util.optimization_barrier %0 : !stream.resource<*>
    %2 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<*>{%c262144000}
    %3 = util.optimization_barrier %2 : !stream.resource<*>
    %4 = stream.resource.size %1 : !stream.resource<*>
    %5 = stream.resource.size %3 : !stream.resource<*>
    %6 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%1[%c0 to %4 for %4], %3[%c0 to %5 for %5]) : (!stream.resource<*>{%4}, !stream.resource<*>{%5}) -> !stream.resource<*>{%c64000}
    %7 = stream.async.transfer %6 : !stream.resource<*>{%c64000} -> !stream.resource<external>{%c64000}
    %8 = stream.tensor.export %7 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %9 = stream.async.splat %c27648_i16 : i16 -> !stream.resource<*>{%c64000}
    %10 = stream.async.transfer %9 : !stream.resource<*>{%c64000} -> !stream.resource<external>{%c64000}
    %11 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%8, %11) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %c0 = arith.constant 0 : index
    %0 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<*>{%c8192}
    %1 = util.optimization_barrier %0 : !stream.resource<*>
    %2 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<*>{%c262144000}
    %3 = util.optimization_barrier %2 : !stream.resource<*>
    %4 = stream.resource.size %1 : !stream.resource<*>
    %5 = stream.resource.size %3 : !stream.resource<*>
    %6 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%1[%c0 to %4 for %4], %3[%c0 to %5 for %5]) : (!stream.resource<*>{%4}, !stream.resource<*>{%5}) -> !stream.resource<*>{%c64000}
    %7 = stream.async.transfer %6 : !stream.resource<*>{%c64000} -> !stream.resource<external>{%c64000}
    %8 = stream.tensor.export %7 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %9 = stream.async.splat %c27648_i16 : i16 -> !stream.resource<*>{%c64000}
    %10 = stream.async.transfer %9 : !stream.resource<*>{%c64000} -> !stream.resource<external>{%c64000}
    %11 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%8, %11) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %c0 = arith.constant 0 : index
    %0 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<*>{%c8192}
    %1 = util.optimization_barrier %0 : !stream.resource<*>
    %2 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<*>{%c262144000}
    %3 = util.optimization_barrier %2 : !stream.resource<*>
    %4 = stream.resource.size %1 : !stream.resource<*>
    %5 = stream.resource.size %3 : !stream.resource<*>
    %6 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%1[%c0 to %4 for %4], %3[%c0 to %5 for %5]) : (!stream.resource<*>{%4}, !stream.resource<*>{%5}) -> !stream.resource<*>{%c64000}
    %7 = stream.async.transfer %6 : !stream.resource<*>{%c64000} -> !stream.resource<external>{%c64000}
    %8 = stream.tensor.export %7 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %9 = stream.async.splat %c27648_i16 : i16 -> !stream.resource<*>{%c64000}
    %10 = stream.async.transfer %9 : !stream.resource<*>{%c64000} -> !stream.resource<external>{%c64000}
    %11 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%8, %11) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %c0 = arith.constant 0 : index
    %0 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<*>{%c8192}
    %1 = util.optimization_barrier %0 : !stream.resource<*>
    %2 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<*>{%c262144000}
    %3 = util.optimization_barrier %2 : !stream.resource<*>
    %4 = stream.resource.size %1 : !stream.resource<*>
    %5 = stream.resource.size %3 : !stream.resource<*>
    %6 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%1[%c0 to %4 for %4], %3[%c0 to %5 for %5]) : (!stream.resource<*>{%4}, !stream.resource<*>{%5}) -> !stream.resource<*>{%c64000}
    %7 = stream.async.transfer %6 : !stream.resource<*>{%c64000} -> !stream.resource<external>{%c64000}
    %8 = stream.tensor.export %7 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %9 = stream.async.splat %c27648_i16 : i16 -> !stream.resource<*>{%c64000}
    %10 = stream.async.transfer %9 : !stream.resource<*>{%c64000} -> !stream.resource<external>{%c64000}
    %11 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%8, %11) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After MaterializeCopyOnWrite (iree-stream-materialize-copy-on-write) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After MaterializeCopyOnWrite (iree-stream-materialize-copy-on-write) //----- //
func.func private @_main() {
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %c0 = arith.constant 0 : index
  %0 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<*>{%c8192}
  %1 = util.optimization_barrier %0 : !stream.resource<*>
  %2 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<*>{%c262144000}
  %3 = util.optimization_barrier %2 : !stream.resource<*>
  %4 = stream.resource.size %1 : !stream.resource<*>
  %5 = stream.resource.size %3 : !stream.resource<*>
  %6 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%1[%c0 to %4 for %4], %3[%c0 to %5 for %5]) : (!stream.resource<*>{%4}, !stream.resource<*>{%5}) -> !stream.resource<*>{%c64000}
  %7 = stream.async.transfer %6 : !stream.resource<*>{%c64000} -> !stream.resource<external>{%c64000}
  %8 = stream.tensor.export %7 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  %9 = stream.async.splat %c27648_i16 : i16 -> !stream.resource<*>{%c64000}
  %10 = stream.async.transfer %9 : !stream.resource<*>{%c64000} -> !stream.resource<external>{%c64000}
  %11 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  check.expect_eq(%8, %11) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After ElideAsyncCopies (iree-stream-elide-async-copies) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %c0 = arith.constant 0 : index
    %0 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<*>{%c8192}
    %1 = util.optimization_barrier %0 : !stream.resource<*>
    %2 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<*>{%c262144000}
    %3 = util.optimization_barrier %2 : !stream.resource<*>
    %4 = stream.resource.size %1 : !stream.resource<*>
    %5 = stream.resource.size %3 : !stream.resource<*>
    %6 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%1[%c0 to %4 for %4], %3[%c0 to %5 for %5]) : (!stream.resource<*>{%4}, !stream.resource<*>{%5}) -> !stream.resource<*>{%c64000}
    %7 = stream.async.transfer %6 : !stream.resource<*>{%c64000} -> !stream.resource<external>{%c64000}
    %8 = stream.tensor.export %7 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %9 = stream.async.splat %c27648_i16 : i16 -> !stream.resource<*>{%c64000}
    %10 = stream.async.transfer %9 : !stream.resource<*>{%c64000} -> !stream.resource<external>{%c64000}
    %11 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%8, %11) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After EmplaceAllocations (iree-stream-emplace-allocations) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func private @_main() {
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %c0 = arith.constant 0 : index
  %0 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<*>{%c8192}
  %1 = util.optimization_barrier %0 : !stream.resource<*>
  %2 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<*>{%c262144000}
  %3 = util.optimization_barrier %2 : !stream.resource<*>
  %4 = stream.resource.size %1 : !stream.resource<*>
  %5 = stream.resource.size %3 : !stream.resource<*>
  %6 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%1[%c0 to %4 for %4], %3[%c0 to %5 for %5]) : (!stream.resource<*>{%4}, !stream.resource<*>{%5}) -> !stream.resource<*>{%c64000}
  %7 = stream.async.transfer %6 : !stream.resource<*>{%c64000} -> !stream.resource<external>{%c64000}
  %8 = stream.tensor.export %7 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  %9 = stream.async.splat %c27648_i16 : i16 -> !stream.resource<*>{%c64000}
  %10 = stream.async.transfer %9 : !stream.resource<*>{%c64000} -> !stream.resource<external>{%c64000}
  %11 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  check.expect_eq(%8, %11) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After EmplaceAllocations (iree-stream-emplace-allocations) //----- //
func.func private @_main() {
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %c0 = arith.constant 0 : index
  %0 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<*>{%c8192}
  %1 = util.optimization_barrier %0 : !stream.resource<*>
  %2 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<*>{%c262144000}
  %3 = util.optimization_barrier %2 : !stream.resource<*>
  %4 = stream.resource.size %1 : !stream.resource<*>
  %5 = stream.resource.size %3 : !stream.resource<*>
  %6 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%1[%c0 to %4 for %4], %3[%c0 to %5 for %5]) : (!stream.resource<*>{%4}, !stream.resource<*>{%5}) -> !stream.resource<*>{%c64000}
  %7 = stream.async.transfer %6 : !stream.resource<*>{%c64000} -> !stream.resource<external>{%c64000}
  %8 = stream.tensor.export %7 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  %9 = stream.async.splat %c27648_i16 : i16 -> !stream.resource<*>{%c64000}
  %10 = stream.async.transfer %9 : !stream.resource<*>{%c64000} -> !stream.resource<external>{%c64000}
  %11 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  check.expect_eq(%8, %11) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After RefineUsage (iree-stream-refine-usage) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %c0 = arith.constant 0 : index
    %0 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c8192}
    %1 = util.optimization_barrier %0 : !stream.resource<transient>
    %2 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c262144000}
    %3 = util.optimization_barrier %2 : !stream.resource<transient>
    %4 = stream.resource.size %1 : !stream.resource<transient>
    %5 = stream.resource.size %3 : !stream.resource<transient>
    %6 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%1[%c0 to %4 for %4], %3[%c0 to %5 for %5]) : (!stream.resource<transient>{%4}, !stream.resource<transient>{%5}) -> !stream.resource<external>{%c64000}
    %7 = stream.tensor.export %6 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %8 = stream.async.splat %c27648_i16 : i16 -> !stream.resource<external>{%c64000}
    %9 = stream.tensor.export %8 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%7, %9) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func private @_main() {
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %c0 = arith.constant 0 : index
  %0 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c8192}
  %1 = util.optimization_barrier %0 : !stream.resource<transient>
  %2 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c262144000}
  %3 = util.optimization_barrier %2 : !stream.resource<transient>
  %4 = stream.resource.size %1 : !stream.resource<transient>
  %5 = stream.resource.size %3 : !stream.resource<transient>
  %6 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%1[%c0 to %4 for %4], %3[%c0 to %5 for %5]) : (!stream.resource<transient>{%4}, !stream.resource<transient>{%5}) -> !stream.resource<external>{%c64000}
  %7 = stream.tensor.export %6 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  %8 = stream.async.splat %c27648_i16 : i16 -> !stream.resource<external>{%c64000}
  %9 = stream.tensor.export %8 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  check.expect_eq(%7, %9) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func private @_main() {
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %c0 = arith.constant 0 : index
  %0 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c8192}
  %1 = util.optimization_barrier %0 : !stream.resource<transient>
  %2 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c262144000}
  %3 = util.optimization_barrier %2 : !stream.resource<transient>
  %4 = stream.resource.size %1 : !stream.resource<transient>
  %5 = stream.resource.size %3 : !stream.resource<transient>
  %6 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%1[%c0 to %4 for %4], %3[%c0 to %5 for %5]) : (!stream.resource<transient>{%4}, !stream.resource<transient>{%5}) -> !stream.resource<external>{%c64000}
  %7 = stream.tensor.export %6 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  %8 = stream.async.splat %c27648_i16 : i16 -> !stream.resource<external>{%c64000}
  %9 = stream.tensor.export %8 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  check.expect_eq(%7, %9) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func private @_main() {
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %c0 = arith.constant 0 : index
  %0 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c8192}
  %1 = util.optimization_barrier %0 : !stream.resource<transient>
  %2 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c262144000}
  %3 = util.optimization_barrier %2 : !stream.resource<transient>
  %4 = stream.resource.size %1 : !stream.resource<transient>
  %5 = stream.resource.size %3 : !stream.resource<transient>
  %6 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%1[%c0 to %4 for %4], %3[%c0 to %5 for %5]) : (!stream.resource<transient>{%4}, !stream.resource<transient>{%5}) -> !stream.resource<external>{%c64000}
  %7 = stream.tensor.export %6 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  %8 = stream.async.splat %c27648_i16 : i16 -> !stream.resource<external>{%c64000}
  %9 = stream.tensor.export %8 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  check.expect_eq(%7, %9) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %c0 = arith.constant 0 : index
    %0 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c8192}
    %1 = util.optimization_barrier %0 : !stream.resource<transient>
    %2 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c262144000}
    %3 = util.optimization_barrier %2 : !stream.resource<transient>
    %4 = stream.resource.size %1 : !stream.resource<transient>
    %5 = stream.resource.size %3 : !stream.resource<transient>
    %6 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%1[%c0 to %4 for %4], %3[%c0 to %5 for %5]) : (!stream.resource<transient>{%4}, !stream.resource<transient>{%5}) -> !stream.resource<external>{%c64000}
    %7 = stream.tensor.export %6 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %8 = stream.async.splat %c27648_i16 : i16 -> !stream.resource<external>{%c64000}
    %9 = stream.tensor.export %8 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%7, %9) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %c0 = arith.constant 0 : index
    %0 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c8192}
    %1 = util.optimization_barrier %0 : !stream.resource<transient>
    %2 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c262144000}
    %3 = util.optimization_barrier %2 : !stream.resource<transient>
    %4 = stream.resource.size %1 : !stream.resource<transient>
    %5 = stream.resource.size %3 : !stream.resource<transient>
    %6 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%1[%c0 to %4 for %4], %3[%c0 to %5 for %5]) : (!stream.resource<transient>{%4}, !stream.resource<transient>{%5}) -> !stream.resource<external>{%c64000}
    %7 = stream.tensor.export %6 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %8 = stream.async.splat %c27648_i16 : i16 -> !stream.resource<external>{%c64000}
    %9 = stream.tensor.export %8 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%7, %9) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %c0 = arith.constant 0 : index
    %0 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c8192}
    %1 = util.optimization_barrier %0 : !stream.resource<transient>
    %2 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c262144000}
    %3 = util.optimization_barrier %2 : !stream.resource<transient>
    %4 = stream.resource.size %1 : !stream.resource<transient>
    %5 = stream.resource.size %3 : !stream.resource<transient>
    %6 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%1[%c0 to %4 for %4], %3[%c0 to %5 for %5]) : (!stream.resource<transient>{%4}, !stream.resource<transient>{%5}) -> !stream.resource<external>{%c64000}
    %7 = stream.tensor.export %6 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %8 = stream.async.splat %c27648_i16 : i16 -> !stream.resource<external>{%c64000}
    %9 = stream.tensor.export %8 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%7, %9) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %c0 = arith.constant 0 : index
    %0 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c8192}
    %1 = util.optimization_barrier %0 : !stream.resource<transient>
    %2 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c262144000}
    %3 = util.optimization_barrier %2 : !stream.resource<transient>
    %4 = stream.resource.size %1 : !stream.resource<transient>
    %5 = stream.resource.size %3 : !stream.resource<transient>
    %6 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%1[%c0 to %4 for %4], %3[%c0 to %5 for %5]) : (!stream.resource<transient>{%4}, !stream.resource<transient>{%5}) -> !stream.resource<external>{%c64000}
    %7 = stream.tensor.export %6 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %8 = stream.async.splat %c27648_i16 : i16 -> !stream.resource<external>{%c64000}
    %9 = stream.tensor.export %8 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%7, %9) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After VerifyAsyncAccessRanges (iree-stream-verify-async-access-ranges) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %c0 = arith.constant 0 : index
    %0 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c8192}
    %1 = util.optimization_barrier %0 : !stream.resource<transient>
    %2 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c262144000}
    %3 = util.optimization_barrier %2 : !stream.resource<transient>
    %4 = stream.resource.size %1 : !stream.resource<transient>
    %5 = stream.resource.size %3 : !stream.resource<transient>
    %6 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%1[%c0 to %4 for %4], %3[%c0 to %5 for %5]) : (!stream.resource<transient>{%4}, !stream.resource<transient>{%5}) -> !stream.resource<external>{%c64000}
    %7 = stream.tensor.export %6 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %8 = stream.async.splat %c27648_i16 : i16 -> !stream.resource<external>{%c64000}
    %9 = stream.tensor.export %8 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%7, %9) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After ScheduleExecution (iree-stream-schedule-execution) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After ScheduleConcurrency (iree-stream-schedule-concurrency) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After ScheduleExecution (iree-stream-schedule-execution) //----- //
func.func private @_main() {
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %c0 = arith.constant 0 : index
  %results, %result_timepoint = stream.async.execute with() -> !stream.resource<transient>{%c8192} {
    %9 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c8192}
    stream.yield %9 : !stream.resource<transient>{%c8192}
  } => !stream.timepoint
  %0 = stream.timepoint.await %result_timepoint => %results : !stream.resource<transient>{%c8192}
  %1 = util.optimization_barrier %0 : !stream.resource<transient>
  %results_0, %result_timepoint_1 = stream.async.execute with() -> !stream.resource<transient>{%c262144000} {
    %9 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c262144000}
    stream.yield %9 : !stream.resource<transient>{%c262144000}
  } => !stream.timepoint
  %2 = stream.timepoint.await %result_timepoint_1 => %results_0 : !stream.resource<transient>{%c262144000}
  %3 = util.optimization_barrier %2 : !stream.resource<transient>
  %4 = stream.resource.size %1 : !stream.resource<transient>
  %5 = stream.resource.size %3 : !stream.resource<transient>
  %results_2:2, %result_timepoint_3 = stream.async.execute with(%1 as %arg0: !stream.resource<transient>{%4}, %3 as %arg1: !stream.resource<transient>{%5}) -> (!stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}) {
    %9 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0[%c0 to %4 for %4], %arg1[%c0 to %5 for %5]) : (!stream.resource<transient>{%4}, !stream.resource<transient>{%5}) -> !stream.resource<external>{%c64000}
    %10 = stream.async.splat %c27648_i16 : i16 -> !stream.resource<external>{%c64000}
    stream.yield %9, %10 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
  } => !stream.timepoint
  %6:2 = stream.timepoint.await %result_timepoint_3 => %results_2#1, %results_2#0 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
  %7 = stream.tensor.export %6#0 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  %8 = stream.tensor.export %6#1 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  check.expect_eq(%8, %7) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After ScheduleConcurrency (iree-stream-schedule-concurrency) //----- //
func.func private @_main() {
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %c0 = arith.constant 0 : index
  %results, %result_timepoint = stream.async.execute with() -> !stream.resource<transient>{%c8192} {
    %9 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c8192}
    stream.yield %9 : !stream.resource<transient>{%c8192}
  } => !stream.timepoint
  %0 = stream.timepoint.await %result_timepoint => %results : !stream.resource<transient>{%c8192}
  %1 = util.optimization_barrier %0 : !stream.resource<transient>
  %results_0, %result_timepoint_1 = stream.async.execute with() -> !stream.resource<transient>{%c262144000} {
    %9 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c262144000}
    stream.yield %9 : !stream.resource<transient>{%c262144000}
  } => !stream.timepoint
  %2 = stream.timepoint.await %result_timepoint_1 => %results_0 : !stream.resource<transient>{%c262144000}
  %3 = util.optimization_barrier %2 : !stream.resource<transient>
  %4 = stream.resource.size %1 : !stream.resource<transient>
  %5 = stream.resource.size %3 : !stream.resource<transient>
  %results_2:2, %result_timepoint_3 = stream.async.execute with(%1 as %arg0: !stream.resource<transient>{%4}, %3 as %arg1: !stream.resource<transient>{%5}) -> (!stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}) {
    %9:2 = stream.async.concurrent with(%arg0 as %arg2: !stream.resource<transient>{%4}, %arg1 as %arg3: !stream.resource<transient>{%5}) -> (!stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}) {
      %10 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg2[%c0 to %4 for %4], %arg3[%c0 to %5 for %5]) : (!stream.resource<transient>{%4}, !stream.resource<transient>{%5}) -> !stream.resource<external>{%c64000}
      %11 = stream.async.splat %c27648_i16 : i16 -> !stream.resource<external>{%c64000}
      stream.yield %10, %11 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
    }
    stream.yield %9#0, %9#1 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
  } => !stream.timepoint
  %6:2 = stream.timepoint.await %result_timepoint_3 => %results_2#1, %results_2#0 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
  %7 = stream.tensor.export %6#0 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  %8 = stream.tensor.export %6#1 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  check.expect_eq(%8, %7) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After PropagateTimepoints (iree-stream-propagate-timepoints) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %c0 = arith.constant 0 : index
    %results, %result_timepoint = stream.async.execute with() -> !stream.resource<transient>{%c8192} {
      %12 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c8192}
      stream.yield %12 : !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %0 = stream.timepoint.await %result_timepoint => %results : !stream.resource<transient>{%c8192}
    %1 = util.optimization_barrier %0 : !stream.resource<transient>
    %results_0, %result_timepoint_1 = stream.async.execute with() -> !stream.resource<transient>{%c262144000} {
      %12 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c262144000}
      stream.yield %12 : !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %2 = stream.timepoint.await %result_timepoint_1 => %results_0 : !stream.resource<transient>{%c262144000}
    %3 = util.optimization_barrier %2 : !stream.resource<transient>
    %4 = stream.resource.size %1 : !stream.resource<transient>
    %5 = stream.resource.size %3 : !stream.resource<transient>
    %6 = stream.timepoint.immediate => !stream.timepoint
    %7 = stream.timepoint.immediate => !stream.timepoint
    %8 = stream.timepoint.join max(%6, %7) => !stream.timepoint
    %results_2:2, %result_timepoint_3 = stream.async.execute await(%8) => with(%1 as %arg0: !stream.resource<transient>{%4}, %3 as %arg1: !stream.resource<transient>{%5}) -> (!stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}) {
      %12:2 = stream.async.concurrent with(%arg0 as %arg2: !stream.resource<transient>{%4}, %arg1 as %arg3: !stream.resource<transient>{%5}) -> (!stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}) {
        %13 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg2[%c0 to %4 for %4], %arg3[%c0 to %5 for %5]) : (!stream.resource<transient>{%4}, !stream.resource<transient>{%5}) -> !stream.resource<external>{%c64000}
        %14 = stream.async.splat %c27648_i16 : i16 -> !stream.resource<external>{%c64000}
        stream.yield %13, %14 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
      }
      stream.yield %12#0, %12#1 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
    } => !stream.timepoint
    %9:2 = stream.timepoint.await %result_timepoint_3 => %results_2#1, %results_2#0 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
    %10 = stream.tensor.export %9#0 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %11 = stream.tensor.export %9#1 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%11, %10) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After MaterializeBuiltins (iree-stream-materialize-builtins) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %c0 = arith.constant 0 : index
    %results, %result_timepoint = stream.async.execute with() -> !stream.resource<transient>{%c8192} {
      %12 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c8192}
      stream.yield %12 : !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %0 = stream.timepoint.await %result_timepoint => %results : !stream.resource<transient>{%c8192}
    %1 = util.optimization_barrier %0 : !stream.resource<transient>
    %results_0, %result_timepoint_1 = stream.async.execute with() -> !stream.resource<transient>{%c262144000} {
      %12 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c262144000}
      stream.yield %12 : !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %2 = stream.timepoint.await %result_timepoint_1 => %results_0 : !stream.resource<transient>{%c262144000}
    %3 = util.optimization_barrier %2 : !stream.resource<transient>
    %4 = stream.resource.size %1 : !stream.resource<transient>
    %5 = stream.resource.size %3 : !stream.resource<transient>
    %6 = stream.timepoint.immediate => !stream.timepoint
    %7 = stream.timepoint.immediate => !stream.timepoint
    %8 = stream.timepoint.join max(%6, %7) => !stream.timepoint
    %results_2:2, %result_timepoint_3 = stream.async.execute await(%8) => with(%1 as %arg0: !stream.resource<transient>{%4}, %3 as %arg1: !stream.resource<transient>{%5}) -> (!stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}) {
      %12:2 = stream.async.concurrent with(%arg0 as %arg2: !stream.resource<transient>{%4}, %arg1 as %arg3: !stream.resource<transient>{%5}) -> (!stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}) {
        %13 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg2[%c0 to %4 for %4], %arg3[%c0 to %5 for %5]) : (!stream.resource<transient>{%4}, !stream.resource<transient>{%5}) -> !stream.resource<external>{%c64000}
        %14 = stream.async.splat %c27648_i16 : i16 -> !stream.resource<external>{%c64000}
        stream.yield %13, %14 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
      }
      stream.yield %12#0, %12#1 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
    } => !stream.timepoint
    %9:2 = stream.timepoint.await %result_timepoint_3 => %results_2#1, %results_2#0 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
    %10 = stream.tensor.export %9#0 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %11 = stream.tensor.export %9#1 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%11, %10) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func private @_main() {
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %c0 = arith.constant 0 : index
  %results, %result_timepoint = stream.async.execute with() -> !stream.resource<transient>{%c8192} {
    %9 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c8192}
    stream.yield %9 : !stream.resource<transient>{%c8192}
  } => !stream.timepoint
  %0 = stream.timepoint.await %result_timepoint => %results : !stream.resource<transient>{%c8192}
  %1 = util.optimization_barrier %0 : !stream.resource<transient>
  %results_0, %result_timepoint_1 = stream.async.execute with() -> !stream.resource<transient>{%c262144000} {
    %9 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c262144000}
    stream.yield %9 : !stream.resource<transient>{%c262144000}
  } => !stream.timepoint
  %2 = stream.timepoint.await %result_timepoint_1 => %results_0 : !stream.resource<transient>{%c262144000}
  %3 = util.optimization_barrier %2 : !stream.resource<transient>
  %4 = stream.resource.size %1 : !stream.resource<transient>
  %5 = stream.resource.size %3 : !stream.resource<transient>
  %results_2:2, %result_timepoint_3 = stream.async.execute with(%1 as %arg0: !stream.resource<transient>{%4}, %3 as %arg1: !stream.resource<transient>{%5}) -> (!stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}) {
    %9:2 = stream.async.concurrent with(%arg0 as %arg2: !stream.resource<transient>{%4}, %arg1 as %arg3: !stream.resource<transient>{%5}) -> (!stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}) {
      %10 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg2[%c0 to %4 for %4], %arg3[%c0 to %5 for %5]) : (!stream.resource<transient>{%4}, !stream.resource<transient>{%5}) -> !stream.resource<external>{%c64000}
      %11 = stream.async.splat %c27648_i16 : i16 -> !stream.resource<external>{%c64000}
      stream.yield %10, %11 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
    }
    stream.yield %9#0, %9#1 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
  } => !stream.timepoint
  %6:2 = stream.timepoint.await %result_timepoint_3 => %results_2#1, %results_2#0 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
  %7 = stream.tensor.export %6#0 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  %8 = stream.tensor.export %6#1 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  check.expect_eq(%8, %7) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func private @_main() {
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %c0 = arith.constant 0 : index
  %results, %result_timepoint = stream.async.execute with() -> !stream.resource<transient>{%c8192} {
    %9 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c8192}
    stream.yield %9 : !stream.resource<transient>{%c8192}
  } => !stream.timepoint
  %0 = stream.timepoint.await %result_timepoint => %results : !stream.resource<transient>{%c8192}
  %1 = util.optimization_barrier %0 : !stream.resource<transient>
  %results_0, %result_timepoint_1 = stream.async.execute with() -> !stream.resource<transient>{%c262144000} {
    %9 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c262144000}
    stream.yield %9 : !stream.resource<transient>{%c262144000}
  } => !stream.timepoint
  %2 = stream.timepoint.await %result_timepoint_1 => %results_0 : !stream.resource<transient>{%c262144000}
  %3 = util.optimization_barrier %2 : !stream.resource<transient>
  %4 = stream.resource.size %1 : !stream.resource<transient>
  %5 = stream.resource.size %3 : !stream.resource<transient>
  %results_2:2, %result_timepoint_3 = stream.async.execute with(%1 as %arg0: !stream.resource<transient>{%4}, %3 as %arg1: !stream.resource<transient>{%5}) -> (!stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}) {
    %9:2 = stream.async.concurrent with(%arg0 as %arg2: !stream.resource<transient>{%4}, %arg1 as %arg3: !stream.resource<transient>{%5}) -> (!stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}) {
      %10 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg2[%c0 to %4 for %4], %arg3[%c0 to %5 for %5]) : (!stream.resource<transient>{%4}, !stream.resource<transient>{%5}) -> !stream.resource<external>{%c64000}
      %11 = stream.async.splat %c27648_i16 : i16 -> !stream.resource<external>{%c64000}
      stream.yield %10, %11 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
    }
    stream.yield %9#0, %9#1 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
  } => !stream.timepoint
  %6:2 = stream.timepoint.await %result_timepoint_3 => %results_2#1, %results_2#0 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
  %7 = stream.tensor.export %6#0 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  %8 = stream.tensor.export %6#1 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  check.expect_eq(%8, %7) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func private @_main() {
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %c0 = arith.constant 0 : index
  %results, %result_timepoint = stream.async.execute with() -> !stream.resource<transient>{%c8192} {
    %9 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c8192}
    stream.yield %9 : !stream.resource<transient>{%c8192}
  } => !stream.timepoint
  %0 = stream.timepoint.await %result_timepoint => %results : !stream.resource<transient>{%c8192}
  %1 = util.optimization_barrier %0 : !stream.resource<transient>
  %results_0, %result_timepoint_1 = stream.async.execute with() -> !stream.resource<transient>{%c262144000} {
    %9 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c262144000}
    stream.yield %9 : !stream.resource<transient>{%c262144000}
  } => !stream.timepoint
  %2 = stream.timepoint.await %result_timepoint_1 => %results_0 : !stream.resource<transient>{%c262144000}
  %3 = util.optimization_barrier %2 : !stream.resource<transient>
  %4 = stream.resource.size %1 : !stream.resource<transient>
  %5 = stream.resource.size %3 : !stream.resource<transient>
  %results_2:2, %result_timepoint_3 = stream.async.execute with(%1 as %arg0: !stream.resource<transient>{%4}, %3 as %arg1: !stream.resource<transient>{%5}) -> (!stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}) {
    %9:2 = stream.async.concurrent with(%arg0 as %arg2: !stream.resource<transient>{%4}, %arg1 as %arg3: !stream.resource<transient>{%5}) -> (!stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}) {
      %10 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg2[%c0 to %4 for %4], %arg3[%c0 to %5 for %5]) : (!stream.resource<transient>{%4}, !stream.resource<transient>{%5}) -> !stream.resource<external>{%c64000}
      %11 = stream.async.splat %c27648_i16 : i16 -> !stream.resource<external>{%c64000}
      stream.yield %10, %11 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
    }
    stream.yield %9#0, %9#1 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
  } => !stream.timepoint
  %6:2 = stream.timepoint.await %result_timepoint_3 => %results_2#1, %results_2#0 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
  %7 = stream.tensor.export %6#0 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  %8 = stream.tensor.export %6#1 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  check.expect_eq(%8, %7) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %c0 = arith.constant 0 : index
    %results, %result_timepoint = stream.async.execute with() -> !stream.resource<transient>{%c8192} {
      %9 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c8192}
      stream.yield %9 : !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %0 = stream.timepoint.await %result_timepoint => %results : !stream.resource<transient>{%c8192}
    %1 = util.optimization_barrier %0 : !stream.resource<transient>
    %results_0, %result_timepoint_1 = stream.async.execute with() -> !stream.resource<transient>{%c262144000} {
      %9 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c262144000}
      stream.yield %9 : !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %2 = stream.timepoint.await %result_timepoint_1 => %results_0 : !stream.resource<transient>{%c262144000}
    %3 = util.optimization_barrier %2 : !stream.resource<transient>
    %4 = stream.resource.size %1 : !stream.resource<transient>
    %5 = stream.resource.size %3 : !stream.resource<transient>
    %results_2:2, %result_timepoint_3 = stream.async.execute with(%1 as %arg0: !stream.resource<transient>{%4}, %3 as %arg1: !stream.resource<transient>{%5}) -> (!stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}) {
      %9:2 = stream.async.concurrent with(%arg0 as %arg2: !stream.resource<transient>{%4}, %arg1 as %arg3: !stream.resource<transient>{%5}) -> (!stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}) {
        %10 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg2[%c0 to %4 for %4], %arg3[%c0 to %5 for %5]) : (!stream.resource<transient>{%4}, !stream.resource<transient>{%5}) -> !stream.resource<external>{%c64000}
        %11 = stream.async.splat %c27648_i16 : i16 -> !stream.resource<external>{%c64000}
        stream.yield %10, %11 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
      }
      stream.yield %9#0, %9#1 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
    } => !stream.timepoint
    %6:2 = stream.timepoint.await %result_timepoint_3 => %results_2#1, %results_2#0 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
    %7 = stream.tensor.export %6#0 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %8 = stream.tensor.export %6#1 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%8, %7) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %c0 = arith.constant 0 : index
    %results, %result_timepoint = stream.async.execute with() -> !stream.resource<transient>{%c8192} {
      %9 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c8192}
      stream.yield %9 : !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %0 = stream.timepoint.await %result_timepoint => %results : !stream.resource<transient>{%c8192}
    %1 = util.optimization_barrier %0 : !stream.resource<transient>
    %results_0, %result_timepoint_1 = stream.async.execute with() -> !stream.resource<transient>{%c262144000} {
      %9 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c262144000}
      stream.yield %9 : !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %2 = stream.timepoint.await %result_timepoint_1 => %results_0 : !stream.resource<transient>{%c262144000}
    %3 = util.optimization_barrier %2 : !stream.resource<transient>
    %4 = stream.resource.size %1 : !stream.resource<transient>
    %5 = stream.resource.size %3 : !stream.resource<transient>
    %results_2:2, %result_timepoint_3 = stream.async.execute with(%1 as %arg0: !stream.resource<transient>{%4}, %3 as %arg1: !stream.resource<transient>{%5}) -> (!stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}) {
      %9:2 = stream.async.concurrent with(%arg0 as %arg2: !stream.resource<transient>{%4}, %arg1 as %arg3: !stream.resource<transient>{%5}) -> (!stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}) {
        %10 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg2[%c0 to %4 for %4], %arg3[%c0 to %5 for %5]) : (!stream.resource<transient>{%4}, !stream.resource<transient>{%5}) -> !stream.resource<external>{%c64000}
        %11 = stream.async.splat %c27648_i16 : i16 -> !stream.resource<external>{%c64000}
        stream.yield %10, %11 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
      }
      stream.yield %9#0, %9#1 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
    } => !stream.timepoint
    %6:2 = stream.timepoint.await %result_timepoint_3 => %results_2#1, %results_2#0 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
    %7 = stream.tensor.export %6#0 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %8 = stream.tensor.export %6#1 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%8, %7) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %c0 = arith.constant 0 : index
    %results, %result_timepoint = stream.async.execute with() -> !stream.resource<transient>{%c8192} {
      %9 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c8192}
      stream.yield %9 : !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %0 = stream.timepoint.await %result_timepoint => %results : !stream.resource<transient>{%c8192}
    %1 = util.optimization_barrier %0 : !stream.resource<transient>
    %results_0, %result_timepoint_1 = stream.async.execute with() -> !stream.resource<transient>{%c262144000} {
      %9 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c262144000}
      stream.yield %9 : !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %2 = stream.timepoint.await %result_timepoint_1 => %results_0 : !stream.resource<transient>{%c262144000}
    %3 = util.optimization_barrier %2 : !stream.resource<transient>
    %4 = stream.resource.size %1 : !stream.resource<transient>
    %5 = stream.resource.size %3 : !stream.resource<transient>
    %results_2:2, %result_timepoint_3 = stream.async.execute with(%1 as %arg0: !stream.resource<transient>{%4}, %3 as %arg1: !stream.resource<transient>{%5}) -> (!stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}) {
      %9:2 = stream.async.concurrent with(%arg0 as %arg2: !stream.resource<transient>{%4}, %arg1 as %arg3: !stream.resource<transient>{%5}) -> (!stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}) {
        %10 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg2[%c0 to %4 for %4], %arg3[%c0 to %5 for %5]) : (!stream.resource<transient>{%4}, !stream.resource<transient>{%5}) -> !stream.resource<external>{%c64000}
        %11 = stream.async.splat %c27648_i16 : i16 -> !stream.resource<external>{%c64000}
        stream.yield %10, %11 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
      }
      stream.yield %9#0, %9#1 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
    } => !stream.timepoint
    %6:2 = stream.timepoint.await %result_timepoint_3 => %results_2#1, %results_2#0 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
    %7 = stream.tensor.export %6#0 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %8 = stream.tensor.export %6#1 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%8, %7) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %c0 = arith.constant 0 : index
    %results, %result_timepoint = stream.async.execute with() -> !stream.resource<transient>{%c8192} {
      %9 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c8192}
      stream.yield %9 : !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %0 = stream.timepoint.await %result_timepoint => %results : !stream.resource<transient>{%c8192}
    %1 = util.optimization_barrier %0 : !stream.resource<transient>
    %results_0, %result_timepoint_1 = stream.async.execute with() -> !stream.resource<transient>{%c262144000} {
      %9 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c262144000}
      stream.yield %9 : !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %2 = stream.timepoint.await %result_timepoint_1 => %results_0 : !stream.resource<transient>{%c262144000}
    %3 = util.optimization_barrier %2 : !stream.resource<transient>
    %4 = stream.resource.size %1 : !stream.resource<transient>
    %5 = stream.resource.size %3 : !stream.resource<transient>
    %results_2:2, %result_timepoint_3 = stream.async.execute with(%1 as %arg0: !stream.resource<transient>{%4}, %3 as %arg1: !stream.resource<transient>{%5}) -> (!stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}) {
      %9:2 = stream.async.concurrent with(%arg0 as %arg2: !stream.resource<transient>{%4}, %arg1 as %arg3: !stream.resource<transient>{%5}) -> (!stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}) {
        %10 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg2[%c0 to %4 for %4], %arg3[%c0 to %5 for %5]) : (!stream.resource<transient>{%4}, !stream.resource<transient>{%5}) -> !stream.resource<external>{%c64000}
        %11 = stream.async.splat %c27648_i16 : i16 -> !stream.resource<external>{%c64000}
        stream.yield %10, %11 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
      }
      stream.yield %9#0, %9#1 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
    } => !stream.timepoint
    %6:2 = stream.timepoint.await %result_timepoint_3 => %results_2#1, %results_2#0 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
    %7 = stream.tensor.export %6#0 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %8 = stream.tensor.export %6#1 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%8, %7) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After VerifyLoweringToAsync (iree-stream-verify-lowering-to-async) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %c0 = arith.constant 0 : index
    %results, %result_timepoint = stream.async.execute with() -> !stream.resource<transient>{%c8192} {
      %9 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c8192}
      stream.yield %9 : !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %0 = stream.timepoint.await %result_timepoint => %results : !stream.resource<transient>{%c8192}
    %1 = util.optimization_barrier %0 : !stream.resource<transient>
    %results_0, %result_timepoint_1 = stream.async.execute with() -> !stream.resource<transient>{%c262144000} {
      %9 = stream.async.splat %c15360_i16 : i16 -> !stream.resource<transient>{%c262144000}
      stream.yield %9 : !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %2 = stream.timepoint.await %result_timepoint_1 => %results_0 : !stream.resource<transient>{%c262144000}
    %3 = util.optimization_barrier %2 : !stream.resource<transient>
    %4 = stream.resource.size %1 : !stream.resource<transient>
    %5 = stream.resource.size %3 : !stream.resource<transient>
    %results_2:2, %result_timepoint_3 = stream.async.execute with(%1 as %arg0: !stream.resource<transient>{%4}, %3 as %arg1: !stream.resource<transient>{%5}) -> (!stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}) {
      %9:2 = stream.async.concurrent with(%arg0 as %arg2: !stream.resource<transient>{%4}, %arg1 as %arg3: !stream.resource<transient>{%5}) -> (!stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}) {
        %10 = stream.async.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg2[%c0 to %4 for %4], %arg3[%c0 to %5 for %5]) : (!stream.resource<transient>{%4}, !stream.resource<transient>{%5}) -> !stream.resource<external>{%c64000}
        %11 = stream.async.splat %c27648_i16 : i16 -> !stream.resource<external>{%c64000}
        stream.yield %10, %11 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
      }
      stream.yield %9#0, %9#1 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
    } => !stream.timepoint
    %6:2 = stream.timepoint.await %result_timepoint_3 => %results_2#1, %results_2#0 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
    %7 = stream.tensor.export %6#0 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %8 = stream.tensor.export %6#1 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%8, %7) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After ScheduleAllocation (iree-stream-schedule-allocation) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %c0 = arith.constant 0 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %c0_0 = arith.constant 0 : index
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0_0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_1, %result_timepoint_2 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %c0_3 = arith.constant 0 : index
    %3 = stream.cmd.execute await(%result_timepoint_2) => with(%result_1 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0_3 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_1 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %c0_4 = arith.constant 0 : index
    %8:3 = stream.resource.pack slices({
      [0, 0] = %c64000,
      [0, 0] = %c64000
    }) : index
    %result_5, %result_timepoint_6 = stream.resource.alloca uninitialized : !stream.resource<external>{%8#0} => !stream.timepoint
    %9 = stream.resource.subview %result_5[%8#1] : !stream.resource<external>{%8#0} -> !stream.resource<external>{%c64000}
    %10 = stream.resource.subview %result_5[%8#2] : !stream.resource<external>{%8#0} -> !stream.resource<external>{%c64000}
    %11 = stream.cmd.execute await(%result_timepoint_6) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %9 as %arg2: !stream.resource<external>{%c64000}, %10 as %arg3: !stream.resource<external>{%c64000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0_4 for %c64000] : !stream.resource<external>{%c64000}
        }
        stream.cmd.fill %c27648_i16, %arg3[%c0_4 for %c64000] : i16 -> !stream.resource<external>{%c64000}
      }
    } => !stream.timepoint
    %12:2 = stream.timepoint.await %11 => %10, %9 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
    %13 = stream.tensor.export %12#0 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %14 = stream.tensor.export %12#1 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%14, %13) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After PackConstants (iree-stream-pack-constants) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After LayoutSlices (iree-stream-layout-slices) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After PackConstants (iree-stream-pack-constants) //----- //
func.func private @_main() {
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %c0 = arith.constant 0 : index
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
  %c0_0 = arith.constant 0 : index
  %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0_0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
  } => !stream.timepoint
  %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
  %2 = util.optimization_barrier %1 : !stream.resource<transient>
  %result_1, %result_timepoint_2 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
  %c0_3 = arith.constant 0 : index
  %3 = stream.cmd.execute await(%result_timepoint_2) => with(%result_1 as %arg0: !stream.resource<transient>{%c262144000}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0_3 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result_1 : !stream.resource<transient>{%c262144000}
  %5 = util.optimization_barrier %4 : !stream.resource<transient>
  %6 = stream.resource.size %2 : !stream.resource<transient>
  %7 = stream.resource.size %5 : !stream.resource<transient>
  %c0_4 = arith.constant 0 : index
  %8:3 = stream.resource.pack slices({
    [0, 0] = %c64000,
    [0, 0] = %c64000
  }) : index
  %result_5, %result_timepoint_6 = stream.resource.alloca uninitialized : !stream.resource<external>{%8#0} => !stream.timepoint
  %9 = stream.resource.subview %result_5[%8#1] : !stream.resource<external>{%8#0} -> !stream.resource<external>{%c64000}
  %10 = stream.resource.subview %result_5[%8#2] : !stream.resource<external>{%8#0} -> !stream.resource<external>{%c64000}
  %11 = stream.cmd.execute await(%result_timepoint_6) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %9 as %arg2: !stream.resource<external>{%c64000}, %10 as %arg3: !stream.resource<external>{%c64000}) {
    stream.cmd.concurrent {
      stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
        ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
        ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
        wo %arg2[%c0_4 for %c64000] : !stream.resource<external>{%c64000}
      }
      stream.cmd.fill %c27648_i16, %arg3[%c0_4 for %c64000] : i16 -> !stream.resource<external>{%c64000}
    }
  } => !stream.timepoint
  %12:2 = stream.timepoint.await %11 => %10, %9 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
  %13 = stream.tensor.export %12#0 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  %14 = stream.tensor.export %12#1 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  check.expect_eq(%14, %13) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After LayoutSlices (iree-stream-layout-slices) //----- //
func.func private @_main() {
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %c0 = arith.constant 0 : index
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
  %c0_0 = arith.constant 0 : index
  %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0_0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
  } => !stream.timepoint
  %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
  %2 = util.optimization_barrier %1 : !stream.resource<transient>
  %result_1, %result_timepoint_2 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
  %c0_3 = arith.constant 0 : index
  %3 = stream.cmd.execute await(%result_timepoint_2) => with(%result_1 as %arg0: !stream.resource<transient>{%c262144000}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0_3 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result_1 : !stream.resource<transient>{%c262144000}
  %5 = util.optimization_barrier %4 : !stream.resource<transient>
  %6 = stream.resource.size %2 : !stream.resource<transient>
  %7 = stream.resource.size %5 : !stream.resource<transient>
  %c0_4 = arith.constant 0 : index
  %c0_5 = arith.constant 0 : index
  %c64000_6 = arith.constant 64000 : index
  %c64000_7 = arith.constant 64000 : index
  %c128000 = arith.constant 128000 : index
  %c128000_8 = arith.constant 128000 : index
  %result_9, %result_timepoint_10 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000_8} => !stream.timepoint
  %8 = stream.resource.subview %result_9[%c0_5] : !stream.resource<external>{%c128000_8} -> !stream.resource<external>{%c64000}
  %9 = stream.resource.subview %result_9[%c64000_7] : !stream.resource<external>{%c128000_8} -> !stream.resource<external>{%c64000}
  %10 = stream.cmd.execute await(%result_timepoint_10) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %8 as %arg2: !stream.resource<external>{%c64000}, %9 as %arg3: !stream.resource<external>{%c64000}) {
    stream.cmd.concurrent {
      stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
        ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
        ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
        wo %arg2[%c0_4 for %c64000] : !stream.resource<external>{%c64000}
      }
      stream.cmd.fill %c27648_i16, %arg3[%c0_4 for %c64000] : i16 -> !stream.resource<external>{%c64000}
    }
  } => !stream.timepoint
  %11:2 = stream.timepoint.await %10 => %9, %8 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
  %12 = stream.tensor.export %11#0 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  %13 = stream.tensor.export %11#1 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  check.expect_eq(%13, %12) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After PropagateSubranges (iree-util-propagate-subranges) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %c0_0 = arith.constant 0 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %c0_1 = arith.constant 0 : index
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0_1 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %c0_4 = arith.constant 0 : index
    %3 = stream.cmd.execute await(%result_timepoint_3) => with(%result_2 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0_4 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_2 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %c0_5 = arith.constant 0 : index
    %c0_6 = arith.constant 0 : index
    %c64000_7 = arith.constant 64000 : index
    %c64000_8 = arith.constant 64000 : index
    %c128000 = arith.constant 128000 : index
    %c128000_9 = arith.constant 128000 : index
    %result_10, %result_timepoint_11 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000_9} => !stream.timepoint
    %8 = stream.resource.subview %result_10[%c0_6] : !stream.resource<external>{%c128000_9} -> !stream.resource<external>{%c64000}
    %9 = stream.resource.subview %result_10[%c64000_8] : !stream.resource<external>{%c128000_9} -> !stream.resource<external>{%c64000}
    %10 = stream.cmd.execute await(%result_timepoint_11) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %8 as %arg2: !stream.resource<external>{%c64000}, %9 as %arg3: !stream.resource<external>{%c64000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
          ro %arg0[%c0_0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0_0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0_5 for %c64000] : !stream.resource<external>{%c64000}
        }
        stream.cmd.fill %c27648_i16, %arg3[%c0_5 for %c64000] : i16 -> !stream.resource<external>{%c64000}
      }
    } => !stream.timepoint
    %11:2 = stream.timepoint.await %10 => %9, %8 : !stream.resource<external>{%c64000}, !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %11#0 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11#1 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func private @_main() {
  %c128000 = arith.constant 128000 : index
  %c0 = arith.constant 0 : index
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
  %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
  } => !stream.timepoint
  %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
  %2 = util.optimization_barrier %1 : !stream.resource<transient>
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
  %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
  %5 = util.optimization_barrier %4 : !stream.resource<transient>
  %6 = stream.resource.size %2 : !stream.resource<transient>
  %7 = stream.resource.size %5 : !stream.resource<transient>
  %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
  %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
    stream.cmd.concurrent {
      stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
        ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
        ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
        wo %arg2[%c0 for %c64000] : !stream.resource<external>{%c128000}
      }
      stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
    }
  } => !stream.timepoint
  %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
  %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
  %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
  %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  check.expect_eq(%13, %12) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func private @_main() {
  %c128000 = arith.constant 128000 : index
  %c0 = arith.constant 0 : index
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
  %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
  } => !stream.timepoint
  %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
  %2 = util.optimization_barrier %1 : !stream.resource<transient>
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
  %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
  %5 = util.optimization_barrier %4 : !stream.resource<transient>
  %6 = stream.resource.size %2 : !stream.resource<transient>
  %7 = stream.resource.size %5 : !stream.resource<transient>
  %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
  %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
    stream.cmd.concurrent {
      stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
        ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
        ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
        wo %arg2[%c0 for %c64000] : !stream.resource<external>{%c128000}
      }
      stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
    }
  } => !stream.timepoint
  %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
  %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
  %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
  %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  check.expect_eq(%13, %12) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func private @_main() {
  %c128000 = arith.constant 128000 : index
  %c0 = arith.constant 0 : index
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
  %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
  } => !stream.timepoint
  %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
  %2 = util.optimization_barrier %1 : !stream.resource<transient>
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
  %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
  %5 = util.optimization_barrier %4 : !stream.resource<transient>
  %6 = stream.resource.size %2 : !stream.resource<transient>
  %7 = stream.resource.size %5 : !stream.resource<transient>
  %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
  %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
    stream.cmd.concurrent {
      stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
        ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
        ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
        wo %arg2[%c0 for %c64000] : !stream.resource<external>{%c128000}
      }
      stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
    }
  } => !stream.timepoint
  %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
  %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
  %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
  %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  check.expect_eq(%13, %12) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c64000] : !stream.resource<external>{%c128000}
        }
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c64000] : !stream.resource<external>{%c128000}
        }
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c64000] : !stream.resource<external>{%c128000}
        }
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c64000] : !stream.resource<external>{%c128000}
        }
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After VerifyLoweringToCmd (iree-stream-verify-lowering-to-cmd) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c64000] : !stream.resource<external>{%c128000}
        }
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func private @_main() {
  %c128000 = arith.constant 128000 : index
  %c0 = arith.constant 0 : index
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
  %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
  } => !stream.timepoint
  %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
  %2 = util.optimization_barrier %1 : !stream.resource<transient>
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
  %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
  %5 = util.optimization_barrier %4 : !stream.resource<transient>
  %6 = stream.resource.size %2 : !stream.resource<transient>
  %7 = stream.resource.size %5 : !stream.resource<transient>
  %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
  %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
    stream.cmd.concurrent {
      stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
        ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
        ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
        wo %arg2[%c0 for %c64000] : !stream.resource<external>{%c128000}
      }
      stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
    }
  } => !stream.timepoint
  %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
  %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
  %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
  %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  check.expect_eq(%13, %12) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func private @_main() {
  %c128000 = arith.constant 128000 : index
  %c0 = arith.constant 0 : index
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
  %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
  } => !stream.timepoint
  %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
  %2 = util.optimization_barrier %1 : !stream.resource<transient>
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
  %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
  %5 = util.optimization_barrier %4 : !stream.resource<transient>
  %6 = stream.resource.size %2 : !stream.resource<transient>
  %7 = stream.resource.size %5 : !stream.resource<transient>
  %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
  %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
    stream.cmd.concurrent {
      stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
        ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
        ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
        wo %arg2[%c0 for %c64000] : !stream.resource<external>{%c128000}
      }
      stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
    }
  } => !stream.timepoint
  %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
  %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
  %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
  %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  check.expect_eq(%13, %12) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func private @_main() {
  %c128000 = arith.constant 128000 : index
  %c0 = arith.constant 0 : index
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
  %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
  } => !stream.timepoint
  %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
  %2 = util.optimization_barrier %1 : !stream.resource<transient>
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
  %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
  %5 = util.optimization_barrier %4 : !stream.resource<transient>
  %6 = stream.resource.size %2 : !stream.resource<transient>
  %7 = stream.resource.size %5 : !stream.resource<transient>
  %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
  %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
    stream.cmd.concurrent {
      stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
        ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
        ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
        wo %arg2[%c0 for %c64000] : !stream.resource<external>{%c128000}
      }
      stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
    }
  } => !stream.timepoint
  %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
  %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
  %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
  %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  check.expect_eq(%13, %12) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c64000] : !stream.resource<external>{%c128000}
        }
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c64000] : !stream.resource<external>{%c128000}
        }
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c64000] : !stream.resource<external>{%c128000}
        }
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c64000] : !stream.resource<external>{%c128000}
        }
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
func.func private @_main() {
  %c128000 = arith.constant 128000 : index
  %c0 = arith.constant 0 : index
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
  %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
  } => !stream.timepoint
  %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
  %2 = util.optimization_barrier %1 : !stream.resource<transient>
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
  %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
  %5 = util.optimization_barrier %4 : !stream.resource<transient>
  %6 = stream.resource.size %2 : !stream.resource<transient>
  %7 = stream.resource.size %5 : !stream.resource<transient>
  %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
  %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
    stream.cmd.concurrent {
      stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
        ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
        ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
        wo %arg2[%c0 for %c64000] : !stream.resource<external>{%c128000}
      }
      stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
    }
  } => !stream.timepoint
  %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
  %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
  %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
  %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  check.expect_eq(%13, %12) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func private @_main() {
  %c128000 = arith.constant 128000 : index
  %c0 = arith.constant 0 : index
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
  %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
  } => !stream.timepoint
  %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
  %2 = util.optimization_barrier %1 : !stream.resource<transient>
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
  %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
  %5 = util.optimization_barrier %4 : !stream.resource<transient>
  %6 = stream.resource.size %2 : !stream.resource<transient>
  %7 = stream.resource.size %5 : !stream.resource<transient>
  %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
  %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
    stream.cmd.concurrent {
      stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
        ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
        ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
        wo %arg2[%c0 for %c64000] : !stream.resource<external>{%c128000}
      }
      stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
    }
  } => !stream.timepoint
  %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
  %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
  %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
  %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  check.expect_eq(%13, %12) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func private @_main() {
  %c128000 = arith.constant 128000 : index
  %c0 = arith.constant 0 : index
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
  %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
  } => !stream.timepoint
  %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
  %2 = util.optimization_barrier %1 : !stream.resource<transient>
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
  %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
  %5 = util.optimization_barrier %4 : !stream.resource<transient>
  %6 = stream.resource.size %2 : !stream.resource<transient>
  %7 = stream.resource.size %5 : !stream.resource<transient>
  %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
  %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
    stream.cmd.concurrent {
      stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
        ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
        ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
        wo %arg2[%c0 for %c64000] : !stream.resource<external>{%c128000}
      }
      stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
    }
  } => !stream.timepoint
  %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
  %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
  %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
  %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  check.expect_eq(%13, %12) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func private @_main() {
  %c128000 = arith.constant 128000 : index
  %c0 = arith.constant 0 : index
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
  %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
  } => !stream.timepoint
  %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
  %2 = util.optimization_barrier %1 : !stream.resource<transient>
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
  %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
  %5 = util.optimization_barrier %4 : !stream.resource<transient>
  %6 = stream.resource.size %2 : !stream.resource<transient>
  %7 = stream.resource.size %5 : !stream.resource<transient>
  %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
  %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
    stream.cmd.concurrent {
      stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
        ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
        ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
        wo %arg2[%c0 for %c64000] : !stream.resource<external>{%c128000}
      }
      stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
    }
  } => !stream.timepoint
  %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
  %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
  %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
  %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  check.expect_eq(%13, %12) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan], iree.fixedpoint.iteration = 0 : index} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c64000] : !stream.resource<external>{%c128000}
        }
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan], iree.fixedpoint.iteration = 0 : index} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c64000] : !stream.resource<external>{%c128000}
        }
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan], iree.fixedpoint.iteration = 0 : index} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c64000] : !stream.resource<external>{%c128000}
        }
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan], iree.fixedpoint.iteration = 0 : index} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c64000] : !stream.resource<external>{%c128000}
        }
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After ElideTimepoints (iree-stream-elide-timepoints) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan], iree.fixedpoint.iteration = 0 : index} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c64000] : !stream.resource<external>{%c128000}
        }
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After FixedPointIterator (iree-util-fixed-point-iterator) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c64000] : !stream.resource<external>{%c128000}
        }
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After FuseDispatchBindings (iree-stream-fuse-dispatch-bindings) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: index, %arg4: index, %arg5: index) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%arg3] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%arg4] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%arg5] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %c0_4 = arith.constant 0 : index
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%c0, %c0, %c0 : index, index, index) {
          ro %arg0[%c0_4 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0_4 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0_4 for %c128000] : !stream.resource<external>{%c128000}
        }
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After AnnotateDispatchArguments (iree-stream-annotate-dispatch-arguments) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: index {stream.values = [0 : index]}, %arg4: index {stream.values = [0 : index]}, %arg5: index {stream.values = [0 : index]}) {
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%arg3] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%arg4] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%arg5] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %c0_4 = arith.constant 0 : index
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%c0, %c0, %c0 : index, index, index) {
          ro %arg0[%c0_4 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0_4 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0_4 for %c128000] : !stream.resource<external>{%c128000}
        }
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After PackDispatchOperands (iree-stream-pack-dispatch-operands) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) {
        %0 = arith.extui %arg3 : i32 to i64
        %1 = arith.extui %arg4 : i32 to i64
        %c32_i64 = arith.constant 32 : i64
        %2 = arith.shli %1, %c32_i64 : i64
        %3 = arith.ori %0, %2 : i64
        %4 = arith.index_castui %3 {stream.values = [0 : index]} : i64 to index
        %5 = arith.extui %arg5 : i32 to i64
        %6 = arith.extui %arg6 : i32 to i64
        %c32_i64_0 = arith.constant 32 : i64
        %7 = arith.shli %6, %c32_i64_0 : i64
        %8 = arith.ori %5, %7 : i64
        %9 = arith.index_castui %8 {stream.values = [0 : index]} : i64 to index
        %10 = arith.extui %arg7 : i32 to i64
        %11 = arith.extui %arg8 : i32 to i64
        %c32_i64_1 = arith.constant 32 : i64
        %12 = arith.shli %11, %c32_i64_1 : i64
        %13 = arith.ori %10, %12 : i64
        %14 = arith.index_castui %13 {stream.values = [0 : index]} : i64 to index
        %cst = arith.constant 0.000000e+00 : f16
        %c0 = arith.constant 0 : index
        %15 = stream.binding.subspan %arg0[%4] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %16 = stream.binding.subspan %arg1[%9] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %17 = stream.binding.subspan %arg2[%14] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %18 = flow.dispatch.tensor.load %15, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %19 = flow.dispatch.tensor.load %16, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %20 = tensor.empty() : tensor<1x32000xf16>
        %21 = linalg.fill ins(%cst : f16) outs(%20 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %22 = linalg.matmul_transpose_b ins(%18, %19 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%21 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %22, %17, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %c0_4 = arith.constant 0 : index
    %c0_i64 = arith.constant 0 : i64
    %c0_i32 = arith.constant 0 : i32
    %c32_i64 = arith.constant 32 : i64
    %c0_i64_5 = arith.constant 0 : i64
    %c0_i32_6 = arith.constant 0 : i32
    %c0_i64_7 = arith.constant 0 : i64
    %c0_i32_8 = arith.constant 0 : i32
    %c32_i64_9 = arith.constant 32 : i64
    %c0_i64_10 = arith.constant 0 : i64
    %c0_i32_11 = arith.constant 0 : i32
    %c0_i64_12 = arith.constant 0 : i64
    %c0_i32_13 = arith.constant 0 : i32
    %c32_i64_14 = arith.constant 32 : i64
    %c0_i64_15 = arith.constant 0 : i64
    %c0_i32_16 = arith.constant 0 : i32
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%c0_i32, %c0_i32_6, %c0_i32_8, %c0_i32_11, %c0_i32_13, %c0_i32_16 : i32, i32, i32, i32, i32, i32) {
          ro %arg0[%c0_4 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0_4 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0_4 for %c128000] : !stream.resource<external>{%c128000}
        }
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func private @_main() {
  %c0_i32 = arith.constant 0 : i32
  %c128000 = arith.constant 128000 : index
  %c0 = arith.constant 0 : index
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
  %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
  } => !stream.timepoint
  %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
  %2 = util.optimization_barrier %1 : !stream.resource<transient>
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
  %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
  %5 = util.optimization_barrier %4 : !stream.resource<transient>
  %6 = stream.resource.size %2 : !stream.resource<transient>
  %7 = stream.resource.size %5 : !stream.resource<transient>
  %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
  %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
    stream.cmd.concurrent {
      stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
        ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
        ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
        wo %arg2[%c0 for %c128000] : !stream.resource<external>{%c128000}
      }
      stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
    }
  } => !stream.timepoint
  %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
  %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
  %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
  %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  check.expect_eq(%13, %12) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func private @_main() {
  %c0_i32 = arith.constant 0 : i32
  %c128000 = arith.constant 128000 : index
  %c0 = arith.constant 0 : index
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
  %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
  } => !stream.timepoint
  %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
  %2 = util.optimization_barrier %1 : !stream.resource<transient>
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
  %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
  %5 = util.optimization_barrier %4 : !stream.resource<transient>
  %6 = stream.resource.size %2 : !stream.resource<transient>
  %7 = stream.resource.size %5 : !stream.resource<transient>
  %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
  %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
    stream.cmd.concurrent {
      stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
        ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
        ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
        wo %arg2[%c0 for %c128000] : !stream.resource<external>{%c128000}
      }
      stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
    }
  } => !stream.timepoint
  %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
  %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
  %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
  %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  check.expect_eq(%13, %12) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func private @_main() {
  %c0_i32 = arith.constant 0 : i32
  %c128000 = arith.constant 128000 : index
  %c0 = arith.constant 0 : index
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
  %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
  } => !stream.timepoint
  %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
  %2 = util.optimization_barrier %1 : !stream.resource<transient>
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
  %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
  %5 = util.optimization_barrier %4 : !stream.resource<transient>
  %6 = stream.resource.size %2 : !stream.resource<transient>
  %7 = stream.resource.size %5 : !stream.resource<transient>
  %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
  %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
    stream.cmd.concurrent {
      stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
        ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
        ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
        wo %arg2[%c0 for %c128000] : !stream.resource<external>{%c128000}
      }
      stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
    }
  } => !stream.timepoint
  %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
  %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
  %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
  %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  check.expect_eq(%13, %12) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) {
        %cst = arith.constant 0.000000e+00 : f16
        %c32_i64 = arith.constant 32 : i64
        %0 = arith.extui %arg3 : i32 to i64
        %1 = arith.extui %arg4 : i32 to i64
        %2 = arith.shli %1, %c32_i64 : i64
        %3 = arith.ori %0, %2 : i64
        %4 = arith.index_castui %3 {stream.values = [0 : index]} : i64 to index
        %5 = arith.extui %arg5 : i32 to i64
        %6 = arith.extui %arg6 : i32 to i64
        %7 = arith.shli %6, %c32_i64 : i64
        %8 = arith.ori %5, %7 : i64
        %9 = arith.index_castui %8 {stream.values = [0 : index]} : i64 to index
        %10 = arith.extui %arg7 : i32 to i64
        %11 = arith.extui %arg8 : i32 to i64
        %12 = arith.shli %11, %c32_i64 : i64
        %13 = arith.ori %10, %12 : i64
        %14 = arith.index_castui %13 {stream.values = [0 : index]} : i64 to index
        %15 = stream.binding.subspan %arg0[%4] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %16 = stream.binding.subspan %arg1[%9] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %17 = stream.binding.subspan %arg2[%14] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %18 = flow.dispatch.tensor.load %15, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %19 = flow.dispatch.tensor.load %16, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %20 = tensor.empty() : tensor<1x32000xf16>
        %21 = linalg.fill ins(%cst : f16) outs(%20 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %22 = linalg.matmul_transpose_b ins(%18, %19 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%21 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %22, %17, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c0_i32 = arith.constant 0 : i32
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c128000] : !stream.resource<external>{%c128000}
        }
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) {
        %cst = arith.constant 0.000000e+00 : f16
        %c32_i64 = arith.constant 32 : i64
        %0 = arith.extui %arg3 : i32 to i64
        %1 = arith.extui %arg4 : i32 to i64
        %2 = arith.shli %1, %c32_i64 : i64
        %3 = arith.ori %0, %2 : i64
        %4 = arith.index_castui %3 {stream.values = [0 : index]} : i64 to index
        %5 = arith.extui %arg5 : i32 to i64
        %6 = arith.extui %arg6 : i32 to i64
        %7 = arith.shli %6, %c32_i64 : i64
        %8 = arith.ori %5, %7 : i64
        %9 = arith.index_castui %8 {stream.values = [0 : index]} : i64 to index
        %10 = arith.extui %arg7 : i32 to i64
        %11 = arith.extui %arg8 : i32 to i64
        %12 = arith.shli %11, %c32_i64 : i64
        %13 = arith.ori %10, %12 : i64
        %14 = arith.index_castui %13 {stream.values = [0 : index]} : i64 to index
        %15 = stream.binding.subspan %arg0[%4] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %16 = stream.binding.subspan %arg1[%9] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %17 = stream.binding.subspan %arg2[%14] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %18 = flow.dispatch.tensor.load %15, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %19 = flow.dispatch.tensor.load %16, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %20 = tensor.empty() : tensor<1x32000xf16>
        %21 = linalg.fill ins(%cst : f16) outs(%20 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %22 = linalg.matmul_transpose_b ins(%18, %19 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%21 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %22, %17, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c0_i32 = arith.constant 0 : i32
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c128000] : !stream.resource<external>{%c128000}
        }
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) {
        %cst = arith.constant 0.000000e+00 : f16
        %c32_i64 = arith.constant 32 : i64
        %0 = arith.extui %arg3 : i32 to i64
        %1 = arith.extui %arg4 : i32 to i64
        %2 = arith.shli %1, %c32_i64 : i64
        %3 = arith.ori %0, %2 : i64
        %4 = arith.index_castui %3 {stream.values = [0 : index]} : i64 to index
        %5 = arith.extui %arg5 : i32 to i64
        %6 = arith.extui %arg6 : i32 to i64
        %7 = arith.shli %6, %c32_i64 : i64
        %8 = arith.ori %5, %7 : i64
        %9 = arith.index_castui %8 {stream.values = [0 : index]} : i64 to index
        %10 = arith.extui %arg7 : i32 to i64
        %11 = arith.extui %arg8 : i32 to i64
        %12 = arith.shli %11, %c32_i64 : i64
        %13 = arith.ori %10, %12 : i64
        %14 = arith.index_castui %13 {stream.values = [0 : index]} : i64 to index
        %15 = stream.binding.subspan %arg0[%4] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %16 = stream.binding.subspan %arg1[%9] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %17 = stream.binding.subspan %arg2[%14] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %18 = flow.dispatch.tensor.load %15, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %19 = flow.dispatch.tensor.load %16, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %20 = tensor.empty() : tensor<1x32000xf16>
        %21 = linalg.fill ins(%cst : f16) outs(%20 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %22 = linalg.matmul_transpose_b ins(%18, %19 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%21 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %22, %17, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c0_i32 = arith.constant 0 : i32
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c128000] : !stream.resource<external>{%c128000}
        }
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) {
        %cst = arith.constant 0.000000e+00 : f16
        %c32_i64 = arith.constant 32 : i64
        %0 = arith.extui %arg3 : i32 to i64
        %1 = arith.extui %arg4 : i32 to i64
        %2 = arith.shli %1, %c32_i64 : i64
        %3 = arith.ori %0, %2 : i64
        %4 = arith.index_castui %3 {stream.values = [0 : index]} : i64 to index
        %5 = arith.extui %arg5 : i32 to i64
        %6 = arith.extui %arg6 : i32 to i64
        %7 = arith.shli %6, %c32_i64 : i64
        %8 = arith.ori %5, %7 : i64
        %9 = arith.index_castui %8 {stream.values = [0 : index]} : i64 to index
        %10 = arith.extui %arg7 : i32 to i64
        %11 = arith.extui %arg8 : i32 to i64
        %12 = arith.shli %11, %c32_i64 : i64
        %13 = arith.ori %10, %12 : i64
        %14 = arith.index_castui %13 {stream.values = [0 : index]} : i64 to index
        %15 = stream.binding.subspan %arg0[%4] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %16 = stream.binding.subspan %arg1[%9] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %17 = stream.binding.subspan %arg2[%14] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %18 = flow.dispatch.tensor.load %15, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %19 = flow.dispatch.tensor.load %16, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %20 = tensor.empty() : tensor<1x32000xf16>
        %21 = linalg.fill ins(%cst : f16) outs(%20 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %22 = linalg.matmul_transpose_b ins(%18, %19 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%21 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %22, %17, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c0_i32 = arith.constant 0 : i32
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c128000] : !stream.resource<external>{%c128000}
        }
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After FoldUniformOperands (iree-stream-fold-uniform-operands) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0.000000e+00 : f16
        %c32_i64 = arith.constant 32 : i64
        %0 = arith.extui %c0_i32 : i32 to i64
        %1 = arith.extui %c0_i32 : i32 to i64
        %2 = arith.shli %1, %c32_i64 : i64
        %3 = arith.ori %0, %2 : i64
        %4 = arith.index_castui %3 {stream.values = [0 : index]} : i64 to index
        %5 = arith.extui %c0_i32 : i32 to i64
        %6 = arith.extui %c0_i32 : i32 to i64
        %7 = arith.shli %6, %c32_i64 : i64
        %8 = arith.ori %5, %7 : i64
        %9 = arith.index_castui %8 {stream.values = [0 : index]} : i64 to index
        %10 = arith.extui %c0_i32 : i32 to i64
        %11 = arith.extui %c0_i32 : i32 to i64
        %12 = arith.shli %11, %c32_i64 : i64
        %13 = arith.ori %10, %12 : i64
        %14 = arith.index_castui %13 {stream.values = [0 : index]} : i64 to index
        %15 = stream.binding.subspan %arg0[%4] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %16 = stream.binding.subspan %arg1[%9] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %17 = stream.binding.subspan %arg2[%14] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %18 = flow.dispatch.tensor.load %15, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %19 = flow.dispatch.tensor.load %16, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %20 = tensor.empty() : tensor<1x32000xf16>
        %21 = linalg.fill ins(%cst : f16) outs(%20 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %22 = linalg.matmul_transpose_b ins(%18, %19 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%21 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %22, %17, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c0_i32 = arith.constant 0 : i32
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c128000] : !stream.resource<external>{%c128000}
        }
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func private @_main() {
  %c128000 = arith.constant 128000 : index
  %c0 = arith.constant 0 : index
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
  %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
  } => !stream.timepoint
  %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
  %2 = util.optimization_barrier %1 : !stream.resource<transient>
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
  %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
  %5 = util.optimization_barrier %4 : !stream.resource<transient>
  %6 = stream.resource.size %2 : !stream.resource<transient>
  %7 = stream.resource.size %5 : !stream.resource<transient>
  %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
  %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
    stream.cmd.concurrent {
      stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
        ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
        ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
        wo %arg2[%c0 for %c128000] : !stream.resource<external>{%c128000}
      }
      stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
    }
  } => !stream.timepoint
  %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
  %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
  %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
  %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  check.expect_eq(%13, %12) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func private @_main() {
  %c128000 = arith.constant 128000 : index
  %c0 = arith.constant 0 : index
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
  %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
  } => !stream.timepoint
  %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
  %2 = util.optimization_barrier %1 : !stream.resource<transient>
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
  %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
  %5 = util.optimization_barrier %4 : !stream.resource<transient>
  %6 = stream.resource.size %2 : !stream.resource<transient>
  %7 = stream.resource.size %5 : !stream.resource<transient>
  %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
  %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
    stream.cmd.concurrent {
      stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
        ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
        ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
        wo %arg2[%c0 for %c128000] : !stream.resource<external>{%c128000}
      }
      stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
    }
  } => !stream.timepoint
  %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
  %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
  %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
  %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  check.expect_eq(%13, %12) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func private @_main() {
  %c128000 = arith.constant 128000 : index
  %c0 = arith.constant 0 : index
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
  %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
  } => !stream.timepoint
  %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
  %2 = util.optimization_barrier %1 : !stream.resource<transient>
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
  %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
  %5 = util.optimization_barrier %4 : !stream.resource<transient>
  %6 = stream.resource.size %2 : !stream.resource<transient>
  %7 = stream.resource.size %5 : !stream.resource<transient>
  %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
  %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
    stream.cmd.concurrent {
      stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
        ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
        ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
        wo %arg2[%c0 for %c128000] : !stream.resource<external>{%c128000}
      }
      stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
    }
  } => !stream.timepoint
  %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
  %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
  %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
  %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  check.expect_eq(%13, %12) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0.000000e+00 : f16
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c128000] : !stream.resource<external>{%c128000}
        }
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0.000000e+00 : f16
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c128000] : !stream.resource<external>{%c128000}
        }
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0.000000e+00 : f16
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c128000] : !stream.resource<external>{%c128000}
        }
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0.000000e+00 : f16
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c128000] : !stream.resource<external>{%c128000}
        }
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0.000000e+00 : f16
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c128000] : !stream.resource<external>{%c128000}
        }
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0.000000e+00 : f16
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c128000] : !stream.resource<external>{%c128000}
        }
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0.000000e+00 : f16
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c128000] : !stream.resource<external>{%c128000}
        }
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0.000000e+00 : f16
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c128000] : !stream.resource<external>{%c128000}
        }
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func private @_main() {
  %c128000 = arith.constant 128000 : index
  %c0 = arith.constant 0 : index
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
  %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
  } => !stream.timepoint
  %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
  %2 = util.optimization_barrier %1 : !stream.resource<transient>
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
  %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
  %5 = util.optimization_barrier %4 : !stream.resource<transient>
  %6 = stream.resource.size %2 : !stream.resource<transient>
  %7 = stream.resource.size %5 : !stream.resource<transient>
  %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
  %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
    stream.cmd.concurrent {
      stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
        ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
        ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
        wo %arg2[%c0 for %c128000] : !stream.resource<external>{%c128000}
      }
      stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
    }
  } => !stream.timepoint
  %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
  %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
  %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
  %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  check.expect_eq(%13, %12) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0.000000e+00 : f16
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c128000] : !stream.resource<external>{%c128000}
        }
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0.000000e+00 : f16
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c128000] : !stream.resource<external>{%c128000}
        }
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0.000000e+00 : f16
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c128000] : !stream.resource<external>{%c128000}
        }
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::AssignTargetDevicesPass (iree-hal-assign-target-devices) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0.000000e+00 : f16
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c128000] : !stream.resource<external>{%c128000}
        }
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::VerifyTargetEnvironmentPass (iree-hal-verify-target-environment) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  stream.executable private @_main_dispatch_0 {
    stream.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0.000000e+00 : f16
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c128000] : !stream.resource<external>{%c128000}
        }
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::(anonymous namespace)::MaterializeInterfacesPass (iree-hal-materialize-interfaces) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.variant public @vulkan_spirv_fb target(#executable_target_vulkan_spirv_fb) {
      hal.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 ordinal(0) layout(#pipeline_layout) {
      ^bb0(%arg0: !hal.device):
        %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
          %c0 = arith.constant 0 : index
          %cst = arith.constant 0.000000e+00 : f16
          %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
          %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
          %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
          %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
          %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
          %5 = tensor.empty() : tensor<1x32000xf16>
          %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
          %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
          flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
          return
        }
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@vulkan_spirv_fb::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c128000] : !stream.resource<external>{%c128000}
        } attributes {hal.interface.bindings = [#hal.interface.binding<0, 0>, #hal.interface.binding<0, 1>, #hal.interface.binding<0, 2>]}
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::DumpExecutableSourcesPass (iree-hal-dump-executable-sources) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.variant public @vulkan_spirv_fb target(#executable_target_vulkan_spirv_fb) {
      hal.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 ordinal(0) layout(#pipeline_layout) {
      ^bb0(%arg0: !hal.device):
        %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
          %c0 = arith.constant 0 : index
          %cst = arith.constant 0.000000e+00 : f16
          %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
          %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
          %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
          %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
          %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
          %5 = tensor.empty() : tensor<1x32000xf16>
          %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
          %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
          flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
          return
        }
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@vulkan_spirv_fb::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c128000] : !stream.resource<external>{%c128000}
        } attributes {hal.interface.bindings = [#hal.interface.binding<0, 0>, #hal.interface.binding<0, 1>, #hal.interface.binding<0, 2>]}
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After CPUMaterializeUpperBoundTileSize (iree-codegen-cpu-materialize-upper-bound-tile-size) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After CPUMaterializeUpperBoundTileSize (iree-codegen-cpu-materialize-upper-bound-tile-size) //----- //
func.func private @_main() {
  %c128000 = arith.constant 128000 : index
  %c0 = arith.constant 0 : index
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
  %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
  } => !stream.timepoint
  %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
  %2 = util.optimization_barrier %1 : !stream.resource<transient>
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
  %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
    stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
  %5 = util.optimization_barrier %4 : !stream.resource<transient>
  %6 = stream.resource.size %2 : !stream.resource<transient>
  %7 = stream.resource.size %5 : !stream.resource<transient>
  %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
  %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
    stream.cmd.concurrent {
      stream.cmd.dispatch @_main_dispatch_0::@vulkan_spirv_fb::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
        ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
        ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
        wo %arg2[%c0 for %c128000] : !stream.resource<external>{%c128000}
      } attributes {hal.interface.bindings = [#hal.interface.binding<0, 0>, #hal.interface.binding<0, 1>, #hal.interface.binding<0, 2>]}
      stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
    }
  } => !stream.timepoint
  %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
  %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
  %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
  %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
  check.expect_eq(%13, %12) : tensor<1x32000xf16>
  return
}

// -----// IR Dump After TypePropagation (iree-codegen-type-propagation) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f16
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
  %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
  %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
  %5 = tensor.empty() : tensor<1x32000xf16>
  %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
  return
}

// -----// IR Dump After BubbleUpOrdinalOps (iree-codegen-bubble-up-ordinal-ops) //----- //
module {
  func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f16
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
    %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
    %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
    %5 = tensor.empty() : tensor<1x32000xf16>
    %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
    return
  }
}

// -----// IR Dump After BufferizeCopyOnlyDispatches (iree-codegen-bufferize-copy-only-dispatches) //----- //
module {
  func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f16
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
    %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
    %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
    %5 = tensor.empty() : tensor<1x32000xf16>
    %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
    flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
    return
  }
}

// -----// IR Dump After DecomposeSoftmax (iree-linalg-ext-decompose-softmax) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f16
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
  %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
  %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
  %5 = tensor.empty() : tensor<1x32000xf16>
  %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
  return
}

// -----// IR Dump After MaterializeUserConfigs (iree-codegen-materialize-user-configs) //----- //
hal.executable.variant public @vulkan_spirv_fb target(<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>) {
  hal.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>) {
  ^bb0(%arg0: !hal.device):
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    hal.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
      %c0 = arith.constant 0 : index
      %cst = arith.constant 0.000000e+00 : f16
      %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
      %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
      %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
      %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
      %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
      %5 = tensor.empty() : tensor<1x32000xf16>
      %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
      %7 = linalg.matmul_transpose_b ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
      flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
      return
    }
  }
}

// -----// IR Dump After SPIRVGeneralizeNamedOps (iree-spirv-generalize-named-ops) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f16
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
  %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
  %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
  %5 = tensor.empty() : tensor<1x32000xf16>
  %6 = linalg.fill ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) {
  ^bb0(%in: f16, %in_0: f16, %out: f16):
    %8 = arith.mulf %in, %in_0 : f16
    %9 = arith.addf %out, %8 : f16
    linalg.yield %9 : f16
  } -> tensor<1x32000xf16>
  flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
  return
}

figuring configuration for generic op
trying to deduce config as reduction...
JAKUB: matvec tile config
// -----// IR Dump After SPIRVSelectLoweringStrategy (iree-spirv-select-lowering-strategy-pass) //----- //
hal.executable.variant public @vulkan_spirv_fb target(<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>) {
  hal.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>) attributes {translation_info = #iree_codegen.translation_info<SPIRVSubgroupReduce>, workgroup_size = [64 : index, 1 : index, 1 : index]} {
  ^bb0(%arg0: !hal.device):
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    hal.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
      %c0 = arith.constant 0 : index
      %cst = arith.constant 0.000000e+00 : f16
      %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
      %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
      %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
      %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
      %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
      %5 = tensor.empty() : tensor<1x32000xf16>
      %6 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 1], [0, 0, 4096]]>} ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
      %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 1], [0, 0, 4096]]>} {
      ^bb0(%in: f16, %in_0: f16, %out: f16):
        %8 = arith.mulf %in, %in_0 : f16
        %9 = arith.addf %out, %8 : f16
        linalg.yield %9 : f16
      } -> tensor<1x32000xf16>
      flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
      return
    }
  }
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::ConfigureTargetExecutableVariantsPass (iree-hal-configure-target-executable-variants) //----- //
hal.executable.variant public @vulkan_spirv_fb target(<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>) {
  hal.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>) attributes {translation_info = #iree_codegen.translation_info<SPIRVSubgroupReduce>, workgroup_size = [64 : index, 1 : index, 1 : index]} {
  ^bb0(%arg0: !hal.device):
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    hal.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
      %c0 = arith.constant 0 : index
      %cst = arith.constant 0.000000e+00 : f16
      %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
      %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
      %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
      %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
      %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
      %5 = tensor.empty() : tensor<1x32000xf16>
      %6 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 1], [0, 0, 4096]]>} ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
      %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 1], [0, 0, 4096]]>} {
      ^bb0(%in: f16, %in_0: f16, %out: f16):
        %8 = arith.mulf %in, %in_0 : f16
        %9 = arith.addf %out, %8 : f16
        linalg.yield %9 : f16
      } -> tensor<1x32000xf16>
      flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
      return
    }
  }
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::ConfigureExecutablesPass (iree-hal-configure-executables) //----- //
hal.executable private @_main_dispatch_0 {
  hal.executable.variant public @vulkan_spirv_fb target(<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>) {
    hal.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>) attributes {translation_info = #iree_codegen.translation_info<SPIRVSubgroupReduce>, workgroup_size = [64 : index, 1 : index, 1 : index]} {
    ^bb0(%arg0: !hal.device):
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      hal.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0.000000e+00 : f16
        %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
        %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
        %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
        %5 = tensor.empty() : tensor<1x32000xf16>
        %6 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 1], [0, 0, 4096]]>} ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
        %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 1], [0, 0, 4096]]>} {
        ^bb0(%in: f16, %in_0: f16, %out: f16):
          %8 = arith.mulf %in, %in_0 : f16
          %9 = arith.addf %out, %8 : f16
          linalg.yield %9 : f16
        } -> tensor<1x32000xf16>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
        return
      }
    }
  }
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::DumpExecutableSourcesPass (iree-hal-dump-executable-sources) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[1, 1], [0, 0, 4096]]>
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#map = affine_map<(d0, d1, d2) -> (d0, d2)>
#map1 = affine_map<(d0, d1, d2) -> (d1, d2)>
#map2 = affine_map<(d0, d1, d2) -> (d0, d1)>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<SPIRVSubgroupReduce>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.variant public @vulkan_spirv_fb target(#executable_target_vulkan_spirv_fb) {
      hal.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
          %c0 = arith.constant 0 : index
          %cst = arith.constant 0.000000e+00 : f16
          %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
          %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
          %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
          %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
          %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
          %5 = tensor.empty() : tensor<1x32000xf16>
          %6 = linalg.fill {lowering_config = #config} ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
          %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) attrs =  {lowering_config = #config} {
          ^bb0(%in: f16, %in_0: f16, %out: f16):
            %8 = arith.mulf %in, %in_0 : f16
            %9 = arith.addf %out, %8 : f16
            linalg.yield %9 : f16
          } -> tensor<1x32000xf16>
          flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
          return
        }
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@vulkan_spirv_fb::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c128000] : !stream.resource<external>{%c128000}
        } attributes {hal.interface.bindings = [#hal.interface.binding<0, 0>, #hal.interface.binding<0, 1>, #hal.interface.binding<0, 2>]}
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


/home/jakub/iree/bench/vmt.mlir:0:0: remark: Executable benchmarks were requested but none were generated. Run with --debug-only=iree-dump-executable-benchmarks for more details.

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::DumpExecutableBenchmarksPass (iree-hal-dump-executable-benchmarks) //----- //
#config = #iree_codegen.lowering_config<tile_sizes = [[1, 1], [0, 0, 4096]]>
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#map = affine_map<(d0, d1, d2) -> (d0, d2)>
#map1 = affine_map<(d0, d1, d2) -> (d1, d2)>
#map2 = affine_map<(d0, d1, d2) -> (d0, d1)>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<SPIRVSubgroupReduce>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.variant public @vulkan_spirv_fb target(#executable_target_vulkan_spirv_fb) {
      hal.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
          %c0 = arith.constant 0 : index
          %cst = arith.constant 0.000000e+00 : f16
          %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
          %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
          %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
          %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
          %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [32000, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<32000x4096xf16>
          %5 = tensor.empty() : tensor<1x32000xf16>
          %6 = linalg.fill {lowering_config = #config} ins(%cst : f16) outs(%5 : tensor<1x32000xf16>) -> tensor<1x32000xf16>
          %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<1x4096xf16>, tensor<32000x4096xf16>) outs(%6 : tensor<1x32000xf16>) attrs =  {lowering_config = #config} {
          ^bb0(%in: f16, %in_0: f16, %out: f16):
            %8 = arith.mulf %in, %in_0 : f16
            %9 = arith.addf %out, %8 : f16
            linalg.yield %9 : f16
          } -> tensor<1x32000xf16>
          flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [1, 32000], strides = [1, 1] : tensor<1x32000xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
          return
        }
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c8192} => !stream.timepoint
    %0 = stream.cmd.execute await(%result_timepoint) => with(%result as %arg0: !stream.resource<transient>{%c8192}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c8192] : i16 -> !stream.resource<transient>{%c8192}
    } => !stream.timepoint
    %1 = stream.timepoint.await %0 => %result : !stream.resource<transient>{%c8192}
    %2 = util.optimization_barrier %1 : !stream.resource<transient>
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized : !stream.resource<transient>{%c262144000} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint_1) => with(%result_0 as %arg0: !stream.resource<transient>{%c262144000}) {
      stream.cmd.fill %c15360_i16, %arg0[%c0 for %c262144000] : i16 -> !stream.resource<transient>{%c262144000}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result_0 : !stream.resource<transient>{%c262144000}
    %5 = util.optimization_barrier %4 : !stream.resource<transient>
    %6 = stream.resource.size %2 : !stream.resource<transient>
    %7 = stream.resource.size %5 : !stream.resource<transient>
    %result_2, %result_timepoint_3 = stream.resource.alloca uninitialized : !stream.resource<external>{%c128000} => !stream.timepoint
    %8 = stream.cmd.execute await(%result_timepoint_3) => with(%2 as %arg0: !stream.resource<transient>{%6}, %5 as %arg1: !stream.resource<transient>{%7}, %result_2 as %arg2: !stream.resource<external>{%c128000}) {
      stream.cmd.concurrent {
        stream.cmd.dispatch @_main_dispatch_0::@vulkan_spirv_fb::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 {
          ro %arg0[%c0 for %6] : !stream.resource<transient>{%6},
          ro %arg1[%c0 for %7] : !stream.resource<transient>{%7},
          wo %arg2[%c0 for %c128000] : !stream.resource<external>{%c128000}
        } attributes {hal.interface.bindings = [#hal.interface.binding<0, 0>, #hal.interface.binding<0, 1>, #hal.interface.binding<0, 2>]}
        stream.cmd.fill %c27648_i16, %arg2[%c64000 for %c64000] : i16 -> !stream.resource<external>{%c128000}
      }
    } => !stream.timepoint
    %9 = stream.timepoint.await %8 => %result_2 : !stream.resource<external>{%c128000}
    %10 = stream.resource.subview %9[%c64000] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %11 = stream.resource.subview %9[%c0] : !stream.resource<external>{%c128000} -> !stream.resource<external>{%c64000}
    %12 = stream.tensor.export %10 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    %13 = stream.tensor.export %11 : tensor<1x32000xf16> in !stream.resource<external>{%c64000} -> tensor<1x32000xf16>
    check.expect_eq(%13, %12) : tensor<1x32000xf16>
    return
  }
}


// -----// IR Dump After TileAndDistributeToWorkgroups (iree-codegen-tile-and-distribute-to-workgroups) //----- //
hal.executable.variant public @vulkan_spirv_fb target(<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>) {
  hal.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>) attributes {translation_info = #iree_codegen.translation_info<SPIRVSubgroupReduce>, workgroup_size = [64 : index, 1 : index, 1 : index]} {
  ^bb0(%arg0: !hal.device):
    %c32000 = arith.constant 32000 : index
    %c1 = arith.constant 1 : index
    hal.return %c32000, %c1, %c1 : index, index, index
  }
  builtin.module {
    func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
      %c0 = arith.constant 0 : index
      %cst = arith.constant 0.000000e+00 : f16
      %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
      %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
      %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
      %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
      %workgroup_id_x = hal.interface.workgroup.id[0] : index
      %4 = flow.dispatch.tensor.load %1, offsets = [%workgroup_id_x, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<1x4096xf16>
      %5 = tensor.empty() : tensor<1x1xf16>
      %6 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 1], [0, 0, 4096]]>} ins(%cst : f16) outs(%5 : tensor<1x1xf16>) -> tensor<1x1xf16>
      %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<1x4096xf16>, tensor<1x4096xf16>) outs(%6 : tensor<1x1xf16>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 1], [0, 0, 4096]]>} {
      ^bb0(%in: f16, %in_0: f16, %out: f16):
        %8 = arith.mulf %in, %in_0 : f16
        %9 = arith.addf %out, %8 : f16
        linalg.yield %9 : f16
      } -> tensor<1x1xf16>
      flow.dispatch.tensor.store %7, %2, offsets = [0, %workgroup_id_x], sizes = [1, 1], strides = [1, 1] : tensor<1x1xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
      return
    }
  }
}

// -----// IR Dump After FuseTensorPadWithConsumer (iree-codegen-fuse-tensor-pad-with-consumer) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f16
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
  %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = flow.dispatch.tensor.load %1, offsets = [%workgroup_id_x, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<1x4096xf16>
  %5 = tensor.empty() : tensor<1x1xf16>
  %6 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 1], [0, 0, 4096]]>} ins(%cst : f16) outs(%5 : tensor<1x1xf16>) -> tensor<1x1xf16>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<1x4096xf16>, tensor<1x4096xf16>) outs(%6 : tensor<1x1xf16>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 1], [0, 0, 4096]]>} {
  ^bb0(%in: f16, %in_0: f16, %out: f16):
    %8 = arith.mulf %in, %in_0 : f16
    %9 = arith.addf %out, %8 : f16
    linalg.yield %9 : f16
  } -> tensor<1x1xf16>
  flow.dispatch.tensor.store %7, %2, offsets = [0, %workgroup_id_x], sizes = [1, 1], strides = [1, 1] : tensor<1x1xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
  return
}

// -----// IR Dump After ConvertToDestinationPassingStyle (iree-codegen-convert-to-destination-passing-style) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f16
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %3 = flow.dispatch.tensor.load %2, offsets = [0, %workgroup_id_x], sizes = [1, 1], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>> -> tensor<1x1xf16>
  %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
  %workgroup_id_x_0 = hal.interface.workgroup.id[0] : index
  %5 = flow.dispatch.tensor.load %1, offsets = [%workgroup_id_x_0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<1x4096xf16>
  %6 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 1], [0, 0, 4096]]>} ins(%cst : f16) outs(%3 : tensor<1x1xf16>) -> tensor<1x1xf16>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%4, %5 : tensor<1x4096xf16>, tensor<1x4096xf16>) outs(%6 : tensor<1x1xf16>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 1], [0, 0, 4096]]>} {
  ^bb0(%in: f16, %in_1: f16, %out: f16):
    %8 = arith.mulf %in, %in_1 : f16
    %9 = arith.addf %out, %8 : f16
    linalg.yield %9 : f16
  } -> tensor<1x1xf16>
  flow.dispatch.tensor.store %7, %2, offsets = [0, %workgroup_id_x_0], sizes = [1, 1], strides = [1, 1] : tensor<1x1xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module {
  func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f16
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %3 = flow.dispatch.tensor.load %2, offsets = [0, %workgroup_id_x], sizes = [1, 1], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>> -> tensor<1x1xf16>
    %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
    %workgroup_id_x_0 = hal.interface.workgroup.id[0] : index
    %5 = flow.dispatch.tensor.load %1, offsets = [%workgroup_id_x_0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<1x4096xf16>
    %6 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 1], [0, 0, 4096]]>} ins(%cst : f16) outs(%3 : tensor<1x1xf16>) -> tensor<1x1xf16>
    %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%4, %5 : tensor<1x4096xf16>, tensor<1x4096xf16>) outs(%6 : tensor<1x1xf16>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 1], [0, 0, 4096]]>} {
    ^bb0(%in: f16, %in_1: f16, %out: f16):
      %8 = arith.mulf %in, %in_1 : f16
      %9 = arith.addf %out, %8 : f16
      linalg.yield %9 : f16
    } -> tensor<1x1xf16>
    flow.dispatch.tensor.store %7, %2, offsets = [0, %workgroup_id_x_0], sizes = [1, 1], strides = [1, 1] : tensor<1x1xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
    return
  }
}

// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f16
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %3 = flow.dispatch.tensor.load %2, offsets = [0, %workgroup_id_x], sizes = [1, 1], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>> -> tensor<1x1xf16>
    %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
    %5 = flow.dispatch.tensor.load %1, offsets = [%workgroup_id_x, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<1x4096xf16>
    %6 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 1], [0, 0, 4096]]>} ins(%cst : f16) outs(%3 : tensor<1x1xf16>) -> tensor<1x1xf16>
    %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%4, %5 : tensor<1x4096xf16>, tensor<1x4096xf16>) outs(%6 : tensor<1x1xf16>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 1], [0, 0, 4096]]>} {
    ^bb0(%in: f16, %in_0: f16, %out: f16):
      %8 = arith.mulf %in, %in_0 : f16
      %9 = arith.addf %out, %8 : f16
      linalg.yield %9 : f16
    } -> tensor<1x1xf16>
    flow.dispatch.tensor.store %7, %2, offsets = [0, %workgroup_id_x], sizes = [1, 1], strides = [1, 1] : tensor<1x1xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
    return
  }
}

// -----// IR Dump After RematerializeParallelOps (iree-codegen-rematerialize-parallel-ops) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f16
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %3 = flow.dispatch.tensor.load %2, offsets = [0, %workgroup_id_x], sizes = [1, 1], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>> -> tensor<1x1xf16>
  %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
  %5 = flow.dispatch.tensor.load %1, offsets = [%workgroup_id_x, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<1x4096xf16>
  %6 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 1], [0, 0, 4096]]>} ins(%cst : f16) outs(%3 : tensor<1x1xf16>) -> tensor<1x1xf16>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%4, %5 : tensor<1x4096xf16>, tensor<1x4096xf16>) outs(%6 : tensor<1x1xf16>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 1], [0, 0, 4096]]>} {
  ^bb0(%in: f16, %in_0: f16, %out: f16):
    %8 = arith.mulf %in, %in_0 : f16
    %9 = arith.addf %out, %8 : f16
    linalg.yield %9 : f16
  } -> tensor<1x1xf16>
  flow.dispatch.tensor.store %7, %2, offsets = [0, %workgroup_id_x], sizes = [1, 1], strides = [1, 1] : tensor<1x1xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f16
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %3 = flow.dispatch.tensor.load %2, offsets = [0, %workgroup_id_x], sizes = [1, 1], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>> -> tensor<1x1xf16>
  %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
  %5 = flow.dispatch.tensor.load %1, offsets = [%workgroup_id_x, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<1x4096xf16>
  %6 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 1], [0, 0, 4096]]>} ins(%cst : f16) outs(%3 : tensor<1x1xf16>) -> tensor<1x1xf16>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%4, %5 : tensor<1x4096xf16>, tensor<1x4096xf16>) outs(%6 : tensor<1x1xf16>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 1], [0, 0, 4096]]>} {
  ^bb0(%in: f16, %in_0: f16, %out: f16):
    %8 = arith.mulf %in, %in_0 : f16
    %9 = arith.addf %out, %8 : f16
    linalg.yield %9 : f16
  } -> tensor<1x1xf16>
  flow.dispatch.tensor.store %7, %2, offsets = [0, %workgroup_id_x], sizes = [1, 1], strides = [1, 1] : tensor<1x1xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
  return
}

JAKUB: GPU tile reduction on:
%7 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%4, %5 : tensor<1x4096xf16>, tensor<1x4096xf16>) outs(%6 : tensor<1x1xf16>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 1], [0, 0, 4096]]>} {
^bb0(%in: f16, %in_0: f16, %out: f16):
  %8 = arith.mulf %in, %in_0 : f16
  %9 = arith.addf %out, %8 : f16
  linalg.yield %9 : f16
} -> tensor<1x1xf16>
JAKUB: tileReduction on:
%7 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%4, %5 : tensor<1x4096xf16>, tensor<1x4096xf16>) outs(%6 : tensor<1x1xf16>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 1], [0, 0, 4096]]>} {
^bb0(%in: f16, %in_0: f16, %out: f16):
  %8 = arith.mulf %in, %in_0 : f16
  %9 = arith.addf %out, %8 : f16
  linalg.yield %9 : f16
} -> tensor<1x1xf16>
JAKUB: lowering config attr: #iree_codegen.lowering_config<tile_sizes = [[1, 1], [0, 0, 4096]]>
JAKUB: tiling sizes: 0, 0, 4096
JAKUB: Tiling succeeded
// -----// IR Dump After GPUTileReduction (iree-codegen-gpu-tile-reduction) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f16
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %3 = flow.dispatch.tensor.load %2, offsets = [0, %workgroup_id_x], sizes = [1, 1], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>> -> tensor<1x1xf16>
  %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
  %5 = flow.dispatch.tensor.load %1, offsets = [%workgroup_id_x, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<1x4096xf16>
  %6 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 1], [0, 0, 4096]]>} ins(%cst : f16) outs(%3 : tensor<1x1xf16>) -> tensor<1x1xf16>
  %c0_0 = arith.constant 0 : index
  %c0_1 = arith.constant 0 : index
  %c4096 = arith.constant 4096 : index
  %7 = tensor.empty() : tensor<1x1x4096xf16>
  %cst_2 = arith.constant 0.000000e+00 : f16
  %8 = linalg.fill ins(%cst_2 : f16) outs(%7 : tensor<1x1x4096xf16>) -> tensor<1x1x4096xf16>
  %c0_3 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c0_4 = arith.constant 0 : index
  %c0_5 = arith.constant 0 : index
  %c1_6 = arith.constant 1 : index
  %c0_7 = arith.constant 0 : index
  %c0_8 = arith.constant 0 : index
  %c4096_9 = arith.constant 4096 : index
  %c4096_10 = arith.constant 4096 : index
  %9 = scf.for %arg0 = %c0_8 to %c4096_9 step %c4096_10 iter_args(%arg1 = %8) -> (tensor<1x1x4096xf16>) {
    %extracted_slice = tensor.extract_slice %4[0, %arg0] [1, 4096] [1, 1] : tensor<1x4096xf16> to tensor<1x4096xf16>
    %extracted_slice_11 = tensor.extract_slice %5[0, %arg0] [1, 4096] [1, 1] : tensor<1x4096xf16> to tensor<1x4096xf16>
    %extracted_slice_12 = tensor.extract_slice %arg1[0, 0, 0] [%c1, %c1_6, 4096] [1, 1, 1] : tensor<1x1x4096xf16> to tensor<?x?x4096xf16>
    %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel"]} ins(%extracted_slice, %extracted_slice_11 : tensor<1x4096xf16>, tensor<1x4096xf16>) outs(%extracted_slice_12 : tensor<?x?x4096xf16>) {
    ^bb0(%in: f16, %in_16: f16, %out: f16):
      %12 = arith.mulf %in, %in_16 : f16
      %13 = arith.addf %out, %12 : f16
      linalg.yield %13 : f16
    } -> tensor<?x?x4096xf16>
    %c0_13 = arith.constant 0 : index
    %dim = tensor.dim %11, %c0_13 : tensor<?x?x4096xf16>
    %c1_14 = arith.constant 1 : index
    %dim_15 = tensor.dim %11, %c1_14 : tensor<?x?x4096xf16>
    %inserted_slice = tensor.insert_slice %11 into %arg1[0, 0, 0] [%dim, %dim_15, 4096] [1, 1, 1] : tensor<?x?x4096xf16> into tensor<1x1x4096xf16>
    scf.yield %inserted_slice : tensor<1x1x4096xf16>
  }
  %10 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%9 : tensor<1x1x4096xf16>) outs(%6 : tensor<1x1xf16>) {
  ^bb0(%in: f16, %out: f16):
    %11 = arith.addf %in, %out : f16
    linalg.yield %11 : f16
  } -> tensor<1x1xf16>
  flow.dispatch.tensor.store %10, %2, offsets = [0, %workgroup_id_x], sizes = [1, 1], strides = [1, 1] : tensor<1x1xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f16
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %3 = flow.dispatch.tensor.load %2, offsets = [0, %workgroup_id_x], sizes = [1, 1], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>> -> tensor<1x1xf16>
  %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
  %5 = flow.dispatch.tensor.load %1, offsets = [%workgroup_id_x, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<1x4096xf16>
  %6 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 1], [0, 0, 4096]]>} ins(%cst : f16) outs(%3 : tensor<1x1xf16>) -> tensor<1x1xf16>
  %7 = tensor.empty() : tensor<1x1x4096xf16>
  %8 = linalg.fill ins(%cst : f16) outs(%7 : tensor<1x1x4096xf16>) -> tensor<1x1x4096xf16>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel"]} ins(%4, %5 : tensor<1x4096xf16>, tensor<1x4096xf16>) outs(%8 : tensor<1x1x4096xf16>) {
  ^bb0(%in: f16, %in_0: f16, %out: f16):
    %11 = arith.mulf %in, %in_0 : f16
    %12 = arith.addf %out, %11 : f16
    linalg.yield %12 : f16
  } -> tensor<1x1x4096xf16>
  %10 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%9 : tensor<1x1x4096xf16>) outs(%6 : tensor<1x1xf16>) {
  ^bb0(%in: f16, %out: f16):
    %11 = arith.addf %in, %out : f16
    linalg.yield %11 : f16
  } -> tensor<1x1xf16>
  flow.dispatch.tensor.store %10, %2, offsets = [0, %workgroup_id_x], sizes = [1, 1], strides = [1, 1] : tensor<1x1xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f16
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %3 = flow.dispatch.tensor.load %2, offsets = [0, %workgroup_id_x], sizes = [1, 1], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>> -> tensor<1x1xf16>
  %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
  %5 = flow.dispatch.tensor.load %1, offsets = [%workgroup_id_x, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<1x4096xf16>
  %6 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 1], [0, 0, 4096]]>} ins(%cst : f16) outs(%3 : tensor<1x1xf16>) -> tensor<1x1xf16>
  %7 = tensor.empty() : tensor<1x1x4096xf16>
  %8 = linalg.fill ins(%cst : f16) outs(%7 : tensor<1x1x4096xf16>) -> tensor<1x1x4096xf16>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel"]} ins(%4, %5 : tensor<1x4096xf16>, tensor<1x4096xf16>) outs(%8 : tensor<1x1x4096xf16>) {
  ^bb0(%in: f16, %in_0: f16, %out: f16):
    %11 = arith.mulf %in, %in_0 : f16
    %12 = arith.addf %out, %11 : f16
    linalg.yield %12 : f16
  } -> tensor<1x1x4096xf16>
  %10 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%9 : tensor<1x1x4096xf16>) outs(%6 : tensor<1x1xf16>) {
  ^bb0(%in: f16, %out: f16):
    %11 = arith.addf %in, %out : f16
    linalg.yield %11 : f16
  } -> tensor<1x1xf16>
  flow.dispatch.tensor.store %10, %2, offsets = [0, %workgroup_id_x], sizes = [1, 1], strides = [1, 1] : tensor<1x1xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
  return
}

// -----// IR Dump After GenericVectorization (iree-codegen-generic-vectorization) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %cst = arith.constant dense<0.000000e+00> : vector<1x1x4096xf16>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<1x1xf16>
  %c0 = arith.constant 0 : index
  %cst_1 = arith.constant 0.000000e+00 : f16
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %3 = flow.dispatch.tensor.load %2, offsets = [0, %workgroup_id_x], sizes = [1, 1], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>> -> tensor<1x1xf16>
  %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
  %5 = flow.dispatch.tensor.load %1, offsets = [%workgroup_id_x, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<1x4096xf16>
  %6 = vector.transfer_write %cst_0, %3[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, tensor<1x1xf16>
  %7 = vector.transfer_read %4[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (d0, 0, d1)>} : tensor<1x4096xf16>, vector<1x1x4096xf16>
  %8 = vector.transfer_read %5[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (0, d0, d1)>} : tensor<1x4096xf16>, vector<1x1x4096xf16>
  %9 = arith.mulf %7, %8 : vector<1x1x4096xf16>
  %10 = arith.addf %9, %cst : vector<1x1x4096xf16>
  %11 = vector.multi_reduction <add>, %10, %cst_0 [2] : vector<1x1x4096xf16> to vector<1x1xf16>
  %12 = vector.transfer_write %11, %6[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, tensor<1x1xf16>
  flow.dispatch.tensor.store %12, %2, offsets = [0, %workgroup_id_x], sizes = [1, 1], strides = [1, 1] : tensor<1x1xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
  return
}

// -----// IR Dump After HoistRedundantVectorTransfers (iree-codegen-hoist-redundant-vector-transfers) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %cst = arith.constant dense<0.000000e+00> : vector<1x1x4096xf16>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<1x1xf16>
  %c0 = arith.constant 0 : index
  %cst_1 = arith.constant 0.000000e+00 : f16
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %3 = flow.dispatch.tensor.load %2, offsets = [0, %workgroup_id_x], sizes = [1, 1], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>> -> tensor<1x1xf16>
  %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
  %5 = flow.dispatch.tensor.load %1, offsets = [%workgroup_id_x, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<1x4096xf16>
  %6 = vector.transfer_write %cst_0, %3[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, tensor<1x1xf16>
  %7 = vector.transfer_read %4[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (d0, 0, d1)>} : tensor<1x4096xf16>, vector<1x1x4096xf16>
  %8 = vector.transfer_read %5[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (0, d0, d1)>} : tensor<1x4096xf16>, vector<1x1x4096xf16>
  %9 = arith.mulf %7, %8 : vector<1x1x4096xf16>
  %10 = arith.addf %9, %cst : vector<1x1x4096xf16>
  %11 = vector.multi_reduction <add>, %10, %cst_0 [2] : vector<1x1x4096xf16> to vector<1x1xf16>
  %12 = vector.transfer_write %11, %6[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, tensor<1x1xf16>
  flow.dispatch.tensor.store %12, %2, offsets = [0, %workgroup_id_x], sizes = [1, 1], strides = [1, 1] : tensor<1x1xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %cst = arith.constant dense<0.000000e+00> : vector<1x1x4096xf16>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<1x1xf16>
  %c0 = arith.constant 0 : index
  %cst_1 = arith.constant 0.000000e+00 : f16
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %3 = flow.dispatch.tensor.load %2, offsets = [0, %workgroup_id_x], sizes = [1, 1], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>> -> tensor<1x1xf16>
  %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
  %5 = flow.dispatch.tensor.load %1, offsets = [%workgroup_id_x, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<1x4096xf16>
  %6 = vector.transfer_read %4[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (d0, 0, d1)>} : tensor<1x4096xf16>, vector<1x1x4096xf16>
  %7 = vector.transfer_read %5[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (0, d0, d1)>} : tensor<1x4096xf16>, vector<1x1x4096xf16>
  %8 = arith.mulf %6, %7 : vector<1x1x4096xf16>
  %9 = arith.addf %8, %cst : vector<1x1x4096xf16>
  %10 = vector.multi_reduction <add>, %9, %cst_0 [2] : vector<1x1x4096xf16> to vector<1x1xf16>
  %11 = vector.transfer_write %10, %3[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, tensor<1x1xf16>
  flow.dispatch.tensor.store %11, %2, offsets = [0, %workgroup_id_x], sizes = [1, 1], strides = [1, 1] : tensor<1x1xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %cst = arith.constant dense<0.000000e+00> : vector<1x1x4096xf16>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<1x1xf16>
  %c0 = arith.constant 0 : index
  %cst_1 = arith.constant 0.000000e+00 : f16
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %3 = flow.dispatch.tensor.load %2, offsets = [0, %workgroup_id_x], sizes = [1, 1], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>> -> tensor<1x1xf16>
  %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
  %5 = flow.dispatch.tensor.load %1, offsets = [%workgroup_id_x, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<1x4096xf16>
  %6 = vector.transfer_read %4[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (d0, 0, d1)>} : tensor<1x4096xf16>, vector<1x1x4096xf16>
  %7 = vector.transfer_read %5[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (0, d0, d1)>} : tensor<1x4096xf16>, vector<1x1x4096xf16>
  %8 = arith.mulf %6, %7 : vector<1x1x4096xf16>
  %9 = arith.addf %8, %cst : vector<1x1x4096xf16>
  %10 = vector.multi_reduction <add>, %9, %cst_0 [2] : vector<1x1x4096xf16> to vector<1x1xf16>
  %11 = vector.transfer_write %10, %3[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, tensor<1x1xf16>
  flow.dispatch.tensor.store %11, %2, offsets = [0, %workgroup_id_x], sizes = [1, 1], strides = [1, 1] : tensor<1x1xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
  return
}

// -----// IR Dump After LoopInvariantCodeMotion (loop-invariant-code-motion) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %cst = arith.constant dense<0.000000e+00> : vector<1x1x4096xf16>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<1x1xf16>
  %c0 = arith.constant 0 : index
  %cst_1 = arith.constant 0.000000e+00 : f16
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %3 = flow.dispatch.tensor.load %2, offsets = [0, %workgroup_id_x], sizes = [1, 1], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>> -> tensor<1x1xf16>
  %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
  %5 = flow.dispatch.tensor.load %1, offsets = [%workgroup_id_x, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<1x4096xf16>
  %6 = vector.transfer_read %4[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (d0, 0, d1)>} : tensor<1x4096xf16>, vector<1x1x4096xf16>
  %7 = vector.transfer_read %5[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (0, d0, d1)>} : tensor<1x4096xf16>, vector<1x1x4096xf16>
  %8 = arith.mulf %6, %7 : vector<1x1x4096xf16>
  %9 = arith.addf %8, %cst : vector<1x1x4096xf16>
  %10 = vector.multi_reduction <add>, %9, %cst_0 [2] : vector<1x1x4096xf16> to vector<1x1xf16>
  %11 = vector.transfer_write %10, %3[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, tensor<1x1xf16>
  flow.dispatch.tensor.store %11, %2, offsets = [0, %workgroup_id_x], sizes = [1, 1], strides = [1, 1] : tensor<1x1xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %cst = arith.constant dense<0.000000e+00> : vector<1x1x4096xf16>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<1x1xf16>
  %c0 = arith.constant 0 : index
  %cst_1 = arith.constant 0.000000e+00 : f16
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %3 = flow.dispatch.tensor.load %2, offsets = [0, %workgroup_id_x], sizes = [1, 1], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>> -> tensor<1x1xf16>
  %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
  %5 = flow.dispatch.tensor.load %1, offsets = [%workgroup_id_x, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<1x4096xf16>
  %6 = vector.transfer_read %4[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (d0, 0, d1)>} : tensor<1x4096xf16>, vector<1x1x4096xf16>
  %7 = vector.transfer_read %5[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (0, d0, d1)>} : tensor<1x4096xf16>, vector<1x1x4096xf16>
  %8 = arith.mulf %6, %7 : vector<1x1x4096xf16>
  %9 = arith.addf %8, %cst : vector<1x1x4096xf16>
  %10 = vector.multi_reduction <add>, %9, %cst_0 [2] : vector<1x1x4096xf16> to vector<1x1xf16>
  %11 = vector.transfer_write %10, %3[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, tensor<1x1xf16>
  flow.dispatch.tensor.store %11, %2, offsets = [0, %workgroup_id_x], sizes = [1, 1], strides = [1, 1] : tensor<1x1xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %cst = arith.constant dense<0.000000e+00> : vector<1x1x4096xf16>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<1x1xf16>
  %c0 = arith.constant 0 : index
  %cst_1 = arith.constant 0.000000e+00 : f16
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %3 = flow.dispatch.tensor.load %2, offsets = [0, %workgroup_id_x], sizes = [1, 1], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>> -> tensor<1x1xf16>
  %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
  %5 = flow.dispatch.tensor.load %1, offsets = [%workgroup_id_x, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<1x4096xf16>
  %6 = vector.transfer_read %4[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (d0, 0, d1)>} : tensor<1x4096xf16>, vector<1x1x4096xf16>
  %7 = vector.transfer_read %5[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (0, d0, d1)>} : tensor<1x4096xf16>, vector<1x1x4096xf16>
  %8 = arith.mulf %6, %7 : vector<1x1x4096xf16>
  %9 = arith.addf %8, %cst : vector<1x1x4096xf16>
  %10 = vector.multi_reduction <add>, %9, %cst_0 [2] : vector<1x1x4096xf16> to vector<1x1xf16>
  %11 = vector.transfer_write %10, %3[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, tensor<1x1xf16>
  flow.dispatch.tensor.store %11, %2, offsets = [0, %workgroup_id_x], sizes = [1, 1], strides = [1, 1] : tensor<1x1xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
  return
}

// -----// IR Dump After ResolveShapedTypeResultDims (resolve-shaped-type-result-dims) //----- //
module {
  func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
    %cst = arith.constant dense<0.000000e+00> : vector<1x1x4096xf16>
    %cst_0 = arith.constant dense<0.000000e+00> : vector<1x1xf16>
    %c0 = arith.constant 0 : index
    %cst_1 = arith.constant 0.000000e+00 : f16
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %3 = flow.dispatch.tensor.load %2, offsets = [0, %workgroup_id_x], sizes = [1, 1], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>> -> tensor<1x1xf16>
    %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
    %5 = flow.dispatch.tensor.load %1, offsets = [%workgroup_id_x, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<1x4096xf16>
    %6 = vector.transfer_read %4[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (d0, 0, d1)>} : tensor<1x4096xf16>, vector<1x1x4096xf16>
    %7 = vector.transfer_read %5[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (0, d0, d1)>} : tensor<1x4096xf16>, vector<1x1x4096xf16>
    %8 = arith.mulf %6, %7 : vector<1x1x4096xf16>
    %9 = arith.addf %8, %cst : vector<1x1x4096xf16>
    %10 = vector.multi_reduction <add>, %9, %cst_0 [2] : vector<1x1x4096xf16> to vector<1x1xf16>
    %11 = vector.transfer_write %10, %3[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, tensor<1x1xf16>
    flow.dispatch.tensor.store %11, %2, offsets = [0, %workgroup_id_x], sizes = [1, 1], strides = [1, 1] : tensor<1x1xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
    return
  }
}

// -----// IR Dump After EliminateEmptyTensors (iree-eliminate-empty-tensors) //----- //
module {
  func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
    %cst = arith.constant dense<0.000000e+00> : vector<1x1x4096xf16>
    %cst_0 = arith.constant dense<0.000000e+00> : vector<1x1xf16>
    %c0 = arith.constant 0 : index
    %cst_1 = arith.constant 0.000000e+00 : f16
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %3 = flow.dispatch.tensor.load %2, offsets = [0, %workgroup_id_x], sizes = [1, 1], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>> -> tensor<1x1xf16>
    %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
    %5 = flow.dispatch.tensor.load %1, offsets = [%workgroup_id_x, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<1x4096xf16>
    %6 = vector.transfer_read %4[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (d0, 0, d1)>} : tensor<1x4096xf16>, vector<1x1x4096xf16>
    %7 = vector.transfer_read %5[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (0, d0, d1)>} : tensor<1x4096xf16>, vector<1x1x4096xf16>
    %8 = arith.mulf %6, %7 : vector<1x1x4096xf16>
    %9 = arith.addf %8, %cst : vector<1x1x4096xf16>
    %10 = vector.multi_reduction <add>, %9, %cst_0 [2] : vector<1x1x4096xf16> to vector<1x1xf16>
    %11 = vector.transfer_write %10, %3[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, tensor<1x1xf16>
    flow.dispatch.tensor.store %11, %2, offsets = [0, %workgroup_id_x], sizes = [1, 1], strides = [1, 1] : tensor<1x1xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
    return
  }
}

// -----// IR Dump After EmptyTensorToAllocTensor (empty-tensor-to-alloc-tensor) //----- //
module {
  func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
    %cst = arith.constant dense<0.000000e+00> : vector<1x1x4096xf16>
    %cst_0 = arith.constant dense<0.000000e+00> : vector<1x1xf16>
    %c0 = arith.constant 0 : index
    %cst_1 = arith.constant 0.000000e+00 : f16
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %3 = flow.dispatch.tensor.load %2, offsets = [0, %workgroup_id_x], sizes = [1, 1], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>> -> tensor<1x1xf16>
    %4 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<1x4096xf16>> -> tensor<1x4096xf16>
    %5 = flow.dispatch.tensor.load %1, offsets = [%workgroup_id_x, 0], sizes = [1, 4096], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<32000x4096xf16>> -> tensor<1x4096xf16>
    %6 = vector.transfer_read %4[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (d0, 0, d1)>} : tensor<1x4096xf16>, vector<1x1x4096xf16>
    %7 = vector.transfer_read %5[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (0, d0, d1)>} : tensor<1x4096xf16>, vector<1x1x4096xf16>
    %8 = arith.mulf %6, %7 : vector<1x1x4096xf16>
    %9 = arith.addf %8, %cst : vector<1x1x4096xf16>
    %10 = vector.multi_reduction <add>, %9, %cst_0 [2] : vector<1x1x4096xf16> to vector<1x1xf16>
    %11 = vector.transfer_write %10, %3[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, tensor<1x1xf16>
    flow.dispatch.tensor.store %11, %2, offsets = [0, %workgroup_id_x], sizes = [1, 1], strides = [1, 1] : tensor<1x1xf16> -> !flow.dispatch.tensor<writeonly:tensor<1x32000xf16>>
    return
  }
}

// -----// IR Dump After IREEComprehensiveBufferize (iree-codegen-iree-comprehensive-bufferize) //----- //
module {
  func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
    %cst = arith.constant dense<0.000000e+00> : vector<1x1x4096xf16>
    %cst_0 = arith.constant dense<0.000000e+00> : vector<1x1xf16>
    %c0 = arith.constant 0 : index
    %cst_1 = arith.constant 0.000000e+00 : f16
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %0, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %1, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %2, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %subview = memref.subview %2[0, %workgroup_id_x] [1, 1] [1, 1] : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>> to memref<1x1xf16, strided<[32000, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_2 = memref.subview %1[%workgroup_id_x, 0] [1, 4096] [1, 1] : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>> to memref<1x4096xf16, strided<[4096, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %3 = vector.transfer_read %0[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (d0, 0, d1)>} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<1x1x4096xf16>
    %4 = vector.transfer_read %subview_2[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (0, d0, d1)>} : memref<1x4096xf16, strided<[4096, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<1x1x4096xf16>
    %5 = arith.mulf %3, %4 : vector<1x1x4096xf16>
    %6 = arith.addf %5, %cst : vector<1x1x4096xf16>
    %7 = vector.multi_reduction <add>, %6, %cst_0 [2] : vector<1x1x4096xf16> to vector<1x1xf16>
    vector.transfer_write %7, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[32000, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_3 = memref.subview %2[0, %workgroup_id_x] [1, 1] [1, 1] : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>> to memref<1x1xf16, strided<[32000, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    memref.copy %subview, %subview_3 : memref<1x1xf16, strided<[32000, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<1x1xf16, strided<[32000, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    return
  }
}

// -----// IR Dump After ResolveShapedTypeResultDims (resolve-shaped-type-result-dims) //----- //
module {
  func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
    %cst = arith.constant dense<0.000000e+00> : vector<1x1x4096xf16>
    %cst_0 = arith.constant dense<0.000000e+00> : vector<1x1xf16>
    %c0 = arith.constant 0 : index
    %cst_1 = arith.constant 0.000000e+00 : f16
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %0, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %1, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %2, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %subview = memref.subview %2[0, %workgroup_id_x] [1, 1] [1, 1] : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>> to memref<1x1xf16, strided<[32000, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_2 = memref.subview %1[%workgroup_id_x, 0] [1, 4096] [1, 1] : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>> to memref<1x4096xf16, strided<[4096, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %3 = vector.transfer_read %0[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (d0, 0, d1)>} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<1x1x4096xf16>
    %4 = vector.transfer_read %subview_2[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (0, d0, d1)>} : memref<1x4096xf16, strided<[4096, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<1x1x4096xf16>
    %5 = arith.mulf %3, %4 : vector<1x1x4096xf16>
    %6 = arith.addf %5, %cst : vector<1x1x4096xf16>
    %7 = vector.multi_reduction <add>, %6, %cst_0 [2] : vector<1x1x4096xf16> to vector<1x1xf16>
    vector.transfer_write %7, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[32000, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_3 = memref.subview %2[0, %workgroup_id_x] [1, 1] [1, 1] : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>> to memref<1x1xf16, strided<[32000, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    memref.copy %subview, %subview_3 : memref<1x1xf16, strided<[32000, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<1x1xf16, strided<[32000, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %cst = arith.constant dense<0.000000e+00> : vector<1x1x4096xf16>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<1x1xf16>
  %c0 = arith.constant 0 : index
  %cst_1 = arith.constant 0.000000e+00 : f16
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %subview = memref.subview %2[0, %workgroup_id_x] [1, 1] [1, 1] : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>> to memref<1x1xf16, strided<[32000, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_2 = memref.subview %1[%workgroup_id_x, 0] [1, 4096] [1, 1] : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>> to memref<1x4096xf16, strided<[4096, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = vector.transfer_read %0[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (d0, 0, d1)>} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<1x1x4096xf16>
  %4 = vector.transfer_read %subview_2[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (0, d0, d1)>} : memref<1x4096xf16, strided<[4096, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<1x1x4096xf16>
  %5 = arith.mulf %3, %4 : vector<1x1x4096xf16>
  %6 = arith.addf %5, %cst : vector<1x1x4096xf16>
  %7 = vector.multi_reduction <add>, %6, %cst_0 [2] : vector<1x1x4096xf16> to vector<1x1xf16>
  vector.transfer_write %7, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[32000, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_3 = memref.subview %2[0, %workgroup_id_x] [1, 1] [1, 1] : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>> to memref<1x1xf16, strided<[32000, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  memref.copy %subview, %subview_3 : memref<1x1xf16, strided<[32000, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<1x1xf16, strided<[32000, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %cst = arith.constant dense<0.000000e+00> : vector<1x1x4096xf16>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<1x1xf16>
  %c0 = arith.constant 0 : index
  %cst_1 = arith.constant 0.000000e+00 : f16
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %subview = memref.subview %2[0, %workgroup_id_x] [1, 1] [1, 1] : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>> to memref<1x1xf16, strided<[32000, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_2 = memref.subview %1[%workgroup_id_x, 0] [1, 4096] [1, 1] : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>> to memref<1x4096xf16, strided<[4096, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = vector.transfer_read %0[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (d0, 0, d1)>} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<1x1x4096xf16>
  %4 = vector.transfer_read %subview_2[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (0, d0, d1)>} : memref<1x4096xf16, strided<[4096, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<1x1x4096xf16>
  %5 = arith.mulf %3, %4 : vector<1x1x4096xf16>
  %6 = arith.addf %5, %cst : vector<1x1x4096xf16>
  %7 = vector.multi_reduction <add>, %6, %cst_0 [2] : vector<1x1x4096xf16> to vector<1x1xf16>
  vector.transfer_write %7, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[32000, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  memref.copy %subview, %subview : memref<1x1xf16, strided<[32000, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<1x1xf16, strided<[32000, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %cst = arith.constant dense<0.000000e+00> : vector<1x1x4096xf16>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<1x1xf16>
  %c0 = arith.constant 0 : index
  %cst_1 = arith.constant 0.000000e+00 : f16
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %subview = memref.subview %2[0, %workgroup_id_x] [1, 1] [1, 1] : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>> to memref<1x1xf16, strided<[32000, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_2 = memref.subview %1[%workgroup_id_x, 0] [1, 4096] [1, 1] : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>> to memref<1x4096xf16, strided<[4096, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = vector.transfer_read %0[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (d0, 0, d1)>} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<1x1x4096xf16>
  %4 = vector.transfer_read %subview_2[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (0, d0, d1)>} : memref<1x4096xf16, strided<[4096, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<1x1x4096xf16>
  %5 = arith.mulf %3, %4 : vector<1x1x4096xf16>
  %6 = arith.addf %5, %cst : vector<1x1x4096xf16>
  %7 = vector.multi_reduction <add>, %6, %cst_0 [2] : vector<1x1x4096xf16> to vector<1x1xf16>
  vector.transfer_write %7, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[32000, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  return
}

// -----// IR Dump After CleanupBufferAllocView (iree-codegen-cleanup-buffer-alloc-view) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %cst = arith.constant dense<0.000000e+00> : vector<1x1x4096xf16>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<1x1xf16>
  %c0 = arith.constant 0 : index
  %cst_1 = arith.constant 0.000000e+00 : f16
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %subview = memref.subview %2[0, %workgroup_id_x] [1, 1] [1, 1] : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>> to memref<1x1xf16, strided<[32000, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_2 = memref.subview %1[%workgroup_id_x, 0] [1, 4096] [1, 1] : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>> to memref<1x4096xf16, strided<[4096, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = vector.transfer_read %0[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (d0, 0, d1)>} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<1x1x4096xf16>
  %4 = vector.transfer_read %subview_2[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (0, d0, d1)>} : memref<1x4096xf16, strided<[4096, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<1x1x4096xf16>
  %5 = arith.mulf %3, %4 : vector<1x1x4096xf16>
  %6 = arith.addf %5, %cst : vector<1x1x4096xf16>
  %7 = vector.multi_reduction <add>, %6, %cst_0 [2] : vector<1x1x4096xf16> to vector<1x1xf16>
  vector.transfer_write %7, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[32000, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  return
}

// -----// IR Dump After SPIRVDistribute (iree-spirv-distribute) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %cst = arith.constant dense<0.000000e+00> : vector<1x1x4096xf16>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<1x1xf16>
  %c0 = arith.constant 0 : index
  %cst_1 = arith.constant 0.000000e+00 : f16
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %subview = memref.subview %2[0, %workgroup_id_x] [1, 1] [1, 1] : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>> to memref<1x1xf16, strided<[32000, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_2 = memref.subview %1[%workgroup_id_x, 0] [1, 4096] [1, 1] : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>> to memref<1x4096xf16, strided<[4096, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = vector.transfer_read %0[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (d0, 0, d1)>} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<1x1x4096xf16>
  %4 = vector.transfer_read %subview_2[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (0, d0, d1)>} : memref<1x4096xf16, strided<[4096, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<1x1x4096xf16>
  %5 = arith.mulf %3, %4 : vector<1x1x4096xf16>
  %6 = arith.addf %5, %cst : vector<1x1x4096xf16>
  %7 = vector.multi_reduction <add>, %6, %cst_0 [2] : vector<1x1x4096xf16> to vector<1x1xf16>
  vector.transfer_write %7, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[32000, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  return
}

// -----// IR Dump After ResolveShapedTypeResultDims (resolve-shaped-type-result-dims) //----- //
module {
  func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
    %cst = arith.constant dense<0.000000e+00> : vector<1x1x4096xf16>
    %cst_0 = arith.constant dense<0.000000e+00> : vector<1x1xf16>
    %c0 = arith.constant 0 : index
    %cst_1 = arith.constant 0.000000e+00 : f16
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %0, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %1, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %2, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %subview = memref.subview %2[0, %workgroup_id_x] [1, 1] [1, 1] : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>> to memref<1x1xf16, strided<[32000, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_2 = memref.subview %1[%workgroup_id_x, 0] [1, 4096] [1, 1] : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>> to memref<1x4096xf16, strided<[4096, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %3 = vector.transfer_read %0[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (d0, 0, d1)>} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<1x1x4096xf16>
    %4 = vector.transfer_read %subview_2[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (0, d0, d1)>} : memref<1x4096xf16, strided<[4096, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<1x1x4096xf16>
    %5 = arith.mulf %3, %4 : vector<1x1x4096xf16>
    %6 = arith.addf %5, %cst : vector<1x1x4096xf16>
    %7 = vector.multi_reduction <add>, %6, %cst_0 [2] : vector<1x1x4096xf16> to vector<1x1xf16>
    vector.transfer_write %7, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[32000, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %cst = arith.constant dense<0.000000e+00> : vector<1x1x4096xf16>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<1x1xf16>
  %c0 = arith.constant 0 : index
  %cst_1 = arith.constant 0.000000e+00 : f16
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %subview = memref.subview %2[0, %workgroup_id_x] [1, 1] [1, 1] : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>> to memref<1x1xf16, strided<[32000, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_2 = memref.subview %1[%workgroup_id_x, 0] [1, 4096] [1, 1] : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>> to memref<1x4096xf16, strided<[4096, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = vector.transfer_read %0[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (d0, 0, d1)>} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<1x1x4096xf16>
  %4 = vector.transfer_read %subview_2[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (0, d0, d1)>} : memref<1x4096xf16, strided<[4096, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<1x1x4096xf16>
  %5 = arith.mulf %3, %4 : vector<1x1x4096xf16>
  %6 = arith.addf %5, %cst : vector<1x1x4096xf16>
  %7 = vector.multi_reduction <add>, %6, %cst_0 [2] : vector<1x1x4096xf16> to vector<1x1xf16>
  vector.transfer_write %7, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[32000, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %cst = arith.constant dense<0.000000e+00> : vector<1x1x4096xf16>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<1x1xf16>
  %c0 = arith.constant 0 : index
  %cst_1 = arith.constant 0.000000e+00 : f16
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %subview = memref.subview %2[0, %workgroup_id_x] [1, 1] [1, 1] : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>> to memref<1x1xf16, strided<[32000, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_2 = memref.subview %1[%workgroup_id_x, 0] [1, 4096] [1, 1] : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>> to memref<1x4096xf16, strided<[4096, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = vector.transfer_read %0[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (d0, 0, d1)>} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<1x1x4096xf16>
  %4 = vector.transfer_read %subview_2[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (0, d0, d1)>} : memref<1x4096xf16, strided<[4096, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<1x1x4096xf16>
  %5 = arith.mulf %3, %4 : vector<1x1x4096xf16>
  %6 = arith.addf %5, %cst : vector<1x1x4096xf16>
  %7 = vector.multi_reduction <add>, %6, %cst_0 [2] : vector<1x1x4096xf16> to vector<1x1xf16>
  vector.transfer_write %7, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[32000, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  return
}

// -----// IR Dump After CleanupBufferAllocView (iree-codegen-cleanup-buffer-alloc-view) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %cst = arith.constant dense<0.000000e+00> : vector<1x1x4096xf16>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<1x1xf16>
  %c0 = arith.constant 0 : index
  %cst_1 = arith.constant 0.000000e+00 : f16
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %subview = memref.subview %2[0, %workgroup_id_x] [1, 1] [1, 1] : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>> to memref<1x1xf16, strided<[32000, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_2 = memref.subview %1[%workgroup_id_x, 0] [1, 4096] [1, 1] : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>> to memref<1x4096xf16, strided<[4096, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = vector.transfer_read %0[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (d0, 0, d1)>} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<1x1x4096xf16>
  %4 = vector.transfer_read %subview_2[%c0, %c0], %cst_1 {in_bounds = [true, true, true], permutation_map = affine_map<(d0, d1) -> (0, d0, d1)>} : memref<1x4096xf16, strided<[4096, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<1x1x4096xf16>
  %5 = arith.mulf %3, %4 : vector<1x1x4096xf16>
  %6 = arith.addf %5, %cst : vector<1x1x4096xf16>
  %7 = vector.multi_reduction <add>, %6, %cst_0 [2] : vector<1x1x4096xf16> to vector<1x1xf16>
  vector.transfer_write %7, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[32000, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  return
}

// -----// IR Dump After OptimizeVectorTransfer (iree-codegen-optimize-vector-transfer) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %cst = arith.constant dense<0.000000e+00> : vector<4096xf16>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<1x1xf16>
  %c0 = arith.constant 0 : index
  %cst_1 = arith.constant 0.000000e+00 : f16
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %subview = memref.subview %2[0, %workgroup_id_x] [1, 1] [1, 1] : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>> to memref<1x1xf16, strided<[32000, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_2 = memref.subview %1[%workgroup_id_x, 0] [1, 4096] [1, 1] : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>> to memref<1x4096xf16, strided<[4096, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = vector.transfer_read %0[%c0, %c0], %cst_1 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<4096xf16>
  %4 = vector.transfer_read %subview_2[%c0, %c0], %cst_1 {in_bounds = [true]} : memref<1x4096xf16, strided<[4096, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4096xf16>
  %5 = arith.mulf %3, %4 : vector<4096xf16>
  %6 = arith.addf %5, %cst : vector<4096xf16>
  %7 = vector.broadcast %6 : vector<4096xf16> to vector<1x1x4096xf16>
  %8 = vector.multi_reduction <add>, %7, %cst_0 [2] : vector<1x1x4096xf16> to vector<1x1xf16>
  %9 = vector.extract %8[0] : vector<1xf16> from vector<1x1xf16>
  vector.transfer_write %9, %subview[%c0, %c0] {in_bounds = [true]} : vector<1xf16>, memref<1x1xf16, strided<[32000, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  return
}

// -----// IR Dump After FoldMemRefAliasOps (fold-memref-alias-ops) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %c0 = arith.constant 0 : index
  %cst = arith.constant dense<0.000000e+00> : vector<4096xf16>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<1x1xf16>
  %cst_1 = arith.constant 0.000000e+00 : f16
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %3 = vector.transfer_read %0[%c0, %c0], %cst_1 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<4096xf16>
  %4 = vector.transfer_read %1[%workgroup_id_x, %c0], %cst_1 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<4096xf16>
  %5 = arith.mulf %3, %4 : vector<4096xf16>
  %6 = arith.addf %5, %cst : vector<4096xf16>
  %7 = vector.broadcast %6 : vector<4096xf16> to vector<1x1x4096xf16>
  %8 = vector.multi_reduction <add>, %7, %cst_0 [2] : vector<1x1x4096xf16> to vector<1x1xf16>
  %9 = vector.extract %8[0] : vector<1xf16> from vector<1x1xf16>
  vector.transfer_write %9, %2[%c0, %workgroup_id_x] {in_bounds = [true]} : vector<1xf16>, memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  return
}

// -----// IR Dump After LoopInvariantCodeMotion (loop-invariant-code-motion) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %c0 = arith.constant 0 : index
  %cst = arith.constant dense<0.000000e+00> : vector<4096xf16>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<1x1xf16>
  %cst_1 = arith.constant 0.000000e+00 : f16
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %3 = vector.transfer_read %0[%c0, %c0], %cst_1 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<4096xf16>
  %4 = vector.transfer_read %1[%workgroup_id_x, %c0], %cst_1 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<4096xf16>
  %5 = arith.mulf %3, %4 : vector<4096xf16>
  %6 = arith.addf %5, %cst : vector<4096xf16>
  %7 = vector.broadcast %6 : vector<4096xf16> to vector<1x1x4096xf16>
  %8 = vector.multi_reduction <add>, %7, %cst_0 [2] : vector<1x1x4096xf16> to vector<1x1xf16>
  %9 = vector.extract %8[0] : vector<1xf16> from vector<1x1xf16>
  vector.transfer_write %9, %2[%c0, %workgroup_id_x] {in_bounds = [true]} : vector<1xf16>, memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %c0 = arith.constant 0 : index
  %cst = arith.constant dense<0.000000e+00> : vector<4096xf16>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<1x1xf16>
  %cst_1 = arith.constant 0.000000e+00 : f16
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %3 = vector.transfer_read %0[%c0, %c0], %cst_1 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<4096xf16>
  %4 = vector.transfer_read %1[%workgroup_id_x, %c0], %cst_1 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<4096xf16>
  %5 = arith.mulf %3, %4 : vector<4096xf16>
  %6 = arith.addf %5, %cst : vector<4096xf16>
  %7 = vector.broadcast %6 : vector<4096xf16> to vector<1x1x4096xf16>
  %8 = vector.multi_reduction <add>, %7, %cst_0 [2] : vector<1x1x4096xf16> to vector<1x1xf16>
  %9 = vector.extract %8[0] : vector<1xf16> from vector<1x1xf16>
  vector.transfer_write %9, %2[%c0, %workgroup_id_x] {in_bounds = [true]} : vector<1xf16>, memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %c0 = arith.constant 0 : index
  %cst = arith.constant dense<0.000000e+00> : vector<4096xf16>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<1x1xf16>
  %cst_1 = arith.constant 0.000000e+00 : f16
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %3 = vector.transfer_read %0[%c0, %c0], %cst_1 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<4096xf16>
  %4 = vector.transfer_read %1[%workgroup_id_x, %c0], %cst_1 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<4096xf16>
  %5 = arith.mulf %3, %4 : vector<4096xf16>
  %6 = arith.addf %5, %cst : vector<4096xf16>
  %7 = vector.broadcast %6 : vector<4096xf16> to vector<1x1x4096xf16>
  %8 = vector.multi_reduction <add>, %7, %cst_0 [2] : vector<1x1x4096xf16> to vector<1x1xf16>
  %9 = vector.extract %8[0] : vector<1xf16> from vector<1x1xf16>
  vector.transfer_write %9, %2[%c0, %workgroup_id_x] {in_bounds = [true]} : vector<1xf16>, memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  return
}

// -----// IR Dump After ForOpCanonicalization (iree-codegen-canonicalize-scf-for) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %c0 = arith.constant 0 : index
  %cst = arith.constant dense<0.000000e+00> : vector<4096xf16>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<1x1xf16>
  %cst_1 = arith.constant 0.000000e+00 : f16
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %3 = vector.transfer_read %0[%c0, %c0], %cst_1 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<4096xf16>
  %4 = vector.transfer_read %1[%workgroup_id_x, %c0], %cst_1 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<4096xf16>
  %5 = arith.mulf %3, %4 : vector<4096xf16>
  %6 = arith.addf %5, %cst : vector<4096xf16>
  %7 = vector.broadcast %6 : vector<4096xf16> to vector<1x1x4096xf16>
  %8 = vector.multi_reduction <add>, %7, %cst_0 [2] : vector<1x1x4096xf16> to vector<1x1xf16>
  %9 = vector.extract %8[0] : vector<1xf16> from vector<1x1xf16>
  vector.transfer_write %9, %2[%c0, %workgroup_id_x] {in_bounds = [true]} : vector<1xf16>, memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %c0 = arith.constant 0 : index
  %cst = arith.constant dense<0.000000e+00> : vector<4096xf16>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<1x1xf16>
  %cst_1 = arith.constant 0.000000e+00 : f16
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %3 = vector.transfer_read %0[%c0, %c0], %cst_1 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<4096xf16>
  %4 = vector.transfer_read %1[%workgroup_id_x, %c0], %cst_1 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<4096xf16>
  %5 = arith.mulf %3, %4 : vector<4096xf16>
  %6 = arith.addf %5, %cst : vector<4096xf16>
  %7 = vector.broadcast %6 : vector<4096xf16> to vector<1x1x4096xf16>
  %8 = vector.multi_reduction <add>, %7, %cst_0 [2] : vector<1x1x4096xf16> to vector<1x1xf16>
  %9 = vector.extract %8[0] : vector<1xf16> from vector<1x1xf16>
  vector.transfer_write %9, %2[%c0, %workgroup_id_x] {in_bounds = [true]} : vector<1xf16>, memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  return
}

//--- after step #0: before vector reduction to gpu ---//
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %c0 = arith.constant 0 : index
  %cst = arith.constant dense<0.000000e+00> : vector<4096xf16>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<1x1xf16>
  %cst_1 = arith.constant 0.000000e+00 : f16
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %3 = vector.transfer_read %0[%c0, %c0], %cst_1 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<4096xf16>
  %4 = vector.transfer_read %1[%workgroup_id_x, %c0], %cst_1 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<4096xf16>
  %5 = arith.mulf %3, %4 : vector<4096xf16>
  %6 = arith.addf %5, %cst : vector<4096xf16>
  %7 = vector.broadcast %6 : vector<4096xf16> to vector<1x1x4096xf16>
  %8 = vector.multi_reduction <add>, %7, %cst_0 [2] : vector<1x1x4096xf16> to vector<1x1xf16>
  %9 = vector.extract %8[0] : vector<1xf16> from vector<1x1xf16>
  vector.transfer_write %9, %2[%c0, %workgroup_id_x] {in_bounds = [true]} : vector<1xf16>, memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  return
}

Get native vector sharpe for: vector.transfer_write %9, %2[%c0, %workgroup_id_x] {in_bounds = [true]} : vector<1xf16>, memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
Get native vector sharpe for: %8 = vector.multi_reduction <add>, %7, %cst_0 [2] : vector<1x1x4096xf16> to vector<1x1xf16>
Transfer op with elem type: f16, preferred width: 512
Get native vector sharpe for: %24 = vector.multi_reduction <add>, %23, %22 [2] : vector<1x1x512xf16> to vector<1x1xf16>
Get native vector sharpe for: %25 = vector.multi_reduction <add>, %24, %22 [2] : vector<1x1x512xf16> to vector<1x1xf16>
Get native vector sharpe for: %22 = vector.multi_reduction <add>, %21, %20 [2] : vector<1x1x512xf16> to vector<1x1xf16>
Get native vector sharpe for: %23 = vector.multi_reduction <add>, %22, %20 [2] : vector<1x1x512xf16> to vector<1x1xf16>
Get native vector sharpe for: %20 = vector.multi_reduction <add>, %19, %18 [2] : vector<1x1x512xf16> to vector<1x1xf16>
Get native vector sharpe for: %21 = vector.multi_reduction <add>, %20, %18 [2] : vector<1x1x512xf16> to vector<1x1xf16>
Get native vector sharpe for: %18 = vector.multi_reduction <add>, %17, %16 [2] : vector<1x1x512xf16> to vector<1x1xf16>
Get native vector sharpe for: %19 = vector.multi_reduction <add>, %18, %16 [2] : vector<1x1x512xf16> to vector<1x1xf16>
Get native vector sharpe for: %16 = vector.multi_reduction <add>, %15, %14 [2] : vector<1x1x512xf16> to vector<1x1xf16>
Get native vector sharpe for: %17 = vector.multi_reduction <add>, %16, %14 [2] : vector<1x1x512xf16> to vector<1x1xf16>
Get native vector sharpe for: %14 = vector.multi_reduction <add>, %13, %12 [2] : vector<1x1x512xf16> to vector<1x1xf16>
Get native vector sharpe for: %15 = vector.multi_reduction <add>, %14, %12 [2] : vector<1x1x512xf16> to vector<1x1xf16>
Get native vector sharpe for: %12 = vector.multi_reduction <add>, %11, %10 [2] : vector<1x1x512xf16> to vector<1x1xf16>
Get native vector sharpe for: %13 = vector.multi_reduction <add>, %12, %10 [2] : vector<1x1x512xf16> to vector<1x1xf16>
Get native vector sharpe for: %10 = vector.multi_reduction <add>, %8, %9 [2] : vector<1x1x512xf16> to vector<1x1xf16>
Get native vector sharpe for: %9 = vector.multi_reduction <add>, %8, %cst_0 [2] : vector<1x1x512xf16> to vector<1x1xf16>
Get native vector sharpe for: %10 = vector.multi_reduction <add>, %9, %cst_0 [2] : vector<1x1x512xf16> to vector<1x1xf16>
Get native vector sharpe for: %6 = arith.addf %5, %cst : vector<4096xf16>
Transfer op with elem type: f16, preferred width: 512
Get native vector sharpe for: %36 = arith.addf %34, %35 : vector<512xf16>
Get native vector sharpe for: %35 = arith.addf %34, %cst_3 : vector<512xf16>
Get native vector sharpe for: %35 = arith.addf %34, %cst : vector<512xf16>
Get native vector sharpe for: %32 = arith.addf %30, %31 : vector<512xf16>
Get native vector sharpe for: %31 = arith.addf %30, %cst_4 : vector<512xf16>
Get native vector sharpe for: %31 = arith.addf %30, %cst : vector<512xf16>
Get native vector sharpe for: %28 = arith.addf %26, %27 : vector<512xf16>
Get native vector sharpe for: %27 = arith.addf %26, %cst_4 : vector<512xf16>
Get native vector sharpe for: %27 = arith.addf %26, %cst : vector<512xf16>
Get native vector sharpe for: %24 = arith.addf %22, %23 : vector<512xf16>
Get native vector sharpe for: %23 = arith.addf %22, %cst_4 : vector<512xf16>
Get native vector sharpe for: %23 = arith.addf %22, %cst : vector<512xf16>
Get native vector sharpe for: %20 = arith.addf %18, %19 : vector<512xf16>
Get native vector sharpe for: %19 = arith.addf %18, %cst_4 : vector<512xf16>
Get native vector sharpe for: %19 = arith.addf %18, %cst : vector<512xf16>
Get native vector sharpe for: %16 = arith.addf %14, %15 : vector<512xf16>
Get native vector sharpe for: %15 = arith.addf %14, %cst_4 : vector<512xf16>
Get native vector sharpe for: %15 = arith.addf %14, %cst : vector<512xf16>
Get native vector sharpe for: %12 = arith.addf %10, %11 : vector<512xf16>
Get native vector sharpe for: %11 = arith.addf %10, %cst_4 : vector<512xf16>
Get native vector sharpe for: %11 = arith.addf %10, %cst : vector<512xf16>
Get native vector sharpe for: %8 = arith.addf %6, %7 : vector<512xf16>
Get native vector sharpe for: %7 = arith.addf %6, %cst_4 : vector<512xf16>
Get native vector sharpe for: %7 = arith.addf %6, %cst : vector<512xf16>
Get native vector sharpe for: %5 = arith.mulf %3, %4 : vector<4096xf16>
Transfer op with elem type: f16, preferred width: 512
Get native vector sharpe for: %37 = arith.addf %7, %cst : vector<512xf16>
Get native vector sharpe for: %38 = arith.addf %11, %cst : vector<512xf16>
Get native vector sharpe for: %39 = arith.addf %15, %cst : vector<512xf16>
Get native vector sharpe for: %40 = arith.addf %19, %cst : vector<512xf16>
Get native vector sharpe for: %41 = arith.addf %23, %cst : vector<512xf16>
Get native vector sharpe for: %42 = arith.addf %27, %cst : vector<512xf16>
Get native vector sharpe for: %43 = arith.addf %31, %cst : vector<512xf16>
Get native vector sharpe for: %44 = arith.addf %35, %cst : vector<512xf16>
Get native vector sharpe for: %35 = arith.mulf %33, %34 : vector<512xf16>
Get native vector sharpe for: %31 = arith.mulf %29, %30 : vector<512xf16>
Get native vector sharpe for: %27 = arith.mulf %25, %26 : vector<512xf16>
Get native vector sharpe for: %23 = arith.mulf %21, %22 : vector<512xf16>
Get native vector sharpe for: %19 = arith.mulf %17, %18 : vector<512xf16>
Get native vector sharpe for: %15 = arith.mulf %13, %14 : vector<512xf16>
Get native vector sharpe for: %11 = arith.mulf %9, %10 : vector<512xf16>
Get native vector sharpe for: %7 = arith.mulf %5, %6 : vector<512xf16>
Get native vector sharpe for: %4 = vector.transfer_read %1[%workgroup_id_x, %c0], %cst_2 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<4096xf16>
Transfer op with elem type: f16, preferred width: 512
Get native vector sharpe for: %29 = arith.mulf %28, %5 : vector<512xf16>
Get native vector sharpe for: %31 = arith.mulf %30, %8 : vector<512xf16>
Get native vector sharpe for: %33 = arith.mulf %32, %11 : vector<512xf16>
Get native vector sharpe for: %35 = arith.mulf %34, %14 : vector<512xf16>
Get native vector sharpe for: %37 = arith.mulf %36, %17 : vector<512xf16>
Get native vector sharpe for: %39 = arith.mulf %38, %20 : vector<512xf16>
Get native vector sharpe for: %41 = arith.mulf %40, %23 : vector<512xf16>
Get native vector sharpe for: %43 = arith.mulf %42, %26 : vector<512xf16>
Get native vector sharpe for: %26 = vector.transfer_read %1[%workgroup_id_x, %25], %cst_2 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %25 = vector.transfer_read %1[%workgroup_id_x, %c3584], %cst_2 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %23 = vector.transfer_read %1[%workgroup_id_x, %22], %cst_2 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %22 = vector.transfer_read %1[%workgroup_id_x, %c3072], %cst_2 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %20 = vector.transfer_read %1[%workgroup_id_x, %19], %cst_2 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %19 = vector.transfer_read %1[%workgroup_id_x, %c2560], %cst_2 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %17 = vector.transfer_read %1[%workgroup_id_x, %16], %cst_2 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %16 = vector.transfer_read %1[%workgroup_id_x, %c2048], %cst_2 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %14 = vector.transfer_read %1[%workgroup_id_x, %13], %cst_2 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %13 = vector.transfer_read %1[%workgroup_id_x, %c1536], %cst_2 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %11 = vector.transfer_read %1[%workgroup_id_x, %10], %cst_2 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %10 = vector.transfer_read %1[%workgroup_id_x, %c1024], %cst_2 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %8 = vector.transfer_read %1[%workgroup_id_x, %7], %cst_2 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %7 = vector.transfer_read %1[%workgroup_id_x, %c512], %cst_2 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %5 = vector.transfer_read %1[%workgroup_id_x, %4], %cst_2 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %4 = vector.transfer_read %1[%workgroup_id_x, %c0], %cst_2 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %3 = vector.transfer_read %0[%c0, %c0], %cst_2 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<4096xf16>
Transfer op with elem type: f16, preferred width: 512
Get native vector sharpe for: %35 = arith.mulf %4, %27 : vector<512xf16>
Get native vector sharpe for: %36 = arith.mulf %7, %28 : vector<512xf16>
Get native vector sharpe for: %37 = arith.mulf %10, %29 : vector<512xf16>
Get native vector sharpe for: %38 = arith.mulf %13, %30 : vector<512xf16>
Get native vector sharpe for: %39 = arith.mulf %16, %31 : vector<512xf16>
Get native vector sharpe for: %40 = arith.mulf %19, %32 : vector<512xf16>
Get native vector sharpe for: %41 = arith.mulf %22, %33 : vector<512xf16>
Get native vector sharpe for: %42 = arith.mulf %25, %34 : vector<512xf16>
Get native vector sharpe for: %25 = vector.transfer_read %0[%c0, %24], %cst_2 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %24 = vector.transfer_read %0[%c0, %c3584], %cst_2 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %22 = vector.transfer_read %0[%c0, %21], %cst_2 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %21 = vector.transfer_read %0[%c0, %c3072], %cst_2 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %19 = vector.transfer_read %0[%c0, %18], %cst_2 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %18 = vector.transfer_read %0[%c0, %c2560], %cst_2 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %16 = vector.transfer_read %0[%c0, %15], %cst_2 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %15 = vector.transfer_read %0[%c0, %c2048], %cst_2 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %13 = vector.transfer_read %0[%c0, %12], %cst_2 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %12 = vector.transfer_read %0[%c0, %c1536], %cst_2 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %10 = vector.transfer_read %0[%c0, %9], %cst_2 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %9 = vector.transfer_read %0[%c0, %c1024], %cst_2 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %7 = vector.transfer_read %0[%c0, %6], %cst_2 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %6 = vector.transfer_read %0[%c0, %c512], %cst_2 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %4 = vector.transfer_read %0[%c0, %3], %cst_2 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %3 = vector.transfer_read %0[%c0, %c0], %cst_2 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: vector.transfer_write %51, %2[%c0, %workgroup_id_x] {in_bounds = [true]} : vector<1xf16>, memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
Get native vector sharpe for: %50 = vector.multi_reduction <add>, %49, %48 [2] : vector<1x1x512xf16> to vector<1x1xf16>
Get native vector sharpe for: %48 = vector.multi_reduction <add>, %47, %46 [2] : vector<1x1x512xf16> to vector<1x1xf16>
Get native vector sharpe for: %46 = vector.multi_reduction <add>, %45, %44 [2] : vector<1x1x512xf16> to vector<1x1xf16>
Get native vector sharpe for: %44 = vector.multi_reduction <add>, %43, %42 [2] : vector<1x1x512xf16> to vector<1x1xf16>
Get native vector sharpe for: %42 = vector.multi_reduction <add>, %41, %40 [2] : vector<1x1x512xf16> to vector<1x1xf16>
Get native vector sharpe for: %40 = vector.multi_reduction <add>, %39, %38 [2] : vector<1x1x512xf16> to vector<1x1xf16>
Get native vector sharpe for: %38 = vector.multi_reduction <add>, %37, %36 [2] : vector<1x1x512xf16> to vector<1x1xf16>
Get native vector sharpe for: %36 = vector.multi_reduction <add>, %35, %cst_0 [2] : vector<1x1x512xf16> to vector<1x1xf16>
Get native vector sharpe for: %34 = arith.addf %26, %cst : vector<512xf16>
Get native vector sharpe for: %33 = arith.addf %25, %cst : vector<512xf16>
Get native vector sharpe for: %32 = arith.addf %24, %cst : vector<512xf16>
Get native vector sharpe for: %31 = arith.addf %23, %cst : vector<512xf16>
Get native vector sharpe for: %30 = arith.addf %22, %cst : vector<512xf16>
Get native vector sharpe for: %29 = arith.addf %21, %cst : vector<512xf16>
Get native vector sharpe for: %28 = arith.addf %20, %cst : vector<512xf16>
Get native vector sharpe for: %27 = arith.addf %19, %cst : vector<512xf16>
Get native vector sharpe for: %26 = arith.mulf %10, %18 : vector<512xf16>
Get native vector sharpe for: %25 = arith.mulf %9, %17 : vector<512xf16>
Get native vector sharpe for: %24 = arith.mulf %8, %16 : vector<512xf16>
Get native vector sharpe for: %23 = arith.mulf %7, %15 : vector<512xf16>
Get native vector sharpe for: %22 = arith.mulf %6, %14 : vector<512xf16>
Get native vector sharpe for: %21 = arith.mulf %5, %13 : vector<512xf16>
Get native vector sharpe for: %20 = arith.mulf %4, %12 : vector<512xf16>
Get native vector sharpe for: %19 = arith.mulf %3, %11 : vector<512xf16>
Get native vector sharpe for: %18 = vector.transfer_read %1[%workgroup_id_x, %c3584], %cst_1 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %17 = vector.transfer_read %1[%workgroup_id_x, %c3072], %cst_1 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %16 = vector.transfer_read %1[%workgroup_id_x, %c2560], %cst_1 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %15 = vector.transfer_read %1[%workgroup_id_x, %c2048], %cst_1 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %14 = vector.transfer_read %1[%workgroup_id_x, %c1536], %cst_1 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %13 = vector.transfer_read %1[%workgroup_id_x, %c1024], %cst_1 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %12 = vector.transfer_read %1[%workgroup_id_x, %c512], %cst_1 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %11 = vector.transfer_read %1[%workgroup_id_x, %c0], %cst_1 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %10 = vector.transfer_read %0[%c0, %c3584], %cst_1 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %9 = vector.transfer_read %0[%c0, %c3072], %cst_1 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %8 = vector.transfer_read %0[%c0, %c2560], %cst_1 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %7 = vector.transfer_read %0[%c0, %c2048], %cst_1 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %6 = vector.transfer_read %0[%c0, %c1536], %cst_1 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %5 = vector.transfer_read %0[%c0, %c1024], %cst_1 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %4 = vector.transfer_read %0[%c0, %c512], %cst_1 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
Get native vector sharpe for: %3 = vector.transfer_read %0[%c0, %c0], %cst_1 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
//--- after unrolling vector ops ---//
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1536 = arith.constant 1536 : index
  %c2048 = arith.constant 2048 : index
  %c2560 = arith.constant 2560 : index
  %c3072 = arith.constant 3072 : index
  %c3584 = arith.constant 3584 : index
  %cst = arith.constant dense<0.000000e+00> : vector<512xf16>
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant dense<0.000000e+00> : vector<1x1xf16>
  %cst_1 = arith.constant 0.000000e+00 : f16
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %3 = vector.transfer_read %0[%c0, %c0], %cst_1 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
  %4 = vector.transfer_read %0[%c0, %c512], %cst_1 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
  %5 = vector.transfer_read %0[%c0, %c1024], %cst_1 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
  %6 = vector.transfer_read %0[%c0, %c1536], %cst_1 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
  %7 = vector.transfer_read %0[%c0, %c2048], %cst_1 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
  %8 = vector.transfer_read %0[%c0, %c2560], %cst_1 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
  %9 = vector.transfer_read %0[%c0, %c3072], %cst_1 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
  %10 = vector.transfer_read %0[%c0, %c3584], %cst_1 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
  %11 = vector.transfer_read %1[%workgroup_id_x, %c0], %cst_1 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
  %12 = vector.transfer_read %1[%workgroup_id_x, %c512], %cst_1 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
  %13 = vector.transfer_read %1[%workgroup_id_x, %c1024], %cst_1 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
  %14 = vector.transfer_read %1[%workgroup_id_x, %c1536], %cst_1 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
  %15 = vector.transfer_read %1[%workgroup_id_x, %c2048], %cst_1 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
  %16 = vector.transfer_read %1[%workgroup_id_x, %c2560], %cst_1 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
  %17 = vector.transfer_read %1[%workgroup_id_x, %c3072], %cst_1 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
  %18 = vector.transfer_read %1[%workgroup_id_x, %c3584], %cst_1 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
  %19 = arith.mulf %3, %11 : vector<512xf16>
  %20 = arith.mulf %4, %12 : vector<512xf16>
  %21 = arith.mulf %5, %13 : vector<512xf16>
  %22 = arith.mulf %6, %14 : vector<512xf16>
  %23 = arith.mulf %7, %15 : vector<512xf16>
  %24 = arith.mulf %8, %16 : vector<512xf16>
  %25 = arith.mulf %9, %17 : vector<512xf16>
  %26 = arith.mulf %10, %18 : vector<512xf16>
  %27 = arith.addf %19, %cst : vector<512xf16>
  %28 = arith.addf %20, %cst : vector<512xf16>
  %29 = arith.addf %21, %cst : vector<512xf16>
  %30 = arith.addf %22, %cst : vector<512xf16>
  %31 = arith.addf %23, %cst : vector<512xf16>
  %32 = arith.addf %24, %cst : vector<512xf16>
  %33 = arith.addf %25, %cst : vector<512xf16>
  %34 = arith.addf %26, %cst : vector<512xf16>
  %35 = vector.broadcast %27 : vector<512xf16> to vector<1x1x512xf16>
  %36 = vector.multi_reduction <add>, %35, %cst_0 [2] : vector<1x1x512xf16> to vector<1x1xf16>
  %37 = vector.broadcast %28 : vector<512xf16> to vector<1x1x512xf16>
  %38 = vector.multi_reduction <add>, %37, %36 [2] : vector<1x1x512xf16> to vector<1x1xf16>
  %39 = vector.broadcast %29 : vector<512xf16> to vector<1x1x512xf16>
  %40 = vector.multi_reduction <add>, %39, %38 [2] : vector<1x1x512xf16> to vector<1x1xf16>
  %41 = vector.broadcast %30 : vector<512xf16> to vector<1x1x512xf16>
  %42 = vector.multi_reduction <add>, %41, %40 [2] : vector<1x1x512xf16> to vector<1x1xf16>
  %43 = vector.broadcast %31 : vector<512xf16> to vector<1x1x512xf16>
  %44 = vector.multi_reduction <add>, %43, %42 [2] : vector<1x1x512xf16> to vector<1x1xf16>
  %45 = vector.broadcast %32 : vector<512xf16> to vector<1x1x512xf16>
  %46 = vector.multi_reduction <add>, %45, %44 [2] : vector<1x1x512xf16> to vector<1x1xf16>
  %47 = vector.broadcast %33 : vector<512xf16> to vector<1x1x512xf16>
  %48 = vector.multi_reduction <add>, %47, %46 [2] : vector<1x1x512xf16> to vector<1x1xf16>
  %49 = vector.broadcast %34 : vector<512xf16> to vector<1x1x512xf16>
  %50 = vector.multi_reduction <add>, %49, %48 [2] : vector<1x1x512xf16> to vector<1x1xf16>
  %51 = vector.extract %50[0] : vector<1xf16> from vector<1x1xf16>
  vector.transfer_write %51, %2[%c0, %workgroup_id_x] {in_bounds = [true]} : vector<1xf16>, memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  return
}

	ZERO found!
	ZERO found!
	ZERO found!
	ZERO found!
	ZERO found!
	ZERO found!
	ZERO found!
//--- after step #1: preprocessing reduction ops ---//
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %cst = arith.constant 0.000000e+00 : f16
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1536 = arith.constant 1536 : index
  %c2048 = arith.constant 2048 : index
  %c2560 = arith.constant 2560 : index
  %c3072 = arith.constant 3072 : index
  %c3584 = arith.constant 3584 : index
  %cst_0 = arith.constant dense<0.000000e+00> : vector<512xf16>
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %3 = vector.transfer_read %0[%c0, %c0], %cst {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
  %4 = vector.transfer_read %0[%c0, %c512], %cst {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
  %5 = vector.transfer_read %0[%c0, %c1024], %cst {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
  %6 = vector.transfer_read %0[%c0, %c1536], %cst {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
  %7 = vector.transfer_read %0[%c0, %c2048], %cst {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
  %8 = vector.transfer_read %0[%c0, %c2560], %cst {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
  %9 = vector.transfer_read %0[%c0, %c3072], %cst {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
  %10 = vector.transfer_read %0[%c0, %c3584], %cst {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
  %11 = vector.transfer_read %1[%workgroup_id_x, %c0], %cst {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
  %12 = vector.transfer_read %1[%workgroup_id_x, %c512], %cst {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
  %13 = vector.transfer_read %1[%workgroup_id_x, %c1024], %cst {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
  %14 = vector.transfer_read %1[%workgroup_id_x, %c1536], %cst {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
  %15 = vector.transfer_read %1[%workgroup_id_x, %c2048], %cst {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
  %16 = vector.transfer_read %1[%workgroup_id_x, %c2560], %cst {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
  %17 = vector.transfer_read %1[%workgroup_id_x, %c3072], %cst {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
  %18 = vector.transfer_read %1[%workgroup_id_x, %c3584], %cst {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
  %19 = arith.mulf %3, %11 : vector<512xf16>
  %20 = arith.mulf %4, %12 : vector<512xf16>
  %21 = arith.mulf %5, %13 : vector<512xf16>
  %22 = arith.mulf %6, %14 : vector<512xf16>
  %23 = arith.mulf %7, %15 : vector<512xf16>
  %24 = arith.mulf %8, %16 : vector<512xf16>
  %25 = arith.mulf %9, %17 : vector<512xf16>
  %26 = arith.mulf %10, %18 : vector<512xf16>
  %27 = arith.addf %26, %cst_0 : vector<512xf16>
  %28 = arith.addf %25, %27 : vector<512xf16>
  %29 = arith.addf %24, %28 : vector<512xf16>
  %30 = arith.addf %23, %29 : vector<512xf16>
  %31 = arith.addf %22, %30 : vector<512xf16>
  %32 = arith.addf %21, %31 : vector<512xf16>
  %33 = arith.addf %20, %32 : vector<512xf16>
  %34 = arith.addf %19, %33 : vector<512xf16>
  %35 = vector.reduction <add>, %34, %cst : vector<512xf16> into f16
  %36 = vector.broadcast %35 : f16 to vector<1xf16>
  vector.transfer_write %36, %2[%c0, %workgroup_id_x] {in_bounds = [true]} : vector<1xf16>, memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  return
}

//--- after step #2: wrapping code with the warp execute op ---//
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %0 = gpu.thread_id  x
  %c64 = arith.constant 64 : index
  vector.warp_execute_on_lane_0(%0)[64] {
    %cst = arith.constant 0.000000e+00 : f16
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1536 = arith.constant 1536 : index
    %c2048 = arith.constant 2048 : index
    %c2560 = arith.constant 2560 : index
    %c3072 = arith.constant 3072 : index
    %c3584 = arith.constant 3584 : index
    %cst_0 = arith.constant dense<0.000000e+00> : vector<512xf16>
    %c0 = arith.constant 0 : index
    %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %1, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
    %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %2, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
    %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %3, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = vector.transfer_read %1[%c0, %c0], %cst {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
    %5 = vector.transfer_read %1[%c0, %c512], %cst {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
    %6 = vector.transfer_read %1[%c0, %c1024], %cst {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
    %7 = vector.transfer_read %1[%c0, %c1536], %cst {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
    %8 = vector.transfer_read %1[%c0, %c2048], %cst {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
    %9 = vector.transfer_read %1[%c0, %c2560], %cst {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
    %10 = vector.transfer_read %1[%c0, %c3072], %cst {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
    %11 = vector.transfer_read %1[%c0, %c3584], %cst {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
    %12 = vector.transfer_read %2[%workgroup_id_x, %c0], %cst {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
    %13 = vector.transfer_read %2[%workgroup_id_x, %c512], %cst {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
    %14 = vector.transfer_read %2[%workgroup_id_x, %c1024], %cst {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
    %15 = vector.transfer_read %2[%workgroup_id_x, %c1536], %cst {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
    %16 = vector.transfer_read %2[%workgroup_id_x, %c2048], %cst {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
    %17 = vector.transfer_read %2[%workgroup_id_x, %c2560], %cst {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
    %18 = vector.transfer_read %2[%workgroup_id_x, %c3072], %cst {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
    %19 = vector.transfer_read %2[%workgroup_id_x, %c3584], %cst {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
    %20 = arith.mulf %4, %12 : vector<512xf16>
    %21 = arith.mulf %5, %13 : vector<512xf16>
    %22 = arith.mulf %6, %14 : vector<512xf16>
    %23 = arith.mulf %7, %15 : vector<512xf16>
    %24 = arith.mulf %8, %16 : vector<512xf16>
    %25 = arith.mulf %9, %17 : vector<512xf16>
    %26 = arith.mulf %10, %18 : vector<512xf16>
    %27 = arith.mulf %11, %19 : vector<512xf16>
    %28 = arith.addf %27, %cst_0 : vector<512xf16>
    %29 = arith.addf %26, %28 : vector<512xf16>
    %30 = arith.addf %25, %29 : vector<512xf16>
    %31 = arith.addf %24, %30 : vector<512xf16>
    %32 = arith.addf %23, %31 : vector<512xf16>
    %33 = arith.addf %22, %32 : vector<512xf16>
    %34 = arith.addf %21, %33 : vector<512xf16>
    %35 = arith.addf %20, %34 : vector<512xf16>
    %36 = vector.reduction <add>, %35, %cst : vector<512xf16> into f16
    %37 = vector.broadcast %36 : f16 to vector<1xf16>
    vector.transfer_write %37, %3[%c0, %workgroup_id_x] {in_bounds = [true]} : vector<1xf16>, memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  }
  return
}

//--- after step #3: hosting uniform code ---//
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %0 = gpu.thread_id  x
  %c64 = arith.constant 64 : index
  %cst = arith.constant 0.000000e+00 : f16
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1536 = arith.constant 1536 : index
  %c2048 = arith.constant 2048 : index
  %c2560 = arith.constant 2560 : index
  %c3072 = arith.constant 3072 : index
  %c3584 = arith.constant 3584 : index
  %c0 = arith.constant 0 : index
  %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %3, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  vector.warp_execute_on_lane_0(%0)[64] {
    %cst_0 = arith.constant dense<0.000000e+00> : vector<512xf16>
    %4 = vector.transfer_read %1[%c0, %c0], %cst {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
    %5 = vector.transfer_read %1[%c0, %c512], %cst {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
    %6 = vector.transfer_read %1[%c0, %c1024], %cst {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
    %7 = vector.transfer_read %1[%c0, %c1536], %cst {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
    %8 = vector.transfer_read %1[%c0, %c2048], %cst {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
    %9 = vector.transfer_read %1[%c0, %c2560], %cst {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
    %10 = vector.transfer_read %1[%c0, %c3072], %cst {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
    %11 = vector.transfer_read %1[%c0, %c3584], %cst {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
    %12 = vector.transfer_read %2[%workgroup_id_x, %c0], %cst {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
    %13 = vector.transfer_read %2[%workgroup_id_x, %c512], %cst {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
    %14 = vector.transfer_read %2[%workgroup_id_x, %c1024], %cst {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
    %15 = vector.transfer_read %2[%workgroup_id_x, %c1536], %cst {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
    %16 = vector.transfer_read %2[%workgroup_id_x, %c2048], %cst {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
    %17 = vector.transfer_read %2[%workgroup_id_x, %c2560], %cst {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
    %18 = vector.transfer_read %2[%workgroup_id_x, %c3072], %cst {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
    %19 = vector.transfer_read %2[%workgroup_id_x, %c3584], %cst {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<512xf16>
    %20 = arith.mulf %4, %12 : vector<512xf16>
    %21 = arith.mulf %5, %13 : vector<512xf16>
    %22 = arith.mulf %6, %14 : vector<512xf16>
    %23 = arith.mulf %7, %15 : vector<512xf16>
    %24 = arith.mulf %8, %16 : vector<512xf16>
    %25 = arith.mulf %9, %17 : vector<512xf16>
    %26 = arith.mulf %10, %18 : vector<512xf16>
    %27 = arith.mulf %11, %19 : vector<512xf16>
    %28 = arith.addf %27, %cst_0 : vector<512xf16>
    %29 = arith.addf %26, %28 : vector<512xf16>
    %30 = arith.addf %25, %29 : vector<512xf16>
    %31 = arith.addf %24, %30 : vector<512xf16>
    %32 = arith.addf %23, %31 : vector<512xf16>
    %33 = arith.addf %22, %32 : vector<512xf16>
    %34 = arith.addf %21, %33 : vector<512xf16>
    %35 = arith.addf %20, %34 : vector<512xf16>
    %36 = vector.reduction <add>, %35, %cst : vector<512xf16> into f16
    %37 = vector.broadcast %36 : f16 to vector<1xf16>
    vector.transfer_write %37, %3[%c0, %workgroup_id_x] {in_bounds = [true]} : vector<1xf16>, memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  }
  return
}

//--- after step #4: propagating distribution ---//
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %cst = arith.constant dense<0.000000e+00> : vector<8xf16>
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f16
  %0 = gpu.thread_id  x
  %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %3, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3584)>()[%0]
  %5 = vector.transfer_read %2[%workgroup_id_x, %4], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %6 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3584)>()[%0]
  %7 = vector.transfer_read %1[%c0, %6], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %8 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3072)>()[%0]
  %9 = vector.transfer_read %2[%workgroup_id_x, %8], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %10 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3072)>()[%0]
  %11 = vector.transfer_read %1[%c0, %10], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %12 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2560)>()[%0]
  %13 = vector.transfer_read %2[%workgroup_id_x, %12], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %14 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2560)>()[%0]
  %15 = vector.transfer_read %1[%c0, %14], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %16 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2048)>()[%0]
  %17 = vector.transfer_read %2[%workgroup_id_x, %16], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %18 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2048)>()[%0]
  %19 = vector.transfer_read %1[%c0, %18], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %20 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1536)>()[%0]
  %21 = vector.transfer_read %2[%workgroup_id_x, %20], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %22 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1536)>()[%0]
  %23 = vector.transfer_read %1[%c0, %22], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %24 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1024)>()[%0]
  %25 = vector.transfer_read %2[%workgroup_id_x, %24], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %26 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1024)>()[%0]
  %27 = vector.transfer_read %1[%c0, %26], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %28 = affine.apply affine_map<()[s0] -> (s0 * 8 + 512)>()[%0]
  %29 = vector.transfer_read %2[%workgroup_id_x, %28], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %30 = affine.apply affine_map<()[s0] -> (s0 * 8 + 512)>()[%0]
  %31 = vector.transfer_read %1[%c0, %30], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %32 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%0]
  %33 = vector.transfer_read %2[%workgroup_id_x, %32], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %34 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%0]
  %35 = vector.transfer_read %1[%c0, %34], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %36 = arith.mulf %7, %5 : vector<8xf16>
  %37 = arith.addf %36, %cst : vector<8xf16>
  %38 = arith.mulf %11, %9 : vector<8xf16>
  %39 = arith.addf %38, %37 : vector<8xf16>
  %40 = arith.mulf %15, %13 : vector<8xf16>
  %41 = arith.addf %40, %39 : vector<8xf16>
  %42 = arith.mulf %19, %17 : vector<8xf16>
  %43 = arith.addf %42, %41 : vector<8xf16>
  %44 = arith.mulf %23, %21 : vector<8xf16>
  %45 = arith.addf %44, %43 : vector<8xf16>
  %46 = arith.mulf %27, %25 : vector<8xf16>
  %47 = arith.addf %46, %45 : vector<8xf16>
  %48 = arith.mulf %31, %29 : vector<8xf16>
  %49 = arith.addf %48, %47 : vector<8xf16>
  %50 = arith.mulf %35, %33 : vector<8xf16>
  %51 = arith.addf %50, %49 : vector<8xf16>
  %52 = vector.reduction <add>, %51 : vector<8xf16> into f16
  %53 = gpu.subgroup_reduce  add %52 : (f16) -> f16
  %54 = arith.addf %53, %cst_0 : f16
  %55 = vector.broadcast %54 : f16 to vector<1xf16>
  vector.warp_execute_on_lane_0(%0)[64] {
    vector.transfer_write %55, %3[%c0, %workgroup_id_x] {in_bounds = [true]} : vector<1xf16>, memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  }
  return
}

//--- after step #5: lowering remaining ops ---//
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %c0 = arith.constant 0 : index
  %cst = arith.constant dense<0.000000e+00> : vector<8xf16>
  %cst_0 = arith.constant 0.000000e+00 : f16
  %0 = gpu.thread_id  x
  %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %3, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3584)>()[%0]
  %5 = vector.transfer_read %2[%workgroup_id_x, %4], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %6 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3584)>()[%0]
  %7 = vector.transfer_read %1[%c0, %6], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %8 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3072)>()[%0]
  %9 = vector.transfer_read %2[%workgroup_id_x, %8], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %10 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3072)>()[%0]
  %11 = vector.transfer_read %1[%c0, %10], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %12 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2560)>()[%0]
  %13 = vector.transfer_read %2[%workgroup_id_x, %12], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %14 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2560)>()[%0]
  %15 = vector.transfer_read %1[%c0, %14], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %16 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2048)>()[%0]
  %17 = vector.transfer_read %2[%workgroup_id_x, %16], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %18 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2048)>()[%0]
  %19 = vector.transfer_read %1[%c0, %18], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %20 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1536)>()[%0]
  %21 = vector.transfer_read %2[%workgroup_id_x, %20], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %22 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1536)>()[%0]
  %23 = vector.transfer_read %1[%c0, %22], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %24 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1024)>()[%0]
  %25 = vector.transfer_read %2[%workgroup_id_x, %24], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %26 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1024)>()[%0]
  %27 = vector.transfer_read %1[%c0, %26], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %28 = affine.apply affine_map<()[s0] -> (s0 * 8 + 512)>()[%0]
  %29 = vector.transfer_read %2[%workgroup_id_x, %28], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %30 = affine.apply affine_map<()[s0] -> (s0 * 8 + 512)>()[%0]
  %31 = vector.transfer_read %1[%c0, %30], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %32 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%0]
  %33 = vector.transfer_read %2[%workgroup_id_x, %32], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %34 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%0]
  %35 = vector.transfer_read %1[%c0, %34], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %36 = arith.mulf %7, %5 : vector<8xf16>
  %37 = arith.addf %36, %cst : vector<8xf16>
  %38 = arith.mulf %11, %9 : vector<8xf16>
  %39 = arith.addf %38, %37 : vector<8xf16>
  %40 = arith.mulf %15, %13 : vector<8xf16>
  %41 = arith.addf %40, %39 : vector<8xf16>
  %42 = arith.mulf %19, %17 : vector<8xf16>
  %43 = arith.addf %42, %41 : vector<8xf16>
  %44 = arith.mulf %23, %21 : vector<8xf16>
  %45 = arith.addf %44, %43 : vector<8xf16>
  %46 = arith.mulf %27, %25 : vector<8xf16>
  %47 = arith.addf %46, %45 : vector<8xf16>
  %48 = arith.mulf %31, %29 : vector<8xf16>
  %49 = arith.addf %48, %47 : vector<8xf16>
  %50 = arith.mulf %35, %33 : vector<8xf16>
  %51 = arith.addf %50, %49 : vector<8xf16>
  %52 = vector.reduction <add>, %51 : vector<8xf16> into f16
  %53 = gpu.subgroup_reduce  add %52 : (f16) -> f16
  %54 = arith.addf %53, %cst_0 : f16
  %55 = vector.broadcast %54 : f16 to vector<1xf16>
  %56 = arith.cmpi eq, %0, %c0 : index
  scf.if %56 {
    vector.transfer_write %55, %3[%c0, %workgroup_id_x] {in_bounds = [true]} : vector<1xf16>, memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  }
  return
}

// -----// IR Dump After VectorReductionToGPU (iree-codegen-vector-reduction-to-gpu) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %c0 = arith.constant 0 : index
  %cst = arith.constant dense<0.000000e+00> : vector<8xf16>
  %cst_0 = arith.constant 0.000000e+00 : f16
  %0 = gpu.thread_id  x
  %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %3, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3584)>()[%0]
  %5 = vector.transfer_read %2[%workgroup_id_x, %4], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %6 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3584)>()[%0]
  %7 = vector.transfer_read %1[%c0, %6], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %8 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3072)>()[%0]
  %9 = vector.transfer_read %2[%workgroup_id_x, %8], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %10 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3072)>()[%0]
  %11 = vector.transfer_read %1[%c0, %10], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %12 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2560)>()[%0]
  %13 = vector.transfer_read %2[%workgroup_id_x, %12], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %14 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2560)>()[%0]
  %15 = vector.transfer_read %1[%c0, %14], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %16 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2048)>()[%0]
  %17 = vector.transfer_read %2[%workgroup_id_x, %16], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %18 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2048)>()[%0]
  %19 = vector.transfer_read %1[%c0, %18], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %20 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1536)>()[%0]
  %21 = vector.transfer_read %2[%workgroup_id_x, %20], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %22 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1536)>()[%0]
  %23 = vector.transfer_read %1[%c0, %22], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %24 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1024)>()[%0]
  %25 = vector.transfer_read %2[%workgroup_id_x, %24], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %26 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1024)>()[%0]
  %27 = vector.transfer_read %1[%c0, %26], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %28 = affine.apply affine_map<()[s0] -> (s0 * 8 + 512)>()[%0]
  %29 = vector.transfer_read %2[%workgroup_id_x, %28], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %30 = affine.apply affine_map<()[s0] -> (s0 * 8 + 512)>()[%0]
  %31 = vector.transfer_read %1[%c0, %30], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %32 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%0]
  %33 = vector.transfer_read %2[%workgroup_id_x, %32], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %34 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%0]
  %35 = vector.transfer_read %1[%c0, %34], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %36 = arith.mulf %7, %5 : vector<8xf16>
  %37 = arith.addf %36, %cst : vector<8xf16>
  %38 = arith.mulf %11, %9 : vector<8xf16>
  %39 = arith.addf %38, %37 : vector<8xf16>
  %40 = arith.mulf %15, %13 : vector<8xf16>
  %41 = arith.addf %40, %39 : vector<8xf16>
  %42 = arith.mulf %19, %17 : vector<8xf16>
  %43 = arith.addf %42, %41 : vector<8xf16>
  %44 = arith.mulf %23, %21 : vector<8xf16>
  %45 = arith.addf %44, %43 : vector<8xf16>
  %46 = arith.mulf %27, %25 : vector<8xf16>
  %47 = arith.addf %46, %45 : vector<8xf16>
  %48 = arith.mulf %31, %29 : vector<8xf16>
  %49 = arith.addf %48, %47 : vector<8xf16>
  %50 = arith.mulf %35, %33 : vector<8xf16>
  %51 = arith.addf %50, %49 : vector<8xf16>
  %52 = vector.reduction <add>, %51 : vector<8xf16> into f16
  %53 = gpu.subgroup_reduce  add %52 : (f16) -> f16
  %54 = arith.addf %53, %cst_0 : f16
  %55 = vector.broadcast %54 : f16 to vector<1xf16>
  %56 = arith.cmpi eq, %0, %c0 : index
  scf.if %56 {
    vector.transfer_write %55, %3[%c0, %workgroup_id_x] {in_bounds = [true]} : vector<1xf16>, memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  }
  return
}

// -----// IR Dump After SPIRVInitialVectorLowering (iree-spirv-initial-vector-lowering) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %c0 = arith.constant 0 : index
  %cst = arith.constant dense<0.000000e+00> : vector<8xf16>
  %cst_0 = arith.constant 0.000000e+00 : f16
  %0 = gpu.thread_id  x
  %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %3, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3584)>()[%0]
  %5 = vector.transfer_read %2[%workgroup_id_x, %4], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %6 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3584)>()[%0]
  %7 = vector.transfer_read %1[%c0, %6], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %8 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3072)>()[%0]
  %9 = vector.transfer_read %2[%workgroup_id_x, %8], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %10 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3072)>()[%0]
  %11 = vector.transfer_read %1[%c0, %10], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %12 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2560)>()[%0]
  %13 = vector.transfer_read %2[%workgroup_id_x, %12], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %14 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2560)>()[%0]
  %15 = vector.transfer_read %1[%c0, %14], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %16 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2048)>()[%0]
  %17 = vector.transfer_read %2[%workgroup_id_x, %16], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %18 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2048)>()[%0]
  %19 = vector.transfer_read %1[%c0, %18], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %20 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1536)>()[%0]
  %21 = vector.transfer_read %2[%workgroup_id_x, %20], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %22 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1536)>()[%0]
  %23 = vector.transfer_read %1[%c0, %22], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %24 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1024)>()[%0]
  %25 = vector.transfer_read %2[%workgroup_id_x, %24], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %26 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1024)>()[%0]
  %27 = vector.transfer_read %1[%c0, %26], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %28 = affine.apply affine_map<()[s0] -> (s0 * 8 + 512)>()[%0]
  %29 = vector.transfer_read %2[%workgroup_id_x, %28], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %30 = affine.apply affine_map<()[s0] -> (s0 * 8 + 512)>()[%0]
  %31 = vector.transfer_read %1[%c0, %30], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %32 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%0]
  %33 = vector.transfer_read %2[%workgroup_id_x, %32], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %34 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%0]
  %35 = vector.transfer_read %1[%c0, %34], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %36 = vector.extract_strided_slice %7 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %37 = vector.extract_strided_slice %5 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %38 = arith.mulf %36, %37 : vector<4xf16>
  %39 = vector.extract_strided_slice %7 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %40 = vector.extract_strided_slice %5 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %41 = arith.mulf %39, %40 : vector<4xf16>
  %42 = vector.extract_strided_slice %cst {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %43 = arith.addf %38, %42 : vector<4xf16>
  %44 = vector.extract_strided_slice %cst {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %45 = arith.addf %41, %44 : vector<4xf16>
  %46 = vector.extract_strided_slice %11 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %47 = vector.extract_strided_slice %9 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %48 = arith.mulf %46, %47 : vector<4xf16>
  %49 = vector.extract_strided_slice %11 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %50 = vector.extract_strided_slice %9 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %51 = arith.mulf %49, %50 : vector<4xf16>
  %52 = arith.addf %48, %43 : vector<4xf16>
  %53 = arith.addf %51, %45 : vector<4xf16>
  %54 = vector.extract_strided_slice %15 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %55 = vector.extract_strided_slice %13 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %56 = arith.mulf %54, %55 : vector<4xf16>
  %57 = vector.extract_strided_slice %15 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %58 = vector.extract_strided_slice %13 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %59 = arith.mulf %57, %58 : vector<4xf16>
  %60 = arith.addf %56, %52 : vector<4xf16>
  %61 = arith.addf %59, %53 : vector<4xf16>
  %62 = vector.extract_strided_slice %19 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %63 = vector.extract_strided_slice %17 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %64 = arith.mulf %62, %63 : vector<4xf16>
  %65 = vector.extract_strided_slice %19 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %66 = vector.extract_strided_slice %17 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %67 = arith.mulf %65, %66 : vector<4xf16>
  %68 = arith.addf %64, %60 : vector<4xf16>
  %69 = arith.addf %67, %61 : vector<4xf16>
  %70 = vector.extract_strided_slice %23 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %71 = vector.extract_strided_slice %21 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %72 = arith.mulf %70, %71 : vector<4xf16>
  %73 = vector.extract_strided_slice %23 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %74 = vector.extract_strided_slice %21 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %75 = arith.mulf %73, %74 : vector<4xf16>
  %76 = arith.addf %72, %68 : vector<4xf16>
  %77 = arith.addf %75, %69 : vector<4xf16>
  %78 = vector.extract_strided_slice %27 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %79 = vector.extract_strided_slice %25 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %80 = arith.mulf %78, %79 : vector<4xf16>
  %81 = vector.extract_strided_slice %27 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %82 = vector.extract_strided_slice %25 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %83 = arith.mulf %81, %82 : vector<4xf16>
  %84 = arith.addf %80, %76 : vector<4xf16>
  %85 = arith.addf %83, %77 : vector<4xf16>
  %86 = vector.extract_strided_slice %31 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %87 = vector.extract_strided_slice %29 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %88 = arith.mulf %86, %87 : vector<4xf16>
  %89 = vector.extract_strided_slice %31 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %90 = vector.extract_strided_slice %29 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %91 = arith.mulf %89, %90 : vector<4xf16>
  %92 = arith.addf %88, %84 : vector<4xf16>
  %93 = arith.addf %91, %85 : vector<4xf16>
  %94 = vector.extract_strided_slice %35 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %95 = vector.extract_strided_slice %33 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %96 = arith.mulf %94, %95 : vector<4xf16>
  %97 = vector.extract_strided_slice %35 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %98 = vector.extract_strided_slice %33 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %99 = arith.mulf %97, %98 : vector<4xf16>
  %100 = arith.addf %96, %92 : vector<4xf16>
  %101 = arith.addf %99, %93 : vector<4xf16>
  %102 = vector.reduction <add>, %100 : vector<4xf16> into f16
  %103 = vector.reduction <add>, %101 : vector<4xf16> into f16
  %104 = arith.addf %102, %103 : f16
  %105 = gpu.subgroup_reduce  add %104 : (f16) -> f16
  %106 = arith.addf %105, %cst_0 : f16
  %107 = vector.broadcast %106 : f16 to vector<1xf16>
  %108 = arith.cmpi eq, %0, %c0 : index
  scf.if %108 {
    vector.transfer_write %107, %3[%c0, %workgroup_id_x] {in_bounds = [true]} : vector<1xf16>, memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  }
  return
}

// -----// IR Dump After HoistRedundantVectorTransfers (iree-codegen-hoist-redundant-vector-transfers) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %c0 = arith.constant 0 : index
  %cst = arith.constant dense<0.000000e+00> : vector<8xf16>
  %cst_0 = arith.constant 0.000000e+00 : f16
  %0 = gpu.thread_id  x
  %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %3, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3584)>()[%0]
  %5 = vector.transfer_read %2[%workgroup_id_x, %4], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %6 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3584)>()[%0]
  %7 = vector.transfer_read %1[%c0, %6], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %8 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3072)>()[%0]
  %9 = vector.transfer_read %2[%workgroup_id_x, %8], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %10 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3072)>()[%0]
  %11 = vector.transfer_read %1[%c0, %10], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %12 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2560)>()[%0]
  %13 = vector.transfer_read %2[%workgroup_id_x, %12], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %14 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2560)>()[%0]
  %15 = vector.transfer_read %1[%c0, %14], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %16 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2048)>()[%0]
  %17 = vector.transfer_read %2[%workgroup_id_x, %16], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %18 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2048)>()[%0]
  %19 = vector.transfer_read %1[%c0, %18], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %20 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1536)>()[%0]
  %21 = vector.transfer_read %2[%workgroup_id_x, %20], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %22 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1536)>()[%0]
  %23 = vector.transfer_read %1[%c0, %22], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %24 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1024)>()[%0]
  %25 = vector.transfer_read %2[%workgroup_id_x, %24], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %26 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1024)>()[%0]
  %27 = vector.transfer_read %1[%c0, %26], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %28 = affine.apply affine_map<()[s0] -> (s0 * 8 + 512)>()[%0]
  %29 = vector.transfer_read %2[%workgroup_id_x, %28], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %30 = affine.apply affine_map<()[s0] -> (s0 * 8 + 512)>()[%0]
  %31 = vector.transfer_read %1[%c0, %30], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %32 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%0]
  %33 = vector.transfer_read %2[%workgroup_id_x, %32], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %34 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%0]
  %35 = vector.transfer_read %1[%c0, %34], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %36 = vector.extract_strided_slice %7 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %37 = vector.extract_strided_slice %5 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %38 = arith.mulf %36, %37 : vector<4xf16>
  %39 = vector.extract_strided_slice %7 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %40 = vector.extract_strided_slice %5 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %41 = arith.mulf %39, %40 : vector<4xf16>
  %42 = vector.extract_strided_slice %cst {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %43 = arith.addf %38, %42 : vector<4xf16>
  %44 = vector.extract_strided_slice %cst {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %45 = arith.addf %41, %44 : vector<4xf16>
  %46 = vector.extract_strided_slice %11 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %47 = vector.extract_strided_slice %9 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %48 = arith.mulf %46, %47 : vector<4xf16>
  %49 = vector.extract_strided_slice %11 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %50 = vector.extract_strided_slice %9 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %51 = arith.mulf %49, %50 : vector<4xf16>
  %52 = arith.addf %48, %43 : vector<4xf16>
  %53 = arith.addf %51, %45 : vector<4xf16>
  %54 = vector.extract_strided_slice %15 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %55 = vector.extract_strided_slice %13 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %56 = arith.mulf %54, %55 : vector<4xf16>
  %57 = vector.extract_strided_slice %15 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %58 = vector.extract_strided_slice %13 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %59 = arith.mulf %57, %58 : vector<4xf16>
  %60 = arith.addf %56, %52 : vector<4xf16>
  %61 = arith.addf %59, %53 : vector<4xf16>
  %62 = vector.extract_strided_slice %19 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %63 = vector.extract_strided_slice %17 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %64 = arith.mulf %62, %63 : vector<4xf16>
  %65 = vector.extract_strided_slice %19 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %66 = vector.extract_strided_slice %17 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %67 = arith.mulf %65, %66 : vector<4xf16>
  %68 = arith.addf %64, %60 : vector<4xf16>
  %69 = arith.addf %67, %61 : vector<4xf16>
  %70 = vector.extract_strided_slice %23 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %71 = vector.extract_strided_slice %21 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %72 = arith.mulf %70, %71 : vector<4xf16>
  %73 = vector.extract_strided_slice %23 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %74 = vector.extract_strided_slice %21 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %75 = arith.mulf %73, %74 : vector<4xf16>
  %76 = arith.addf %72, %68 : vector<4xf16>
  %77 = arith.addf %75, %69 : vector<4xf16>
  %78 = vector.extract_strided_slice %27 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %79 = vector.extract_strided_slice %25 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %80 = arith.mulf %78, %79 : vector<4xf16>
  %81 = vector.extract_strided_slice %27 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %82 = vector.extract_strided_slice %25 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %83 = arith.mulf %81, %82 : vector<4xf16>
  %84 = arith.addf %80, %76 : vector<4xf16>
  %85 = arith.addf %83, %77 : vector<4xf16>
  %86 = vector.extract_strided_slice %31 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %87 = vector.extract_strided_slice %29 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %88 = arith.mulf %86, %87 : vector<4xf16>
  %89 = vector.extract_strided_slice %31 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %90 = vector.extract_strided_slice %29 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %91 = arith.mulf %89, %90 : vector<4xf16>
  %92 = arith.addf %88, %84 : vector<4xf16>
  %93 = arith.addf %91, %85 : vector<4xf16>
  %94 = vector.extract_strided_slice %35 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %95 = vector.extract_strided_slice %33 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %96 = arith.mulf %94, %95 : vector<4xf16>
  %97 = vector.extract_strided_slice %35 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %98 = vector.extract_strided_slice %33 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %99 = arith.mulf %97, %98 : vector<4xf16>
  %100 = arith.addf %96, %92 : vector<4xf16>
  %101 = arith.addf %99, %93 : vector<4xf16>
  %102 = vector.reduction <add>, %100 : vector<4xf16> into f16
  %103 = vector.reduction <add>, %101 : vector<4xf16> into f16
  %104 = arith.addf %102, %103 : f16
  %105 = gpu.subgroup_reduce  add %104 : (f16) -> f16
  %106 = arith.addf %105, %cst_0 : f16
  %107 = vector.broadcast %106 : f16 to vector<1xf16>
  %108 = arith.cmpi eq, %0, %c0 : index
  scf.if %108 {
    vector.transfer_write %107, %3[%c0, %workgroup_id_x] {in_bounds = [true]} : vector<1xf16>, memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  }
  return
}

// -----// IR Dump After SPIRVFinalVectorLowering (iree-spirv-final-vector-lowering) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f16
  %0 = gpu.thread_id  x
  %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %3, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3584)>()[%0]
  %5 = vector.transfer_read %2[%workgroup_id_x, %4], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %6 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3584)>()[%0]
  %7 = vector.transfer_read %1[%c0, %6], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %8 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3072)>()[%0]
  %9 = vector.transfer_read %2[%workgroup_id_x, %8], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %10 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3072)>()[%0]
  %11 = vector.transfer_read %1[%c0, %10], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %12 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2560)>()[%0]
  %13 = vector.transfer_read %2[%workgroup_id_x, %12], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %14 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2560)>()[%0]
  %15 = vector.transfer_read %1[%c0, %14], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %16 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2048)>()[%0]
  %17 = vector.transfer_read %2[%workgroup_id_x, %16], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %18 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2048)>()[%0]
  %19 = vector.transfer_read %1[%c0, %18], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %20 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1536)>()[%0]
  %21 = vector.transfer_read %2[%workgroup_id_x, %20], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %22 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1536)>()[%0]
  %23 = vector.transfer_read %1[%c0, %22], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %24 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1024)>()[%0]
  %25 = vector.transfer_read %2[%workgroup_id_x, %24], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %26 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1024)>()[%0]
  %27 = vector.transfer_read %1[%c0, %26], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %28 = affine.apply affine_map<()[s0] -> (s0 * 8 + 512)>()[%0]
  %29 = vector.transfer_read %2[%workgroup_id_x, %28], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %30 = affine.apply affine_map<()[s0] -> (s0 * 8 + 512)>()[%0]
  %31 = vector.transfer_read %1[%c0, %30], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %32 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%0]
  %33 = vector.transfer_read %2[%workgroup_id_x, %32], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %34 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%0]
  %35 = vector.transfer_read %1[%c0, %34], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %36 = vector.extract_strided_slice %7 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %37 = vector.extract_strided_slice %5 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %38 = arith.mulf %36, %37 : vector<4xf16>
  %39 = vector.extract_strided_slice %7 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %40 = vector.extract_strided_slice %5 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %41 = arith.mulf %39, %40 : vector<4xf16>
  %42 = arith.addf %38, %cst : vector<4xf16>
  %43 = arith.addf %41, %cst : vector<4xf16>
  %44 = vector.extract_strided_slice %11 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %45 = vector.extract_strided_slice %9 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %46 = arith.mulf %44, %45 : vector<4xf16>
  %47 = vector.extract_strided_slice %11 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %48 = vector.extract_strided_slice %9 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %49 = arith.mulf %47, %48 : vector<4xf16>
  %50 = arith.addf %46, %42 : vector<4xf16>
  %51 = arith.addf %49, %43 : vector<4xf16>
  %52 = vector.extract_strided_slice %15 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %53 = vector.extract_strided_slice %13 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %54 = arith.mulf %52, %53 : vector<4xf16>
  %55 = vector.extract_strided_slice %15 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %56 = vector.extract_strided_slice %13 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %57 = arith.mulf %55, %56 : vector<4xf16>
  %58 = arith.addf %54, %50 : vector<4xf16>
  %59 = arith.addf %57, %51 : vector<4xf16>
  %60 = vector.extract_strided_slice %19 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %61 = vector.extract_strided_slice %17 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %62 = arith.mulf %60, %61 : vector<4xf16>
  %63 = vector.extract_strided_slice %19 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %64 = vector.extract_strided_slice %17 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %65 = arith.mulf %63, %64 : vector<4xf16>
  %66 = arith.addf %62, %58 : vector<4xf16>
  %67 = arith.addf %65, %59 : vector<4xf16>
  %68 = vector.extract_strided_slice %23 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %69 = vector.extract_strided_slice %21 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %70 = arith.mulf %68, %69 : vector<4xf16>
  %71 = vector.extract_strided_slice %23 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %72 = vector.extract_strided_slice %21 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %73 = arith.mulf %71, %72 : vector<4xf16>
  %74 = arith.addf %70, %66 : vector<4xf16>
  %75 = arith.addf %73, %67 : vector<4xf16>
  %76 = vector.extract_strided_slice %27 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %77 = vector.extract_strided_slice %25 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %78 = arith.mulf %76, %77 : vector<4xf16>
  %79 = vector.extract_strided_slice %27 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %80 = vector.extract_strided_slice %25 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %81 = arith.mulf %79, %80 : vector<4xf16>
  %82 = arith.addf %78, %74 : vector<4xf16>
  %83 = arith.addf %81, %75 : vector<4xf16>
  %84 = vector.extract_strided_slice %31 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %85 = vector.extract_strided_slice %29 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %86 = arith.mulf %84, %85 : vector<4xf16>
  %87 = vector.extract_strided_slice %31 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %88 = vector.extract_strided_slice %29 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %89 = arith.mulf %87, %88 : vector<4xf16>
  %90 = arith.addf %86, %82 : vector<4xf16>
  %91 = arith.addf %89, %83 : vector<4xf16>
  %92 = vector.extract_strided_slice %35 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %93 = vector.extract_strided_slice %33 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %94 = arith.mulf %92, %93 : vector<4xf16>
  %95 = vector.extract_strided_slice %35 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %96 = vector.extract_strided_slice %33 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %97 = arith.mulf %95, %96 : vector<4xf16>
  %98 = arith.addf %94, %90 : vector<4xf16>
  %99 = arith.addf %97, %91 : vector<4xf16>
  %100 = vector.reduction <add>, %98 : vector<4xf16> into f16
  %101 = vector.reduction <add>, %99 : vector<4xf16> into f16
  %102 = arith.addf %100, %101 : f16
  %103 = gpu.subgroup_reduce  add %102 : (f16) -> f16
  %104 = arith.addf %103, %cst_0 : f16
  %105 = vector.splat %104 : vector<1xf16>
  %106 = arith.cmpi eq, %0, %c0 : index
  scf.if %106 {
    vector.transfer_write %105, %3[%c0, %workgroup_id_x] {in_bounds = [true]} : vector<1xf16>, memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module {
  func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
    %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f16
    %0 = gpu.thread_id  x
    %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %1, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
    %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %2, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
    %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %3, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3584)>()[%0]
    %5 = vector.transfer_read %2[%workgroup_id_x, %4], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %6 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3584)>()[%0]
    %7 = vector.transfer_read %1[%c0, %6], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %8 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3072)>()[%0]
    %9 = vector.transfer_read %2[%workgroup_id_x, %8], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %10 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3072)>()[%0]
    %11 = vector.transfer_read %1[%c0, %10], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %12 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2560)>()[%0]
    %13 = vector.transfer_read %2[%workgroup_id_x, %12], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %14 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2560)>()[%0]
    %15 = vector.transfer_read %1[%c0, %14], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %16 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2048)>()[%0]
    %17 = vector.transfer_read %2[%workgroup_id_x, %16], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %18 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2048)>()[%0]
    %19 = vector.transfer_read %1[%c0, %18], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %20 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1536)>()[%0]
    %21 = vector.transfer_read %2[%workgroup_id_x, %20], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %22 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1536)>()[%0]
    %23 = vector.transfer_read %1[%c0, %22], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %24 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1024)>()[%0]
    %25 = vector.transfer_read %2[%workgroup_id_x, %24], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %26 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1024)>()[%0]
    %27 = vector.transfer_read %1[%c0, %26], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %28 = affine.apply affine_map<()[s0] -> (s0 * 8 + 512)>()[%0]
    %29 = vector.transfer_read %2[%workgroup_id_x, %28], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %30 = affine.apply affine_map<()[s0] -> (s0 * 8 + 512)>()[%0]
    %31 = vector.transfer_read %1[%c0, %30], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %32 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%0]
    %33 = vector.transfer_read %2[%workgroup_id_x, %32], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %34 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%0]
    %35 = vector.transfer_read %1[%c0, %34], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %36 = vector.extract_strided_slice %7 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %37 = vector.extract_strided_slice %5 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %38 = arith.mulf %36, %37 : vector<4xf16>
    %39 = vector.extract_strided_slice %7 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %40 = vector.extract_strided_slice %5 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %41 = arith.mulf %39, %40 : vector<4xf16>
    %42 = arith.addf %38, %cst : vector<4xf16>
    %43 = arith.addf %41, %cst : vector<4xf16>
    %44 = vector.extract_strided_slice %11 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %45 = vector.extract_strided_slice %9 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %46 = arith.mulf %44, %45 : vector<4xf16>
    %47 = vector.extract_strided_slice %11 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %48 = vector.extract_strided_slice %9 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %49 = arith.mulf %47, %48 : vector<4xf16>
    %50 = arith.addf %46, %42 : vector<4xf16>
    %51 = arith.addf %49, %43 : vector<4xf16>
    %52 = vector.extract_strided_slice %15 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %53 = vector.extract_strided_slice %13 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %54 = arith.mulf %52, %53 : vector<4xf16>
    %55 = vector.extract_strided_slice %15 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %56 = vector.extract_strided_slice %13 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %57 = arith.mulf %55, %56 : vector<4xf16>
    %58 = arith.addf %54, %50 : vector<4xf16>
    %59 = arith.addf %57, %51 : vector<4xf16>
    %60 = vector.extract_strided_slice %19 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %61 = vector.extract_strided_slice %17 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %62 = arith.mulf %60, %61 : vector<4xf16>
    %63 = vector.extract_strided_slice %19 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %64 = vector.extract_strided_slice %17 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %65 = arith.mulf %63, %64 : vector<4xf16>
    %66 = arith.addf %62, %58 : vector<4xf16>
    %67 = arith.addf %65, %59 : vector<4xf16>
    %68 = vector.extract_strided_slice %23 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %69 = vector.extract_strided_slice %21 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %70 = arith.mulf %68, %69 : vector<4xf16>
    %71 = vector.extract_strided_slice %23 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %72 = vector.extract_strided_slice %21 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %73 = arith.mulf %71, %72 : vector<4xf16>
    %74 = arith.addf %70, %66 : vector<4xf16>
    %75 = arith.addf %73, %67 : vector<4xf16>
    %76 = vector.extract_strided_slice %27 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %77 = vector.extract_strided_slice %25 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %78 = arith.mulf %76, %77 : vector<4xf16>
    %79 = vector.extract_strided_slice %27 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %80 = vector.extract_strided_slice %25 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %81 = arith.mulf %79, %80 : vector<4xf16>
    %82 = arith.addf %78, %74 : vector<4xf16>
    %83 = arith.addf %81, %75 : vector<4xf16>
    %84 = vector.extract_strided_slice %31 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %85 = vector.extract_strided_slice %29 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %86 = arith.mulf %84, %85 : vector<4xf16>
    %87 = vector.extract_strided_slice %31 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %88 = vector.extract_strided_slice %29 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %89 = arith.mulf %87, %88 : vector<4xf16>
    %90 = arith.addf %86, %82 : vector<4xf16>
    %91 = arith.addf %89, %83 : vector<4xf16>
    %92 = vector.extract_strided_slice %35 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %93 = vector.extract_strided_slice %33 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %94 = arith.mulf %92, %93 : vector<4xf16>
    %95 = vector.extract_strided_slice %35 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %96 = vector.extract_strided_slice %33 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %97 = arith.mulf %95, %96 : vector<4xf16>
    %98 = arith.addf %94, %90 : vector<4xf16>
    %99 = arith.addf %97, %91 : vector<4xf16>
    %100 = vector.reduction <add>, %98 : vector<4xf16> into f16
    %101 = vector.reduction <add>, %99 : vector<4xf16> into f16
    %102 = arith.addf %100, %101 : f16
    %103 = gpu.subgroup_reduce  add %102 : (f16) -> f16
    %104 = arith.addf %103, %cst_0 : f16
    %105 = vector.splat %104 : vector<1xf16>
    %106 = arith.cmpi eq, %0, %c0 : index
    scf.if %106 {
      vector.transfer_write %105, %3[%c0, %workgroup_id_x] {in_bounds = [true]} : vector<1xf16>, memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    }
    return
  }
}

// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
    %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f16
    %0 = gpu.thread_id  x
    %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %1, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
    %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %2, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
    %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %3, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3584)>()[%0]
    %5 = vector.transfer_read %2[%workgroup_id_x, %4], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %6 = vector.transfer_read %1[%c0, %4], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %7 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3072)>()[%0]
    %8 = vector.transfer_read %2[%workgroup_id_x, %7], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %9 = vector.transfer_read %1[%c0, %7], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %10 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2560)>()[%0]
    %11 = vector.transfer_read %2[%workgroup_id_x, %10], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %12 = vector.transfer_read %1[%c0, %10], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %13 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2048)>()[%0]
    %14 = vector.transfer_read %2[%workgroup_id_x, %13], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %15 = vector.transfer_read %1[%c0, %13], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %16 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1536)>()[%0]
    %17 = vector.transfer_read %2[%workgroup_id_x, %16], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %18 = vector.transfer_read %1[%c0, %16], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %19 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1024)>()[%0]
    %20 = vector.transfer_read %2[%workgroup_id_x, %19], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %21 = vector.transfer_read %1[%c0, %19], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %22 = affine.apply affine_map<()[s0] -> (s0 * 8 + 512)>()[%0]
    %23 = vector.transfer_read %2[%workgroup_id_x, %22], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %24 = vector.transfer_read %1[%c0, %22], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %25 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%0]
    %26 = vector.transfer_read %2[%workgroup_id_x, %25], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %27 = vector.transfer_read %1[%c0, %25], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %28 = vector.extract_strided_slice %6 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %29 = vector.extract_strided_slice %5 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %30 = arith.mulf %28, %29 : vector<4xf16>
    %31 = vector.extract_strided_slice %6 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %32 = vector.extract_strided_slice %5 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %33 = arith.mulf %31, %32 : vector<4xf16>
    %34 = arith.addf %30, %cst : vector<4xf16>
    %35 = arith.addf %33, %cst : vector<4xf16>
    %36 = vector.extract_strided_slice %9 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %37 = vector.extract_strided_slice %8 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %38 = arith.mulf %36, %37 : vector<4xf16>
    %39 = vector.extract_strided_slice %9 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %40 = vector.extract_strided_slice %8 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %41 = arith.mulf %39, %40 : vector<4xf16>
    %42 = arith.addf %38, %34 : vector<4xf16>
    %43 = arith.addf %41, %35 : vector<4xf16>
    %44 = vector.extract_strided_slice %12 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %45 = vector.extract_strided_slice %11 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %46 = arith.mulf %44, %45 : vector<4xf16>
    %47 = vector.extract_strided_slice %12 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %48 = vector.extract_strided_slice %11 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %49 = arith.mulf %47, %48 : vector<4xf16>
    %50 = arith.addf %46, %42 : vector<4xf16>
    %51 = arith.addf %49, %43 : vector<4xf16>
    %52 = vector.extract_strided_slice %15 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %53 = vector.extract_strided_slice %14 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %54 = arith.mulf %52, %53 : vector<4xf16>
    %55 = vector.extract_strided_slice %15 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %56 = vector.extract_strided_slice %14 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %57 = arith.mulf %55, %56 : vector<4xf16>
    %58 = arith.addf %54, %50 : vector<4xf16>
    %59 = arith.addf %57, %51 : vector<4xf16>
    %60 = vector.extract_strided_slice %18 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %61 = vector.extract_strided_slice %17 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %62 = arith.mulf %60, %61 : vector<4xf16>
    %63 = vector.extract_strided_slice %18 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %64 = vector.extract_strided_slice %17 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %65 = arith.mulf %63, %64 : vector<4xf16>
    %66 = arith.addf %62, %58 : vector<4xf16>
    %67 = arith.addf %65, %59 : vector<4xf16>
    %68 = vector.extract_strided_slice %21 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %69 = vector.extract_strided_slice %20 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %70 = arith.mulf %68, %69 : vector<4xf16>
    %71 = vector.extract_strided_slice %21 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %72 = vector.extract_strided_slice %20 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %73 = arith.mulf %71, %72 : vector<4xf16>
    %74 = arith.addf %70, %66 : vector<4xf16>
    %75 = arith.addf %73, %67 : vector<4xf16>
    %76 = vector.extract_strided_slice %24 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %77 = vector.extract_strided_slice %23 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %78 = arith.mulf %76, %77 : vector<4xf16>
    %79 = vector.extract_strided_slice %24 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %80 = vector.extract_strided_slice %23 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %81 = arith.mulf %79, %80 : vector<4xf16>
    %82 = arith.addf %78, %74 : vector<4xf16>
    %83 = arith.addf %81, %75 : vector<4xf16>
    %84 = vector.extract_strided_slice %27 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %85 = vector.extract_strided_slice %26 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %86 = arith.mulf %84, %85 : vector<4xf16>
    %87 = vector.extract_strided_slice %27 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %88 = vector.extract_strided_slice %26 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %89 = arith.mulf %87, %88 : vector<4xf16>
    %90 = arith.addf %86, %82 : vector<4xf16>
    %91 = arith.addf %89, %83 : vector<4xf16>
    %92 = vector.reduction <add>, %90 : vector<4xf16> into f16
    %93 = vector.reduction <add>, %91 : vector<4xf16> into f16
    %94 = arith.addf %92, %93 : f16
    %95 = gpu.subgroup_reduce  add %94 : (f16) -> f16
    %96 = arith.addf %95, %cst_0 : f16
    %97 = vector.splat %96 : vector<1xf16>
    %98 = arith.cmpi eq, %0, %c0 : index
    scf.if %98 {
      vector.transfer_write %97, %3[%c0, %workgroup_id_x] {in_bounds = [true]} : vector<1xf16>, memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    }
    return
  }
}

// -----// IR Dump After SPIRVLowerExecutableTarget (iree-spirv-lower-executable-target-pass) //----- //
hal.executable.variant public @vulkan_spirv_fb target(<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>) {
  hal.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>) attributes {translation_info = #iree_codegen.translation_info<SPIRVSubgroupReduce>, workgroup_size = [64 : index, 1 : index, 1 : index]} {
  ^bb0(%arg0: !hal.device):
    %c32000 = arith.constant 32000 : index
    %c1 = arith.constant 1 : index
    hal.return %c32000, %c1, %c1 : index, index, index
  }
  builtin.module {
    func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
      %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
      %c0 = arith.constant 0 : index
      %cst_0 = arith.constant 0.000000e+00 : f16
      %0 = gpu.thread_id  x
      %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
      memref.assume_alignment %1, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
      %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
      memref.assume_alignment %2, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
      %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
      memref.assume_alignment %3, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
      %workgroup_id_x = hal.interface.workgroup.id[0] : index
      %4 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3584)>()[%0]
      %5 = vector.transfer_read %2[%workgroup_id_x, %4], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
      %6 = vector.transfer_read %1[%c0, %4], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
      %7 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3072)>()[%0]
      %8 = vector.transfer_read %2[%workgroup_id_x, %7], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
      %9 = vector.transfer_read %1[%c0, %7], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
      %10 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2560)>()[%0]
      %11 = vector.transfer_read %2[%workgroup_id_x, %10], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
      %12 = vector.transfer_read %1[%c0, %10], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
      %13 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2048)>()[%0]
      %14 = vector.transfer_read %2[%workgroup_id_x, %13], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
      %15 = vector.transfer_read %1[%c0, %13], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
      %16 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1536)>()[%0]
      %17 = vector.transfer_read %2[%workgroup_id_x, %16], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
      %18 = vector.transfer_read %1[%c0, %16], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
      %19 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1024)>()[%0]
      %20 = vector.transfer_read %2[%workgroup_id_x, %19], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
      %21 = vector.transfer_read %1[%c0, %19], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
      %22 = affine.apply affine_map<()[s0] -> (s0 * 8 + 512)>()[%0]
      %23 = vector.transfer_read %2[%workgroup_id_x, %22], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
      %24 = vector.transfer_read %1[%c0, %22], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
      %25 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%0]
      %26 = vector.transfer_read %2[%workgroup_id_x, %25], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
      %27 = vector.transfer_read %1[%c0, %25], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
      %28 = vector.extract_strided_slice %6 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
      %29 = vector.extract_strided_slice %5 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
      %30 = arith.mulf %28, %29 : vector<4xf16>
      %31 = vector.extract_strided_slice %6 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
      %32 = vector.extract_strided_slice %5 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
      %33 = arith.mulf %31, %32 : vector<4xf16>
      %34 = arith.addf %30, %cst : vector<4xf16>
      %35 = arith.addf %33, %cst : vector<4xf16>
      %36 = vector.extract_strided_slice %9 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
      %37 = vector.extract_strided_slice %8 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
      %38 = arith.mulf %36, %37 : vector<4xf16>
      %39 = vector.extract_strided_slice %9 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
      %40 = vector.extract_strided_slice %8 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
      %41 = arith.mulf %39, %40 : vector<4xf16>
      %42 = arith.addf %38, %34 : vector<4xf16>
      %43 = arith.addf %41, %35 : vector<4xf16>
      %44 = vector.extract_strided_slice %12 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
      %45 = vector.extract_strided_slice %11 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
      %46 = arith.mulf %44, %45 : vector<4xf16>
      %47 = vector.extract_strided_slice %12 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
      %48 = vector.extract_strided_slice %11 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
      %49 = arith.mulf %47, %48 : vector<4xf16>
      %50 = arith.addf %46, %42 : vector<4xf16>
      %51 = arith.addf %49, %43 : vector<4xf16>
      %52 = vector.extract_strided_slice %15 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
      %53 = vector.extract_strided_slice %14 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
      %54 = arith.mulf %52, %53 : vector<4xf16>
      %55 = vector.extract_strided_slice %15 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
      %56 = vector.extract_strided_slice %14 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
      %57 = arith.mulf %55, %56 : vector<4xf16>
      %58 = arith.addf %54, %50 : vector<4xf16>
      %59 = arith.addf %57, %51 : vector<4xf16>
      %60 = vector.extract_strided_slice %18 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
      %61 = vector.extract_strided_slice %17 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
      %62 = arith.mulf %60, %61 : vector<4xf16>
      %63 = vector.extract_strided_slice %18 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
      %64 = vector.extract_strided_slice %17 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
      %65 = arith.mulf %63, %64 : vector<4xf16>
      %66 = arith.addf %62, %58 : vector<4xf16>
      %67 = arith.addf %65, %59 : vector<4xf16>
      %68 = vector.extract_strided_slice %21 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
      %69 = vector.extract_strided_slice %20 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
      %70 = arith.mulf %68, %69 : vector<4xf16>
      %71 = vector.extract_strided_slice %21 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
      %72 = vector.extract_strided_slice %20 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
      %73 = arith.mulf %71, %72 : vector<4xf16>
      %74 = arith.addf %70, %66 : vector<4xf16>
      %75 = arith.addf %73, %67 : vector<4xf16>
      %76 = vector.extract_strided_slice %24 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
      %77 = vector.extract_strided_slice %23 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
      %78 = arith.mulf %76, %77 : vector<4xf16>
      %79 = vector.extract_strided_slice %24 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
      %80 = vector.extract_strided_slice %23 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
      %81 = arith.mulf %79, %80 : vector<4xf16>
      %82 = arith.addf %78, %74 : vector<4xf16>
      %83 = arith.addf %81, %75 : vector<4xf16>
      %84 = vector.extract_strided_slice %27 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
      %85 = vector.extract_strided_slice %26 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
      %86 = arith.mulf %84, %85 : vector<4xf16>
      %87 = vector.extract_strided_slice %27 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
      %88 = vector.extract_strided_slice %26 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
      %89 = arith.mulf %87, %88 : vector<4xf16>
      %90 = arith.addf %86, %82 : vector<4xf16>
      %91 = arith.addf %89, %83 : vector<4xf16>
      %92 = vector.reduction <add>, %90 : vector<4xf16> into f16
      %93 = vector.reduction <add>, %91 : vector<4xf16> into f16
      %94 = arith.addf %92, %93 : f16
      %95 = gpu.subgroup_reduce  add %94 : (f16) -> f16
      %96 = arith.addf %95, %cst_0 : f16
      %97 = vector.splat %96 : vector<1xf16>
      %98 = arith.cmpi eq, %0, %c0 : index
      scf.if %98 {
        vector.transfer_write %97, %3[%c0, %workgroup_id_x] {in_bounds = [true]} : vector<1xf16>, memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
      }
      return
    }
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module {
  func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
    %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f16
    %0 = gpu.thread_id  x
    %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %1, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
    %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %2, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
    %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %3, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3584)>()[%0]
    %5 = vector.transfer_read %2[%workgroup_id_x, %4], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %6 = vector.transfer_read %1[%c0, %4], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %7 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3072)>()[%0]
    %8 = vector.transfer_read %2[%workgroup_id_x, %7], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %9 = vector.transfer_read %1[%c0, %7], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %10 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2560)>()[%0]
    %11 = vector.transfer_read %2[%workgroup_id_x, %10], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %12 = vector.transfer_read %1[%c0, %10], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %13 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2048)>()[%0]
    %14 = vector.transfer_read %2[%workgroup_id_x, %13], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %15 = vector.transfer_read %1[%c0, %13], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %16 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1536)>()[%0]
    %17 = vector.transfer_read %2[%workgroup_id_x, %16], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %18 = vector.transfer_read %1[%c0, %16], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %19 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1024)>()[%0]
    %20 = vector.transfer_read %2[%workgroup_id_x, %19], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %21 = vector.transfer_read %1[%c0, %19], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %22 = affine.apply affine_map<()[s0] -> (s0 * 8 + 512)>()[%0]
    %23 = vector.transfer_read %2[%workgroup_id_x, %22], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %24 = vector.transfer_read %1[%c0, %22], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %25 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%0]
    %26 = vector.transfer_read %2[%workgroup_id_x, %25], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %27 = vector.transfer_read %1[%c0, %25], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %28 = vector.extract_strided_slice %6 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %29 = vector.extract_strided_slice %5 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %30 = arith.mulf %28, %29 : vector<4xf16>
    %31 = vector.extract_strided_slice %6 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %32 = vector.extract_strided_slice %5 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %33 = arith.mulf %31, %32 : vector<4xf16>
    %34 = arith.addf %30, %cst : vector<4xf16>
    %35 = arith.addf %33, %cst : vector<4xf16>
    %36 = vector.extract_strided_slice %9 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %37 = vector.extract_strided_slice %8 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %38 = arith.mulf %36, %37 : vector<4xf16>
    %39 = vector.extract_strided_slice %9 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %40 = vector.extract_strided_slice %8 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %41 = arith.mulf %39, %40 : vector<4xf16>
    %42 = arith.addf %38, %34 : vector<4xf16>
    %43 = arith.addf %41, %35 : vector<4xf16>
    %44 = vector.extract_strided_slice %12 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %45 = vector.extract_strided_slice %11 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %46 = arith.mulf %44, %45 : vector<4xf16>
    %47 = vector.extract_strided_slice %12 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %48 = vector.extract_strided_slice %11 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %49 = arith.mulf %47, %48 : vector<4xf16>
    %50 = arith.addf %46, %42 : vector<4xf16>
    %51 = arith.addf %49, %43 : vector<4xf16>
    %52 = vector.extract_strided_slice %15 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %53 = vector.extract_strided_slice %14 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %54 = arith.mulf %52, %53 : vector<4xf16>
    %55 = vector.extract_strided_slice %15 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %56 = vector.extract_strided_slice %14 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %57 = arith.mulf %55, %56 : vector<4xf16>
    %58 = arith.addf %54, %50 : vector<4xf16>
    %59 = arith.addf %57, %51 : vector<4xf16>
    %60 = vector.extract_strided_slice %18 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %61 = vector.extract_strided_slice %17 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %62 = arith.mulf %60, %61 : vector<4xf16>
    %63 = vector.extract_strided_slice %18 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %64 = vector.extract_strided_slice %17 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %65 = arith.mulf %63, %64 : vector<4xf16>
    %66 = arith.addf %62, %58 : vector<4xf16>
    %67 = arith.addf %65, %59 : vector<4xf16>
    %68 = vector.extract_strided_slice %21 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %69 = vector.extract_strided_slice %20 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %70 = arith.mulf %68, %69 : vector<4xf16>
    %71 = vector.extract_strided_slice %21 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %72 = vector.extract_strided_slice %20 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %73 = arith.mulf %71, %72 : vector<4xf16>
    %74 = arith.addf %70, %66 : vector<4xf16>
    %75 = arith.addf %73, %67 : vector<4xf16>
    %76 = vector.extract_strided_slice %24 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %77 = vector.extract_strided_slice %23 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %78 = arith.mulf %76, %77 : vector<4xf16>
    %79 = vector.extract_strided_slice %24 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %80 = vector.extract_strided_slice %23 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %81 = arith.mulf %79, %80 : vector<4xf16>
    %82 = arith.addf %78, %74 : vector<4xf16>
    %83 = arith.addf %81, %75 : vector<4xf16>
    %84 = vector.extract_strided_slice %27 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %85 = vector.extract_strided_slice %26 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %86 = arith.mulf %84, %85 : vector<4xf16>
    %87 = vector.extract_strided_slice %27 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %88 = vector.extract_strided_slice %26 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %89 = arith.mulf %87, %88 : vector<4xf16>
    %90 = arith.addf %86, %82 : vector<4xf16>
    %91 = arith.addf %89, %83 : vector<4xf16>
    %92 = vector.reduction <add>, %90 : vector<4xf16> into f16
    %93 = vector.reduction <add>, %91 : vector<4xf16> into f16
    %94 = arith.addf %92, %93 : f16
    %95 = gpu.subgroup_reduce  add %94 : (f16) -> f16
    %96 = arith.addf %95, %cst_0 : f16
    %97 = vector.splat %96 : vector<1xf16>
    %98 = arith.cmpi eq, %0, %c0 : index
    scf.if %98 {
      vector.transfer_write %97, %3[%c0, %workgroup_id_x] {in_bounds = [true]} : vector<1xf16>, memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    }
    return
  }
}

// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
    %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f16
    %0 = gpu.thread_id  x
    %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %1, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
    %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %2, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
    %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %3, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3584)>()[%0]
    %5 = vector.transfer_read %2[%workgroup_id_x, %4], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %6 = vector.transfer_read %1[%c0, %4], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %7 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3072)>()[%0]
    %8 = vector.transfer_read %2[%workgroup_id_x, %7], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %9 = vector.transfer_read %1[%c0, %7], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %10 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2560)>()[%0]
    %11 = vector.transfer_read %2[%workgroup_id_x, %10], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %12 = vector.transfer_read %1[%c0, %10], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %13 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2048)>()[%0]
    %14 = vector.transfer_read %2[%workgroup_id_x, %13], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %15 = vector.transfer_read %1[%c0, %13], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %16 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1536)>()[%0]
    %17 = vector.transfer_read %2[%workgroup_id_x, %16], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %18 = vector.transfer_read %1[%c0, %16], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %19 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1024)>()[%0]
    %20 = vector.transfer_read %2[%workgroup_id_x, %19], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %21 = vector.transfer_read %1[%c0, %19], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %22 = affine.apply affine_map<()[s0] -> (s0 * 8 + 512)>()[%0]
    %23 = vector.transfer_read %2[%workgroup_id_x, %22], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %24 = vector.transfer_read %1[%c0, %22], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %25 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%0]
    %26 = vector.transfer_read %2[%workgroup_id_x, %25], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %27 = vector.transfer_read %1[%c0, %25], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %28 = vector.extract_strided_slice %6 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %29 = vector.extract_strided_slice %5 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %30 = arith.mulf %28, %29 : vector<4xf16>
    %31 = vector.extract_strided_slice %6 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %32 = vector.extract_strided_slice %5 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %33 = arith.mulf %31, %32 : vector<4xf16>
    %34 = arith.addf %30, %cst : vector<4xf16>
    %35 = arith.addf %33, %cst : vector<4xf16>
    %36 = vector.extract_strided_slice %9 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %37 = vector.extract_strided_slice %8 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %38 = arith.mulf %36, %37 : vector<4xf16>
    %39 = vector.extract_strided_slice %9 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %40 = vector.extract_strided_slice %8 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %41 = arith.mulf %39, %40 : vector<4xf16>
    %42 = arith.addf %38, %34 : vector<4xf16>
    %43 = arith.addf %41, %35 : vector<4xf16>
    %44 = vector.extract_strided_slice %12 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %45 = vector.extract_strided_slice %11 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %46 = arith.mulf %44, %45 : vector<4xf16>
    %47 = vector.extract_strided_slice %12 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %48 = vector.extract_strided_slice %11 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %49 = arith.mulf %47, %48 : vector<4xf16>
    %50 = arith.addf %46, %42 : vector<4xf16>
    %51 = arith.addf %49, %43 : vector<4xf16>
    %52 = vector.extract_strided_slice %15 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %53 = vector.extract_strided_slice %14 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %54 = arith.mulf %52, %53 : vector<4xf16>
    %55 = vector.extract_strided_slice %15 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %56 = vector.extract_strided_slice %14 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %57 = arith.mulf %55, %56 : vector<4xf16>
    %58 = arith.addf %54, %50 : vector<4xf16>
    %59 = arith.addf %57, %51 : vector<4xf16>
    %60 = vector.extract_strided_slice %18 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %61 = vector.extract_strided_slice %17 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %62 = arith.mulf %60, %61 : vector<4xf16>
    %63 = vector.extract_strided_slice %18 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %64 = vector.extract_strided_slice %17 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %65 = arith.mulf %63, %64 : vector<4xf16>
    %66 = arith.addf %62, %58 : vector<4xf16>
    %67 = arith.addf %65, %59 : vector<4xf16>
    %68 = vector.extract_strided_slice %21 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %69 = vector.extract_strided_slice %20 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %70 = arith.mulf %68, %69 : vector<4xf16>
    %71 = vector.extract_strided_slice %21 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %72 = vector.extract_strided_slice %20 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %73 = arith.mulf %71, %72 : vector<4xf16>
    %74 = arith.addf %70, %66 : vector<4xf16>
    %75 = arith.addf %73, %67 : vector<4xf16>
    %76 = vector.extract_strided_slice %24 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %77 = vector.extract_strided_slice %23 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %78 = arith.mulf %76, %77 : vector<4xf16>
    %79 = vector.extract_strided_slice %24 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %80 = vector.extract_strided_slice %23 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %81 = arith.mulf %79, %80 : vector<4xf16>
    %82 = arith.addf %78, %74 : vector<4xf16>
    %83 = arith.addf %81, %75 : vector<4xf16>
    %84 = vector.extract_strided_slice %27 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %85 = vector.extract_strided_slice %26 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %86 = arith.mulf %84, %85 : vector<4xf16>
    %87 = vector.extract_strided_slice %27 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %88 = vector.extract_strided_slice %26 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %89 = arith.mulf %87, %88 : vector<4xf16>
    %90 = arith.addf %86, %82 : vector<4xf16>
    %91 = arith.addf %89, %83 : vector<4xf16>
    %92 = vector.reduction <add>, %90 : vector<4xf16> into f16
    %93 = vector.reduction <add>, %91 : vector<4xf16> into f16
    %94 = arith.addf %92, %93 : f16
    %95 = gpu.subgroup_reduce  add %94 : (f16) -> f16
    %96 = arith.addf %95, %cst_0 : f16
    %97 = vector.splat %96 : vector<1xf16>
    %98 = arith.cmpi eq, %0, %c0 : index
    scf.if %98 {
      vector.transfer_write %97, %3[%c0, %workgroup_id_x] {in_bounds = [true]} : vector<1xf16>, memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    }
    return
  }
}

// -----// IR Dump After ConvertComplexToStandard (convert-complex-to-standard) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f16
  %0 = gpu.thread_id  x
  %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %3, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3584)>()[%0]
  %5 = vector.transfer_read %2[%workgroup_id_x, %4], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %6 = vector.transfer_read %1[%c0, %4], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %7 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3072)>()[%0]
  %8 = vector.transfer_read %2[%workgroup_id_x, %7], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %9 = vector.transfer_read %1[%c0, %7], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %10 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2560)>()[%0]
  %11 = vector.transfer_read %2[%workgroup_id_x, %10], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %12 = vector.transfer_read %1[%c0, %10], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %13 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2048)>()[%0]
  %14 = vector.transfer_read %2[%workgroup_id_x, %13], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %15 = vector.transfer_read %1[%c0, %13], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %16 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1536)>()[%0]
  %17 = vector.transfer_read %2[%workgroup_id_x, %16], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %18 = vector.transfer_read %1[%c0, %16], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %19 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1024)>()[%0]
  %20 = vector.transfer_read %2[%workgroup_id_x, %19], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %21 = vector.transfer_read %1[%c0, %19], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %22 = affine.apply affine_map<()[s0] -> (s0 * 8 + 512)>()[%0]
  %23 = vector.transfer_read %2[%workgroup_id_x, %22], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %24 = vector.transfer_read %1[%c0, %22], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %25 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%0]
  %26 = vector.transfer_read %2[%workgroup_id_x, %25], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %27 = vector.transfer_read %1[%c0, %25], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %28 = vector.extract_strided_slice %6 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %29 = vector.extract_strided_slice %5 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %30 = arith.mulf %28, %29 : vector<4xf16>
  %31 = vector.extract_strided_slice %6 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %32 = vector.extract_strided_slice %5 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %33 = arith.mulf %31, %32 : vector<4xf16>
  %34 = arith.addf %30, %cst : vector<4xf16>
  %35 = arith.addf %33, %cst : vector<4xf16>
  %36 = vector.extract_strided_slice %9 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %37 = vector.extract_strided_slice %8 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %38 = arith.mulf %36, %37 : vector<4xf16>
  %39 = vector.extract_strided_slice %9 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %40 = vector.extract_strided_slice %8 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %41 = arith.mulf %39, %40 : vector<4xf16>
  %42 = arith.addf %38, %34 : vector<4xf16>
  %43 = arith.addf %41, %35 : vector<4xf16>
  %44 = vector.extract_strided_slice %12 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %45 = vector.extract_strided_slice %11 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %46 = arith.mulf %44, %45 : vector<4xf16>
  %47 = vector.extract_strided_slice %12 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %48 = vector.extract_strided_slice %11 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %49 = arith.mulf %47, %48 : vector<4xf16>
  %50 = arith.addf %46, %42 : vector<4xf16>
  %51 = arith.addf %49, %43 : vector<4xf16>
  %52 = vector.extract_strided_slice %15 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %53 = vector.extract_strided_slice %14 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %54 = arith.mulf %52, %53 : vector<4xf16>
  %55 = vector.extract_strided_slice %15 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %56 = vector.extract_strided_slice %14 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %57 = arith.mulf %55, %56 : vector<4xf16>
  %58 = arith.addf %54, %50 : vector<4xf16>
  %59 = arith.addf %57, %51 : vector<4xf16>
  %60 = vector.extract_strided_slice %18 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %61 = vector.extract_strided_slice %17 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %62 = arith.mulf %60, %61 : vector<4xf16>
  %63 = vector.extract_strided_slice %18 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %64 = vector.extract_strided_slice %17 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %65 = arith.mulf %63, %64 : vector<4xf16>
  %66 = arith.addf %62, %58 : vector<4xf16>
  %67 = arith.addf %65, %59 : vector<4xf16>
  %68 = vector.extract_strided_slice %21 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %69 = vector.extract_strided_slice %20 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %70 = arith.mulf %68, %69 : vector<4xf16>
  %71 = vector.extract_strided_slice %21 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %72 = vector.extract_strided_slice %20 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %73 = arith.mulf %71, %72 : vector<4xf16>
  %74 = arith.addf %70, %66 : vector<4xf16>
  %75 = arith.addf %73, %67 : vector<4xf16>
  %76 = vector.extract_strided_slice %24 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %77 = vector.extract_strided_slice %23 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %78 = arith.mulf %76, %77 : vector<4xf16>
  %79 = vector.extract_strided_slice %24 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %80 = vector.extract_strided_slice %23 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %81 = arith.mulf %79, %80 : vector<4xf16>
  %82 = arith.addf %78, %74 : vector<4xf16>
  %83 = arith.addf %81, %75 : vector<4xf16>
  %84 = vector.extract_strided_slice %27 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %85 = vector.extract_strided_slice %26 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %86 = arith.mulf %84, %85 : vector<4xf16>
  %87 = vector.extract_strided_slice %27 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %88 = vector.extract_strided_slice %26 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %89 = arith.mulf %87, %88 : vector<4xf16>
  %90 = arith.addf %86, %82 : vector<4xf16>
  %91 = arith.addf %89, %83 : vector<4xf16>
  %92 = vector.reduction <add>, %90 : vector<4xf16> into f16
  %93 = vector.reduction <add>, %91 : vector<4xf16> into f16
  %94 = arith.addf %92, %93 : f16
  %95 = gpu.subgroup_reduce  add %94 : (f16) -> f16
  %96 = arith.addf %95, %cst_0 : f16
  %97 = vector.splat %96 : vector<1xf16>
  %98 = arith.cmpi eq, %0, %c0 : index
  scf.if %98 {
    vector.transfer_write %97, %3[%c0, %workgroup_id_x] {in_bounds = [true]} : vector<1xf16>, memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  }
  return
}

// -----// IR Dump After PolynomialApproximationPass (iree-codegen-polynomial-approximation) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f16
  %0 = gpu.thread_id  x
  %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %3, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3584)>()[%0]
  %5 = vector.transfer_read %2[%workgroup_id_x, %4], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %6 = vector.transfer_read %1[%c0, %4], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %7 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3072)>()[%0]
  %8 = vector.transfer_read %2[%workgroup_id_x, %7], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %9 = vector.transfer_read %1[%c0, %7], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %10 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2560)>()[%0]
  %11 = vector.transfer_read %2[%workgroup_id_x, %10], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %12 = vector.transfer_read %1[%c0, %10], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %13 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2048)>()[%0]
  %14 = vector.transfer_read %2[%workgroup_id_x, %13], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %15 = vector.transfer_read %1[%c0, %13], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %16 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1536)>()[%0]
  %17 = vector.transfer_read %2[%workgroup_id_x, %16], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %18 = vector.transfer_read %1[%c0, %16], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %19 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1024)>()[%0]
  %20 = vector.transfer_read %2[%workgroup_id_x, %19], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %21 = vector.transfer_read %1[%c0, %19], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %22 = affine.apply affine_map<()[s0] -> (s0 * 8 + 512)>()[%0]
  %23 = vector.transfer_read %2[%workgroup_id_x, %22], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %24 = vector.transfer_read %1[%c0, %22], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %25 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%0]
  %26 = vector.transfer_read %2[%workgroup_id_x, %25], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %27 = vector.transfer_read %1[%c0, %25], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %28 = vector.extract_strided_slice %6 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %29 = vector.extract_strided_slice %5 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %30 = arith.mulf %28, %29 : vector<4xf16>
  %31 = vector.extract_strided_slice %6 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %32 = vector.extract_strided_slice %5 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %33 = arith.mulf %31, %32 : vector<4xf16>
  %34 = arith.addf %30, %cst : vector<4xf16>
  %35 = arith.addf %33, %cst : vector<4xf16>
  %36 = vector.extract_strided_slice %9 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %37 = vector.extract_strided_slice %8 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %38 = arith.mulf %36, %37 : vector<4xf16>
  %39 = vector.extract_strided_slice %9 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %40 = vector.extract_strided_slice %8 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %41 = arith.mulf %39, %40 : vector<4xf16>
  %42 = arith.addf %38, %34 : vector<4xf16>
  %43 = arith.addf %41, %35 : vector<4xf16>
  %44 = vector.extract_strided_slice %12 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %45 = vector.extract_strided_slice %11 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %46 = arith.mulf %44, %45 : vector<4xf16>
  %47 = vector.extract_strided_slice %12 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %48 = vector.extract_strided_slice %11 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %49 = arith.mulf %47, %48 : vector<4xf16>
  %50 = arith.addf %46, %42 : vector<4xf16>
  %51 = arith.addf %49, %43 : vector<4xf16>
  %52 = vector.extract_strided_slice %15 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %53 = vector.extract_strided_slice %14 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %54 = arith.mulf %52, %53 : vector<4xf16>
  %55 = vector.extract_strided_slice %15 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %56 = vector.extract_strided_slice %14 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %57 = arith.mulf %55, %56 : vector<4xf16>
  %58 = arith.addf %54, %50 : vector<4xf16>
  %59 = arith.addf %57, %51 : vector<4xf16>
  %60 = vector.extract_strided_slice %18 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %61 = vector.extract_strided_slice %17 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %62 = arith.mulf %60, %61 : vector<4xf16>
  %63 = vector.extract_strided_slice %18 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %64 = vector.extract_strided_slice %17 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %65 = arith.mulf %63, %64 : vector<4xf16>
  %66 = arith.addf %62, %58 : vector<4xf16>
  %67 = arith.addf %65, %59 : vector<4xf16>
  %68 = vector.extract_strided_slice %21 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %69 = vector.extract_strided_slice %20 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %70 = arith.mulf %68, %69 : vector<4xf16>
  %71 = vector.extract_strided_slice %21 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %72 = vector.extract_strided_slice %20 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %73 = arith.mulf %71, %72 : vector<4xf16>
  %74 = arith.addf %70, %66 : vector<4xf16>
  %75 = arith.addf %73, %67 : vector<4xf16>
  %76 = vector.extract_strided_slice %24 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %77 = vector.extract_strided_slice %23 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %78 = arith.mulf %76, %77 : vector<4xf16>
  %79 = vector.extract_strided_slice %24 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %80 = vector.extract_strided_slice %23 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %81 = arith.mulf %79, %80 : vector<4xf16>
  %82 = arith.addf %78, %74 : vector<4xf16>
  %83 = arith.addf %81, %75 : vector<4xf16>
  %84 = vector.extract_strided_slice %27 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %85 = vector.extract_strided_slice %26 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %86 = arith.mulf %84, %85 : vector<4xf16>
  %87 = vector.extract_strided_slice %27 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %88 = vector.extract_strided_slice %26 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %89 = arith.mulf %87, %88 : vector<4xf16>
  %90 = arith.addf %86, %82 : vector<4xf16>
  %91 = arith.addf %89, %83 : vector<4xf16>
  %92 = vector.reduction <add>, %90 : vector<4xf16> into f16
  %93 = vector.reduction <add>, %91 : vector<4xf16> into f16
  %94 = arith.addf %92, %93 : f16
  %95 = gpu.subgroup_reduce  add %94 : (f16) -> f16
  %96 = arith.addf %95, %cst_0 : f16
  %97 = vector.splat %96 : vector<1xf16>
  %98 = arith.cmpi eq, %0, %c0 : index
  scf.if %98 {
    vector.transfer_write %97, %3[%c0, %workgroup_id_x] {in_bounds = [true]} : vector<1xf16>, memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  }
  return
}

// -----// IR Dump After PadDynamicAlloc (iree-codegen-pad-dynamic-alloc) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f16
  %0 = gpu.thread_id  x
  %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %3, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3584)>()[%0]
  %5 = vector.transfer_read %2[%workgroup_id_x, %4], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %6 = vector.transfer_read %1[%c0, %4], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %7 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3072)>()[%0]
  %8 = vector.transfer_read %2[%workgroup_id_x, %7], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %9 = vector.transfer_read %1[%c0, %7], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %10 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2560)>()[%0]
  %11 = vector.transfer_read %2[%workgroup_id_x, %10], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %12 = vector.transfer_read %1[%c0, %10], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %13 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2048)>()[%0]
  %14 = vector.transfer_read %2[%workgroup_id_x, %13], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %15 = vector.transfer_read %1[%c0, %13], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %16 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1536)>()[%0]
  %17 = vector.transfer_read %2[%workgroup_id_x, %16], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %18 = vector.transfer_read %1[%c0, %16], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %19 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1024)>()[%0]
  %20 = vector.transfer_read %2[%workgroup_id_x, %19], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %21 = vector.transfer_read %1[%c0, %19], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %22 = affine.apply affine_map<()[s0] -> (s0 * 8 + 512)>()[%0]
  %23 = vector.transfer_read %2[%workgroup_id_x, %22], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %24 = vector.transfer_read %1[%c0, %22], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %25 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%0]
  %26 = vector.transfer_read %2[%workgroup_id_x, %25], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %27 = vector.transfer_read %1[%c0, %25], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %28 = vector.extract_strided_slice %6 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %29 = vector.extract_strided_slice %5 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %30 = arith.mulf %28, %29 : vector<4xf16>
  %31 = vector.extract_strided_slice %6 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %32 = vector.extract_strided_slice %5 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %33 = arith.mulf %31, %32 : vector<4xf16>
  %34 = arith.addf %30, %cst : vector<4xf16>
  %35 = arith.addf %33, %cst : vector<4xf16>
  %36 = vector.extract_strided_slice %9 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %37 = vector.extract_strided_slice %8 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %38 = arith.mulf %36, %37 : vector<4xf16>
  %39 = vector.extract_strided_slice %9 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %40 = vector.extract_strided_slice %8 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %41 = arith.mulf %39, %40 : vector<4xf16>
  %42 = arith.addf %38, %34 : vector<4xf16>
  %43 = arith.addf %41, %35 : vector<4xf16>
  %44 = vector.extract_strided_slice %12 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %45 = vector.extract_strided_slice %11 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %46 = arith.mulf %44, %45 : vector<4xf16>
  %47 = vector.extract_strided_slice %12 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %48 = vector.extract_strided_slice %11 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %49 = arith.mulf %47, %48 : vector<4xf16>
  %50 = arith.addf %46, %42 : vector<4xf16>
  %51 = arith.addf %49, %43 : vector<4xf16>
  %52 = vector.extract_strided_slice %15 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %53 = vector.extract_strided_slice %14 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %54 = arith.mulf %52, %53 : vector<4xf16>
  %55 = vector.extract_strided_slice %15 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %56 = vector.extract_strided_slice %14 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %57 = arith.mulf %55, %56 : vector<4xf16>
  %58 = arith.addf %54, %50 : vector<4xf16>
  %59 = arith.addf %57, %51 : vector<4xf16>
  %60 = vector.extract_strided_slice %18 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %61 = vector.extract_strided_slice %17 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %62 = arith.mulf %60, %61 : vector<4xf16>
  %63 = vector.extract_strided_slice %18 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %64 = vector.extract_strided_slice %17 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %65 = arith.mulf %63, %64 : vector<4xf16>
  %66 = arith.addf %62, %58 : vector<4xf16>
  %67 = arith.addf %65, %59 : vector<4xf16>
  %68 = vector.extract_strided_slice %21 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %69 = vector.extract_strided_slice %20 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %70 = arith.mulf %68, %69 : vector<4xf16>
  %71 = vector.extract_strided_slice %21 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %72 = vector.extract_strided_slice %20 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %73 = arith.mulf %71, %72 : vector<4xf16>
  %74 = arith.addf %70, %66 : vector<4xf16>
  %75 = arith.addf %73, %67 : vector<4xf16>
  %76 = vector.extract_strided_slice %24 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %77 = vector.extract_strided_slice %23 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %78 = arith.mulf %76, %77 : vector<4xf16>
  %79 = vector.extract_strided_slice %24 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %80 = vector.extract_strided_slice %23 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %81 = arith.mulf %79, %80 : vector<4xf16>
  %82 = arith.addf %78, %74 : vector<4xf16>
  %83 = arith.addf %81, %75 : vector<4xf16>
  %84 = vector.extract_strided_slice %27 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %85 = vector.extract_strided_slice %26 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %86 = arith.mulf %84, %85 : vector<4xf16>
  %87 = vector.extract_strided_slice %27 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %88 = vector.extract_strided_slice %26 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %89 = arith.mulf %87, %88 : vector<4xf16>
  %90 = arith.addf %86, %82 : vector<4xf16>
  %91 = arith.addf %89, %83 : vector<4xf16>
  %92 = vector.reduction <add>, %90 : vector<4xf16> into f16
  %93 = vector.reduction <add>, %91 : vector<4xf16> into f16
  %94 = arith.addf %92, %93 : f16
  %95 = gpu.subgroup_reduce  add %94 : (f16) -> f16
  %96 = arith.addf %95, %cst_0 : f16
  %97 = vector.splat %96 : vector<1xf16>
  %98 = arith.cmpi eq, %0, %c0 : index
  scf.if %98 {
    vector.transfer_write %97, %3[%c0, %workgroup_id_x] {in_bounds = [true]} : vector<1xf16>, memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  }
  return
}

// -----// IR Dump After GPUCheckResourceUsage (iree-codegen-gpu-check-resource-usage) //----- //
module {
  func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
    %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f16
    %0 = gpu.thread_id  x
    %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %1, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
    %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %2, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
    %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %3, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3584)>()[%0]
    %5 = vector.transfer_read %2[%workgroup_id_x, %4], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %6 = vector.transfer_read %1[%c0, %4], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %7 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3072)>()[%0]
    %8 = vector.transfer_read %2[%workgroup_id_x, %7], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %9 = vector.transfer_read %1[%c0, %7], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %10 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2560)>()[%0]
    %11 = vector.transfer_read %2[%workgroup_id_x, %10], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %12 = vector.transfer_read %1[%c0, %10], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %13 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2048)>()[%0]
    %14 = vector.transfer_read %2[%workgroup_id_x, %13], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %15 = vector.transfer_read %1[%c0, %13], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %16 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1536)>()[%0]
    %17 = vector.transfer_read %2[%workgroup_id_x, %16], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %18 = vector.transfer_read %1[%c0, %16], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %19 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1024)>()[%0]
    %20 = vector.transfer_read %2[%workgroup_id_x, %19], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %21 = vector.transfer_read %1[%c0, %19], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %22 = affine.apply affine_map<()[s0] -> (s0 * 8 + 512)>()[%0]
    %23 = vector.transfer_read %2[%workgroup_id_x, %22], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %24 = vector.transfer_read %1[%c0, %22], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %25 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%0]
    %26 = vector.transfer_read %2[%workgroup_id_x, %25], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %27 = vector.transfer_read %1[%c0, %25], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %28 = vector.extract_strided_slice %6 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %29 = vector.extract_strided_slice %5 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %30 = arith.mulf %28, %29 : vector<4xf16>
    %31 = vector.extract_strided_slice %6 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %32 = vector.extract_strided_slice %5 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %33 = arith.mulf %31, %32 : vector<4xf16>
    %34 = arith.addf %30, %cst : vector<4xf16>
    %35 = arith.addf %33, %cst : vector<4xf16>
    %36 = vector.extract_strided_slice %9 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %37 = vector.extract_strided_slice %8 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %38 = arith.mulf %36, %37 : vector<4xf16>
    %39 = vector.extract_strided_slice %9 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %40 = vector.extract_strided_slice %8 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %41 = arith.mulf %39, %40 : vector<4xf16>
    %42 = arith.addf %38, %34 : vector<4xf16>
    %43 = arith.addf %41, %35 : vector<4xf16>
    %44 = vector.extract_strided_slice %12 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %45 = vector.extract_strided_slice %11 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %46 = arith.mulf %44, %45 : vector<4xf16>
    %47 = vector.extract_strided_slice %12 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %48 = vector.extract_strided_slice %11 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %49 = arith.mulf %47, %48 : vector<4xf16>
    %50 = arith.addf %46, %42 : vector<4xf16>
    %51 = arith.addf %49, %43 : vector<4xf16>
    %52 = vector.extract_strided_slice %15 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %53 = vector.extract_strided_slice %14 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %54 = arith.mulf %52, %53 : vector<4xf16>
    %55 = vector.extract_strided_slice %15 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %56 = vector.extract_strided_slice %14 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %57 = arith.mulf %55, %56 : vector<4xf16>
    %58 = arith.addf %54, %50 : vector<4xf16>
    %59 = arith.addf %57, %51 : vector<4xf16>
    %60 = vector.extract_strided_slice %18 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %61 = vector.extract_strided_slice %17 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %62 = arith.mulf %60, %61 : vector<4xf16>
    %63 = vector.extract_strided_slice %18 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %64 = vector.extract_strided_slice %17 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %65 = arith.mulf %63, %64 : vector<4xf16>
    %66 = arith.addf %62, %58 : vector<4xf16>
    %67 = arith.addf %65, %59 : vector<4xf16>
    %68 = vector.extract_strided_slice %21 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %69 = vector.extract_strided_slice %20 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %70 = arith.mulf %68, %69 : vector<4xf16>
    %71 = vector.extract_strided_slice %21 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %72 = vector.extract_strided_slice %20 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %73 = arith.mulf %71, %72 : vector<4xf16>
    %74 = arith.addf %70, %66 : vector<4xf16>
    %75 = arith.addf %73, %67 : vector<4xf16>
    %76 = vector.extract_strided_slice %24 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %77 = vector.extract_strided_slice %23 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %78 = arith.mulf %76, %77 : vector<4xf16>
    %79 = vector.extract_strided_slice %24 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %80 = vector.extract_strided_slice %23 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %81 = arith.mulf %79, %80 : vector<4xf16>
    %82 = arith.addf %78, %74 : vector<4xf16>
    %83 = arith.addf %81, %75 : vector<4xf16>
    %84 = vector.extract_strided_slice %27 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %85 = vector.extract_strided_slice %26 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %86 = arith.mulf %84, %85 : vector<4xf16>
    %87 = vector.extract_strided_slice %27 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %88 = vector.extract_strided_slice %26 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %89 = arith.mulf %87, %88 : vector<4xf16>
    %90 = arith.addf %86, %82 : vector<4xf16>
    %91 = arith.addf %89, %83 : vector<4xf16>
    %92 = vector.reduction <add>, %90 : vector<4xf16> into f16
    %93 = vector.reduction <add>, %91 : vector<4xf16> into f16
    %94 = arith.addf %92, %93 : f16
    %95 = gpu.subgroup_reduce  add %94 : (f16) -> f16
    %96 = arith.addf %95, %cst_0 : f16
    %97 = vector.splat %96 : vector<1xf16>
    %98 = arith.cmpi eq, %0, %c0 : index
    scf.if %98 {
      vector.transfer_write %97, %3[%c0, %workgroup_id_x] {in_bounds = [true]} : vector<1xf16>, memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    }
    return
  }
}

// -----// IR Dump After FoldMemRefAliasOps (fold-memref-alias-ops) //----- //
module {
  func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
    %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f16
    %0 = gpu.thread_id  x
    %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %1, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
    %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %2, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
    %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %3, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3584)>()[%0]
    %5 = vector.transfer_read %2[%workgroup_id_x, %4], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %6 = vector.transfer_read %1[%c0, %4], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %7 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3072)>()[%0]
    %8 = vector.transfer_read %2[%workgroup_id_x, %7], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %9 = vector.transfer_read %1[%c0, %7], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %10 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2560)>()[%0]
    %11 = vector.transfer_read %2[%workgroup_id_x, %10], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %12 = vector.transfer_read %1[%c0, %10], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %13 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2048)>()[%0]
    %14 = vector.transfer_read %2[%workgroup_id_x, %13], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %15 = vector.transfer_read %1[%c0, %13], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %16 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1536)>()[%0]
    %17 = vector.transfer_read %2[%workgroup_id_x, %16], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %18 = vector.transfer_read %1[%c0, %16], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %19 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1024)>()[%0]
    %20 = vector.transfer_read %2[%workgroup_id_x, %19], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %21 = vector.transfer_read %1[%c0, %19], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %22 = affine.apply affine_map<()[s0] -> (s0 * 8 + 512)>()[%0]
    %23 = vector.transfer_read %2[%workgroup_id_x, %22], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %24 = vector.transfer_read %1[%c0, %22], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %25 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%0]
    %26 = vector.transfer_read %2[%workgroup_id_x, %25], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %27 = vector.transfer_read %1[%c0, %25], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %28 = vector.extract_strided_slice %6 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %29 = vector.extract_strided_slice %5 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %30 = arith.mulf %28, %29 : vector<4xf16>
    %31 = vector.extract_strided_slice %6 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %32 = vector.extract_strided_slice %5 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %33 = arith.mulf %31, %32 : vector<4xf16>
    %34 = arith.addf %30, %cst : vector<4xf16>
    %35 = arith.addf %33, %cst : vector<4xf16>
    %36 = vector.extract_strided_slice %9 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %37 = vector.extract_strided_slice %8 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %38 = arith.mulf %36, %37 : vector<4xf16>
    %39 = vector.extract_strided_slice %9 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %40 = vector.extract_strided_slice %8 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %41 = arith.mulf %39, %40 : vector<4xf16>
    %42 = arith.addf %38, %34 : vector<4xf16>
    %43 = arith.addf %41, %35 : vector<4xf16>
    %44 = vector.extract_strided_slice %12 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %45 = vector.extract_strided_slice %11 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %46 = arith.mulf %44, %45 : vector<4xf16>
    %47 = vector.extract_strided_slice %12 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %48 = vector.extract_strided_slice %11 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %49 = arith.mulf %47, %48 : vector<4xf16>
    %50 = arith.addf %46, %42 : vector<4xf16>
    %51 = arith.addf %49, %43 : vector<4xf16>
    %52 = vector.extract_strided_slice %15 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %53 = vector.extract_strided_slice %14 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %54 = arith.mulf %52, %53 : vector<4xf16>
    %55 = vector.extract_strided_slice %15 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %56 = vector.extract_strided_slice %14 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %57 = arith.mulf %55, %56 : vector<4xf16>
    %58 = arith.addf %54, %50 : vector<4xf16>
    %59 = arith.addf %57, %51 : vector<4xf16>
    %60 = vector.extract_strided_slice %18 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %61 = vector.extract_strided_slice %17 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %62 = arith.mulf %60, %61 : vector<4xf16>
    %63 = vector.extract_strided_slice %18 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %64 = vector.extract_strided_slice %17 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %65 = arith.mulf %63, %64 : vector<4xf16>
    %66 = arith.addf %62, %58 : vector<4xf16>
    %67 = arith.addf %65, %59 : vector<4xf16>
    %68 = vector.extract_strided_slice %21 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %69 = vector.extract_strided_slice %20 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %70 = arith.mulf %68, %69 : vector<4xf16>
    %71 = vector.extract_strided_slice %21 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %72 = vector.extract_strided_slice %20 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %73 = arith.mulf %71, %72 : vector<4xf16>
    %74 = arith.addf %70, %66 : vector<4xf16>
    %75 = arith.addf %73, %67 : vector<4xf16>
    %76 = vector.extract_strided_slice %24 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %77 = vector.extract_strided_slice %23 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %78 = arith.mulf %76, %77 : vector<4xf16>
    %79 = vector.extract_strided_slice %24 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %80 = vector.extract_strided_slice %23 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %81 = arith.mulf %79, %80 : vector<4xf16>
    %82 = arith.addf %78, %74 : vector<4xf16>
    %83 = arith.addf %81, %75 : vector<4xf16>
    %84 = vector.extract_strided_slice %27 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %85 = vector.extract_strided_slice %26 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %86 = arith.mulf %84, %85 : vector<4xf16>
    %87 = vector.extract_strided_slice %27 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %88 = vector.extract_strided_slice %26 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %89 = arith.mulf %87, %88 : vector<4xf16>
    %90 = arith.addf %86, %82 : vector<4xf16>
    %91 = arith.addf %89, %83 : vector<4xf16>
    %92 = vector.reduction <add>, %90 : vector<4xf16> into f16
    %93 = vector.reduction <add>, %91 : vector<4xf16> into f16
    %94 = arith.addf %92, %93 : f16
    %95 = gpu.subgroup_reduce  add %94 : (f16) -> f16
    %96 = arith.addf %95, %cst_0 : f16
    %97 = vector.splat %96 : vector<1xf16>
    %98 = arith.cmpi eq, %0, %c0 : index
    scf.if %98 {
      vector.transfer_write %97, %3[%c0, %workgroup_id_x] {in_bounds = [true]} : vector<1xf16>, memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    }
    return
  }
}

// -----// IR Dump After EmulateNarrowType (iree-codegen-emulate-narrow-type) //----- //
module {
  func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
    %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f16
    %0 = gpu.thread_id  x
    %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %1, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
    %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %2, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
    %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %3, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3584)>()[%0]
    %5 = vector.transfer_read %2[%workgroup_id_x, %4], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %6 = vector.transfer_read %1[%c0, %4], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %7 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3072)>()[%0]
    %8 = vector.transfer_read %2[%workgroup_id_x, %7], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %9 = vector.transfer_read %1[%c0, %7], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %10 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2560)>()[%0]
    %11 = vector.transfer_read %2[%workgroup_id_x, %10], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %12 = vector.transfer_read %1[%c0, %10], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %13 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2048)>()[%0]
    %14 = vector.transfer_read %2[%workgroup_id_x, %13], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %15 = vector.transfer_read %1[%c0, %13], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %16 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1536)>()[%0]
    %17 = vector.transfer_read %2[%workgroup_id_x, %16], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %18 = vector.transfer_read %1[%c0, %16], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %19 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1024)>()[%0]
    %20 = vector.transfer_read %2[%workgroup_id_x, %19], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %21 = vector.transfer_read %1[%c0, %19], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %22 = affine.apply affine_map<()[s0] -> (s0 * 8 + 512)>()[%0]
    %23 = vector.transfer_read %2[%workgroup_id_x, %22], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %24 = vector.transfer_read %1[%c0, %22], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %25 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%0]
    %26 = vector.transfer_read %2[%workgroup_id_x, %25], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %27 = vector.transfer_read %1[%c0, %25], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %28 = vector.extract_strided_slice %6 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %29 = vector.extract_strided_slice %5 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %30 = arith.mulf %28, %29 : vector<4xf16>
    %31 = vector.extract_strided_slice %6 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %32 = vector.extract_strided_slice %5 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %33 = arith.mulf %31, %32 : vector<4xf16>
    %34 = arith.addf %30, %cst : vector<4xf16>
    %35 = arith.addf %33, %cst : vector<4xf16>
    %36 = vector.extract_strided_slice %9 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %37 = vector.extract_strided_slice %8 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %38 = arith.mulf %36, %37 : vector<4xf16>
    %39 = vector.extract_strided_slice %9 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %40 = vector.extract_strided_slice %8 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %41 = arith.mulf %39, %40 : vector<4xf16>
    %42 = arith.addf %38, %34 : vector<4xf16>
    %43 = arith.addf %41, %35 : vector<4xf16>
    %44 = vector.extract_strided_slice %12 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %45 = vector.extract_strided_slice %11 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %46 = arith.mulf %44, %45 : vector<4xf16>
    %47 = vector.extract_strided_slice %12 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %48 = vector.extract_strided_slice %11 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %49 = arith.mulf %47, %48 : vector<4xf16>
    %50 = arith.addf %46, %42 : vector<4xf16>
    %51 = arith.addf %49, %43 : vector<4xf16>
    %52 = vector.extract_strided_slice %15 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %53 = vector.extract_strided_slice %14 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %54 = arith.mulf %52, %53 : vector<4xf16>
    %55 = vector.extract_strided_slice %15 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %56 = vector.extract_strided_slice %14 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %57 = arith.mulf %55, %56 : vector<4xf16>
    %58 = arith.addf %54, %50 : vector<4xf16>
    %59 = arith.addf %57, %51 : vector<4xf16>
    %60 = vector.extract_strided_slice %18 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %61 = vector.extract_strided_slice %17 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %62 = arith.mulf %60, %61 : vector<4xf16>
    %63 = vector.extract_strided_slice %18 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %64 = vector.extract_strided_slice %17 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %65 = arith.mulf %63, %64 : vector<4xf16>
    %66 = arith.addf %62, %58 : vector<4xf16>
    %67 = arith.addf %65, %59 : vector<4xf16>
    %68 = vector.extract_strided_slice %21 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %69 = vector.extract_strided_slice %20 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %70 = arith.mulf %68, %69 : vector<4xf16>
    %71 = vector.extract_strided_slice %21 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %72 = vector.extract_strided_slice %20 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %73 = arith.mulf %71, %72 : vector<4xf16>
    %74 = arith.addf %70, %66 : vector<4xf16>
    %75 = arith.addf %73, %67 : vector<4xf16>
    %76 = vector.extract_strided_slice %24 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %77 = vector.extract_strided_slice %23 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %78 = arith.mulf %76, %77 : vector<4xf16>
    %79 = vector.extract_strided_slice %24 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %80 = vector.extract_strided_slice %23 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %81 = arith.mulf %79, %80 : vector<4xf16>
    %82 = arith.addf %78, %74 : vector<4xf16>
    %83 = arith.addf %81, %75 : vector<4xf16>
    %84 = vector.extract_strided_slice %27 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %85 = vector.extract_strided_slice %26 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %86 = arith.mulf %84, %85 : vector<4xf16>
    %87 = vector.extract_strided_slice %27 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %88 = vector.extract_strided_slice %26 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %89 = arith.mulf %87, %88 : vector<4xf16>
    %90 = arith.addf %86, %82 : vector<4xf16>
    %91 = arith.addf %89, %83 : vector<4xf16>
    %92 = vector.reduction <add>, %90 : vector<4xf16> into f16
    %93 = vector.reduction <add>, %91 : vector<4xf16> into f16
    %94 = arith.addf %92, %93 : f16
    %95 = gpu.subgroup_reduce  add %94 : (f16) -> f16
    %96 = arith.addf %95, %cst_0 : f16
    %97 = vector.splat %96 : vector<1xf16>
    %98 = arith.cmpi eq, %0, %c0 : index
    scf.if %98 {
      vector.transfer_write %97, %3[%c0, %workgroup_id_x] {in_bounds = [true]} : vector<1xf16>, memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    }
    return
  }
}

// -----// IR Dump After ExpandOps (memref-expand) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f16
  %0 = gpu.thread_id  x
  %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
  %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %3, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3584)>()[%0]
  %5 = vector.transfer_read %2[%workgroup_id_x, %4], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %6 = vector.transfer_read %1[%c0, %4], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %7 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3072)>()[%0]
  %8 = vector.transfer_read %2[%workgroup_id_x, %7], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %9 = vector.transfer_read %1[%c0, %7], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %10 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2560)>()[%0]
  %11 = vector.transfer_read %2[%workgroup_id_x, %10], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %12 = vector.transfer_read %1[%c0, %10], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %13 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2048)>()[%0]
  %14 = vector.transfer_read %2[%workgroup_id_x, %13], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %15 = vector.transfer_read %1[%c0, %13], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %16 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1536)>()[%0]
  %17 = vector.transfer_read %2[%workgroup_id_x, %16], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %18 = vector.transfer_read %1[%c0, %16], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %19 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1024)>()[%0]
  %20 = vector.transfer_read %2[%workgroup_id_x, %19], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %21 = vector.transfer_read %1[%c0, %19], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %22 = affine.apply affine_map<()[s0] -> (s0 * 8 + 512)>()[%0]
  %23 = vector.transfer_read %2[%workgroup_id_x, %22], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %24 = vector.transfer_read %1[%c0, %22], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %25 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%0]
  %26 = vector.transfer_read %2[%workgroup_id_x, %25], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %27 = vector.transfer_read %1[%c0, %25], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
  %28 = vector.extract_strided_slice %6 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %29 = vector.extract_strided_slice %5 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %30 = arith.mulf %28, %29 : vector<4xf16>
  %31 = vector.extract_strided_slice %6 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %32 = vector.extract_strided_slice %5 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %33 = arith.mulf %31, %32 : vector<4xf16>
  %34 = arith.addf %30, %cst : vector<4xf16>
  %35 = arith.addf %33, %cst : vector<4xf16>
  %36 = vector.extract_strided_slice %9 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %37 = vector.extract_strided_slice %8 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %38 = arith.mulf %36, %37 : vector<4xf16>
  %39 = vector.extract_strided_slice %9 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %40 = vector.extract_strided_slice %8 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %41 = arith.mulf %39, %40 : vector<4xf16>
  %42 = arith.addf %38, %34 : vector<4xf16>
  %43 = arith.addf %41, %35 : vector<4xf16>
  %44 = vector.extract_strided_slice %12 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %45 = vector.extract_strided_slice %11 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %46 = arith.mulf %44, %45 : vector<4xf16>
  %47 = vector.extract_strided_slice %12 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %48 = vector.extract_strided_slice %11 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %49 = arith.mulf %47, %48 : vector<4xf16>
  %50 = arith.addf %46, %42 : vector<4xf16>
  %51 = arith.addf %49, %43 : vector<4xf16>
  %52 = vector.extract_strided_slice %15 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %53 = vector.extract_strided_slice %14 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %54 = arith.mulf %52, %53 : vector<4xf16>
  %55 = vector.extract_strided_slice %15 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %56 = vector.extract_strided_slice %14 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %57 = arith.mulf %55, %56 : vector<4xf16>
  %58 = arith.addf %54, %50 : vector<4xf16>
  %59 = arith.addf %57, %51 : vector<4xf16>
  %60 = vector.extract_strided_slice %18 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %61 = vector.extract_strided_slice %17 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %62 = arith.mulf %60, %61 : vector<4xf16>
  %63 = vector.extract_strided_slice %18 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %64 = vector.extract_strided_slice %17 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %65 = arith.mulf %63, %64 : vector<4xf16>
  %66 = arith.addf %62, %58 : vector<4xf16>
  %67 = arith.addf %65, %59 : vector<4xf16>
  %68 = vector.extract_strided_slice %21 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %69 = vector.extract_strided_slice %20 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %70 = arith.mulf %68, %69 : vector<4xf16>
  %71 = vector.extract_strided_slice %21 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %72 = vector.extract_strided_slice %20 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %73 = arith.mulf %71, %72 : vector<4xf16>
  %74 = arith.addf %70, %66 : vector<4xf16>
  %75 = arith.addf %73, %67 : vector<4xf16>
  %76 = vector.extract_strided_slice %24 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %77 = vector.extract_strided_slice %23 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %78 = arith.mulf %76, %77 : vector<4xf16>
  %79 = vector.extract_strided_slice %24 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %80 = vector.extract_strided_slice %23 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %81 = arith.mulf %79, %80 : vector<4xf16>
  %82 = arith.addf %78, %74 : vector<4xf16>
  %83 = arith.addf %81, %75 : vector<4xf16>
  %84 = vector.extract_strided_slice %27 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %85 = vector.extract_strided_slice %26 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %86 = arith.mulf %84, %85 : vector<4xf16>
  %87 = vector.extract_strided_slice %27 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %88 = vector.extract_strided_slice %26 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %89 = arith.mulf %87, %88 : vector<4xf16>
  %90 = arith.addf %86, %82 : vector<4xf16>
  %91 = arith.addf %89, %83 : vector<4xf16>
  %92 = vector.reduction <add>, %90 : vector<4xf16> into f16
  %93 = vector.reduction <add>, %91 : vector<4xf16> into f16
  %94 = arith.addf %92, %93 : f16
  %95 = gpu.subgroup_reduce  add %94 : (f16) -> f16
  %96 = arith.addf %95, %cst_0 : f16
  %97 = vector.splat %96 : vector<1xf16>
  %98 = arith.cmpi eq, %0, %c0 : index
  scf.if %98 {
    vector.transfer_write %97, %3[%c0, %workgroup_id_x] {in_bounds = [true]} : vector<1xf16>, memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module {
  func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
    %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f16
    %0 = gpu.thread_id  x
    %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %1, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
    %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %2, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
    %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %3, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3584)>()[%0]
    %5 = vector.transfer_read %2[%workgroup_id_x, %4], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %6 = vector.transfer_read %1[%c0, %4], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %7 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3072)>()[%0]
    %8 = vector.transfer_read %2[%workgroup_id_x, %7], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %9 = vector.transfer_read %1[%c0, %7], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %10 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2560)>()[%0]
    %11 = vector.transfer_read %2[%workgroup_id_x, %10], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %12 = vector.transfer_read %1[%c0, %10], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %13 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2048)>()[%0]
    %14 = vector.transfer_read %2[%workgroup_id_x, %13], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %15 = vector.transfer_read %1[%c0, %13], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %16 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1536)>()[%0]
    %17 = vector.transfer_read %2[%workgroup_id_x, %16], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %18 = vector.transfer_read %1[%c0, %16], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %19 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1024)>()[%0]
    %20 = vector.transfer_read %2[%workgroup_id_x, %19], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %21 = vector.transfer_read %1[%c0, %19], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %22 = affine.apply affine_map<()[s0] -> (s0 * 8 + 512)>()[%0]
    %23 = vector.transfer_read %2[%workgroup_id_x, %22], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %24 = vector.transfer_read %1[%c0, %22], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %25 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%0]
    %26 = vector.transfer_read %2[%workgroup_id_x, %25], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %27 = vector.transfer_read %1[%c0, %25], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %28 = vector.extract_strided_slice %6 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %29 = vector.extract_strided_slice %5 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %30 = arith.mulf %28, %29 : vector<4xf16>
    %31 = vector.extract_strided_slice %6 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %32 = vector.extract_strided_slice %5 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %33 = arith.mulf %31, %32 : vector<4xf16>
    %34 = arith.addf %30, %cst : vector<4xf16>
    %35 = arith.addf %33, %cst : vector<4xf16>
    %36 = vector.extract_strided_slice %9 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %37 = vector.extract_strided_slice %8 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %38 = arith.mulf %36, %37 : vector<4xf16>
    %39 = vector.extract_strided_slice %9 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %40 = vector.extract_strided_slice %8 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %41 = arith.mulf %39, %40 : vector<4xf16>
    %42 = arith.addf %38, %34 : vector<4xf16>
    %43 = arith.addf %41, %35 : vector<4xf16>
    %44 = vector.extract_strided_slice %12 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %45 = vector.extract_strided_slice %11 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %46 = arith.mulf %44, %45 : vector<4xf16>
    %47 = vector.extract_strided_slice %12 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %48 = vector.extract_strided_slice %11 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %49 = arith.mulf %47, %48 : vector<4xf16>
    %50 = arith.addf %46, %42 : vector<4xf16>
    %51 = arith.addf %49, %43 : vector<4xf16>
    %52 = vector.extract_strided_slice %15 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %53 = vector.extract_strided_slice %14 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %54 = arith.mulf %52, %53 : vector<4xf16>
    %55 = vector.extract_strided_slice %15 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %56 = vector.extract_strided_slice %14 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %57 = arith.mulf %55, %56 : vector<4xf16>
    %58 = arith.addf %54, %50 : vector<4xf16>
    %59 = arith.addf %57, %51 : vector<4xf16>
    %60 = vector.extract_strided_slice %18 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %61 = vector.extract_strided_slice %17 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %62 = arith.mulf %60, %61 : vector<4xf16>
    %63 = vector.extract_strided_slice %18 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %64 = vector.extract_strided_slice %17 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %65 = arith.mulf %63, %64 : vector<4xf16>
    %66 = arith.addf %62, %58 : vector<4xf16>
    %67 = arith.addf %65, %59 : vector<4xf16>
    %68 = vector.extract_strided_slice %21 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %69 = vector.extract_strided_slice %20 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %70 = arith.mulf %68, %69 : vector<4xf16>
    %71 = vector.extract_strided_slice %21 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %72 = vector.extract_strided_slice %20 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %73 = arith.mulf %71, %72 : vector<4xf16>
    %74 = arith.addf %70, %66 : vector<4xf16>
    %75 = arith.addf %73, %67 : vector<4xf16>
    %76 = vector.extract_strided_slice %24 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %77 = vector.extract_strided_slice %23 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %78 = arith.mulf %76, %77 : vector<4xf16>
    %79 = vector.extract_strided_slice %24 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %80 = vector.extract_strided_slice %23 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %81 = arith.mulf %79, %80 : vector<4xf16>
    %82 = arith.addf %78, %74 : vector<4xf16>
    %83 = arith.addf %81, %75 : vector<4xf16>
    %84 = vector.extract_strided_slice %27 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %85 = vector.extract_strided_slice %26 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %86 = arith.mulf %84, %85 : vector<4xf16>
    %87 = vector.extract_strided_slice %27 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %88 = vector.extract_strided_slice %26 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %89 = arith.mulf %87, %88 : vector<4xf16>
    %90 = arith.addf %86, %82 : vector<4xf16>
    %91 = arith.addf %89, %83 : vector<4xf16>
    %92 = vector.reduction <add>, %90 : vector<4xf16> into f16
    %93 = vector.reduction <add>, %91 : vector<4xf16> into f16
    %94 = arith.addf %92, %93 : f16
    %95 = gpu.subgroup_reduce  add %94 : (f16) -> f16
    %96 = arith.addf %95, %cst_0 : f16
    %97 = vector.splat %96 : vector<1xf16>
    %98 = arith.cmpi eq, %0, %c0 : index
    scf.if %98 {
      vector.transfer_write %97, %3[%c0, %workgroup_id_x] {in_bounds = [true]} : vector<1xf16>, memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    }
    return
  }
}

// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
    %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f16
    %0 = gpu.thread_id  x
    %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %1, 64 : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>
    %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %2, 64 : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>
    %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %3, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3584)>()[%0]
    %5 = vector.transfer_read %2[%workgroup_id_x, %4], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %6 = vector.transfer_read %1[%c0, %4], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %7 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3072)>()[%0]
    %8 = vector.transfer_read %2[%workgroup_id_x, %7], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %9 = vector.transfer_read %1[%c0, %7], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %10 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2560)>()[%0]
    %11 = vector.transfer_read %2[%workgroup_id_x, %10], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %12 = vector.transfer_read %1[%c0, %10], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %13 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2048)>()[%0]
    %14 = vector.transfer_read %2[%workgroup_id_x, %13], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %15 = vector.transfer_read %1[%c0, %13], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %16 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1536)>()[%0]
    %17 = vector.transfer_read %2[%workgroup_id_x, %16], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %18 = vector.transfer_read %1[%c0, %16], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %19 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1024)>()[%0]
    %20 = vector.transfer_read %2[%workgroup_id_x, %19], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %21 = vector.transfer_read %1[%c0, %19], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %22 = affine.apply affine_map<()[s0] -> (s0 * 8 + 512)>()[%0]
    %23 = vector.transfer_read %2[%workgroup_id_x, %22], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %24 = vector.transfer_read %1[%c0, %22], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %25 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%0]
    %26 = vector.transfer_read %2[%workgroup_id_x, %25], %cst_0 {in_bounds = [true]} : memref<32000x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %27 = vector.transfer_read %1[%c0, %25], %cst_0 {in_bounds = [true]} : memref<1x4096xf16, #hal.descriptor_type<storage_buffer>>, vector<8xf16>
    %28 = vector.extract_strided_slice %6 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %29 = vector.extract_strided_slice %5 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %30 = arith.mulf %28, %29 : vector<4xf16>
    %31 = vector.extract_strided_slice %6 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %32 = vector.extract_strided_slice %5 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %33 = arith.mulf %31, %32 : vector<4xf16>
    %34 = arith.addf %30, %cst : vector<4xf16>
    %35 = arith.addf %33, %cst : vector<4xf16>
    %36 = vector.extract_strided_slice %9 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %37 = vector.extract_strided_slice %8 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %38 = arith.mulf %36, %37 : vector<4xf16>
    %39 = vector.extract_strided_slice %9 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %40 = vector.extract_strided_slice %8 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %41 = arith.mulf %39, %40 : vector<4xf16>
    %42 = arith.addf %38, %34 : vector<4xf16>
    %43 = arith.addf %41, %35 : vector<4xf16>
    %44 = vector.extract_strided_slice %12 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %45 = vector.extract_strided_slice %11 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %46 = arith.mulf %44, %45 : vector<4xf16>
    %47 = vector.extract_strided_slice %12 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %48 = vector.extract_strided_slice %11 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %49 = arith.mulf %47, %48 : vector<4xf16>
    %50 = arith.addf %46, %42 : vector<4xf16>
    %51 = arith.addf %49, %43 : vector<4xf16>
    %52 = vector.extract_strided_slice %15 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %53 = vector.extract_strided_slice %14 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %54 = arith.mulf %52, %53 : vector<4xf16>
    %55 = vector.extract_strided_slice %15 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %56 = vector.extract_strided_slice %14 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %57 = arith.mulf %55, %56 : vector<4xf16>
    %58 = arith.addf %54, %50 : vector<4xf16>
    %59 = arith.addf %57, %51 : vector<4xf16>
    %60 = vector.extract_strided_slice %18 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %61 = vector.extract_strided_slice %17 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %62 = arith.mulf %60, %61 : vector<4xf16>
    %63 = vector.extract_strided_slice %18 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %64 = vector.extract_strided_slice %17 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %65 = arith.mulf %63, %64 : vector<4xf16>
    %66 = arith.addf %62, %58 : vector<4xf16>
    %67 = arith.addf %65, %59 : vector<4xf16>
    %68 = vector.extract_strided_slice %21 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %69 = vector.extract_strided_slice %20 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %70 = arith.mulf %68, %69 : vector<4xf16>
    %71 = vector.extract_strided_slice %21 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %72 = vector.extract_strided_slice %20 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %73 = arith.mulf %71, %72 : vector<4xf16>
    %74 = arith.addf %70, %66 : vector<4xf16>
    %75 = arith.addf %73, %67 : vector<4xf16>
    %76 = vector.extract_strided_slice %24 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %77 = vector.extract_strided_slice %23 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %78 = arith.mulf %76, %77 : vector<4xf16>
    %79 = vector.extract_strided_slice %24 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %80 = vector.extract_strided_slice %23 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %81 = arith.mulf %79, %80 : vector<4xf16>
    %82 = arith.addf %78, %74 : vector<4xf16>
    %83 = arith.addf %81, %75 : vector<4xf16>
    %84 = vector.extract_strided_slice %27 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %85 = vector.extract_strided_slice %26 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %86 = arith.mulf %84, %85 : vector<4xf16>
    %87 = vector.extract_strided_slice %27 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %88 = vector.extract_strided_slice %26 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %89 = arith.mulf %87, %88 : vector<4xf16>
    %90 = arith.addf %86, %82 : vector<4xf16>
    %91 = arith.addf %89, %83 : vector<4xf16>
    %92 = vector.reduction <add>, %90 : vector<4xf16> into f16
    %93 = vector.reduction <add>, %91 : vector<4xf16> into f16
    %94 = arith.addf %92, %93 : f16
    %95 = gpu.subgroup_reduce  add %94 : (f16) -> f16
    %96 = arith.addf %95, %cst_0 : f16
    %97 = vector.splat %96 : vector<1xf16>
    %98 = arith.cmpi eq, %0, %c0 : index
    scf.if %98 {
      vector.transfer_write %97, %3[%c0, %workgroup_id_x] {in_bounds = [true]} : vector<1xf16>, memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    }
    return
  }
}

// -----// IR Dump After SPIRVVectorizeLoadStore (iree-spirv-vectorize-load-store) //----- //
module {
  func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
    %c8 = arith.constant 8 : index
    %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f16
    %0 = gpu.thread_id  x
    %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %1, 64 : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %2, 64 : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %3, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3584)>()[%0]
    %5 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%4, %c8]
    %6 = memref.load %2[%workgroup_id_x, %5] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %7 = vector.bitcast %6 : vector<4xf32> to vector<8xf16>
    %8 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%4, %c8]
    %9 = memref.load %1[%c0, %8] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %10 = vector.bitcast %9 : vector<4xf32> to vector<8xf16>
    %11 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3072)>()[%0]
    %12 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%11, %c8]
    %13 = memref.load %2[%workgroup_id_x, %12] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %14 = vector.bitcast %13 : vector<4xf32> to vector<8xf16>
    %15 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%11, %c8]
    %16 = memref.load %1[%c0, %15] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %17 = vector.bitcast %16 : vector<4xf32> to vector<8xf16>
    %18 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2560)>()[%0]
    %19 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%18, %c8]
    %20 = memref.load %2[%workgroup_id_x, %19] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %21 = vector.bitcast %20 : vector<4xf32> to vector<8xf16>
    %22 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%18, %c8]
    %23 = memref.load %1[%c0, %22] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %24 = vector.bitcast %23 : vector<4xf32> to vector<8xf16>
    %25 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2048)>()[%0]
    %26 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%25, %c8]
    %27 = memref.load %2[%workgroup_id_x, %26] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %28 = vector.bitcast %27 : vector<4xf32> to vector<8xf16>
    %29 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%25, %c8]
    %30 = memref.load %1[%c0, %29] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %31 = vector.bitcast %30 : vector<4xf32> to vector<8xf16>
    %32 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1536)>()[%0]
    %33 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%32, %c8]
    %34 = memref.load %2[%workgroup_id_x, %33] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %35 = vector.bitcast %34 : vector<4xf32> to vector<8xf16>
    %36 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%32, %c8]
    %37 = memref.load %1[%c0, %36] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %38 = vector.bitcast %37 : vector<4xf32> to vector<8xf16>
    %39 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1024)>()[%0]
    %40 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%39, %c8]
    %41 = memref.load %2[%workgroup_id_x, %40] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %42 = vector.bitcast %41 : vector<4xf32> to vector<8xf16>
    %43 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%39, %c8]
    %44 = memref.load %1[%c0, %43] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %45 = vector.bitcast %44 : vector<4xf32> to vector<8xf16>
    %46 = affine.apply affine_map<()[s0] -> (s0 * 8 + 512)>()[%0]
    %47 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%46, %c8]
    %48 = memref.load %2[%workgroup_id_x, %47] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %49 = vector.bitcast %48 : vector<4xf32> to vector<8xf16>
    %50 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%46, %c8]
    %51 = memref.load %1[%c0, %50] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %52 = vector.bitcast %51 : vector<4xf32> to vector<8xf16>
    %53 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%0]
    %54 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%53, %c8]
    %55 = memref.load %2[%workgroup_id_x, %54] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %56 = vector.bitcast %55 : vector<4xf32> to vector<8xf16>
    %57 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%53, %c8]
    %58 = memref.load %1[%c0, %57] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %59 = vector.bitcast %58 : vector<4xf32> to vector<8xf16>
    %60 = vector.extract_strided_slice %10 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %61 = vector.extract_strided_slice %7 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %62 = arith.mulf %60, %61 : vector<4xf16>
    %63 = vector.extract_strided_slice %10 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %64 = vector.extract_strided_slice %7 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %65 = arith.mulf %63, %64 : vector<4xf16>
    %66 = arith.addf %62, %cst : vector<4xf16>
    %67 = arith.addf %65, %cst : vector<4xf16>
    %68 = vector.extract_strided_slice %17 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %69 = vector.extract_strided_slice %14 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %70 = arith.mulf %68, %69 : vector<4xf16>
    %71 = vector.extract_strided_slice %17 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %72 = vector.extract_strided_slice %14 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %73 = arith.mulf %71, %72 : vector<4xf16>
    %74 = arith.addf %70, %66 : vector<4xf16>
    %75 = arith.addf %73, %67 : vector<4xf16>
    %76 = vector.extract_strided_slice %24 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %77 = vector.extract_strided_slice %21 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %78 = arith.mulf %76, %77 : vector<4xf16>
    %79 = vector.extract_strided_slice %24 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %80 = vector.extract_strided_slice %21 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %81 = arith.mulf %79, %80 : vector<4xf16>
    %82 = arith.addf %78, %74 : vector<4xf16>
    %83 = arith.addf %81, %75 : vector<4xf16>
    %84 = vector.extract_strided_slice %31 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %85 = vector.extract_strided_slice %28 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %86 = arith.mulf %84, %85 : vector<4xf16>
    %87 = vector.extract_strided_slice %31 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %88 = vector.extract_strided_slice %28 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %89 = arith.mulf %87, %88 : vector<4xf16>
    %90 = arith.addf %86, %82 : vector<4xf16>
    %91 = arith.addf %89, %83 : vector<4xf16>
    %92 = vector.extract_strided_slice %38 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %93 = vector.extract_strided_slice %35 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %94 = arith.mulf %92, %93 : vector<4xf16>
    %95 = vector.extract_strided_slice %38 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %96 = vector.extract_strided_slice %35 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %97 = arith.mulf %95, %96 : vector<4xf16>
    %98 = arith.addf %94, %90 : vector<4xf16>
    %99 = arith.addf %97, %91 : vector<4xf16>
    %100 = vector.extract_strided_slice %45 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %101 = vector.extract_strided_slice %42 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %102 = arith.mulf %100, %101 : vector<4xf16>
    %103 = vector.extract_strided_slice %45 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %104 = vector.extract_strided_slice %42 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %105 = arith.mulf %103, %104 : vector<4xf16>
    %106 = arith.addf %102, %98 : vector<4xf16>
    %107 = arith.addf %105, %99 : vector<4xf16>
    %108 = vector.extract_strided_slice %52 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %109 = vector.extract_strided_slice %49 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %110 = arith.mulf %108, %109 : vector<4xf16>
    %111 = vector.extract_strided_slice %52 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %112 = vector.extract_strided_slice %49 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %113 = arith.mulf %111, %112 : vector<4xf16>
    %114 = arith.addf %110, %106 : vector<4xf16>
    %115 = arith.addf %113, %107 : vector<4xf16>
    %116 = vector.extract_strided_slice %59 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %117 = vector.extract_strided_slice %56 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %118 = arith.mulf %116, %117 : vector<4xf16>
    %119 = vector.extract_strided_slice %59 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %120 = vector.extract_strided_slice %56 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
    %121 = arith.mulf %119, %120 : vector<4xf16>
    %122 = arith.addf %118, %114 : vector<4xf16>
    %123 = arith.addf %121, %115 : vector<4xf16>
    %124 = vector.reduction <add>, %122 : vector<4xf16> into f16
    %125 = vector.reduction <add>, %123 : vector<4xf16> into f16
    %126 = arith.addf %124, %125 : f16
    %127 = gpu.subgroup_reduce  add %126 : (f16) -> f16
    %128 = arith.addf %127, %cst_0 : f16
    %129 = arith.cmpi eq, %0, %c0 : index
    scf.if %129 {
      %130 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%workgroup_id_x, %c0]
      memref.store %128, %3[%c0, %130] : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    }
    return
  }
}

// -----// IR Dump After ForOpCanonicalization (iree-codegen-canonicalize-scf-for) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %c8 = arith.constant 8 : index
  %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f16
  %0 = gpu.thread_id  x
  %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %3, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3584)>()[%0]
  %5 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%4, %c8]
  %6 = memref.load %2[%workgroup_id_x, %5] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %7 = vector.bitcast %6 : vector<4xf32> to vector<8xf16>
  %8 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%4, %c8]
  %9 = memref.load %1[%c0, %8] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %10 = vector.bitcast %9 : vector<4xf32> to vector<8xf16>
  %11 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3072)>()[%0]
  %12 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%11, %c8]
  %13 = memref.load %2[%workgroup_id_x, %12] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %14 = vector.bitcast %13 : vector<4xf32> to vector<8xf16>
  %15 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%11, %c8]
  %16 = memref.load %1[%c0, %15] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %17 = vector.bitcast %16 : vector<4xf32> to vector<8xf16>
  %18 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2560)>()[%0]
  %19 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%18, %c8]
  %20 = memref.load %2[%workgroup_id_x, %19] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %21 = vector.bitcast %20 : vector<4xf32> to vector<8xf16>
  %22 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%18, %c8]
  %23 = memref.load %1[%c0, %22] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %24 = vector.bitcast %23 : vector<4xf32> to vector<8xf16>
  %25 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2048)>()[%0]
  %26 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%25, %c8]
  %27 = memref.load %2[%workgroup_id_x, %26] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %28 = vector.bitcast %27 : vector<4xf32> to vector<8xf16>
  %29 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%25, %c8]
  %30 = memref.load %1[%c0, %29] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %31 = vector.bitcast %30 : vector<4xf32> to vector<8xf16>
  %32 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1536)>()[%0]
  %33 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%32, %c8]
  %34 = memref.load %2[%workgroup_id_x, %33] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %35 = vector.bitcast %34 : vector<4xf32> to vector<8xf16>
  %36 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%32, %c8]
  %37 = memref.load %1[%c0, %36] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %38 = vector.bitcast %37 : vector<4xf32> to vector<8xf16>
  %39 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1024)>()[%0]
  %40 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%39, %c8]
  %41 = memref.load %2[%workgroup_id_x, %40] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %42 = vector.bitcast %41 : vector<4xf32> to vector<8xf16>
  %43 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%39, %c8]
  %44 = memref.load %1[%c0, %43] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %45 = vector.bitcast %44 : vector<4xf32> to vector<8xf16>
  %46 = affine.apply affine_map<()[s0] -> (s0 * 8 + 512)>()[%0]
  %47 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%46, %c8]
  %48 = memref.load %2[%workgroup_id_x, %47] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %49 = vector.bitcast %48 : vector<4xf32> to vector<8xf16>
  %50 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%46, %c8]
  %51 = memref.load %1[%c0, %50] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %52 = vector.bitcast %51 : vector<4xf32> to vector<8xf16>
  %53 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%0]
  %54 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%53, %c8]
  %55 = memref.load %2[%workgroup_id_x, %54] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %56 = vector.bitcast %55 : vector<4xf32> to vector<8xf16>
  %57 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%53, %c8]
  %58 = memref.load %1[%c0, %57] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %59 = vector.bitcast %58 : vector<4xf32> to vector<8xf16>
  %60 = vector.extract_strided_slice %10 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %61 = vector.extract_strided_slice %7 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %62 = arith.mulf %60, %61 : vector<4xf16>
  %63 = vector.extract_strided_slice %10 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %64 = vector.extract_strided_slice %7 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %65 = arith.mulf %63, %64 : vector<4xf16>
  %66 = arith.addf %62, %cst : vector<4xf16>
  %67 = arith.addf %65, %cst : vector<4xf16>
  %68 = vector.extract_strided_slice %17 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %69 = vector.extract_strided_slice %14 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %70 = arith.mulf %68, %69 : vector<4xf16>
  %71 = vector.extract_strided_slice %17 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %72 = vector.extract_strided_slice %14 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %73 = arith.mulf %71, %72 : vector<4xf16>
  %74 = arith.addf %70, %66 : vector<4xf16>
  %75 = arith.addf %73, %67 : vector<4xf16>
  %76 = vector.extract_strided_slice %24 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %77 = vector.extract_strided_slice %21 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %78 = arith.mulf %76, %77 : vector<4xf16>
  %79 = vector.extract_strided_slice %24 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %80 = vector.extract_strided_slice %21 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %81 = arith.mulf %79, %80 : vector<4xf16>
  %82 = arith.addf %78, %74 : vector<4xf16>
  %83 = arith.addf %81, %75 : vector<4xf16>
  %84 = vector.extract_strided_slice %31 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %85 = vector.extract_strided_slice %28 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %86 = arith.mulf %84, %85 : vector<4xf16>
  %87 = vector.extract_strided_slice %31 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %88 = vector.extract_strided_slice %28 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %89 = arith.mulf %87, %88 : vector<4xf16>
  %90 = arith.addf %86, %82 : vector<4xf16>
  %91 = arith.addf %89, %83 : vector<4xf16>
  %92 = vector.extract_strided_slice %38 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %93 = vector.extract_strided_slice %35 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %94 = arith.mulf %92, %93 : vector<4xf16>
  %95 = vector.extract_strided_slice %38 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %96 = vector.extract_strided_slice %35 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %97 = arith.mulf %95, %96 : vector<4xf16>
  %98 = arith.addf %94, %90 : vector<4xf16>
  %99 = arith.addf %97, %91 : vector<4xf16>
  %100 = vector.extract_strided_slice %45 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %101 = vector.extract_strided_slice %42 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %102 = arith.mulf %100, %101 : vector<4xf16>
  %103 = vector.extract_strided_slice %45 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %104 = vector.extract_strided_slice %42 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %105 = arith.mulf %103, %104 : vector<4xf16>
  %106 = arith.addf %102, %98 : vector<4xf16>
  %107 = arith.addf %105, %99 : vector<4xf16>
  %108 = vector.extract_strided_slice %52 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %109 = vector.extract_strided_slice %49 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %110 = arith.mulf %108, %109 : vector<4xf16>
  %111 = vector.extract_strided_slice %52 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %112 = vector.extract_strided_slice %49 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %113 = arith.mulf %111, %112 : vector<4xf16>
  %114 = arith.addf %110, %106 : vector<4xf16>
  %115 = arith.addf %113, %107 : vector<4xf16>
  %116 = vector.extract_strided_slice %59 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %117 = vector.extract_strided_slice %56 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %118 = arith.mulf %116, %117 : vector<4xf16>
  %119 = vector.extract_strided_slice %59 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %120 = vector.extract_strided_slice %56 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf16> to vector<4xf16>
  %121 = arith.mulf %119, %120 : vector<4xf16>
  %122 = arith.addf %118, %114 : vector<4xf16>
  %123 = arith.addf %121, %115 : vector<4xf16>
  %124 = vector.reduction <add>, %122 : vector<4xf16> into f16
  %125 = vector.reduction <add>, %123 : vector<4xf16> into f16
  %126 = arith.addf %124, %125 : f16
  %127 = gpu.subgroup_reduce  add %126 : (f16) -> f16
  %128 = arith.addf %127, %cst_0 : f16
  %129 = arith.cmpi eq, %0, %c0 : index
  scf.if %129 {
    %130 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%workgroup_id_x, %c0]
    memref.store %128, %3[%c0, %130] : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  }
  return
}

// -----// IR Dump After OptimizeVectorTransfer (iree-codegen-optimize-vector-transfer) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %c8 = arith.constant 8 : index
  %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f16
  %0 = gpu.thread_id  x
  %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %3, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3584)>()[%0]
  %5 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%4, %c8]
  %6 = memref.load %2[%workgroup_id_x, %5] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %7 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%4, %c8]
  %8 = memref.load %1[%c0, %7] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %9 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3072)>()[%0]
  %10 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%9, %c8]
  %11 = memref.load %2[%workgroup_id_x, %10] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %12 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%9, %c8]
  %13 = memref.load %1[%c0, %12] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %14 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2560)>()[%0]
  %15 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%14, %c8]
  %16 = memref.load %2[%workgroup_id_x, %15] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %17 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%14, %c8]
  %18 = memref.load %1[%c0, %17] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %19 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2048)>()[%0]
  %20 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%19, %c8]
  %21 = memref.load %2[%workgroup_id_x, %20] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %22 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%19, %c8]
  %23 = memref.load %1[%c0, %22] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %24 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1536)>()[%0]
  %25 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%24, %c8]
  %26 = memref.load %2[%workgroup_id_x, %25] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %27 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%24, %c8]
  %28 = memref.load %1[%c0, %27] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %29 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1024)>()[%0]
  %30 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%29, %c8]
  %31 = memref.load %2[%workgroup_id_x, %30] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %32 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%29, %c8]
  %33 = memref.load %1[%c0, %32] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %34 = affine.apply affine_map<()[s0] -> (s0 * 8 + 512)>()[%0]
  %35 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%34, %c8]
  %36 = memref.load %2[%workgroup_id_x, %35] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %37 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%34, %c8]
  %38 = memref.load %1[%c0, %37] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %39 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%0]
  %40 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%39, %c8]
  %41 = memref.load %2[%workgroup_id_x, %40] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %42 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%39, %c8]
  %43 = memref.load %1[%c0, %42] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %44 = vector.extract_strided_slice %8 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %45 = vector.bitcast %44 : vector<2xf32> to vector<4xf16>
  %46 = vector.extract_strided_slice %6 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %47 = vector.bitcast %46 : vector<2xf32> to vector<4xf16>
  %48 = arith.mulf %45, %47 : vector<4xf16>
  %49 = vector.extract_strided_slice %8 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %50 = vector.bitcast %49 : vector<2xf32> to vector<4xf16>
  %51 = vector.extract_strided_slice %6 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %52 = vector.bitcast %51 : vector<2xf32> to vector<4xf16>
  %53 = arith.mulf %50, %52 : vector<4xf16>
  %54 = arith.addf %48, %cst : vector<4xf16>
  %55 = arith.addf %53, %cst : vector<4xf16>
  %56 = vector.extract_strided_slice %13 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %57 = vector.bitcast %56 : vector<2xf32> to vector<4xf16>
  %58 = vector.extract_strided_slice %11 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %59 = vector.bitcast %58 : vector<2xf32> to vector<4xf16>
  %60 = arith.mulf %57, %59 : vector<4xf16>
  %61 = vector.extract_strided_slice %13 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %62 = vector.bitcast %61 : vector<2xf32> to vector<4xf16>
  %63 = vector.extract_strided_slice %11 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %64 = vector.bitcast %63 : vector<2xf32> to vector<4xf16>
  %65 = arith.mulf %62, %64 : vector<4xf16>
  %66 = arith.addf %60, %54 : vector<4xf16>
  %67 = arith.addf %65, %55 : vector<4xf16>
  %68 = vector.extract_strided_slice %18 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %69 = vector.bitcast %68 : vector<2xf32> to vector<4xf16>
  %70 = vector.extract_strided_slice %16 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %71 = vector.bitcast %70 : vector<2xf32> to vector<4xf16>
  %72 = arith.mulf %69, %71 : vector<4xf16>
  %73 = vector.extract_strided_slice %18 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %74 = vector.bitcast %73 : vector<2xf32> to vector<4xf16>
  %75 = vector.extract_strided_slice %16 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %76 = vector.bitcast %75 : vector<2xf32> to vector<4xf16>
  %77 = arith.mulf %74, %76 : vector<4xf16>
  %78 = arith.addf %72, %66 : vector<4xf16>
  %79 = arith.addf %77, %67 : vector<4xf16>
  %80 = vector.extract_strided_slice %23 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %81 = vector.bitcast %80 : vector<2xf32> to vector<4xf16>
  %82 = vector.extract_strided_slice %21 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %83 = vector.bitcast %82 : vector<2xf32> to vector<4xf16>
  %84 = arith.mulf %81, %83 : vector<4xf16>
  %85 = vector.extract_strided_slice %23 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %86 = vector.bitcast %85 : vector<2xf32> to vector<4xf16>
  %87 = vector.extract_strided_slice %21 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %88 = vector.bitcast %87 : vector<2xf32> to vector<4xf16>
  %89 = arith.mulf %86, %88 : vector<4xf16>
  %90 = arith.addf %84, %78 : vector<4xf16>
  %91 = arith.addf %89, %79 : vector<4xf16>
  %92 = vector.extract_strided_slice %28 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %93 = vector.bitcast %92 : vector<2xf32> to vector<4xf16>
  %94 = vector.extract_strided_slice %26 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %95 = vector.bitcast %94 : vector<2xf32> to vector<4xf16>
  %96 = arith.mulf %93, %95 : vector<4xf16>
  %97 = vector.extract_strided_slice %28 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %98 = vector.bitcast %97 : vector<2xf32> to vector<4xf16>
  %99 = vector.extract_strided_slice %26 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %100 = vector.bitcast %99 : vector<2xf32> to vector<4xf16>
  %101 = arith.mulf %98, %100 : vector<4xf16>
  %102 = arith.addf %96, %90 : vector<4xf16>
  %103 = arith.addf %101, %91 : vector<4xf16>
  %104 = vector.extract_strided_slice %33 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %105 = vector.bitcast %104 : vector<2xf32> to vector<4xf16>
  %106 = vector.extract_strided_slice %31 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %107 = vector.bitcast %106 : vector<2xf32> to vector<4xf16>
  %108 = arith.mulf %105, %107 : vector<4xf16>
  %109 = vector.extract_strided_slice %33 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %110 = vector.bitcast %109 : vector<2xf32> to vector<4xf16>
  %111 = vector.extract_strided_slice %31 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %112 = vector.bitcast %111 : vector<2xf32> to vector<4xf16>
  %113 = arith.mulf %110, %112 : vector<4xf16>
  %114 = arith.addf %108, %102 : vector<4xf16>
  %115 = arith.addf %113, %103 : vector<4xf16>
  %116 = vector.extract_strided_slice %38 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %117 = vector.bitcast %116 : vector<2xf32> to vector<4xf16>
  %118 = vector.extract_strided_slice %36 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %119 = vector.bitcast %118 : vector<2xf32> to vector<4xf16>
  %120 = arith.mulf %117, %119 : vector<4xf16>
  %121 = vector.extract_strided_slice %38 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %122 = vector.bitcast %121 : vector<2xf32> to vector<4xf16>
  %123 = vector.extract_strided_slice %36 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %124 = vector.bitcast %123 : vector<2xf32> to vector<4xf16>
  %125 = arith.mulf %122, %124 : vector<4xf16>
  %126 = arith.addf %120, %114 : vector<4xf16>
  %127 = arith.addf %125, %115 : vector<4xf16>
  %128 = vector.extract_strided_slice %43 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %129 = vector.bitcast %128 : vector<2xf32> to vector<4xf16>
  %130 = vector.extract_strided_slice %41 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %131 = vector.bitcast %130 : vector<2xf32> to vector<4xf16>
  %132 = arith.mulf %129, %131 : vector<4xf16>
  %133 = vector.extract_strided_slice %43 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %134 = vector.bitcast %133 : vector<2xf32> to vector<4xf16>
  %135 = vector.extract_strided_slice %41 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %136 = vector.bitcast %135 : vector<2xf32> to vector<4xf16>
  %137 = arith.mulf %134, %136 : vector<4xf16>
  %138 = arith.addf %132, %126 : vector<4xf16>
  %139 = arith.addf %137, %127 : vector<4xf16>
  %140 = vector.reduction <add>, %138 : vector<4xf16> into f16
  %141 = vector.reduction <add>, %139 : vector<4xf16> into f16
  %142 = arith.addf %140, %141 : f16
  %143 = gpu.subgroup_reduce  add %142 : (f16) -> f16
  %144 = arith.addf %143, %cst_0 : f16
  %145 = arith.cmpi eq, %0, %c0 : index
  scf.if %145 {
    %146 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%workgroup_id_x, %c0]
    memref.store %144, %3[%c0, %146] : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  }
  return
}

// -----// IR Dump After SPIRVBreakDownLargeVector (iree-spirv-breakdown-large-vector) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %c8 = arith.constant 8 : index
  %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f16
  %0 = gpu.thread_id  x
  %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %3, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3584)>()[%0]
  %5 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%4, %c8]
  %6 = memref.load %2[%workgroup_id_x, %5] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %7 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%4, %c8]
  %8 = memref.load %1[%c0, %7] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %9 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3072)>()[%0]
  %10 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%9, %c8]
  %11 = memref.load %2[%workgroup_id_x, %10] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %12 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%9, %c8]
  %13 = memref.load %1[%c0, %12] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %14 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2560)>()[%0]
  %15 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%14, %c8]
  %16 = memref.load %2[%workgroup_id_x, %15] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %17 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%14, %c8]
  %18 = memref.load %1[%c0, %17] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %19 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2048)>()[%0]
  %20 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%19, %c8]
  %21 = memref.load %2[%workgroup_id_x, %20] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %22 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%19, %c8]
  %23 = memref.load %1[%c0, %22] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %24 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1536)>()[%0]
  %25 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%24, %c8]
  %26 = memref.load %2[%workgroup_id_x, %25] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %27 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%24, %c8]
  %28 = memref.load %1[%c0, %27] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %29 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1024)>()[%0]
  %30 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%29, %c8]
  %31 = memref.load %2[%workgroup_id_x, %30] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %32 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%29, %c8]
  %33 = memref.load %1[%c0, %32] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %34 = affine.apply affine_map<()[s0] -> (s0 * 8 + 512)>()[%0]
  %35 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%34, %c8]
  %36 = memref.load %2[%workgroup_id_x, %35] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %37 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%34, %c8]
  %38 = memref.load %1[%c0, %37] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %39 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%0]
  %40 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%39, %c8]
  %41 = memref.load %2[%workgroup_id_x, %40] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %42 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%39, %c8]
  %43 = memref.load %1[%c0, %42] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %44 = vector.extract_strided_slice %8 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %45 = vector.bitcast %44 : vector<2xf32> to vector<4xf16>
  %46 = vector.extract_strided_slice %6 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %47 = vector.bitcast %46 : vector<2xf32> to vector<4xf16>
  %48 = arith.mulf %45, %47 : vector<4xf16>
  %49 = vector.extract_strided_slice %8 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %50 = vector.bitcast %49 : vector<2xf32> to vector<4xf16>
  %51 = vector.extract_strided_slice %6 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %52 = vector.bitcast %51 : vector<2xf32> to vector<4xf16>
  %53 = arith.mulf %50, %52 : vector<4xf16>
  %54 = arith.addf %48, %cst : vector<4xf16>
  %55 = arith.addf %53, %cst : vector<4xf16>
  %56 = vector.extract_strided_slice %13 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %57 = vector.bitcast %56 : vector<2xf32> to vector<4xf16>
  %58 = vector.extract_strided_slice %11 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %59 = vector.bitcast %58 : vector<2xf32> to vector<4xf16>
  %60 = arith.mulf %57, %59 : vector<4xf16>
  %61 = vector.extract_strided_slice %13 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %62 = vector.bitcast %61 : vector<2xf32> to vector<4xf16>
  %63 = vector.extract_strided_slice %11 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %64 = vector.bitcast %63 : vector<2xf32> to vector<4xf16>
  %65 = arith.mulf %62, %64 : vector<4xf16>
  %66 = arith.addf %60, %54 : vector<4xf16>
  %67 = arith.addf %65, %55 : vector<4xf16>
  %68 = vector.extract_strided_slice %18 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %69 = vector.bitcast %68 : vector<2xf32> to vector<4xf16>
  %70 = vector.extract_strided_slice %16 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %71 = vector.bitcast %70 : vector<2xf32> to vector<4xf16>
  %72 = arith.mulf %69, %71 : vector<4xf16>
  %73 = vector.extract_strided_slice %18 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %74 = vector.bitcast %73 : vector<2xf32> to vector<4xf16>
  %75 = vector.extract_strided_slice %16 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %76 = vector.bitcast %75 : vector<2xf32> to vector<4xf16>
  %77 = arith.mulf %74, %76 : vector<4xf16>
  %78 = arith.addf %72, %66 : vector<4xf16>
  %79 = arith.addf %77, %67 : vector<4xf16>
  %80 = vector.extract_strided_slice %23 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %81 = vector.bitcast %80 : vector<2xf32> to vector<4xf16>
  %82 = vector.extract_strided_slice %21 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %83 = vector.bitcast %82 : vector<2xf32> to vector<4xf16>
  %84 = arith.mulf %81, %83 : vector<4xf16>
  %85 = vector.extract_strided_slice %23 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %86 = vector.bitcast %85 : vector<2xf32> to vector<4xf16>
  %87 = vector.extract_strided_slice %21 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %88 = vector.bitcast %87 : vector<2xf32> to vector<4xf16>
  %89 = arith.mulf %86, %88 : vector<4xf16>
  %90 = arith.addf %84, %78 : vector<4xf16>
  %91 = arith.addf %89, %79 : vector<4xf16>
  %92 = vector.extract_strided_slice %28 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %93 = vector.bitcast %92 : vector<2xf32> to vector<4xf16>
  %94 = vector.extract_strided_slice %26 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %95 = vector.bitcast %94 : vector<2xf32> to vector<4xf16>
  %96 = arith.mulf %93, %95 : vector<4xf16>
  %97 = vector.extract_strided_slice %28 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %98 = vector.bitcast %97 : vector<2xf32> to vector<4xf16>
  %99 = vector.extract_strided_slice %26 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %100 = vector.bitcast %99 : vector<2xf32> to vector<4xf16>
  %101 = arith.mulf %98, %100 : vector<4xf16>
  %102 = arith.addf %96, %90 : vector<4xf16>
  %103 = arith.addf %101, %91 : vector<4xf16>
  %104 = vector.extract_strided_slice %33 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %105 = vector.bitcast %104 : vector<2xf32> to vector<4xf16>
  %106 = vector.extract_strided_slice %31 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %107 = vector.bitcast %106 : vector<2xf32> to vector<4xf16>
  %108 = arith.mulf %105, %107 : vector<4xf16>
  %109 = vector.extract_strided_slice %33 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %110 = vector.bitcast %109 : vector<2xf32> to vector<4xf16>
  %111 = vector.extract_strided_slice %31 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %112 = vector.bitcast %111 : vector<2xf32> to vector<4xf16>
  %113 = arith.mulf %110, %112 : vector<4xf16>
  %114 = arith.addf %108, %102 : vector<4xf16>
  %115 = arith.addf %113, %103 : vector<4xf16>
  %116 = vector.extract_strided_slice %38 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %117 = vector.bitcast %116 : vector<2xf32> to vector<4xf16>
  %118 = vector.extract_strided_slice %36 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %119 = vector.bitcast %118 : vector<2xf32> to vector<4xf16>
  %120 = arith.mulf %117, %119 : vector<4xf16>
  %121 = vector.extract_strided_slice %38 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %122 = vector.bitcast %121 : vector<2xf32> to vector<4xf16>
  %123 = vector.extract_strided_slice %36 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %124 = vector.bitcast %123 : vector<2xf32> to vector<4xf16>
  %125 = arith.mulf %122, %124 : vector<4xf16>
  %126 = arith.addf %120, %114 : vector<4xf16>
  %127 = arith.addf %125, %115 : vector<4xf16>
  %128 = vector.extract_strided_slice %43 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %129 = vector.bitcast %128 : vector<2xf32> to vector<4xf16>
  %130 = vector.extract_strided_slice %41 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %131 = vector.bitcast %130 : vector<2xf32> to vector<4xf16>
  %132 = arith.mulf %129, %131 : vector<4xf16>
  %133 = vector.extract_strided_slice %43 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %134 = vector.bitcast %133 : vector<2xf32> to vector<4xf16>
  %135 = vector.extract_strided_slice %41 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %136 = vector.bitcast %135 : vector<2xf32> to vector<4xf16>
  %137 = arith.mulf %134, %136 : vector<4xf16>
  %138 = arith.addf %132, %126 : vector<4xf16>
  %139 = arith.addf %137, %127 : vector<4xf16>
  %140 = vector.reduction <add>, %138 : vector<4xf16> into f16
  %141 = vector.reduction <add>, %139 : vector<4xf16> into f16
  %142 = arith.addf %140, %141 : f16
  %143 = gpu.subgroup_reduce  add %142 : (f16) -> f16
  %144 = arith.addf %143, %cst_0 : f16
  %145 = arith.cmpi eq, %0, %c0 : index
  scf.if %145 {
    %146 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%workgroup_id_x, %c0]
    memref.store %144, %3[%c0, %146] : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  }
  return
}

// -----// IR Dump After ForOpCanonicalization (iree-codegen-canonicalize-scf-for) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %c8 = arith.constant 8 : index
  %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f16
  %0 = gpu.thread_id  x
  %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %3, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3584)>()[%0]
  %5 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%4, %c8]
  %6 = memref.load %2[%workgroup_id_x, %5] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %7 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%4, %c8]
  %8 = memref.load %1[%c0, %7] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %9 = affine.apply affine_map<()[s0] -> (s0 * 8 + 3072)>()[%0]
  %10 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%9, %c8]
  %11 = memref.load %2[%workgroup_id_x, %10] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %12 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%9, %c8]
  %13 = memref.load %1[%c0, %12] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %14 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2560)>()[%0]
  %15 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%14, %c8]
  %16 = memref.load %2[%workgroup_id_x, %15] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %17 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%14, %c8]
  %18 = memref.load %1[%c0, %17] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %19 = affine.apply affine_map<()[s0] -> (s0 * 8 + 2048)>()[%0]
  %20 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%19, %c8]
  %21 = memref.load %2[%workgroup_id_x, %20] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %22 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%19, %c8]
  %23 = memref.load %1[%c0, %22] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %24 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1536)>()[%0]
  %25 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%24, %c8]
  %26 = memref.load %2[%workgroup_id_x, %25] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %27 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%24, %c8]
  %28 = memref.load %1[%c0, %27] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %29 = affine.apply affine_map<()[s0] -> (s0 * 8 + 1024)>()[%0]
  %30 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%29, %c8]
  %31 = memref.load %2[%workgroup_id_x, %30] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %32 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%29, %c8]
  %33 = memref.load %1[%c0, %32] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %34 = affine.apply affine_map<()[s0] -> (s0 * 8 + 512)>()[%0]
  %35 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%34, %c8]
  %36 = memref.load %2[%workgroup_id_x, %35] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %37 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%34, %c8]
  %38 = memref.load %1[%c0, %37] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %39 = affine.apply affine_map<()[s0] -> (s0 * 8)>()[%0]
  %40 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%39, %c8]
  %41 = memref.load %2[%workgroup_id_x, %40] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %42 = affine.apply affine_map<()[s0, s1] -> (s0 floordiv s1)>()[%39, %c8]
  %43 = memref.load %1[%c0, %42] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %44 = vector.extract_strided_slice %8 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %45 = vector.bitcast %44 : vector<2xf32> to vector<4xf16>
  %46 = vector.extract_strided_slice %6 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %47 = vector.bitcast %46 : vector<2xf32> to vector<4xf16>
  %48 = arith.mulf %45, %47 : vector<4xf16>
  %49 = vector.extract_strided_slice %8 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %50 = vector.bitcast %49 : vector<2xf32> to vector<4xf16>
  %51 = vector.extract_strided_slice %6 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %52 = vector.bitcast %51 : vector<2xf32> to vector<4xf16>
  %53 = arith.mulf %50, %52 : vector<4xf16>
  %54 = arith.addf %48, %cst : vector<4xf16>
  %55 = arith.addf %53, %cst : vector<4xf16>
  %56 = vector.extract_strided_slice %13 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %57 = vector.bitcast %56 : vector<2xf32> to vector<4xf16>
  %58 = vector.extract_strided_slice %11 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %59 = vector.bitcast %58 : vector<2xf32> to vector<4xf16>
  %60 = arith.mulf %57, %59 : vector<4xf16>
  %61 = vector.extract_strided_slice %13 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %62 = vector.bitcast %61 : vector<2xf32> to vector<4xf16>
  %63 = vector.extract_strided_slice %11 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %64 = vector.bitcast %63 : vector<2xf32> to vector<4xf16>
  %65 = arith.mulf %62, %64 : vector<4xf16>
  %66 = arith.addf %60, %54 : vector<4xf16>
  %67 = arith.addf %65, %55 : vector<4xf16>
  %68 = vector.extract_strided_slice %18 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %69 = vector.bitcast %68 : vector<2xf32> to vector<4xf16>
  %70 = vector.extract_strided_slice %16 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %71 = vector.bitcast %70 : vector<2xf32> to vector<4xf16>
  %72 = arith.mulf %69, %71 : vector<4xf16>
  %73 = vector.extract_strided_slice %18 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %74 = vector.bitcast %73 : vector<2xf32> to vector<4xf16>
  %75 = vector.extract_strided_slice %16 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %76 = vector.bitcast %75 : vector<2xf32> to vector<4xf16>
  %77 = arith.mulf %74, %76 : vector<4xf16>
  %78 = arith.addf %72, %66 : vector<4xf16>
  %79 = arith.addf %77, %67 : vector<4xf16>
  %80 = vector.extract_strided_slice %23 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %81 = vector.bitcast %80 : vector<2xf32> to vector<4xf16>
  %82 = vector.extract_strided_slice %21 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %83 = vector.bitcast %82 : vector<2xf32> to vector<4xf16>
  %84 = arith.mulf %81, %83 : vector<4xf16>
  %85 = vector.extract_strided_slice %23 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %86 = vector.bitcast %85 : vector<2xf32> to vector<4xf16>
  %87 = vector.extract_strided_slice %21 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %88 = vector.bitcast %87 : vector<2xf32> to vector<4xf16>
  %89 = arith.mulf %86, %88 : vector<4xf16>
  %90 = arith.addf %84, %78 : vector<4xf16>
  %91 = arith.addf %89, %79 : vector<4xf16>
  %92 = vector.extract_strided_slice %28 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %93 = vector.bitcast %92 : vector<2xf32> to vector<4xf16>
  %94 = vector.extract_strided_slice %26 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %95 = vector.bitcast %94 : vector<2xf32> to vector<4xf16>
  %96 = arith.mulf %93, %95 : vector<4xf16>
  %97 = vector.extract_strided_slice %28 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %98 = vector.bitcast %97 : vector<2xf32> to vector<4xf16>
  %99 = vector.extract_strided_slice %26 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %100 = vector.bitcast %99 : vector<2xf32> to vector<4xf16>
  %101 = arith.mulf %98, %100 : vector<4xf16>
  %102 = arith.addf %96, %90 : vector<4xf16>
  %103 = arith.addf %101, %91 : vector<4xf16>
  %104 = vector.extract_strided_slice %33 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %105 = vector.bitcast %104 : vector<2xf32> to vector<4xf16>
  %106 = vector.extract_strided_slice %31 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %107 = vector.bitcast %106 : vector<2xf32> to vector<4xf16>
  %108 = arith.mulf %105, %107 : vector<4xf16>
  %109 = vector.extract_strided_slice %33 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %110 = vector.bitcast %109 : vector<2xf32> to vector<4xf16>
  %111 = vector.extract_strided_slice %31 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %112 = vector.bitcast %111 : vector<2xf32> to vector<4xf16>
  %113 = arith.mulf %110, %112 : vector<4xf16>
  %114 = arith.addf %108, %102 : vector<4xf16>
  %115 = arith.addf %113, %103 : vector<4xf16>
  %116 = vector.extract_strided_slice %38 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %117 = vector.bitcast %116 : vector<2xf32> to vector<4xf16>
  %118 = vector.extract_strided_slice %36 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %119 = vector.bitcast %118 : vector<2xf32> to vector<4xf16>
  %120 = arith.mulf %117, %119 : vector<4xf16>
  %121 = vector.extract_strided_slice %38 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %122 = vector.bitcast %121 : vector<2xf32> to vector<4xf16>
  %123 = vector.extract_strided_slice %36 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %124 = vector.bitcast %123 : vector<2xf32> to vector<4xf16>
  %125 = arith.mulf %122, %124 : vector<4xf16>
  %126 = arith.addf %120, %114 : vector<4xf16>
  %127 = arith.addf %125, %115 : vector<4xf16>
  %128 = vector.extract_strided_slice %43 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %129 = vector.bitcast %128 : vector<2xf32> to vector<4xf16>
  %130 = vector.extract_strided_slice %41 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %131 = vector.bitcast %130 : vector<2xf32> to vector<4xf16>
  %132 = arith.mulf %129, %131 : vector<4xf16>
  %133 = vector.extract_strided_slice %43 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %134 = vector.bitcast %133 : vector<2xf32> to vector<4xf16>
  %135 = vector.extract_strided_slice %41 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %136 = vector.bitcast %135 : vector<2xf32> to vector<4xf16>
  %137 = arith.mulf %134, %136 : vector<4xf16>
  %138 = arith.addf %132, %126 : vector<4xf16>
  %139 = arith.addf %137, %127 : vector<4xf16>
  %140 = vector.reduction <add>, %138 : vector<4xf16> into f16
  %141 = vector.reduction <add>, %139 : vector<4xf16> into f16
  %142 = arith.addf %140, %141 : f16
  %143 = gpu.subgroup_reduce  add %142 : (f16) -> f16
  %144 = arith.addf %143, %cst_0 : f16
  %145 = arith.cmpi eq, %0, %c0 : index
  scf.if %145 {
    %146 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%workgroup_id_x, %c0]
    memref.store %144, %3[%c0, %146] : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module {
  func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
    %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f16
    %0 = gpu.thread_id  x
    %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %1, 64 : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %2, 64 : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %3, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = affine.apply affine_map<()[s0] -> (s0 + 448)>()[%0]
    %5 = memref.load %2[%workgroup_id_x, %4] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %6 = affine.apply affine_map<()[s0] -> (s0 + 448)>()[%0]
    %7 = memref.load %1[%c0, %6] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %8 = affine.apply affine_map<()[s0] -> (s0 + 384)>()[%0]
    %9 = memref.load %2[%workgroup_id_x, %8] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %10 = affine.apply affine_map<()[s0] -> (s0 + 384)>()[%0]
    %11 = memref.load %1[%c0, %10] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %12 = affine.apply affine_map<()[s0] -> (s0 + 320)>()[%0]
    %13 = memref.load %2[%workgroup_id_x, %12] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %14 = affine.apply affine_map<()[s0] -> (s0 + 320)>()[%0]
    %15 = memref.load %1[%c0, %14] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %16 = affine.apply affine_map<()[s0] -> (s0 + 256)>()[%0]
    %17 = memref.load %2[%workgroup_id_x, %16] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %18 = affine.apply affine_map<()[s0] -> (s0 + 256)>()[%0]
    %19 = memref.load %1[%c0, %18] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %20 = affine.apply affine_map<()[s0] -> (s0 + 192)>()[%0]
    %21 = memref.load %2[%workgroup_id_x, %20] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %22 = affine.apply affine_map<()[s0] -> (s0 + 192)>()[%0]
    %23 = memref.load %1[%c0, %22] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %24 = affine.apply affine_map<()[s0] -> (s0 + 128)>()[%0]
    %25 = memref.load %2[%workgroup_id_x, %24] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %26 = affine.apply affine_map<()[s0] -> (s0 + 128)>()[%0]
    %27 = memref.load %1[%c0, %26] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %28 = affine.apply affine_map<()[s0] -> (s0 + 64)>()[%0]
    %29 = memref.load %2[%workgroup_id_x, %28] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %30 = affine.apply affine_map<()[s0] -> (s0 + 64)>()[%0]
    %31 = memref.load %1[%c0, %30] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %32 = memref.load %2[%workgroup_id_x, %0] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %33 = memref.load %1[%c0, %0] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %34 = vector.extract_strided_slice %7 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %35 = vector.bitcast %34 : vector<2xf32> to vector<4xf16>
    %36 = vector.extract_strided_slice %5 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %37 = vector.bitcast %36 : vector<2xf32> to vector<4xf16>
    %38 = arith.mulf %35, %37 : vector<4xf16>
    %39 = vector.extract_strided_slice %7 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %40 = vector.bitcast %39 : vector<2xf32> to vector<4xf16>
    %41 = vector.extract_strided_slice %5 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %42 = vector.bitcast %41 : vector<2xf32> to vector<4xf16>
    %43 = arith.mulf %40, %42 : vector<4xf16>
    %44 = arith.addf %38, %cst : vector<4xf16>
    %45 = arith.addf %43, %cst : vector<4xf16>
    %46 = vector.extract_strided_slice %11 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %47 = vector.bitcast %46 : vector<2xf32> to vector<4xf16>
    %48 = vector.extract_strided_slice %9 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %49 = vector.bitcast %48 : vector<2xf32> to vector<4xf16>
    %50 = arith.mulf %47, %49 : vector<4xf16>
    %51 = vector.extract_strided_slice %11 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %52 = vector.bitcast %51 : vector<2xf32> to vector<4xf16>
    %53 = vector.extract_strided_slice %9 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %54 = vector.bitcast %53 : vector<2xf32> to vector<4xf16>
    %55 = arith.mulf %52, %54 : vector<4xf16>
    %56 = arith.addf %50, %44 : vector<4xf16>
    %57 = arith.addf %55, %45 : vector<4xf16>
    %58 = vector.extract_strided_slice %15 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %59 = vector.bitcast %58 : vector<2xf32> to vector<4xf16>
    %60 = vector.extract_strided_slice %13 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %61 = vector.bitcast %60 : vector<2xf32> to vector<4xf16>
    %62 = arith.mulf %59, %61 : vector<4xf16>
    %63 = vector.extract_strided_slice %15 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %64 = vector.bitcast %63 : vector<2xf32> to vector<4xf16>
    %65 = vector.extract_strided_slice %13 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %66 = vector.bitcast %65 : vector<2xf32> to vector<4xf16>
    %67 = arith.mulf %64, %66 : vector<4xf16>
    %68 = arith.addf %62, %56 : vector<4xf16>
    %69 = arith.addf %67, %57 : vector<4xf16>
    %70 = vector.extract_strided_slice %19 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %71 = vector.bitcast %70 : vector<2xf32> to vector<4xf16>
    %72 = vector.extract_strided_slice %17 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %73 = vector.bitcast %72 : vector<2xf32> to vector<4xf16>
    %74 = arith.mulf %71, %73 : vector<4xf16>
    %75 = vector.extract_strided_slice %19 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %76 = vector.bitcast %75 : vector<2xf32> to vector<4xf16>
    %77 = vector.extract_strided_slice %17 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %78 = vector.bitcast %77 : vector<2xf32> to vector<4xf16>
    %79 = arith.mulf %76, %78 : vector<4xf16>
    %80 = arith.addf %74, %68 : vector<4xf16>
    %81 = arith.addf %79, %69 : vector<4xf16>
    %82 = vector.extract_strided_slice %23 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %83 = vector.bitcast %82 : vector<2xf32> to vector<4xf16>
    %84 = vector.extract_strided_slice %21 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %85 = vector.bitcast %84 : vector<2xf32> to vector<4xf16>
    %86 = arith.mulf %83, %85 : vector<4xf16>
    %87 = vector.extract_strided_slice %23 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %88 = vector.bitcast %87 : vector<2xf32> to vector<4xf16>
    %89 = vector.extract_strided_slice %21 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %90 = vector.bitcast %89 : vector<2xf32> to vector<4xf16>
    %91 = arith.mulf %88, %90 : vector<4xf16>
    %92 = arith.addf %86, %80 : vector<4xf16>
    %93 = arith.addf %91, %81 : vector<4xf16>
    %94 = vector.extract_strided_slice %27 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %95 = vector.bitcast %94 : vector<2xf32> to vector<4xf16>
    %96 = vector.extract_strided_slice %25 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %97 = vector.bitcast %96 : vector<2xf32> to vector<4xf16>
    %98 = arith.mulf %95, %97 : vector<4xf16>
    %99 = vector.extract_strided_slice %27 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %100 = vector.bitcast %99 : vector<2xf32> to vector<4xf16>
    %101 = vector.extract_strided_slice %25 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %102 = vector.bitcast %101 : vector<2xf32> to vector<4xf16>
    %103 = arith.mulf %100, %102 : vector<4xf16>
    %104 = arith.addf %98, %92 : vector<4xf16>
    %105 = arith.addf %103, %93 : vector<4xf16>
    %106 = vector.extract_strided_slice %31 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %107 = vector.bitcast %106 : vector<2xf32> to vector<4xf16>
    %108 = vector.extract_strided_slice %29 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %109 = vector.bitcast %108 : vector<2xf32> to vector<4xf16>
    %110 = arith.mulf %107, %109 : vector<4xf16>
    %111 = vector.extract_strided_slice %31 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %112 = vector.bitcast %111 : vector<2xf32> to vector<4xf16>
    %113 = vector.extract_strided_slice %29 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %114 = vector.bitcast %113 : vector<2xf32> to vector<4xf16>
    %115 = arith.mulf %112, %114 : vector<4xf16>
    %116 = arith.addf %110, %104 : vector<4xf16>
    %117 = arith.addf %115, %105 : vector<4xf16>
    %118 = vector.extract_strided_slice %33 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %119 = vector.bitcast %118 : vector<2xf32> to vector<4xf16>
    %120 = vector.extract_strided_slice %32 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %121 = vector.bitcast %120 : vector<2xf32> to vector<4xf16>
    %122 = arith.mulf %119, %121 : vector<4xf16>
    %123 = vector.extract_strided_slice %33 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %124 = vector.bitcast %123 : vector<2xf32> to vector<4xf16>
    %125 = vector.extract_strided_slice %32 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %126 = vector.bitcast %125 : vector<2xf32> to vector<4xf16>
    %127 = arith.mulf %124, %126 : vector<4xf16>
    %128 = arith.addf %122, %116 : vector<4xf16>
    %129 = arith.addf %127, %117 : vector<4xf16>
    %130 = vector.reduction <add>, %128 : vector<4xf16> into f16
    %131 = vector.reduction <add>, %129 : vector<4xf16> into f16
    %132 = arith.addf %130, %131 : f16
    %133 = gpu.subgroup_reduce  add %132 : (f16) -> f16
    %134 = arith.addf %133, %cst_0 : f16
    %135 = arith.cmpi eq, %0, %c0 : index
    scf.if %135 {
      memref.store %134, %3[%c0, %workgroup_id_x] : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    }
    return
  }
}

// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
    %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f16
    %0 = gpu.thread_id  x
    %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %1, 64 : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %2, 64 : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %3, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = affine.apply affine_map<()[s0] -> (s0 + 448)>()[%0]
    %5 = memref.load %2[%workgroup_id_x, %4] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %6 = memref.load %1[%c0, %4] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %7 = affine.apply affine_map<()[s0] -> (s0 + 384)>()[%0]
    %8 = memref.load %2[%workgroup_id_x, %7] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %9 = memref.load %1[%c0, %7] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %10 = affine.apply affine_map<()[s0] -> (s0 + 320)>()[%0]
    %11 = memref.load %2[%workgroup_id_x, %10] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %12 = memref.load %1[%c0, %10] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %13 = affine.apply affine_map<()[s0] -> (s0 + 256)>()[%0]
    %14 = memref.load %2[%workgroup_id_x, %13] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %15 = memref.load %1[%c0, %13] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %16 = affine.apply affine_map<()[s0] -> (s0 + 192)>()[%0]
    %17 = memref.load %2[%workgroup_id_x, %16] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %18 = memref.load %1[%c0, %16] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %19 = affine.apply affine_map<()[s0] -> (s0 + 128)>()[%0]
    %20 = memref.load %2[%workgroup_id_x, %19] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %21 = memref.load %1[%c0, %19] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %22 = affine.apply affine_map<()[s0] -> (s0 + 64)>()[%0]
    %23 = memref.load %2[%workgroup_id_x, %22] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %24 = memref.load %1[%c0, %22] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %25 = memref.load %2[%workgroup_id_x, %0] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %26 = memref.load %1[%c0, %0] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %27 = vector.extract_strided_slice %6 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %28 = vector.bitcast %27 : vector<2xf32> to vector<4xf16>
    %29 = vector.extract_strided_slice %5 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %30 = vector.bitcast %29 : vector<2xf32> to vector<4xf16>
    %31 = arith.mulf %28, %30 : vector<4xf16>
    %32 = vector.extract_strided_slice %6 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %33 = vector.bitcast %32 : vector<2xf32> to vector<4xf16>
    %34 = vector.extract_strided_slice %5 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %35 = vector.bitcast %34 : vector<2xf32> to vector<4xf16>
    %36 = arith.mulf %33, %35 : vector<4xf16>
    %37 = arith.addf %31, %cst : vector<4xf16>
    %38 = arith.addf %36, %cst : vector<4xf16>
    %39 = vector.extract_strided_slice %9 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %40 = vector.bitcast %39 : vector<2xf32> to vector<4xf16>
    %41 = vector.extract_strided_slice %8 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %42 = vector.bitcast %41 : vector<2xf32> to vector<4xf16>
    %43 = arith.mulf %40, %42 : vector<4xf16>
    %44 = vector.extract_strided_slice %9 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %45 = vector.bitcast %44 : vector<2xf32> to vector<4xf16>
    %46 = vector.extract_strided_slice %8 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %47 = vector.bitcast %46 : vector<2xf32> to vector<4xf16>
    %48 = arith.mulf %45, %47 : vector<4xf16>
    %49 = arith.addf %43, %37 : vector<4xf16>
    %50 = arith.addf %48, %38 : vector<4xf16>
    %51 = vector.extract_strided_slice %12 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %52 = vector.bitcast %51 : vector<2xf32> to vector<4xf16>
    %53 = vector.extract_strided_slice %11 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %54 = vector.bitcast %53 : vector<2xf32> to vector<4xf16>
    %55 = arith.mulf %52, %54 : vector<4xf16>
    %56 = vector.extract_strided_slice %12 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %57 = vector.bitcast %56 : vector<2xf32> to vector<4xf16>
    %58 = vector.extract_strided_slice %11 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %59 = vector.bitcast %58 : vector<2xf32> to vector<4xf16>
    %60 = arith.mulf %57, %59 : vector<4xf16>
    %61 = arith.addf %55, %49 : vector<4xf16>
    %62 = arith.addf %60, %50 : vector<4xf16>
    %63 = vector.extract_strided_slice %15 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %64 = vector.bitcast %63 : vector<2xf32> to vector<4xf16>
    %65 = vector.extract_strided_slice %14 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %66 = vector.bitcast %65 : vector<2xf32> to vector<4xf16>
    %67 = arith.mulf %64, %66 : vector<4xf16>
    %68 = vector.extract_strided_slice %15 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %69 = vector.bitcast %68 : vector<2xf32> to vector<4xf16>
    %70 = vector.extract_strided_slice %14 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %71 = vector.bitcast %70 : vector<2xf32> to vector<4xf16>
    %72 = arith.mulf %69, %71 : vector<4xf16>
    %73 = arith.addf %67, %61 : vector<4xf16>
    %74 = arith.addf %72, %62 : vector<4xf16>
    %75 = vector.extract_strided_slice %18 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %76 = vector.bitcast %75 : vector<2xf32> to vector<4xf16>
    %77 = vector.extract_strided_slice %17 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %78 = vector.bitcast %77 : vector<2xf32> to vector<4xf16>
    %79 = arith.mulf %76, %78 : vector<4xf16>
    %80 = vector.extract_strided_slice %18 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %81 = vector.bitcast %80 : vector<2xf32> to vector<4xf16>
    %82 = vector.extract_strided_slice %17 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %83 = vector.bitcast %82 : vector<2xf32> to vector<4xf16>
    %84 = arith.mulf %81, %83 : vector<4xf16>
    %85 = arith.addf %79, %73 : vector<4xf16>
    %86 = arith.addf %84, %74 : vector<4xf16>
    %87 = vector.extract_strided_slice %21 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %88 = vector.bitcast %87 : vector<2xf32> to vector<4xf16>
    %89 = vector.extract_strided_slice %20 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %90 = vector.bitcast %89 : vector<2xf32> to vector<4xf16>
    %91 = arith.mulf %88, %90 : vector<4xf16>
    %92 = vector.extract_strided_slice %21 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %93 = vector.bitcast %92 : vector<2xf32> to vector<4xf16>
    %94 = vector.extract_strided_slice %20 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %95 = vector.bitcast %94 : vector<2xf32> to vector<4xf16>
    %96 = arith.mulf %93, %95 : vector<4xf16>
    %97 = arith.addf %91, %85 : vector<4xf16>
    %98 = arith.addf %96, %86 : vector<4xf16>
    %99 = vector.extract_strided_slice %24 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %100 = vector.bitcast %99 : vector<2xf32> to vector<4xf16>
    %101 = vector.extract_strided_slice %23 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %102 = vector.bitcast %101 : vector<2xf32> to vector<4xf16>
    %103 = arith.mulf %100, %102 : vector<4xf16>
    %104 = vector.extract_strided_slice %24 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %105 = vector.bitcast %104 : vector<2xf32> to vector<4xf16>
    %106 = vector.extract_strided_slice %23 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %107 = vector.bitcast %106 : vector<2xf32> to vector<4xf16>
    %108 = arith.mulf %105, %107 : vector<4xf16>
    %109 = arith.addf %103, %97 : vector<4xf16>
    %110 = arith.addf %108, %98 : vector<4xf16>
    %111 = vector.extract_strided_slice %26 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %112 = vector.bitcast %111 : vector<2xf32> to vector<4xf16>
    %113 = vector.extract_strided_slice %25 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %114 = vector.bitcast %113 : vector<2xf32> to vector<4xf16>
    %115 = arith.mulf %112, %114 : vector<4xf16>
    %116 = vector.extract_strided_slice %26 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %117 = vector.bitcast %116 : vector<2xf32> to vector<4xf16>
    %118 = vector.extract_strided_slice %25 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %119 = vector.bitcast %118 : vector<2xf32> to vector<4xf16>
    %120 = arith.mulf %117, %119 : vector<4xf16>
    %121 = arith.addf %115, %109 : vector<4xf16>
    %122 = arith.addf %120, %110 : vector<4xf16>
    %123 = vector.reduction <add>, %121 : vector<4xf16> into f16
    %124 = vector.reduction <add>, %122 : vector<4xf16> into f16
    %125 = arith.addf %123, %124 : f16
    %126 = gpu.subgroup_reduce  add %125 : (f16) -> f16
    %127 = arith.addf %126, %cst_0 : f16
    %128 = arith.cmpi eq, %0, %c0 : index
    scf.if %128 {
      memref.store %127, %3[%c0, %workgroup_id_x] : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
    }
    return
  }
}

// -----// IR Dump After OptimizeVectorTransfer (iree-codegen-optimize-vector-transfer) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f16
  %0 = gpu.thread_id  x
  %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %3, 64 : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = affine.apply affine_map<()[s0] -> (s0 + 448)>()[%0]
  %5 = memref.load %2[%workgroup_id_x, %4] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %6 = memref.load %1[%c0, %4] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %7 = affine.apply affine_map<()[s0] -> (s0 + 384)>()[%0]
  %8 = memref.load %2[%workgroup_id_x, %7] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %9 = memref.load %1[%c0, %7] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %10 = affine.apply affine_map<()[s0] -> (s0 + 320)>()[%0]
  %11 = memref.load %2[%workgroup_id_x, %10] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %12 = memref.load %1[%c0, %10] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %13 = affine.apply affine_map<()[s0] -> (s0 + 256)>()[%0]
  %14 = memref.load %2[%workgroup_id_x, %13] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %15 = memref.load %1[%c0, %13] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %16 = affine.apply affine_map<()[s0] -> (s0 + 192)>()[%0]
  %17 = memref.load %2[%workgroup_id_x, %16] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %18 = memref.load %1[%c0, %16] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %19 = affine.apply affine_map<()[s0] -> (s0 + 128)>()[%0]
  %20 = memref.load %2[%workgroup_id_x, %19] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %21 = memref.load %1[%c0, %19] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %22 = affine.apply affine_map<()[s0] -> (s0 + 64)>()[%0]
  %23 = memref.load %2[%workgroup_id_x, %22] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %24 = memref.load %1[%c0, %22] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %25 = memref.load %2[%workgroup_id_x, %0] : memref<32000x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %26 = memref.load %1[%c0, %0] : memref<1x512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %27 = vector.extract_strided_slice %6 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %28 = vector.bitcast %27 : vector<2xf32> to vector<4xf16>
  %29 = vector.extract_strided_slice %5 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %30 = vector.bitcast %29 : vector<2xf32> to vector<4xf16>
  %31 = arith.mulf %28, %30 : vector<4xf16>
  %32 = vector.extract_strided_slice %6 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %33 = vector.bitcast %32 : vector<2xf32> to vector<4xf16>
  %34 = vector.extract_strided_slice %5 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %35 = vector.bitcast %34 : vector<2xf32> to vector<4xf16>
  %36 = arith.mulf %33, %35 : vector<4xf16>
  %37 = arith.addf %31, %cst : vector<4xf16>
  %38 = arith.addf %36, %cst : vector<4xf16>
  %39 = vector.extract_strided_slice %9 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %40 = vector.bitcast %39 : vector<2xf32> to vector<4xf16>
  %41 = vector.extract_strided_slice %8 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %42 = vector.bitcast %41 : vector<2xf32> to vector<4xf16>
  %43 = arith.mulf %40, %42 : vector<4xf16>
  %44 = vector.extract_strided_slice %9 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %45 = vector.bitcast %44 : vector<2xf32> to vector<4xf16>
  %46 = vector.extract_strided_slice %8 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %47 = vector.bitcast %46 : vector<2xf32> to vector<4xf16>
  %48 = arith.mulf %45, %47 : vector<4xf16>
  %49 = arith.addf %43, %37 : vector<4xf16>
  %50 = arith.addf %48, %38 : vector<4xf16>
  %51 = vector.extract_strided_slice %12 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %52 = vector.bitcast %51 : vector<2xf32> to vector<4xf16>
  %53 = vector.extract_strided_slice %11 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %54 = vector.bitcast %53 : vector<2xf32> to vector<4xf16>
  %55 = arith.mulf %52, %54 : vector<4xf16>
  %56 = vector.extract_strided_slice %12 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %57 = vector.bitcast %56 : vector<2xf32> to vector<4xf16>
  %58 = vector.extract_strided_slice %11 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %59 = vector.bitcast %58 : vector<2xf32> to vector<4xf16>
  %60 = arith.mulf %57, %59 : vector<4xf16>
  %61 = arith.addf %55, %49 : vector<4xf16>
  %62 = arith.addf %60, %50 : vector<4xf16>
  %63 = vector.extract_strided_slice %15 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %64 = vector.bitcast %63 : vector<2xf32> to vector<4xf16>
  %65 = vector.extract_strided_slice %14 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %66 = vector.bitcast %65 : vector<2xf32> to vector<4xf16>
  %67 = arith.mulf %64, %66 : vector<4xf16>
  %68 = vector.extract_strided_slice %15 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %69 = vector.bitcast %68 : vector<2xf32> to vector<4xf16>
  %70 = vector.extract_strided_slice %14 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %71 = vector.bitcast %70 : vector<2xf32> to vector<4xf16>
  %72 = arith.mulf %69, %71 : vector<4xf16>
  %73 = arith.addf %67, %61 : vector<4xf16>
  %74 = arith.addf %72, %62 : vector<4xf16>
  %75 = vector.extract_strided_slice %18 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %76 = vector.bitcast %75 : vector<2xf32> to vector<4xf16>
  %77 = vector.extract_strided_slice %17 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %78 = vector.bitcast %77 : vector<2xf32> to vector<4xf16>
  %79 = arith.mulf %76, %78 : vector<4xf16>
  %80 = vector.extract_strided_slice %18 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %81 = vector.bitcast %80 : vector<2xf32> to vector<4xf16>
  %82 = vector.extract_strided_slice %17 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %83 = vector.bitcast %82 : vector<2xf32> to vector<4xf16>
  %84 = arith.mulf %81, %83 : vector<4xf16>
  %85 = arith.addf %79, %73 : vector<4xf16>
  %86 = arith.addf %84, %74 : vector<4xf16>
  %87 = vector.extract_strided_slice %21 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %88 = vector.bitcast %87 : vector<2xf32> to vector<4xf16>
  %89 = vector.extract_strided_slice %20 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %90 = vector.bitcast %89 : vector<2xf32> to vector<4xf16>
  %91 = arith.mulf %88, %90 : vector<4xf16>
  %92 = vector.extract_strided_slice %21 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %93 = vector.bitcast %92 : vector<2xf32> to vector<4xf16>
  %94 = vector.extract_strided_slice %20 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %95 = vector.bitcast %94 : vector<2xf32> to vector<4xf16>
  %96 = arith.mulf %93, %95 : vector<4xf16>
  %97 = arith.addf %91, %85 : vector<4xf16>
  %98 = arith.addf %96, %86 : vector<4xf16>
  %99 = vector.extract_strided_slice %24 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %100 = vector.bitcast %99 : vector<2xf32> to vector<4xf16>
  %101 = vector.extract_strided_slice %23 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %102 = vector.bitcast %101 : vector<2xf32> to vector<4xf16>
  %103 = arith.mulf %100, %102 : vector<4xf16>
  %104 = vector.extract_strided_slice %24 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %105 = vector.bitcast %104 : vector<2xf32> to vector<4xf16>
  %106 = vector.extract_strided_slice %23 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %107 = vector.bitcast %106 : vector<2xf32> to vector<4xf16>
  %108 = arith.mulf %105, %107 : vector<4xf16>
  %109 = arith.addf %103, %97 : vector<4xf16>
  %110 = arith.addf %108, %98 : vector<4xf16>
  %111 = vector.extract_strided_slice %26 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %112 = vector.bitcast %111 : vector<2xf32> to vector<4xf16>
  %113 = vector.extract_strided_slice %25 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %114 = vector.bitcast %113 : vector<2xf32> to vector<4xf16>
  %115 = arith.mulf %112, %114 : vector<4xf16>
  %116 = vector.extract_strided_slice %26 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %117 = vector.bitcast %116 : vector<2xf32> to vector<4xf16>
  %118 = vector.extract_strided_slice %25 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %119 = vector.bitcast %118 : vector<2xf32> to vector<4xf16>
  %120 = arith.mulf %117, %119 : vector<4xf16>
  %121 = arith.addf %115, %109 : vector<4xf16>
  %122 = arith.addf %120, %110 : vector<4xf16>
  %123 = vector.reduction <add>, %121 : vector<4xf16> into f16
  %124 = vector.reduction <add>, %122 : vector<4xf16> into f16
  %125 = arith.addf %123, %124 : f16
  %126 = gpu.subgroup_reduce  add %125 : (f16) -> f16
  %127 = arith.addf %126, %cst_0 : f16
  %128 = arith.cmpi eq, %0, %c0 : index
  scf.if %128 {
    memref.store %127, %3[%c0, %workgroup_id_x] : memref<1x32000xf16, #hal.descriptor_type<storage_buffer>>
  }
  return
}

// -----// IR Dump After FlattenMemRefSubspan (iree-codegen-flatten-memref-subspan) //----- //
module {
  func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
    %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f16
    %0 = gpu.thread_id  x
    %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<16384000xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<32000xf16, #hal.descriptor_type<storage_buffer>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 512 + 448)>()[%0, %workgroup_id_x]
    %5 = memref.load %2[%4] : memref<16384000xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %6 = affine.apply affine_map<()[s0] -> (s0 + 448)>()[%0]
    %7 = memref.load %1[%6] : memref<512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 512 + 384)>()[%0, %workgroup_id_x]
    %9 = memref.load %2[%8] : memref<16384000xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %10 = affine.apply affine_map<()[s0] -> (s0 + 384)>()[%0]
    %11 = memref.load %1[%10] : memref<512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 512 + 320)>()[%0, %workgroup_id_x]
    %13 = memref.load %2[%12] : memref<16384000xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %14 = affine.apply affine_map<()[s0] -> (s0 + 320)>()[%0]
    %15 = memref.load %1[%14] : memref<512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %16 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 512 + 256)>()[%0, %workgroup_id_x]
    %17 = memref.load %2[%16] : memref<16384000xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %18 = affine.apply affine_map<()[s0] -> (s0 + 256)>()[%0]
    %19 = memref.load %1[%18] : memref<512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %20 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 512 + 192)>()[%0, %workgroup_id_x]
    %21 = memref.load %2[%20] : memref<16384000xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %22 = affine.apply affine_map<()[s0] -> (s0 + 192)>()[%0]
    %23 = memref.load %1[%22] : memref<512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %24 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 512 + 128)>()[%0, %workgroup_id_x]
    %25 = memref.load %2[%24] : memref<16384000xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %26 = affine.apply affine_map<()[s0] -> (s0 + 128)>()[%0]
    %27 = memref.load %1[%26] : memref<512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %28 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 512 + 64)>()[%0, %workgroup_id_x]
    %29 = memref.load %2[%28] : memref<16384000xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %30 = affine.apply affine_map<()[s0] -> (s0 + 64)>()[%0]
    %31 = memref.load %1[%30] : memref<512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %32 = affine.apply affine_map<()[s0, s1] -> (s0 * 512 + s1)>()[%workgroup_id_x, %0]
    %33 = memref.load %2[%32] : memref<16384000xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %34 = memref.load %1[%0] : memref<512xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %35 = vector.extract_strided_slice %7 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %36 = vector.bitcast %35 : vector<2xf32> to vector<4xf16>
    %37 = vector.extract_strided_slice %5 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %38 = vector.bitcast %37 : vector<2xf32> to vector<4xf16>
    %39 = arith.mulf %36, %38 : vector<4xf16>
    %40 = vector.extract_strided_slice %7 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %41 = vector.bitcast %40 : vector<2xf32> to vector<4xf16>
    %42 = vector.extract_strided_slice %5 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %43 = vector.bitcast %42 : vector<2xf32> to vector<4xf16>
    %44 = arith.mulf %41, %43 : vector<4xf16>
    %45 = arith.addf %39, %cst : vector<4xf16>
    %46 = arith.addf %44, %cst : vector<4xf16>
    %47 = vector.extract_strided_slice %11 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %48 = vector.bitcast %47 : vector<2xf32> to vector<4xf16>
    %49 = vector.extract_strided_slice %9 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %50 = vector.bitcast %49 : vector<2xf32> to vector<4xf16>
    %51 = arith.mulf %48, %50 : vector<4xf16>
    %52 = vector.extract_strided_slice %11 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %53 = vector.bitcast %52 : vector<2xf32> to vector<4xf16>
    %54 = vector.extract_strided_slice %9 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %55 = vector.bitcast %54 : vector<2xf32> to vector<4xf16>
    %56 = arith.mulf %53, %55 : vector<4xf16>
    %57 = arith.addf %51, %45 : vector<4xf16>
    %58 = arith.addf %56, %46 : vector<4xf16>
    %59 = vector.extract_strided_slice %15 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %60 = vector.bitcast %59 : vector<2xf32> to vector<4xf16>
    %61 = vector.extract_strided_slice %13 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %62 = vector.bitcast %61 : vector<2xf32> to vector<4xf16>
    %63 = arith.mulf %60, %62 : vector<4xf16>
    %64 = vector.extract_strided_slice %15 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %65 = vector.bitcast %64 : vector<2xf32> to vector<4xf16>
    %66 = vector.extract_strided_slice %13 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %67 = vector.bitcast %66 : vector<2xf32> to vector<4xf16>
    %68 = arith.mulf %65, %67 : vector<4xf16>
    %69 = arith.addf %63, %57 : vector<4xf16>
    %70 = arith.addf %68, %58 : vector<4xf16>
    %71 = vector.extract_strided_slice %19 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %72 = vector.bitcast %71 : vector<2xf32> to vector<4xf16>
    %73 = vector.extract_strided_slice %17 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %74 = vector.bitcast %73 : vector<2xf32> to vector<4xf16>
    %75 = arith.mulf %72, %74 : vector<4xf16>
    %76 = vector.extract_strided_slice %19 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %77 = vector.bitcast %76 : vector<2xf32> to vector<4xf16>
    %78 = vector.extract_strided_slice %17 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %79 = vector.bitcast %78 : vector<2xf32> to vector<4xf16>
    %80 = arith.mulf %77, %79 : vector<4xf16>
    %81 = arith.addf %75, %69 : vector<4xf16>
    %82 = arith.addf %80, %70 : vector<4xf16>
    %83 = vector.extract_strided_slice %23 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %84 = vector.bitcast %83 : vector<2xf32> to vector<4xf16>
    %85 = vector.extract_strided_slice %21 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %86 = vector.bitcast %85 : vector<2xf32> to vector<4xf16>
    %87 = arith.mulf %84, %86 : vector<4xf16>
    %88 = vector.extract_strided_slice %23 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %89 = vector.bitcast %88 : vector<2xf32> to vector<4xf16>
    %90 = vector.extract_strided_slice %21 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %91 = vector.bitcast %90 : vector<2xf32> to vector<4xf16>
    %92 = arith.mulf %89, %91 : vector<4xf16>
    %93 = arith.addf %87, %81 : vector<4xf16>
    %94 = arith.addf %92, %82 : vector<4xf16>
    %95 = vector.extract_strided_slice %27 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %96 = vector.bitcast %95 : vector<2xf32> to vector<4xf16>
    %97 = vector.extract_strided_slice %25 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %98 = vector.bitcast %97 : vector<2xf32> to vector<4xf16>
    %99 = arith.mulf %96, %98 : vector<4xf16>
    %100 = vector.extract_strided_slice %27 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %101 = vector.bitcast %100 : vector<2xf32> to vector<4xf16>
    %102 = vector.extract_strided_slice %25 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %103 = vector.bitcast %102 : vector<2xf32> to vector<4xf16>
    %104 = arith.mulf %101, %103 : vector<4xf16>
    %105 = arith.addf %99, %93 : vector<4xf16>
    %106 = arith.addf %104, %94 : vector<4xf16>
    %107 = vector.extract_strided_slice %31 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %108 = vector.bitcast %107 : vector<2xf32> to vector<4xf16>
    %109 = vector.extract_strided_slice %29 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %110 = vector.bitcast %109 : vector<2xf32> to vector<4xf16>
    %111 = arith.mulf %108, %110 : vector<4xf16>
    %112 = vector.extract_strided_slice %31 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %113 = vector.bitcast %112 : vector<2xf32> to vector<4xf16>
    %114 = vector.extract_strided_slice %29 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %115 = vector.bitcast %114 : vector<2xf32> to vector<4xf16>
    %116 = arith.mulf %113, %115 : vector<4xf16>
    %117 = arith.addf %111, %105 : vector<4xf16>
    %118 = arith.addf %116, %106 : vector<4xf16>
    %119 = vector.extract_strided_slice %34 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %120 = vector.bitcast %119 : vector<2xf32> to vector<4xf16>
    %121 = vector.extract_strided_slice %33 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %122 = vector.bitcast %121 : vector<2xf32> to vector<4xf16>
    %123 = arith.mulf %120, %122 : vector<4xf16>
    %124 = vector.extract_strided_slice %34 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %125 = vector.bitcast %124 : vector<2xf32> to vector<4xf16>
    %126 = vector.extract_strided_slice %33 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %127 = vector.bitcast %126 : vector<2xf32> to vector<4xf16>
    %128 = arith.mulf %125, %127 : vector<4xf16>
    %129 = arith.addf %123, %117 : vector<4xf16>
    %130 = arith.addf %128, %118 : vector<4xf16>
    %131 = vector.reduction <add>, %129 : vector<4xf16> into f16
    %132 = vector.reduction <add>, %130 : vector<4xf16> into f16
    %133 = arith.addf %131, %132 : f16
    %134 = gpu.subgroup_reduce  add %133 : (f16) -> f16
    %135 = arith.addf %134, %cst_0 : f16
    %136 = arith.cmpi eq, %0, %c0 : index
    scf.if %136 {
      memref.store %135, %3[%workgroup_id_x] : memref<32000xf16, #hal.descriptor_type<storage_buffer>>
    }
    return
  }
}

// -----// IR Dump After SPIRVEraseStorageBufferStaticShape (iree-spirv-erase-storage-buffer-static-shape) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %c32000 = arith.constant 32000 : index
  %c16384000 = arith.constant 16384000 : index
  %c512 = arith.constant 512 : index
  %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f16
  %0 = gpu.thread_id  x
  %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>{%c512}
  %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>{%c16384000}
  %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<?xf16, #hal.descriptor_type<storage_buffer>>{%c32000}
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 512 + 448)>()[%0, %workgroup_id_x]
  %5 = memref.load %2[%4] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %6 = affine.apply affine_map<()[s0] -> (s0 + 448)>()[%0]
  %7 = memref.load %1[%6] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 512 + 384)>()[%0, %workgroup_id_x]
  %9 = memref.load %2[%8] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %10 = affine.apply affine_map<()[s0] -> (s0 + 384)>()[%0]
  %11 = memref.load %1[%10] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 512 + 320)>()[%0, %workgroup_id_x]
  %13 = memref.load %2[%12] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %14 = affine.apply affine_map<()[s0] -> (s0 + 320)>()[%0]
  %15 = memref.load %1[%14] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %16 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 512 + 256)>()[%0, %workgroup_id_x]
  %17 = memref.load %2[%16] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %18 = affine.apply affine_map<()[s0] -> (s0 + 256)>()[%0]
  %19 = memref.load %1[%18] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %20 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 512 + 192)>()[%0, %workgroup_id_x]
  %21 = memref.load %2[%20] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %22 = affine.apply affine_map<()[s0] -> (s0 + 192)>()[%0]
  %23 = memref.load %1[%22] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %24 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 512 + 128)>()[%0, %workgroup_id_x]
  %25 = memref.load %2[%24] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %26 = affine.apply affine_map<()[s0] -> (s0 + 128)>()[%0]
  %27 = memref.load %1[%26] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %28 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 512 + 64)>()[%0, %workgroup_id_x]
  %29 = memref.load %2[%28] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %30 = affine.apply affine_map<()[s0] -> (s0 + 64)>()[%0]
  %31 = memref.load %1[%30] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %32 = affine.apply affine_map<()[s0, s1] -> (s0 * 512 + s1)>()[%workgroup_id_x, %0]
  %33 = memref.load %2[%32] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %34 = memref.load %1[%0] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
  %35 = vector.extract_strided_slice %7 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %36 = vector.bitcast %35 : vector<2xf32> to vector<4xf16>
  %37 = vector.extract_strided_slice %5 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %38 = vector.bitcast %37 : vector<2xf32> to vector<4xf16>
  %39 = arith.mulf %36, %38 : vector<4xf16>
  %40 = vector.extract_strided_slice %7 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %41 = vector.bitcast %40 : vector<2xf32> to vector<4xf16>
  %42 = vector.extract_strided_slice %5 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %43 = vector.bitcast %42 : vector<2xf32> to vector<4xf16>
  %44 = arith.mulf %41, %43 : vector<4xf16>
  %45 = arith.addf %39, %cst : vector<4xf16>
  %46 = arith.addf %44, %cst : vector<4xf16>
  %47 = vector.extract_strided_slice %11 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %48 = vector.bitcast %47 : vector<2xf32> to vector<4xf16>
  %49 = vector.extract_strided_slice %9 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %50 = vector.bitcast %49 : vector<2xf32> to vector<4xf16>
  %51 = arith.mulf %48, %50 : vector<4xf16>
  %52 = vector.extract_strided_slice %11 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %53 = vector.bitcast %52 : vector<2xf32> to vector<4xf16>
  %54 = vector.extract_strided_slice %9 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %55 = vector.bitcast %54 : vector<2xf32> to vector<4xf16>
  %56 = arith.mulf %53, %55 : vector<4xf16>
  %57 = arith.addf %51, %45 : vector<4xf16>
  %58 = arith.addf %56, %46 : vector<4xf16>
  %59 = vector.extract_strided_slice %15 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %60 = vector.bitcast %59 : vector<2xf32> to vector<4xf16>
  %61 = vector.extract_strided_slice %13 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %62 = vector.bitcast %61 : vector<2xf32> to vector<4xf16>
  %63 = arith.mulf %60, %62 : vector<4xf16>
  %64 = vector.extract_strided_slice %15 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %65 = vector.bitcast %64 : vector<2xf32> to vector<4xf16>
  %66 = vector.extract_strided_slice %13 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %67 = vector.bitcast %66 : vector<2xf32> to vector<4xf16>
  %68 = arith.mulf %65, %67 : vector<4xf16>
  %69 = arith.addf %63, %57 : vector<4xf16>
  %70 = arith.addf %68, %58 : vector<4xf16>
  %71 = vector.extract_strided_slice %19 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %72 = vector.bitcast %71 : vector<2xf32> to vector<4xf16>
  %73 = vector.extract_strided_slice %17 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %74 = vector.bitcast %73 : vector<2xf32> to vector<4xf16>
  %75 = arith.mulf %72, %74 : vector<4xf16>
  %76 = vector.extract_strided_slice %19 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %77 = vector.bitcast %76 : vector<2xf32> to vector<4xf16>
  %78 = vector.extract_strided_slice %17 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %79 = vector.bitcast %78 : vector<2xf32> to vector<4xf16>
  %80 = arith.mulf %77, %79 : vector<4xf16>
  %81 = arith.addf %75, %69 : vector<4xf16>
  %82 = arith.addf %80, %70 : vector<4xf16>
  %83 = vector.extract_strided_slice %23 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %84 = vector.bitcast %83 : vector<2xf32> to vector<4xf16>
  %85 = vector.extract_strided_slice %21 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %86 = vector.bitcast %85 : vector<2xf32> to vector<4xf16>
  %87 = arith.mulf %84, %86 : vector<4xf16>
  %88 = vector.extract_strided_slice %23 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %89 = vector.bitcast %88 : vector<2xf32> to vector<4xf16>
  %90 = vector.extract_strided_slice %21 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %91 = vector.bitcast %90 : vector<2xf32> to vector<4xf16>
  %92 = arith.mulf %89, %91 : vector<4xf16>
  %93 = arith.addf %87, %81 : vector<4xf16>
  %94 = arith.addf %92, %82 : vector<4xf16>
  %95 = vector.extract_strided_slice %27 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %96 = vector.bitcast %95 : vector<2xf32> to vector<4xf16>
  %97 = vector.extract_strided_slice %25 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %98 = vector.bitcast %97 : vector<2xf32> to vector<4xf16>
  %99 = arith.mulf %96, %98 : vector<4xf16>
  %100 = vector.extract_strided_slice %27 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %101 = vector.bitcast %100 : vector<2xf32> to vector<4xf16>
  %102 = vector.extract_strided_slice %25 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %103 = vector.bitcast %102 : vector<2xf32> to vector<4xf16>
  %104 = arith.mulf %101, %103 : vector<4xf16>
  %105 = arith.addf %99, %93 : vector<4xf16>
  %106 = arith.addf %104, %94 : vector<4xf16>
  %107 = vector.extract_strided_slice %31 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %108 = vector.bitcast %107 : vector<2xf32> to vector<4xf16>
  %109 = vector.extract_strided_slice %29 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %110 = vector.bitcast %109 : vector<2xf32> to vector<4xf16>
  %111 = arith.mulf %108, %110 : vector<4xf16>
  %112 = vector.extract_strided_slice %31 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %113 = vector.bitcast %112 : vector<2xf32> to vector<4xf16>
  %114 = vector.extract_strided_slice %29 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %115 = vector.bitcast %114 : vector<2xf32> to vector<4xf16>
  %116 = arith.mulf %113, %115 : vector<4xf16>
  %117 = arith.addf %111, %105 : vector<4xf16>
  %118 = arith.addf %116, %106 : vector<4xf16>
  %119 = vector.extract_strided_slice %34 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %120 = vector.bitcast %119 : vector<2xf32> to vector<4xf16>
  %121 = vector.extract_strided_slice %33 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %122 = vector.bitcast %121 : vector<2xf32> to vector<4xf16>
  %123 = arith.mulf %120, %122 : vector<4xf16>
  %124 = vector.extract_strided_slice %34 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %125 = vector.bitcast %124 : vector<2xf32> to vector<4xf16>
  %126 = vector.extract_strided_slice %33 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %127 = vector.bitcast %126 : vector<2xf32> to vector<4xf16>
  %128 = arith.mulf %125, %127 : vector<4xf16>
  %129 = arith.addf %123, %117 : vector<4xf16>
  %130 = arith.addf %128, %118 : vector<4xf16>
  %131 = vector.reduction <add>, %129 : vector<4xf16> into f16
  %132 = vector.reduction <add>, %130 : vector<4xf16> into f16
  %133 = arith.addf %131, %132 : f16
  %134 = gpu.subgroup_reduce  add %133 : (f16) -> f16
  %135 = arith.addf %134, %cst_0 : f16
  %136 = arith.cmpi eq, %0, %c0 : index
  scf.if %136 {
    memref.store %135, %3[%workgroup_id_x] : memref<?xf16, #hal.descriptor_type<storage_buffer>>
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module {
  func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
    %c32000 = arith.constant 32000 : index
    %c16384000 = arith.constant 16384000 : index
    %c512 = arith.constant 512 : index
    %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f16
    %0 = gpu.thread_id  x
    %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>{%c512}
    %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>{%c16384000}
    %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<?xf16, #hal.descriptor_type<storage_buffer>>{%c32000}
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 512 + 448)>()[%0, %workgroup_id_x]
    %5 = memref.load %2[%4] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %6 = affine.apply affine_map<()[s0] -> (s0 + 448)>()[%0]
    %7 = memref.load %1[%6] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 512 + 384)>()[%0, %workgroup_id_x]
    %9 = memref.load %2[%8] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %10 = affine.apply affine_map<()[s0] -> (s0 + 384)>()[%0]
    %11 = memref.load %1[%10] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 512 + 320)>()[%0, %workgroup_id_x]
    %13 = memref.load %2[%12] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %14 = affine.apply affine_map<()[s0] -> (s0 + 320)>()[%0]
    %15 = memref.load %1[%14] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %16 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 512 + 256)>()[%0, %workgroup_id_x]
    %17 = memref.load %2[%16] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %18 = affine.apply affine_map<()[s0] -> (s0 + 256)>()[%0]
    %19 = memref.load %1[%18] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %20 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 512 + 192)>()[%0, %workgroup_id_x]
    %21 = memref.load %2[%20] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %22 = affine.apply affine_map<()[s0] -> (s0 + 192)>()[%0]
    %23 = memref.load %1[%22] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %24 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 512 + 128)>()[%0, %workgroup_id_x]
    %25 = memref.load %2[%24] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %26 = affine.apply affine_map<()[s0] -> (s0 + 128)>()[%0]
    %27 = memref.load %1[%26] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %28 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 512 + 64)>()[%0, %workgroup_id_x]
    %29 = memref.load %2[%28] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %30 = affine.apply affine_map<()[s0] -> (s0 + 64)>()[%0]
    %31 = memref.load %1[%30] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %32 = affine.apply affine_map<()[s0, s1] -> (s0 * 512 + s1)>()[%workgroup_id_x, %0]
    %33 = memref.load %2[%32] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %34 = memref.load %1[%0] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %35 = vector.extract_strided_slice %7 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %36 = vector.bitcast %35 : vector<2xf32> to vector<4xf16>
    %37 = vector.extract_strided_slice %5 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %38 = vector.bitcast %37 : vector<2xf32> to vector<4xf16>
    %39 = arith.mulf %36, %38 : vector<4xf16>
    %40 = vector.extract_strided_slice %7 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %41 = vector.bitcast %40 : vector<2xf32> to vector<4xf16>
    %42 = vector.extract_strided_slice %5 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %43 = vector.bitcast %42 : vector<2xf32> to vector<4xf16>
    %44 = arith.mulf %41, %43 : vector<4xf16>
    %45 = arith.addf %39, %cst : vector<4xf16>
    %46 = arith.addf %44, %cst : vector<4xf16>
    %47 = vector.extract_strided_slice %11 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %48 = vector.bitcast %47 : vector<2xf32> to vector<4xf16>
    %49 = vector.extract_strided_slice %9 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %50 = vector.bitcast %49 : vector<2xf32> to vector<4xf16>
    %51 = arith.mulf %48, %50 : vector<4xf16>
    %52 = vector.extract_strided_slice %11 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %53 = vector.bitcast %52 : vector<2xf32> to vector<4xf16>
    %54 = vector.extract_strided_slice %9 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %55 = vector.bitcast %54 : vector<2xf32> to vector<4xf16>
    %56 = arith.mulf %53, %55 : vector<4xf16>
    %57 = arith.addf %51, %45 : vector<4xf16>
    %58 = arith.addf %56, %46 : vector<4xf16>
    %59 = vector.extract_strided_slice %15 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %60 = vector.bitcast %59 : vector<2xf32> to vector<4xf16>
    %61 = vector.extract_strided_slice %13 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %62 = vector.bitcast %61 : vector<2xf32> to vector<4xf16>
    %63 = arith.mulf %60, %62 : vector<4xf16>
    %64 = vector.extract_strided_slice %15 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %65 = vector.bitcast %64 : vector<2xf32> to vector<4xf16>
    %66 = vector.extract_strided_slice %13 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %67 = vector.bitcast %66 : vector<2xf32> to vector<4xf16>
    %68 = arith.mulf %65, %67 : vector<4xf16>
    %69 = arith.addf %63, %57 : vector<4xf16>
    %70 = arith.addf %68, %58 : vector<4xf16>
    %71 = vector.extract_strided_slice %19 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %72 = vector.bitcast %71 : vector<2xf32> to vector<4xf16>
    %73 = vector.extract_strided_slice %17 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %74 = vector.bitcast %73 : vector<2xf32> to vector<4xf16>
    %75 = arith.mulf %72, %74 : vector<4xf16>
    %76 = vector.extract_strided_slice %19 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %77 = vector.bitcast %76 : vector<2xf32> to vector<4xf16>
    %78 = vector.extract_strided_slice %17 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %79 = vector.bitcast %78 : vector<2xf32> to vector<4xf16>
    %80 = arith.mulf %77, %79 : vector<4xf16>
    %81 = arith.addf %75, %69 : vector<4xf16>
    %82 = arith.addf %80, %70 : vector<4xf16>
    %83 = vector.extract_strided_slice %23 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %84 = vector.bitcast %83 : vector<2xf32> to vector<4xf16>
    %85 = vector.extract_strided_slice %21 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %86 = vector.bitcast %85 : vector<2xf32> to vector<4xf16>
    %87 = arith.mulf %84, %86 : vector<4xf16>
    %88 = vector.extract_strided_slice %23 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %89 = vector.bitcast %88 : vector<2xf32> to vector<4xf16>
    %90 = vector.extract_strided_slice %21 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %91 = vector.bitcast %90 : vector<2xf32> to vector<4xf16>
    %92 = arith.mulf %89, %91 : vector<4xf16>
    %93 = arith.addf %87, %81 : vector<4xf16>
    %94 = arith.addf %92, %82 : vector<4xf16>
    %95 = vector.extract_strided_slice %27 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %96 = vector.bitcast %95 : vector<2xf32> to vector<4xf16>
    %97 = vector.extract_strided_slice %25 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %98 = vector.bitcast %97 : vector<2xf32> to vector<4xf16>
    %99 = arith.mulf %96, %98 : vector<4xf16>
    %100 = vector.extract_strided_slice %27 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %101 = vector.bitcast %100 : vector<2xf32> to vector<4xf16>
    %102 = vector.extract_strided_slice %25 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %103 = vector.bitcast %102 : vector<2xf32> to vector<4xf16>
    %104 = arith.mulf %101, %103 : vector<4xf16>
    %105 = arith.addf %99, %93 : vector<4xf16>
    %106 = arith.addf %104, %94 : vector<4xf16>
    %107 = vector.extract_strided_slice %31 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %108 = vector.bitcast %107 : vector<2xf32> to vector<4xf16>
    %109 = vector.extract_strided_slice %29 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %110 = vector.bitcast %109 : vector<2xf32> to vector<4xf16>
    %111 = arith.mulf %108, %110 : vector<4xf16>
    %112 = vector.extract_strided_slice %31 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %113 = vector.bitcast %112 : vector<2xf32> to vector<4xf16>
    %114 = vector.extract_strided_slice %29 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %115 = vector.bitcast %114 : vector<2xf32> to vector<4xf16>
    %116 = arith.mulf %113, %115 : vector<4xf16>
    %117 = arith.addf %111, %105 : vector<4xf16>
    %118 = arith.addf %116, %106 : vector<4xf16>
    %119 = vector.extract_strided_slice %34 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %120 = vector.bitcast %119 : vector<2xf32> to vector<4xf16>
    %121 = vector.extract_strided_slice %33 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %122 = vector.bitcast %121 : vector<2xf32> to vector<4xf16>
    %123 = arith.mulf %120, %122 : vector<4xf16>
    %124 = vector.extract_strided_slice %34 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %125 = vector.bitcast %124 : vector<2xf32> to vector<4xf16>
    %126 = vector.extract_strided_slice %33 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %127 = vector.bitcast %126 : vector<2xf32> to vector<4xf16>
    %128 = arith.mulf %125, %127 : vector<4xf16>
    %129 = arith.addf %123, %117 : vector<4xf16>
    %130 = arith.addf %128, %118 : vector<4xf16>
    %131 = vector.reduction <add>, %129 : vector<4xf16> into f16
    %132 = vector.reduction <add>, %130 : vector<4xf16> into f16
    %133 = arith.addf %131, %132 : f16
    %134 = gpu.subgroup_reduce  add %133 : (f16) -> f16
    %135 = arith.addf %134, %cst_0 : f16
    %136 = arith.cmpi eq, %0, %c0 : index
    scf.if %136 {
      memref.store %135, %3[%workgroup_id_x] : memref<?xf16, #hal.descriptor_type<storage_buffer>>
    }
    return
  }
}

// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
    %c32000 = arith.constant 32000 : index
    %c16384000 = arith.constant 16384000 : index
    %c512 = arith.constant 512 : index
    %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f16
    %0 = gpu.thread_id  x
    %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>{%c512}
    %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>{%c16384000}
    %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<?xf16, #hal.descriptor_type<storage_buffer>>{%c32000}
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 512 + 448)>()[%0, %workgroup_id_x]
    %5 = memref.load %2[%4] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %6 = affine.apply affine_map<()[s0] -> (s0 + 448)>()[%0]
    %7 = memref.load %1[%6] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %8 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 512 + 384)>()[%0, %workgroup_id_x]
    %9 = memref.load %2[%8] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %10 = affine.apply affine_map<()[s0] -> (s0 + 384)>()[%0]
    %11 = memref.load %1[%10] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %12 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 512 + 320)>()[%0, %workgroup_id_x]
    %13 = memref.load %2[%12] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %14 = affine.apply affine_map<()[s0] -> (s0 + 320)>()[%0]
    %15 = memref.load %1[%14] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %16 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 512 + 256)>()[%0, %workgroup_id_x]
    %17 = memref.load %2[%16] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %18 = affine.apply affine_map<()[s0] -> (s0 + 256)>()[%0]
    %19 = memref.load %1[%18] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %20 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 512 + 192)>()[%0, %workgroup_id_x]
    %21 = memref.load %2[%20] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %22 = affine.apply affine_map<()[s0] -> (s0 + 192)>()[%0]
    %23 = memref.load %1[%22] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %24 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 512 + 128)>()[%0, %workgroup_id_x]
    %25 = memref.load %2[%24] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %26 = affine.apply affine_map<()[s0] -> (s0 + 128)>()[%0]
    %27 = memref.load %1[%26] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %28 = affine.apply affine_map<()[s0, s1] -> (s0 + s1 * 512 + 64)>()[%0, %workgroup_id_x]
    %29 = memref.load %2[%28] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %30 = affine.apply affine_map<()[s0] -> (s0 + 64)>()[%0]
    %31 = memref.load %1[%30] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %32 = affine.apply affine_map<()[s0, s1] -> (s0 * 512 + s1)>()[%workgroup_id_x, %0]
    %33 = memref.load %2[%32] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %34 = memref.load %1[%0] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %35 = vector.extract_strided_slice %7 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %36 = vector.bitcast %35 : vector<2xf32> to vector<4xf16>
    %37 = vector.extract_strided_slice %5 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %38 = vector.bitcast %37 : vector<2xf32> to vector<4xf16>
    %39 = arith.mulf %36, %38 : vector<4xf16>
    %40 = vector.extract_strided_slice %7 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %41 = vector.bitcast %40 : vector<2xf32> to vector<4xf16>
    %42 = vector.extract_strided_slice %5 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %43 = vector.bitcast %42 : vector<2xf32> to vector<4xf16>
    %44 = arith.mulf %41, %43 : vector<4xf16>
    %45 = arith.addf %39, %cst : vector<4xf16>
    %46 = arith.addf %44, %cst : vector<4xf16>
    %47 = vector.extract_strided_slice %11 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %48 = vector.bitcast %47 : vector<2xf32> to vector<4xf16>
    %49 = vector.extract_strided_slice %9 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %50 = vector.bitcast %49 : vector<2xf32> to vector<4xf16>
    %51 = arith.mulf %48, %50 : vector<4xf16>
    %52 = vector.extract_strided_slice %11 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %53 = vector.bitcast %52 : vector<2xf32> to vector<4xf16>
    %54 = vector.extract_strided_slice %9 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %55 = vector.bitcast %54 : vector<2xf32> to vector<4xf16>
    %56 = arith.mulf %53, %55 : vector<4xf16>
    %57 = arith.addf %51, %45 : vector<4xf16>
    %58 = arith.addf %56, %46 : vector<4xf16>
    %59 = vector.extract_strided_slice %15 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %60 = vector.bitcast %59 : vector<2xf32> to vector<4xf16>
    %61 = vector.extract_strided_slice %13 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %62 = vector.bitcast %61 : vector<2xf32> to vector<4xf16>
    %63 = arith.mulf %60, %62 : vector<4xf16>
    %64 = vector.extract_strided_slice %15 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %65 = vector.bitcast %64 : vector<2xf32> to vector<4xf16>
    %66 = vector.extract_strided_slice %13 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %67 = vector.bitcast %66 : vector<2xf32> to vector<4xf16>
    %68 = arith.mulf %65, %67 : vector<4xf16>
    %69 = arith.addf %63, %57 : vector<4xf16>
    %70 = arith.addf %68, %58 : vector<4xf16>
    %71 = vector.extract_strided_slice %19 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %72 = vector.bitcast %71 : vector<2xf32> to vector<4xf16>
    %73 = vector.extract_strided_slice %17 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %74 = vector.bitcast %73 : vector<2xf32> to vector<4xf16>
    %75 = arith.mulf %72, %74 : vector<4xf16>
    %76 = vector.extract_strided_slice %19 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %77 = vector.bitcast %76 : vector<2xf32> to vector<4xf16>
    %78 = vector.extract_strided_slice %17 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %79 = vector.bitcast %78 : vector<2xf32> to vector<4xf16>
    %80 = arith.mulf %77, %79 : vector<4xf16>
    %81 = arith.addf %75, %69 : vector<4xf16>
    %82 = arith.addf %80, %70 : vector<4xf16>
    %83 = vector.extract_strided_slice %23 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %84 = vector.bitcast %83 : vector<2xf32> to vector<4xf16>
    %85 = vector.extract_strided_slice %21 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %86 = vector.bitcast %85 : vector<2xf32> to vector<4xf16>
    %87 = arith.mulf %84, %86 : vector<4xf16>
    %88 = vector.extract_strided_slice %23 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %89 = vector.bitcast %88 : vector<2xf32> to vector<4xf16>
    %90 = vector.extract_strided_slice %21 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %91 = vector.bitcast %90 : vector<2xf32> to vector<4xf16>
    %92 = arith.mulf %89, %91 : vector<4xf16>
    %93 = arith.addf %87, %81 : vector<4xf16>
    %94 = arith.addf %92, %82 : vector<4xf16>
    %95 = vector.extract_strided_slice %27 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %96 = vector.bitcast %95 : vector<2xf32> to vector<4xf16>
    %97 = vector.extract_strided_slice %25 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %98 = vector.bitcast %97 : vector<2xf32> to vector<4xf16>
    %99 = arith.mulf %96, %98 : vector<4xf16>
    %100 = vector.extract_strided_slice %27 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %101 = vector.bitcast %100 : vector<2xf32> to vector<4xf16>
    %102 = vector.extract_strided_slice %25 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %103 = vector.bitcast %102 : vector<2xf32> to vector<4xf16>
    %104 = arith.mulf %101, %103 : vector<4xf16>
    %105 = arith.addf %99, %93 : vector<4xf16>
    %106 = arith.addf %104, %94 : vector<4xf16>
    %107 = vector.extract_strided_slice %31 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %108 = vector.bitcast %107 : vector<2xf32> to vector<4xf16>
    %109 = vector.extract_strided_slice %29 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %110 = vector.bitcast %109 : vector<2xf32> to vector<4xf16>
    %111 = arith.mulf %108, %110 : vector<4xf16>
    %112 = vector.extract_strided_slice %31 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %113 = vector.bitcast %112 : vector<2xf32> to vector<4xf16>
    %114 = vector.extract_strided_slice %29 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %115 = vector.bitcast %114 : vector<2xf32> to vector<4xf16>
    %116 = arith.mulf %113, %115 : vector<4xf16>
    %117 = arith.addf %111, %105 : vector<4xf16>
    %118 = arith.addf %116, %106 : vector<4xf16>
    %119 = vector.extract_strided_slice %34 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %120 = vector.bitcast %119 : vector<2xf32> to vector<4xf16>
    %121 = vector.extract_strided_slice %33 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %122 = vector.bitcast %121 : vector<2xf32> to vector<4xf16>
    %123 = arith.mulf %120, %122 : vector<4xf16>
    %124 = vector.extract_strided_slice %34 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %125 = vector.bitcast %124 : vector<2xf32> to vector<4xf16>
    %126 = vector.extract_strided_slice %33 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %127 = vector.bitcast %126 : vector<2xf32> to vector<4xf16>
    %128 = arith.mulf %125, %127 : vector<4xf16>
    %129 = arith.addf %123, %117 : vector<4xf16>
    %130 = arith.addf %128, %118 : vector<4xf16>
    %131 = vector.reduction <add>, %129 : vector<4xf16> into f16
    %132 = vector.reduction <add>, %130 : vector<4xf16> into f16
    %133 = arith.addf %131, %132 : f16
    %134 = gpu.subgroup_reduce  add %133 : (f16) -> f16
    %135 = arith.addf %134, %cst_0 : f16
    %136 = arith.cmpi eq, %0, %c0 : index
    scf.if %136 {
      memref.store %135, %3[%workgroup_id_x] : memref<?xf16, #hal.descriptor_type<storage_buffer>>
    }
    return
  }
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
module {
  func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
    %c32000 = arith.constant 32000 : index
    %c16384000 = arith.constant 16384000 : index
    %c512 = arith.constant 512 : index
    %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f16
    %0 = gpu.thread_id  x
    %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>{%c512}
    %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>{%c16384000}
    %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<?xf16, #hal.descriptor_type<storage_buffer>>{%c32000}
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %c512_1 = arith.constant 512 : index
    %4 = arith.muli %workgroup_id_x, %c512_1 : index
    %5 = arith.addi %0, %4 : index
    %c448 = arith.constant 448 : index
    %6 = arith.addi %5, %c448 : index
    %7 = memref.load %2[%6] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %c448_2 = arith.constant 448 : index
    %8 = arith.addi %0, %c448_2 : index
    %9 = memref.load %1[%8] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %c512_3 = arith.constant 512 : index
    %10 = arith.muli %workgroup_id_x, %c512_3 : index
    %11 = arith.addi %0, %10 : index
    %c384 = arith.constant 384 : index
    %12 = arith.addi %11, %c384 : index
    %13 = memref.load %2[%12] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %c384_4 = arith.constant 384 : index
    %14 = arith.addi %0, %c384_4 : index
    %15 = memref.load %1[%14] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %c512_5 = arith.constant 512 : index
    %16 = arith.muli %workgroup_id_x, %c512_5 : index
    %17 = arith.addi %0, %16 : index
    %c320 = arith.constant 320 : index
    %18 = arith.addi %17, %c320 : index
    %19 = memref.load %2[%18] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %c320_6 = arith.constant 320 : index
    %20 = arith.addi %0, %c320_6 : index
    %21 = memref.load %1[%20] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %c512_7 = arith.constant 512 : index
    %22 = arith.muli %workgroup_id_x, %c512_7 : index
    %23 = arith.addi %0, %22 : index
    %c256 = arith.constant 256 : index
    %24 = arith.addi %23, %c256 : index
    %25 = memref.load %2[%24] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %c256_8 = arith.constant 256 : index
    %26 = arith.addi %0, %c256_8 : index
    %27 = memref.load %1[%26] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %c512_9 = arith.constant 512 : index
    %28 = arith.muli %workgroup_id_x, %c512_9 : index
    %29 = arith.addi %0, %28 : index
    %c192 = arith.constant 192 : index
    %30 = arith.addi %29, %c192 : index
    %31 = memref.load %2[%30] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %c192_10 = arith.constant 192 : index
    %32 = arith.addi %0, %c192_10 : index
    %33 = memref.load %1[%32] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %c512_11 = arith.constant 512 : index
    %34 = arith.muli %workgroup_id_x, %c512_11 : index
    %35 = arith.addi %0, %34 : index
    %c128 = arith.constant 128 : index
    %36 = arith.addi %35, %c128 : index
    %37 = memref.load %2[%36] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %c128_12 = arith.constant 128 : index
    %38 = arith.addi %0, %c128_12 : index
    %39 = memref.load %1[%38] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %c512_13 = arith.constant 512 : index
    %40 = arith.muli %workgroup_id_x, %c512_13 : index
    %41 = arith.addi %0, %40 : index
    %c64 = arith.constant 64 : index
    %42 = arith.addi %41, %c64 : index
    %43 = memref.load %2[%42] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %c64_14 = arith.constant 64 : index
    %44 = arith.addi %0, %c64_14 : index
    %45 = memref.load %1[%44] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %c512_15 = arith.constant 512 : index
    %46 = arith.muli %workgroup_id_x, %c512_15 : index
    %47 = arith.addi %46, %0 : index
    %48 = memref.load %2[%47] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %49 = memref.load %1[%0] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %50 = vector.extract_strided_slice %9 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %51 = vector.bitcast %50 : vector<2xf32> to vector<4xf16>
    %52 = vector.extract_strided_slice %7 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %53 = vector.bitcast %52 : vector<2xf32> to vector<4xf16>
    %54 = arith.mulf %51, %53 : vector<4xf16>
    %55 = vector.extract_strided_slice %9 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %56 = vector.bitcast %55 : vector<2xf32> to vector<4xf16>
    %57 = vector.extract_strided_slice %7 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %58 = vector.bitcast %57 : vector<2xf32> to vector<4xf16>
    %59 = arith.mulf %56, %58 : vector<4xf16>
    %60 = arith.addf %54, %cst : vector<4xf16>
    %61 = arith.addf %59, %cst : vector<4xf16>
    %62 = vector.extract_strided_slice %15 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %63 = vector.bitcast %62 : vector<2xf32> to vector<4xf16>
    %64 = vector.extract_strided_slice %13 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %65 = vector.bitcast %64 : vector<2xf32> to vector<4xf16>
    %66 = arith.mulf %63, %65 : vector<4xf16>
    %67 = vector.extract_strided_slice %15 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %68 = vector.bitcast %67 : vector<2xf32> to vector<4xf16>
    %69 = vector.extract_strided_slice %13 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %70 = vector.bitcast %69 : vector<2xf32> to vector<4xf16>
    %71 = arith.mulf %68, %70 : vector<4xf16>
    %72 = arith.addf %66, %60 : vector<4xf16>
    %73 = arith.addf %71, %61 : vector<4xf16>
    %74 = vector.extract_strided_slice %21 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %75 = vector.bitcast %74 : vector<2xf32> to vector<4xf16>
    %76 = vector.extract_strided_slice %19 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %77 = vector.bitcast %76 : vector<2xf32> to vector<4xf16>
    %78 = arith.mulf %75, %77 : vector<4xf16>
    %79 = vector.extract_strided_slice %21 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %80 = vector.bitcast %79 : vector<2xf32> to vector<4xf16>
    %81 = vector.extract_strided_slice %19 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %82 = vector.bitcast %81 : vector<2xf32> to vector<4xf16>
    %83 = arith.mulf %80, %82 : vector<4xf16>
    %84 = arith.addf %78, %72 : vector<4xf16>
    %85 = arith.addf %83, %73 : vector<4xf16>
    %86 = vector.extract_strided_slice %27 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %87 = vector.bitcast %86 : vector<2xf32> to vector<4xf16>
    %88 = vector.extract_strided_slice %25 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %89 = vector.bitcast %88 : vector<2xf32> to vector<4xf16>
    %90 = arith.mulf %87, %89 : vector<4xf16>
    %91 = vector.extract_strided_slice %27 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %92 = vector.bitcast %91 : vector<2xf32> to vector<4xf16>
    %93 = vector.extract_strided_slice %25 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %94 = vector.bitcast %93 : vector<2xf32> to vector<4xf16>
    %95 = arith.mulf %92, %94 : vector<4xf16>
    %96 = arith.addf %90, %84 : vector<4xf16>
    %97 = arith.addf %95, %85 : vector<4xf16>
    %98 = vector.extract_strided_slice %33 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %99 = vector.bitcast %98 : vector<2xf32> to vector<4xf16>
    %100 = vector.extract_strided_slice %31 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %101 = vector.bitcast %100 : vector<2xf32> to vector<4xf16>
    %102 = arith.mulf %99, %101 : vector<4xf16>
    %103 = vector.extract_strided_slice %33 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %104 = vector.bitcast %103 : vector<2xf32> to vector<4xf16>
    %105 = vector.extract_strided_slice %31 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %106 = vector.bitcast %105 : vector<2xf32> to vector<4xf16>
    %107 = arith.mulf %104, %106 : vector<4xf16>
    %108 = arith.addf %102, %96 : vector<4xf16>
    %109 = arith.addf %107, %97 : vector<4xf16>
    %110 = vector.extract_strided_slice %39 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %111 = vector.bitcast %110 : vector<2xf32> to vector<4xf16>
    %112 = vector.extract_strided_slice %37 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %113 = vector.bitcast %112 : vector<2xf32> to vector<4xf16>
    %114 = arith.mulf %111, %113 : vector<4xf16>
    %115 = vector.extract_strided_slice %39 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %116 = vector.bitcast %115 : vector<2xf32> to vector<4xf16>
    %117 = vector.extract_strided_slice %37 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %118 = vector.bitcast %117 : vector<2xf32> to vector<4xf16>
    %119 = arith.mulf %116, %118 : vector<4xf16>
    %120 = arith.addf %114, %108 : vector<4xf16>
    %121 = arith.addf %119, %109 : vector<4xf16>
    %122 = vector.extract_strided_slice %45 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %123 = vector.bitcast %122 : vector<2xf32> to vector<4xf16>
    %124 = vector.extract_strided_slice %43 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %125 = vector.bitcast %124 : vector<2xf32> to vector<4xf16>
    %126 = arith.mulf %123, %125 : vector<4xf16>
    %127 = vector.extract_strided_slice %45 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %128 = vector.bitcast %127 : vector<2xf32> to vector<4xf16>
    %129 = vector.extract_strided_slice %43 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %130 = vector.bitcast %129 : vector<2xf32> to vector<4xf16>
    %131 = arith.mulf %128, %130 : vector<4xf16>
    %132 = arith.addf %126, %120 : vector<4xf16>
    %133 = arith.addf %131, %121 : vector<4xf16>
    %134 = vector.extract_strided_slice %49 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %135 = vector.bitcast %134 : vector<2xf32> to vector<4xf16>
    %136 = vector.extract_strided_slice %48 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %137 = vector.bitcast %136 : vector<2xf32> to vector<4xf16>
    %138 = arith.mulf %135, %137 : vector<4xf16>
    %139 = vector.extract_strided_slice %49 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %140 = vector.bitcast %139 : vector<2xf32> to vector<4xf16>
    %141 = vector.extract_strided_slice %48 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %142 = vector.bitcast %141 : vector<2xf32> to vector<4xf16>
    %143 = arith.mulf %140, %142 : vector<4xf16>
    %144 = arith.addf %138, %132 : vector<4xf16>
    %145 = arith.addf %143, %133 : vector<4xf16>
    %146 = vector.reduction <add>, %144 : vector<4xf16> into f16
    %147 = vector.reduction <add>, %145 : vector<4xf16> into f16
    %148 = arith.addf %146, %147 : f16
    %149 = gpu.subgroup_reduce  add %148 : (f16) -> f16
    %150 = arith.addf %149, %cst_0 : f16
    %151 = arith.cmpi eq, %0, %c0 : index
    scf.if %151 {
      memref.store %150, %3[%workgroup_id_x] : memref<?xf16, #hal.descriptor_type<storage_buffer>>
    }
    return
  }
}

// -----// IR Dump After TosaToArith (tosa-to-arith) //----- //
module {
  func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
    %c32000 = arith.constant 32000 : index
    %c16384000 = arith.constant 16384000 : index
    %c512 = arith.constant 512 : index
    %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f16
    %0 = gpu.thread_id  x
    %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>{%c512}
    %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>{%c16384000}
    %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<?xf16, #hal.descriptor_type<storage_buffer>>{%c32000}
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %c512_1 = arith.constant 512 : index
    %4 = arith.muli %workgroup_id_x, %c512_1 : index
    %5 = arith.addi %0, %4 : index
    %c448 = arith.constant 448 : index
    %6 = arith.addi %5, %c448 : index
    %7 = memref.load %2[%6] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %c448_2 = arith.constant 448 : index
    %8 = arith.addi %0, %c448_2 : index
    %9 = memref.load %1[%8] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %c512_3 = arith.constant 512 : index
    %10 = arith.muli %workgroup_id_x, %c512_3 : index
    %11 = arith.addi %0, %10 : index
    %c384 = arith.constant 384 : index
    %12 = arith.addi %11, %c384 : index
    %13 = memref.load %2[%12] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %c384_4 = arith.constant 384 : index
    %14 = arith.addi %0, %c384_4 : index
    %15 = memref.load %1[%14] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %c512_5 = arith.constant 512 : index
    %16 = arith.muli %workgroup_id_x, %c512_5 : index
    %17 = arith.addi %0, %16 : index
    %c320 = arith.constant 320 : index
    %18 = arith.addi %17, %c320 : index
    %19 = memref.load %2[%18] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %c320_6 = arith.constant 320 : index
    %20 = arith.addi %0, %c320_6 : index
    %21 = memref.load %1[%20] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %c512_7 = arith.constant 512 : index
    %22 = arith.muli %workgroup_id_x, %c512_7 : index
    %23 = arith.addi %0, %22 : index
    %c256 = arith.constant 256 : index
    %24 = arith.addi %23, %c256 : index
    %25 = memref.load %2[%24] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %c256_8 = arith.constant 256 : index
    %26 = arith.addi %0, %c256_8 : index
    %27 = memref.load %1[%26] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %c512_9 = arith.constant 512 : index
    %28 = arith.muli %workgroup_id_x, %c512_9 : index
    %29 = arith.addi %0, %28 : index
    %c192 = arith.constant 192 : index
    %30 = arith.addi %29, %c192 : index
    %31 = memref.load %2[%30] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %c192_10 = arith.constant 192 : index
    %32 = arith.addi %0, %c192_10 : index
    %33 = memref.load %1[%32] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %c512_11 = arith.constant 512 : index
    %34 = arith.muli %workgroup_id_x, %c512_11 : index
    %35 = arith.addi %0, %34 : index
    %c128 = arith.constant 128 : index
    %36 = arith.addi %35, %c128 : index
    %37 = memref.load %2[%36] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %c128_12 = arith.constant 128 : index
    %38 = arith.addi %0, %c128_12 : index
    %39 = memref.load %1[%38] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %c512_13 = arith.constant 512 : index
    %40 = arith.muli %workgroup_id_x, %c512_13 : index
    %41 = arith.addi %0, %40 : index
    %c64 = arith.constant 64 : index
    %42 = arith.addi %41, %c64 : index
    %43 = memref.load %2[%42] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %c64_14 = arith.constant 64 : index
    %44 = arith.addi %0, %c64_14 : index
    %45 = memref.load %1[%44] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %c512_15 = arith.constant 512 : index
    %46 = arith.muli %workgroup_id_x, %c512_15 : index
    %47 = arith.addi %46, %0 : index
    %48 = memref.load %2[%47] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %49 = memref.load %1[%0] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %50 = vector.extract_strided_slice %9 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %51 = vector.bitcast %50 : vector<2xf32> to vector<4xf16>
    %52 = vector.extract_strided_slice %7 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %53 = vector.bitcast %52 : vector<2xf32> to vector<4xf16>
    %54 = arith.mulf %51, %53 : vector<4xf16>
    %55 = vector.extract_strided_slice %9 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %56 = vector.bitcast %55 : vector<2xf32> to vector<4xf16>
    %57 = vector.extract_strided_slice %7 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %58 = vector.bitcast %57 : vector<2xf32> to vector<4xf16>
    %59 = arith.mulf %56, %58 : vector<4xf16>
    %60 = arith.addf %54, %cst : vector<4xf16>
    %61 = arith.addf %59, %cst : vector<4xf16>
    %62 = vector.extract_strided_slice %15 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %63 = vector.bitcast %62 : vector<2xf32> to vector<4xf16>
    %64 = vector.extract_strided_slice %13 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %65 = vector.bitcast %64 : vector<2xf32> to vector<4xf16>
    %66 = arith.mulf %63, %65 : vector<4xf16>
    %67 = vector.extract_strided_slice %15 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %68 = vector.bitcast %67 : vector<2xf32> to vector<4xf16>
    %69 = vector.extract_strided_slice %13 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %70 = vector.bitcast %69 : vector<2xf32> to vector<4xf16>
    %71 = arith.mulf %68, %70 : vector<4xf16>
    %72 = arith.addf %66, %60 : vector<4xf16>
    %73 = arith.addf %71, %61 : vector<4xf16>
    %74 = vector.extract_strided_slice %21 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %75 = vector.bitcast %74 : vector<2xf32> to vector<4xf16>
    %76 = vector.extract_strided_slice %19 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %77 = vector.bitcast %76 : vector<2xf32> to vector<4xf16>
    %78 = arith.mulf %75, %77 : vector<4xf16>
    %79 = vector.extract_strided_slice %21 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %80 = vector.bitcast %79 : vector<2xf32> to vector<4xf16>
    %81 = vector.extract_strided_slice %19 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %82 = vector.bitcast %81 : vector<2xf32> to vector<4xf16>
    %83 = arith.mulf %80, %82 : vector<4xf16>
    %84 = arith.addf %78, %72 : vector<4xf16>
    %85 = arith.addf %83, %73 : vector<4xf16>
    %86 = vector.extract_strided_slice %27 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %87 = vector.bitcast %86 : vector<2xf32> to vector<4xf16>
    %88 = vector.extract_strided_slice %25 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %89 = vector.bitcast %88 : vector<2xf32> to vector<4xf16>
    %90 = arith.mulf %87, %89 : vector<4xf16>
    %91 = vector.extract_strided_slice %27 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %92 = vector.bitcast %91 : vector<2xf32> to vector<4xf16>
    %93 = vector.extract_strided_slice %25 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %94 = vector.bitcast %93 : vector<2xf32> to vector<4xf16>
    %95 = arith.mulf %92, %94 : vector<4xf16>
    %96 = arith.addf %90, %84 : vector<4xf16>
    %97 = arith.addf %95, %85 : vector<4xf16>
    %98 = vector.extract_strided_slice %33 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %99 = vector.bitcast %98 : vector<2xf32> to vector<4xf16>
    %100 = vector.extract_strided_slice %31 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %101 = vector.bitcast %100 : vector<2xf32> to vector<4xf16>
    %102 = arith.mulf %99, %101 : vector<4xf16>
    %103 = vector.extract_strided_slice %33 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %104 = vector.bitcast %103 : vector<2xf32> to vector<4xf16>
    %105 = vector.extract_strided_slice %31 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %106 = vector.bitcast %105 : vector<2xf32> to vector<4xf16>
    %107 = arith.mulf %104, %106 : vector<4xf16>
    %108 = arith.addf %102, %96 : vector<4xf16>
    %109 = arith.addf %107, %97 : vector<4xf16>
    %110 = vector.extract_strided_slice %39 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %111 = vector.bitcast %110 : vector<2xf32> to vector<4xf16>
    %112 = vector.extract_strided_slice %37 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %113 = vector.bitcast %112 : vector<2xf32> to vector<4xf16>
    %114 = arith.mulf %111, %113 : vector<4xf16>
    %115 = vector.extract_strided_slice %39 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %116 = vector.bitcast %115 : vector<2xf32> to vector<4xf16>
    %117 = vector.extract_strided_slice %37 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %118 = vector.bitcast %117 : vector<2xf32> to vector<4xf16>
    %119 = arith.mulf %116, %118 : vector<4xf16>
    %120 = arith.addf %114, %108 : vector<4xf16>
    %121 = arith.addf %119, %109 : vector<4xf16>
    %122 = vector.extract_strided_slice %45 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %123 = vector.bitcast %122 : vector<2xf32> to vector<4xf16>
    %124 = vector.extract_strided_slice %43 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %125 = vector.bitcast %124 : vector<2xf32> to vector<4xf16>
    %126 = arith.mulf %123, %125 : vector<4xf16>
    %127 = vector.extract_strided_slice %45 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %128 = vector.bitcast %127 : vector<2xf32> to vector<4xf16>
    %129 = vector.extract_strided_slice %43 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %130 = vector.bitcast %129 : vector<2xf32> to vector<4xf16>
    %131 = arith.mulf %128, %130 : vector<4xf16>
    %132 = arith.addf %126, %120 : vector<4xf16>
    %133 = arith.addf %131, %121 : vector<4xf16>
    %134 = vector.extract_strided_slice %49 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %135 = vector.bitcast %134 : vector<2xf32> to vector<4xf16>
    %136 = vector.extract_strided_slice %48 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %137 = vector.bitcast %136 : vector<2xf32> to vector<4xf16>
    %138 = arith.mulf %135, %137 : vector<4xf16>
    %139 = vector.extract_strided_slice %49 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %140 = vector.bitcast %139 : vector<2xf32> to vector<4xf16>
    %141 = vector.extract_strided_slice %48 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %142 = vector.bitcast %141 : vector<2xf32> to vector<4xf16>
    %143 = arith.mulf %140, %142 : vector<4xf16>
    %144 = arith.addf %138, %132 : vector<4xf16>
    %145 = arith.addf %143, %133 : vector<4xf16>
    %146 = vector.reduction <add>, %144 : vector<4xf16> into f16
    %147 = vector.reduction <add>, %145 : vector<4xf16> into f16
    %148 = arith.addf %146, %147 : f16
    %149 = gpu.subgroup_reduce  add %148 : (f16) -> f16
    %150 = arith.addf %149, %cst_0 : f16
    %151 = arith.cmpi eq, %0, %c0 : index
    scf.if %151 {
      memref.store %150, %3[%workgroup_id_x] : memref<?xf16, #hal.descriptor_type<storage_buffer>>
    }
    return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module {
  func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
    %c64 = arith.constant 64 : index
    %c128 = arith.constant 128 : index
    %c192 = arith.constant 192 : index
    %c256 = arith.constant 256 : index
    %c320 = arith.constant 320 : index
    %c384 = arith.constant 384 : index
    %c448 = arith.constant 448 : index
    %c32000 = arith.constant 32000 : index
    %c16384000 = arith.constant 16384000 : index
    %c512 = arith.constant 512 : index
    %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f16
    %0 = gpu.thread_id  x
    %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>{%c512}
    %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>{%c16384000}
    %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<?xf16, #hal.descriptor_type<storage_buffer>>{%c32000}
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = arith.muli %workgroup_id_x, %c512 : index
    %5 = arith.addi %0, %4 : index
    %6 = arith.addi %5, %c448 : index
    %7 = memref.load %2[%6] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %8 = arith.addi %0, %c448 : index
    %9 = memref.load %1[%8] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %10 = arith.muli %workgroup_id_x, %c512 : index
    %11 = arith.addi %0, %10 : index
    %12 = arith.addi %11, %c384 : index
    %13 = memref.load %2[%12] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %14 = arith.addi %0, %c384 : index
    %15 = memref.load %1[%14] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %16 = arith.muli %workgroup_id_x, %c512 : index
    %17 = arith.addi %0, %16 : index
    %18 = arith.addi %17, %c320 : index
    %19 = memref.load %2[%18] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %20 = arith.addi %0, %c320 : index
    %21 = memref.load %1[%20] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %22 = arith.muli %workgroup_id_x, %c512 : index
    %23 = arith.addi %0, %22 : index
    %24 = arith.addi %23, %c256 : index
    %25 = memref.load %2[%24] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %26 = arith.addi %0, %c256 : index
    %27 = memref.load %1[%26] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %28 = arith.muli %workgroup_id_x, %c512 : index
    %29 = arith.addi %0, %28 : index
    %30 = arith.addi %29, %c192 : index
    %31 = memref.load %2[%30] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %32 = arith.addi %0, %c192 : index
    %33 = memref.load %1[%32] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %34 = arith.muli %workgroup_id_x, %c512 : index
    %35 = arith.addi %0, %34 : index
    %36 = arith.addi %35, %c128 : index
    %37 = memref.load %2[%36] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %38 = arith.addi %0, %c128 : index
    %39 = memref.load %1[%38] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %40 = arith.muli %workgroup_id_x, %c512 : index
    %41 = arith.addi %0, %40 : index
    %42 = arith.addi %41, %c64 : index
    %43 = memref.load %2[%42] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %44 = arith.addi %0, %c64 : index
    %45 = memref.load %1[%44] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %46 = arith.muli %workgroup_id_x, %c512 : index
    %47 = arith.addi %46, %0 : index
    %48 = memref.load %2[%47] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %49 = memref.load %1[%0] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %50 = vector.extract_strided_slice %9 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %51 = vector.bitcast %50 : vector<2xf32> to vector<4xf16>
    %52 = vector.extract_strided_slice %7 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %53 = vector.bitcast %52 : vector<2xf32> to vector<4xf16>
    %54 = arith.mulf %51, %53 : vector<4xf16>
    %55 = vector.extract_strided_slice %9 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %56 = vector.bitcast %55 : vector<2xf32> to vector<4xf16>
    %57 = vector.extract_strided_slice %7 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %58 = vector.bitcast %57 : vector<2xf32> to vector<4xf16>
    %59 = arith.mulf %56, %58 : vector<4xf16>
    %60 = arith.addf %54, %cst : vector<4xf16>
    %61 = arith.addf %59, %cst : vector<4xf16>
    %62 = vector.extract_strided_slice %15 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %63 = vector.bitcast %62 : vector<2xf32> to vector<4xf16>
    %64 = vector.extract_strided_slice %13 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %65 = vector.bitcast %64 : vector<2xf32> to vector<4xf16>
    %66 = arith.mulf %63, %65 : vector<4xf16>
    %67 = vector.extract_strided_slice %15 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %68 = vector.bitcast %67 : vector<2xf32> to vector<4xf16>
    %69 = vector.extract_strided_slice %13 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %70 = vector.bitcast %69 : vector<2xf32> to vector<4xf16>
    %71 = arith.mulf %68, %70 : vector<4xf16>
    %72 = arith.addf %66, %60 : vector<4xf16>
    %73 = arith.addf %71, %61 : vector<4xf16>
    %74 = vector.extract_strided_slice %21 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %75 = vector.bitcast %74 : vector<2xf32> to vector<4xf16>
    %76 = vector.extract_strided_slice %19 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %77 = vector.bitcast %76 : vector<2xf32> to vector<4xf16>
    %78 = arith.mulf %75, %77 : vector<4xf16>
    %79 = vector.extract_strided_slice %21 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %80 = vector.bitcast %79 : vector<2xf32> to vector<4xf16>
    %81 = vector.extract_strided_slice %19 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %82 = vector.bitcast %81 : vector<2xf32> to vector<4xf16>
    %83 = arith.mulf %80, %82 : vector<4xf16>
    %84 = arith.addf %78, %72 : vector<4xf16>
    %85 = arith.addf %83, %73 : vector<4xf16>
    %86 = vector.extract_strided_slice %27 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %87 = vector.bitcast %86 : vector<2xf32> to vector<4xf16>
    %88 = vector.extract_strided_slice %25 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %89 = vector.bitcast %88 : vector<2xf32> to vector<4xf16>
    %90 = arith.mulf %87, %89 : vector<4xf16>
    %91 = vector.extract_strided_slice %27 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %92 = vector.bitcast %91 : vector<2xf32> to vector<4xf16>
    %93 = vector.extract_strided_slice %25 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %94 = vector.bitcast %93 : vector<2xf32> to vector<4xf16>
    %95 = arith.mulf %92, %94 : vector<4xf16>
    %96 = arith.addf %90, %84 : vector<4xf16>
    %97 = arith.addf %95, %85 : vector<4xf16>
    %98 = vector.extract_strided_slice %33 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %99 = vector.bitcast %98 : vector<2xf32> to vector<4xf16>
    %100 = vector.extract_strided_slice %31 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %101 = vector.bitcast %100 : vector<2xf32> to vector<4xf16>
    %102 = arith.mulf %99, %101 : vector<4xf16>
    %103 = vector.extract_strided_slice %33 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %104 = vector.bitcast %103 : vector<2xf32> to vector<4xf16>
    %105 = vector.extract_strided_slice %31 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %106 = vector.bitcast %105 : vector<2xf32> to vector<4xf16>
    %107 = arith.mulf %104, %106 : vector<4xf16>
    %108 = arith.addf %102, %96 : vector<4xf16>
    %109 = arith.addf %107, %97 : vector<4xf16>
    %110 = vector.extract_strided_slice %39 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %111 = vector.bitcast %110 : vector<2xf32> to vector<4xf16>
    %112 = vector.extract_strided_slice %37 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %113 = vector.bitcast %112 : vector<2xf32> to vector<4xf16>
    %114 = arith.mulf %111, %113 : vector<4xf16>
    %115 = vector.extract_strided_slice %39 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %116 = vector.bitcast %115 : vector<2xf32> to vector<4xf16>
    %117 = vector.extract_strided_slice %37 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %118 = vector.bitcast %117 : vector<2xf32> to vector<4xf16>
    %119 = arith.mulf %116, %118 : vector<4xf16>
    %120 = arith.addf %114, %108 : vector<4xf16>
    %121 = arith.addf %119, %109 : vector<4xf16>
    %122 = vector.extract_strided_slice %45 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %123 = vector.bitcast %122 : vector<2xf32> to vector<4xf16>
    %124 = vector.extract_strided_slice %43 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %125 = vector.bitcast %124 : vector<2xf32> to vector<4xf16>
    %126 = arith.mulf %123, %125 : vector<4xf16>
    %127 = vector.extract_strided_slice %45 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %128 = vector.bitcast %127 : vector<2xf32> to vector<4xf16>
    %129 = vector.extract_strided_slice %43 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %130 = vector.bitcast %129 : vector<2xf32> to vector<4xf16>
    %131 = arith.mulf %128, %130 : vector<4xf16>
    %132 = arith.addf %126, %120 : vector<4xf16>
    %133 = arith.addf %131, %121 : vector<4xf16>
    %134 = vector.extract_strided_slice %49 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %135 = vector.bitcast %134 : vector<2xf32> to vector<4xf16>
    %136 = vector.extract_strided_slice %48 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %137 = vector.bitcast %136 : vector<2xf32> to vector<4xf16>
    %138 = arith.mulf %135, %137 : vector<4xf16>
    %139 = vector.extract_strided_slice %49 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %140 = vector.bitcast %139 : vector<2xf32> to vector<4xf16>
    %141 = vector.extract_strided_slice %48 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %142 = vector.bitcast %141 : vector<2xf32> to vector<4xf16>
    %143 = arith.mulf %140, %142 : vector<4xf16>
    %144 = arith.addf %138, %132 : vector<4xf16>
    %145 = arith.addf %143, %133 : vector<4xf16>
    %146 = vector.reduction <add>, %144 : vector<4xf16> into f16
    %147 = vector.reduction <add>, %145 : vector<4xf16> into f16
    %148 = arith.addf %146, %147 : f16
    %149 = gpu.subgroup_reduce  add %148 : (f16) -> f16
    %150 = arith.addf %149, %cst_0 : f16
    %151 = arith.cmpi eq, %0, %c0 : index
    scf.if %151 {
      memref.store %150, %3[%workgroup_id_x] : memref<?xf16, #hal.descriptor_type<storage_buffer>>
    }
    return
  }
}

// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
    %c64 = arith.constant 64 : index
    %c128 = arith.constant 128 : index
    %c192 = arith.constant 192 : index
    %c256 = arith.constant 256 : index
    %c320 = arith.constant 320 : index
    %c384 = arith.constant 384 : index
    %c448 = arith.constant 448 : index
    %c32000 = arith.constant 32000 : index
    %c16384000 = arith.constant 16384000 : index
    %c512 = arith.constant 512 : index
    %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f16
    %0 = gpu.thread_id  x
    %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>{%c512}
    %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>{%c16384000}
    %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<?xf16, #hal.descriptor_type<storage_buffer>>{%c32000}
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = arith.muli %workgroup_id_x, %c512 : index
    %5 = arith.addi %0, %4 : index
    %6 = arith.addi %5, %c448 : index
    %7 = memref.load %2[%6] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %8 = arith.addi %0, %c448 : index
    %9 = memref.load %1[%8] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %10 = arith.addi %5, %c384 : index
    %11 = memref.load %2[%10] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %12 = arith.addi %0, %c384 : index
    %13 = memref.load %1[%12] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %14 = arith.addi %5, %c320 : index
    %15 = memref.load %2[%14] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %16 = arith.addi %0, %c320 : index
    %17 = memref.load %1[%16] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %18 = arith.addi %5, %c256 : index
    %19 = memref.load %2[%18] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %20 = arith.addi %0, %c256 : index
    %21 = memref.load %1[%20] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %22 = arith.addi %5, %c192 : index
    %23 = memref.load %2[%22] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %24 = arith.addi %0, %c192 : index
    %25 = memref.load %1[%24] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %26 = arith.addi %5, %c128 : index
    %27 = memref.load %2[%26] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %28 = arith.addi %0, %c128 : index
    %29 = memref.load %1[%28] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %30 = arith.addi %5, %c64 : index
    %31 = memref.load %2[%30] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %32 = arith.addi %0, %c64 : index
    %33 = memref.load %1[%32] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %34 = arith.addi %4, %0 : index
    %35 = memref.load %2[%34] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %36 = memref.load %1[%0] : memref<?xvector<4xf32>, #hal.descriptor_type<storage_buffer>>
    %37 = vector.extract_strided_slice %9 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %38 = vector.bitcast %37 : vector<2xf32> to vector<4xf16>
    %39 = vector.extract_strided_slice %7 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %40 = vector.bitcast %39 : vector<2xf32> to vector<4xf16>
    %41 = arith.mulf %38, %40 : vector<4xf16>
    %42 = vector.extract_strided_slice %9 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %43 = vector.bitcast %42 : vector<2xf32> to vector<4xf16>
    %44 = vector.extract_strided_slice %7 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %45 = vector.bitcast %44 : vector<2xf32> to vector<4xf16>
    %46 = arith.mulf %43, %45 : vector<4xf16>
    %47 = arith.addf %41, %cst : vector<4xf16>
    %48 = arith.addf %46, %cst : vector<4xf16>
    %49 = vector.extract_strided_slice %13 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %50 = vector.bitcast %49 : vector<2xf32> to vector<4xf16>
    %51 = vector.extract_strided_slice %11 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %52 = vector.bitcast %51 : vector<2xf32> to vector<4xf16>
    %53 = arith.mulf %50, %52 : vector<4xf16>
    %54 = vector.extract_strided_slice %13 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %55 = vector.bitcast %54 : vector<2xf32> to vector<4xf16>
    %56 = vector.extract_strided_slice %11 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %57 = vector.bitcast %56 : vector<2xf32> to vector<4xf16>
    %58 = arith.mulf %55, %57 : vector<4xf16>
    %59 = arith.addf %53, %47 : vector<4xf16>
    %60 = arith.addf %58, %48 : vector<4xf16>
    %61 = vector.extract_strided_slice %17 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %62 = vector.bitcast %61 : vector<2xf32> to vector<4xf16>
    %63 = vector.extract_strided_slice %15 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %64 = vector.bitcast %63 : vector<2xf32> to vector<4xf16>
    %65 = arith.mulf %62, %64 : vector<4xf16>
    %66 = vector.extract_strided_slice %17 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %67 = vector.bitcast %66 : vector<2xf32> to vector<4xf16>
    %68 = vector.extract_strided_slice %15 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %69 = vector.bitcast %68 : vector<2xf32> to vector<4xf16>
    %70 = arith.mulf %67, %69 : vector<4xf16>
    %71 = arith.addf %65, %59 : vector<4xf16>
    %72 = arith.addf %70, %60 : vector<4xf16>
    %73 = vector.extract_strided_slice %21 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %74 = vector.bitcast %73 : vector<2xf32> to vector<4xf16>
    %75 = vector.extract_strided_slice %19 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %76 = vector.bitcast %75 : vector<2xf32> to vector<4xf16>
    %77 = arith.mulf %74, %76 : vector<4xf16>
    %78 = vector.extract_strided_slice %21 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %79 = vector.bitcast %78 : vector<2xf32> to vector<4xf16>
    %80 = vector.extract_strided_slice %19 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %81 = vector.bitcast %80 : vector<2xf32> to vector<4xf16>
    %82 = arith.mulf %79, %81 : vector<4xf16>
    %83 = arith.addf %77, %71 : vector<4xf16>
    %84 = arith.addf %82, %72 : vector<4xf16>
    %85 = vector.extract_strided_slice %25 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %86 = vector.bitcast %85 : vector<2xf32> to vector<4xf16>
    %87 = vector.extract_strided_slice %23 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %88 = vector.bitcast %87 : vector<2xf32> to vector<4xf16>
    %89 = arith.mulf %86, %88 : vector<4xf16>
    %90 = vector.extract_strided_slice %25 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %91 = vector.bitcast %90 : vector<2xf32> to vector<4xf16>
    %92 = vector.extract_strided_slice %23 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %93 = vector.bitcast %92 : vector<2xf32> to vector<4xf16>
    %94 = arith.mulf %91, %93 : vector<4xf16>
    %95 = arith.addf %89, %83 : vector<4xf16>
    %96 = arith.addf %94, %84 : vector<4xf16>
    %97 = vector.extract_strided_slice %29 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %98 = vector.bitcast %97 : vector<2xf32> to vector<4xf16>
    %99 = vector.extract_strided_slice %27 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %100 = vector.bitcast %99 : vector<2xf32> to vector<4xf16>
    %101 = arith.mulf %98, %100 : vector<4xf16>
    %102 = vector.extract_strided_slice %29 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %103 = vector.bitcast %102 : vector<2xf32> to vector<4xf16>
    %104 = vector.extract_strided_slice %27 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %105 = vector.bitcast %104 : vector<2xf32> to vector<4xf16>
    %106 = arith.mulf %103, %105 : vector<4xf16>
    %107 = arith.addf %101, %95 : vector<4xf16>
    %108 = arith.addf %106, %96 : vector<4xf16>
    %109 = vector.extract_strided_slice %33 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %110 = vector.bitcast %109 : vector<2xf32> to vector<4xf16>
    %111 = vector.extract_strided_slice %31 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %112 = vector.bitcast %111 : vector<2xf32> to vector<4xf16>
    %113 = arith.mulf %110, %112 : vector<4xf16>
    %114 = vector.extract_strided_slice %33 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %115 = vector.bitcast %114 : vector<2xf32> to vector<4xf16>
    %116 = vector.extract_strided_slice %31 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %117 = vector.bitcast %116 : vector<2xf32> to vector<4xf16>
    %118 = arith.mulf %115, %117 : vector<4xf16>
    %119 = arith.addf %113, %107 : vector<4xf16>
    %120 = arith.addf %118, %108 : vector<4xf16>
    %121 = vector.extract_strided_slice %36 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %122 = vector.bitcast %121 : vector<2xf32> to vector<4xf16>
    %123 = vector.extract_strided_slice %35 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %124 = vector.bitcast %123 : vector<2xf32> to vector<4xf16>
    %125 = arith.mulf %122, %124 : vector<4xf16>
    %126 = vector.extract_strided_slice %36 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %127 = vector.bitcast %126 : vector<2xf32> to vector<4xf16>
    %128 = vector.extract_strided_slice %35 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %129 = vector.bitcast %128 : vector<2xf32> to vector<4xf16>
    %130 = arith.mulf %127, %129 : vector<4xf16>
    %131 = arith.addf %125, %119 : vector<4xf16>
    %132 = arith.addf %130, %120 : vector<4xf16>
    %133 = vector.reduction <add>, %131 : vector<4xf16> into f16
    %134 = vector.reduction <add>, %132 : vector<4xf16> into f16
    %135 = arith.addf %133, %134 : f16
    %136 = gpu.subgroup_reduce  add %135 : (f16) -> f16
    %137 = arith.addf %136, %cst_0 : f16
    %138 = arith.cmpi eq, %0, %c0 : index
    scf.if %138 {
      memref.store %137, %3[%workgroup_id_x] : memref<?xf16, #hal.descriptor_type<storage_buffer>>
    }
    return
  }
}

// -----// IR Dump After SPIRVMapMemRefStorageClass (iree-spirv-map-memref-storage-class) //----- //
func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
  %c64 = arith.constant 64 : index
  %c128 = arith.constant 128 : index
  %c192 = arith.constant 192 : index
  %c256 = arith.constant 256 : index
  %c320 = arith.constant 320 : index
  %c384 = arith.constant 384 : index
  %c448 = arith.constant 448 : index
  %c32000 = arith.constant 32000 : index
  %c16384000 = arith.constant 16384000 : index
  %c512 = arith.constant 512 : index
  %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f16
  %0 = gpu.thread_id  x
  %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>{%c512}
  %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>{%c16384000}
  %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<?xf16, #spirv.storage_class<StorageBuffer>>{%c32000}
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = arith.muli %workgroup_id_x, %c512 : index
  %5 = arith.addi %0, %4 : index
  %6 = arith.addi %5, %c448 : index
  %7 = memref.load %2[%6] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
  %8 = arith.addi %0, %c448 : index
  %9 = memref.load %1[%8] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
  %10 = arith.addi %5, %c384 : index
  %11 = memref.load %2[%10] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
  %12 = arith.addi %0, %c384 : index
  %13 = memref.load %1[%12] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
  %14 = arith.addi %5, %c320 : index
  %15 = memref.load %2[%14] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
  %16 = arith.addi %0, %c320 : index
  %17 = memref.load %1[%16] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
  %18 = arith.addi %5, %c256 : index
  %19 = memref.load %2[%18] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
  %20 = arith.addi %0, %c256 : index
  %21 = memref.load %1[%20] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
  %22 = arith.addi %5, %c192 : index
  %23 = memref.load %2[%22] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
  %24 = arith.addi %0, %c192 : index
  %25 = memref.load %1[%24] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
  %26 = arith.addi %5, %c128 : index
  %27 = memref.load %2[%26] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
  %28 = arith.addi %0, %c128 : index
  %29 = memref.load %1[%28] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
  %30 = arith.addi %5, %c64 : index
  %31 = memref.load %2[%30] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
  %32 = arith.addi %0, %c64 : index
  %33 = memref.load %1[%32] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
  %34 = arith.addi %4, %0 : index
  %35 = memref.load %2[%34] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
  %36 = memref.load %1[%0] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
  %37 = vector.extract_strided_slice %9 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %38 = vector.bitcast %37 : vector<2xf32> to vector<4xf16>
  %39 = vector.extract_strided_slice %7 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %40 = vector.bitcast %39 : vector<2xf32> to vector<4xf16>
  %41 = arith.mulf %38, %40 : vector<4xf16>
  %42 = vector.extract_strided_slice %9 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %43 = vector.bitcast %42 : vector<2xf32> to vector<4xf16>
  %44 = vector.extract_strided_slice %7 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %45 = vector.bitcast %44 : vector<2xf32> to vector<4xf16>
  %46 = arith.mulf %43, %45 : vector<4xf16>
  %47 = arith.addf %41, %cst : vector<4xf16>
  %48 = arith.addf %46, %cst : vector<4xf16>
  %49 = vector.extract_strided_slice %13 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %50 = vector.bitcast %49 : vector<2xf32> to vector<4xf16>
  %51 = vector.extract_strided_slice %11 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %52 = vector.bitcast %51 : vector<2xf32> to vector<4xf16>
  %53 = arith.mulf %50, %52 : vector<4xf16>
  %54 = vector.extract_strided_slice %13 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %55 = vector.bitcast %54 : vector<2xf32> to vector<4xf16>
  %56 = vector.extract_strided_slice %11 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %57 = vector.bitcast %56 : vector<2xf32> to vector<4xf16>
  %58 = arith.mulf %55, %57 : vector<4xf16>
  %59 = arith.addf %53, %47 : vector<4xf16>
  %60 = arith.addf %58, %48 : vector<4xf16>
  %61 = vector.extract_strided_slice %17 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %62 = vector.bitcast %61 : vector<2xf32> to vector<4xf16>
  %63 = vector.extract_strided_slice %15 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %64 = vector.bitcast %63 : vector<2xf32> to vector<4xf16>
  %65 = arith.mulf %62, %64 : vector<4xf16>
  %66 = vector.extract_strided_slice %17 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %67 = vector.bitcast %66 : vector<2xf32> to vector<4xf16>
  %68 = vector.extract_strided_slice %15 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %69 = vector.bitcast %68 : vector<2xf32> to vector<4xf16>
  %70 = arith.mulf %67, %69 : vector<4xf16>
  %71 = arith.addf %65, %59 : vector<4xf16>
  %72 = arith.addf %70, %60 : vector<4xf16>
  %73 = vector.extract_strided_slice %21 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %74 = vector.bitcast %73 : vector<2xf32> to vector<4xf16>
  %75 = vector.extract_strided_slice %19 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %76 = vector.bitcast %75 : vector<2xf32> to vector<4xf16>
  %77 = arith.mulf %74, %76 : vector<4xf16>
  %78 = vector.extract_strided_slice %21 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %79 = vector.bitcast %78 : vector<2xf32> to vector<4xf16>
  %80 = vector.extract_strided_slice %19 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %81 = vector.bitcast %80 : vector<2xf32> to vector<4xf16>
  %82 = arith.mulf %79, %81 : vector<4xf16>
  %83 = arith.addf %77, %71 : vector<4xf16>
  %84 = arith.addf %82, %72 : vector<4xf16>
  %85 = vector.extract_strided_slice %25 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %86 = vector.bitcast %85 : vector<2xf32> to vector<4xf16>
  %87 = vector.extract_strided_slice %23 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %88 = vector.bitcast %87 : vector<2xf32> to vector<4xf16>
  %89 = arith.mulf %86, %88 : vector<4xf16>
  %90 = vector.extract_strided_slice %25 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %91 = vector.bitcast %90 : vector<2xf32> to vector<4xf16>
  %92 = vector.extract_strided_slice %23 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %93 = vector.bitcast %92 : vector<2xf32> to vector<4xf16>
  %94 = arith.mulf %91, %93 : vector<4xf16>
  %95 = arith.addf %89, %83 : vector<4xf16>
  %96 = arith.addf %94, %84 : vector<4xf16>
  %97 = vector.extract_strided_slice %29 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %98 = vector.bitcast %97 : vector<2xf32> to vector<4xf16>
  %99 = vector.extract_strided_slice %27 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %100 = vector.bitcast %99 : vector<2xf32> to vector<4xf16>
  %101 = arith.mulf %98, %100 : vector<4xf16>
  %102 = vector.extract_strided_slice %29 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %103 = vector.bitcast %102 : vector<2xf32> to vector<4xf16>
  %104 = vector.extract_strided_slice %27 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %105 = vector.bitcast %104 : vector<2xf32> to vector<4xf16>
  %106 = arith.mulf %103, %105 : vector<4xf16>
  %107 = arith.addf %101, %95 : vector<4xf16>
  %108 = arith.addf %106, %96 : vector<4xf16>
  %109 = vector.extract_strided_slice %33 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %110 = vector.bitcast %109 : vector<2xf32> to vector<4xf16>
  %111 = vector.extract_strided_slice %31 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %112 = vector.bitcast %111 : vector<2xf32> to vector<4xf16>
  %113 = arith.mulf %110, %112 : vector<4xf16>
  %114 = vector.extract_strided_slice %33 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %115 = vector.bitcast %114 : vector<2xf32> to vector<4xf16>
  %116 = vector.extract_strided_slice %31 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %117 = vector.bitcast %116 : vector<2xf32> to vector<4xf16>
  %118 = arith.mulf %115, %117 : vector<4xf16>
  %119 = arith.addf %113, %107 : vector<4xf16>
  %120 = arith.addf %118, %108 : vector<4xf16>
  %121 = vector.extract_strided_slice %36 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %122 = vector.bitcast %121 : vector<2xf32> to vector<4xf16>
  %123 = vector.extract_strided_slice %35 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %124 = vector.bitcast %123 : vector<2xf32> to vector<4xf16>
  %125 = arith.mulf %122, %124 : vector<4xf16>
  %126 = vector.extract_strided_slice %36 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %127 = vector.bitcast %126 : vector<2xf32> to vector<4xf16>
  %128 = vector.extract_strided_slice %35 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
  %129 = vector.bitcast %128 : vector<2xf32> to vector<4xf16>
  %130 = arith.mulf %127, %129 : vector<4xf16>
  %131 = arith.addf %125, %119 : vector<4xf16>
  %132 = arith.addf %130, %120 : vector<4xf16>
  %133 = vector.reduction <add>, %131 : vector<4xf16> into f16
  %134 = vector.reduction <add>, %132 : vector<4xf16> into f16
  %135 = arith.addf %133, %134 : f16
  %136 = gpu.subgroup_reduce  add %135 : (f16) -> f16
  %137 = arith.addf %136, %cst_0 : f16
  %138 = arith.cmpi eq, %0, %c0 : index
  scf.if %138 {
    memref.store %137, %3[%workgroup_id_x] : memref<?xf16, #spirv.storage_class<StorageBuffer>>
  }
  return
}

// -----// IR Dump After SPIRVEmulateI64 (iree-spirv-emulate-i64) //----- //
module {
  func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
    %c64 = arith.constant 64 : index
    %c128 = arith.constant 128 : index
    %c192 = arith.constant 192 : index
    %c256 = arith.constant 256 : index
    %c320 = arith.constant 320 : index
    %c384 = arith.constant 384 : index
    %c448 = arith.constant 448 : index
    %c32000 = arith.constant 32000 : index
    %c16384000 = arith.constant 16384000 : index
    %c512 = arith.constant 512 : index
    %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f16
    %0 = gpu.thread_id  x
    %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>{%c512}
    %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>{%c16384000}
    %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<?xf16, #spirv.storage_class<StorageBuffer>>{%c32000}
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = arith.muli %workgroup_id_x, %c512 : index
    %5 = arith.addi %0, %4 : index
    %6 = arith.addi %5, %c448 : index
    %7 = memref.load %2[%6] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %8 = arith.addi %0, %c448 : index
    %9 = memref.load %1[%8] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %10 = arith.addi %5, %c384 : index
    %11 = memref.load %2[%10] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %12 = arith.addi %0, %c384 : index
    %13 = memref.load %1[%12] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %14 = arith.addi %5, %c320 : index
    %15 = memref.load %2[%14] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %16 = arith.addi %0, %c320 : index
    %17 = memref.load %1[%16] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %18 = arith.addi %5, %c256 : index
    %19 = memref.load %2[%18] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %20 = arith.addi %0, %c256 : index
    %21 = memref.load %1[%20] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %22 = arith.addi %5, %c192 : index
    %23 = memref.load %2[%22] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %24 = arith.addi %0, %c192 : index
    %25 = memref.load %1[%24] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %26 = arith.addi %5, %c128 : index
    %27 = memref.load %2[%26] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %28 = arith.addi %0, %c128 : index
    %29 = memref.load %1[%28] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %30 = arith.addi %5, %c64 : index
    %31 = memref.load %2[%30] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %32 = arith.addi %0, %c64 : index
    %33 = memref.load %1[%32] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %34 = arith.addi %4, %0 : index
    %35 = memref.load %2[%34] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %36 = memref.load %1[%0] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %37 = vector.extract_strided_slice %9 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %38 = vector.bitcast %37 : vector<2xf32> to vector<4xf16>
    %39 = vector.extract_strided_slice %7 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %40 = vector.bitcast %39 : vector<2xf32> to vector<4xf16>
    %41 = arith.mulf %38, %40 : vector<4xf16>
    %42 = vector.extract_strided_slice %9 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %43 = vector.bitcast %42 : vector<2xf32> to vector<4xf16>
    %44 = vector.extract_strided_slice %7 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %45 = vector.bitcast %44 : vector<2xf32> to vector<4xf16>
    %46 = arith.mulf %43, %45 : vector<4xf16>
    %47 = arith.addf %41, %cst : vector<4xf16>
    %48 = arith.addf %46, %cst : vector<4xf16>
    %49 = vector.extract_strided_slice %13 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %50 = vector.bitcast %49 : vector<2xf32> to vector<4xf16>
    %51 = vector.extract_strided_slice %11 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %52 = vector.bitcast %51 : vector<2xf32> to vector<4xf16>
    %53 = arith.mulf %50, %52 : vector<4xf16>
    %54 = vector.extract_strided_slice %13 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %55 = vector.bitcast %54 : vector<2xf32> to vector<4xf16>
    %56 = vector.extract_strided_slice %11 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %57 = vector.bitcast %56 : vector<2xf32> to vector<4xf16>
    %58 = arith.mulf %55, %57 : vector<4xf16>
    %59 = arith.addf %53, %47 : vector<4xf16>
    %60 = arith.addf %58, %48 : vector<4xf16>
    %61 = vector.extract_strided_slice %17 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %62 = vector.bitcast %61 : vector<2xf32> to vector<4xf16>
    %63 = vector.extract_strided_slice %15 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %64 = vector.bitcast %63 : vector<2xf32> to vector<4xf16>
    %65 = arith.mulf %62, %64 : vector<4xf16>
    %66 = vector.extract_strided_slice %17 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %67 = vector.bitcast %66 : vector<2xf32> to vector<4xf16>
    %68 = vector.extract_strided_slice %15 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %69 = vector.bitcast %68 : vector<2xf32> to vector<4xf16>
    %70 = arith.mulf %67, %69 : vector<4xf16>
    %71 = arith.addf %65, %59 : vector<4xf16>
    %72 = arith.addf %70, %60 : vector<4xf16>
    %73 = vector.extract_strided_slice %21 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %74 = vector.bitcast %73 : vector<2xf32> to vector<4xf16>
    %75 = vector.extract_strided_slice %19 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %76 = vector.bitcast %75 : vector<2xf32> to vector<4xf16>
    %77 = arith.mulf %74, %76 : vector<4xf16>
    %78 = vector.extract_strided_slice %21 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %79 = vector.bitcast %78 : vector<2xf32> to vector<4xf16>
    %80 = vector.extract_strided_slice %19 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %81 = vector.bitcast %80 : vector<2xf32> to vector<4xf16>
    %82 = arith.mulf %79, %81 : vector<4xf16>
    %83 = arith.addf %77, %71 : vector<4xf16>
    %84 = arith.addf %82, %72 : vector<4xf16>
    %85 = vector.extract_strided_slice %25 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %86 = vector.bitcast %85 : vector<2xf32> to vector<4xf16>
    %87 = vector.extract_strided_slice %23 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %88 = vector.bitcast %87 : vector<2xf32> to vector<4xf16>
    %89 = arith.mulf %86, %88 : vector<4xf16>
    %90 = vector.extract_strided_slice %25 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %91 = vector.bitcast %90 : vector<2xf32> to vector<4xf16>
    %92 = vector.extract_strided_slice %23 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %93 = vector.bitcast %92 : vector<2xf32> to vector<4xf16>
    %94 = arith.mulf %91, %93 : vector<4xf16>
    %95 = arith.addf %89, %83 : vector<4xf16>
    %96 = arith.addf %94, %84 : vector<4xf16>
    %97 = vector.extract_strided_slice %29 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %98 = vector.bitcast %97 : vector<2xf32> to vector<4xf16>
    %99 = vector.extract_strided_slice %27 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %100 = vector.bitcast %99 : vector<2xf32> to vector<4xf16>
    %101 = arith.mulf %98, %100 : vector<4xf16>
    %102 = vector.extract_strided_slice %29 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %103 = vector.bitcast %102 : vector<2xf32> to vector<4xf16>
    %104 = vector.extract_strided_slice %27 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %105 = vector.bitcast %104 : vector<2xf32> to vector<4xf16>
    %106 = arith.mulf %103, %105 : vector<4xf16>
    %107 = arith.addf %101, %95 : vector<4xf16>
    %108 = arith.addf %106, %96 : vector<4xf16>
    %109 = vector.extract_strided_slice %33 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %110 = vector.bitcast %109 : vector<2xf32> to vector<4xf16>
    %111 = vector.extract_strided_slice %31 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %112 = vector.bitcast %111 : vector<2xf32> to vector<4xf16>
    %113 = arith.mulf %110, %112 : vector<4xf16>
    %114 = vector.extract_strided_slice %33 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %115 = vector.bitcast %114 : vector<2xf32> to vector<4xf16>
    %116 = vector.extract_strided_slice %31 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %117 = vector.bitcast %116 : vector<2xf32> to vector<4xf16>
    %118 = arith.mulf %115, %117 : vector<4xf16>
    %119 = arith.addf %113, %107 : vector<4xf16>
    %120 = arith.addf %118, %108 : vector<4xf16>
    %121 = vector.extract_strided_slice %36 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %122 = vector.bitcast %121 : vector<2xf32> to vector<4xf16>
    %123 = vector.extract_strided_slice %35 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %124 = vector.bitcast %123 : vector<2xf32> to vector<4xf16>
    %125 = arith.mulf %122, %124 : vector<4xf16>
    %126 = vector.extract_strided_slice %36 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %127 = vector.bitcast %126 : vector<2xf32> to vector<4xf16>
    %128 = vector.extract_strided_slice %35 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %129 = vector.bitcast %128 : vector<2xf32> to vector<4xf16>
    %130 = arith.mulf %127, %129 : vector<4xf16>
    %131 = arith.addf %125, %119 : vector<4xf16>
    %132 = arith.addf %130, %120 : vector<4xf16>
    %133 = vector.reduction <add>, %131 : vector<4xf16> into f16
    %134 = vector.reduction <add>, %132 : vector<4xf16> into f16
    %135 = arith.addf %133, %134 : f16
    %136 = gpu.subgroup_reduce  add %135 : (f16) -> f16
    %137 = arith.addf %136, %cst_0 : f16
    %138 = arith.cmpi eq, %0, %c0 : index
    scf.if %138 {
      memref.store %137, %3[%workgroup_id_x] : memref<?xf16, #spirv.storage_class<StorageBuffer>>
    }
    return
  }
}

// -----// IR Dump After ConvertBf16ArithToF32 (iree-convert-bf16-arith-to-f32) //----- //
module {
  func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
    %c64 = arith.constant 64 : index
    %c128 = arith.constant 128 : index
    %c192 = arith.constant 192 : index
    %c256 = arith.constant 256 : index
    %c320 = arith.constant 320 : index
    %c384 = arith.constant 384 : index
    %c448 = arith.constant 448 : index
    %c32000 = arith.constant 32000 : index
    %c16384000 = arith.constant 16384000 : index
    %c512 = arith.constant 512 : index
    %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f16
    %0 = gpu.thread_id  x
    %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>{%c512}
    %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>{%c16384000}
    %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<?xf16, #spirv.storage_class<StorageBuffer>>{%c32000}
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = arith.muli %workgroup_id_x, %c512 : index
    %5 = arith.addi %0, %4 : index
    %6 = arith.addi %5, %c448 : index
    %7 = memref.load %2[%6] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %8 = arith.addi %0, %c448 : index
    %9 = memref.load %1[%8] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %10 = arith.addi %5, %c384 : index
    %11 = memref.load %2[%10] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %12 = arith.addi %0, %c384 : index
    %13 = memref.load %1[%12] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %14 = arith.addi %5, %c320 : index
    %15 = memref.load %2[%14] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %16 = arith.addi %0, %c320 : index
    %17 = memref.load %1[%16] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %18 = arith.addi %5, %c256 : index
    %19 = memref.load %2[%18] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %20 = arith.addi %0, %c256 : index
    %21 = memref.load %1[%20] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %22 = arith.addi %5, %c192 : index
    %23 = memref.load %2[%22] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %24 = arith.addi %0, %c192 : index
    %25 = memref.load %1[%24] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %26 = arith.addi %5, %c128 : index
    %27 = memref.load %2[%26] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %28 = arith.addi %0, %c128 : index
    %29 = memref.load %1[%28] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %30 = arith.addi %5, %c64 : index
    %31 = memref.load %2[%30] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %32 = arith.addi %0, %c64 : index
    %33 = memref.load %1[%32] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %34 = arith.addi %4, %0 : index
    %35 = memref.load %2[%34] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %36 = memref.load %1[%0] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %37 = vector.extract_strided_slice %9 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %38 = vector.bitcast %37 : vector<2xf32> to vector<4xf16>
    %39 = vector.extract_strided_slice %7 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %40 = vector.bitcast %39 : vector<2xf32> to vector<4xf16>
    %41 = arith.mulf %38, %40 : vector<4xf16>
    %42 = vector.extract_strided_slice %9 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %43 = vector.bitcast %42 : vector<2xf32> to vector<4xf16>
    %44 = vector.extract_strided_slice %7 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %45 = vector.bitcast %44 : vector<2xf32> to vector<4xf16>
    %46 = arith.mulf %43, %45 : vector<4xf16>
    %47 = arith.addf %41, %cst : vector<4xf16>
    %48 = arith.addf %46, %cst : vector<4xf16>
    %49 = vector.extract_strided_slice %13 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %50 = vector.bitcast %49 : vector<2xf32> to vector<4xf16>
    %51 = vector.extract_strided_slice %11 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %52 = vector.bitcast %51 : vector<2xf32> to vector<4xf16>
    %53 = arith.mulf %50, %52 : vector<4xf16>
    %54 = vector.extract_strided_slice %13 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %55 = vector.bitcast %54 : vector<2xf32> to vector<4xf16>
    %56 = vector.extract_strided_slice %11 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %57 = vector.bitcast %56 : vector<2xf32> to vector<4xf16>
    %58 = arith.mulf %55, %57 : vector<4xf16>
    %59 = arith.addf %53, %47 : vector<4xf16>
    %60 = arith.addf %58, %48 : vector<4xf16>
    %61 = vector.extract_strided_slice %17 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %62 = vector.bitcast %61 : vector<2xf32> to vector<4xf16>
    %63 = vector.extract_strided_slice %15 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %64 = vector.bitcast %63 : vector<2xf32> to vector<4xf16>
    %65 = arith.mulf %62, %64 : vector<4xf16>
    %66 = vector.extract_strided_slice %17 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %67 = vector.bitcast %66 : vector<2xf32> to vector<4xf16>
    %68 = vector.extract_strided_slice %15 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %69 = vector.bitcast %68 : vector<2xf32> to vector<4xf16>
    %70 = arith.mulf %67, %69 : vector<4xf16>
    %71 = arith.addf %65, %59 : vector<4xf16>
    %72 = arith.addf %70, %60 : vector<4xf16>
    %73 = vector.extract_strided_slice %21 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %74 = vector.bitcast %73 : vector<2xf32> to vector<4xf16>
    %75 = vector.extract_strided_slice %19 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %76 = vector.bitcast %75 : vector<2xf32> to vector<4xf16>
    %77 = arith.mulf %74, %76 : vector<4xf16>
    %78 = vector.extract_strided_slice %21 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %79 = vector.bitcast %78 : vector<2xf32> to vector<4xf16>
    %80 = vector.extract_strided_slice %19 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %81 = vector.bitcast %80 : vector<2xf32> to vector<4xf16>
    %82 = arith.mulf %79, %81 : vector<4xf16>
    %83 = arith.addf %77, %71 : vector<4xf16>
    %84 = arith.addf %82, %72 : vector<4xf16>
    %85 = vector.extract_strided_slice %25 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %86 = vector.bitcast %85 : vector<2xf32> to vector<4xf16>
    %87 = vector.extract_strided_slice %23 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %88 = vector.bitcast %87 : vector<2xf32> to vector<4xf16>
    %89 = arith.mulf %86, %88 : vector<4xf16>
    %90 = vector.extract_strided_slice %25 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %91 = vector.bitcast %90 : vector<2xf32> to vector<4xf16>
    %92 = vector.extract_strided_slice %23 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %93 = vector.bitcast %92 : vector<2xf32> to vector<4xf16>
    %94 = arith.mulf %91, %93 : vector<4xf16>
    %95 = arith.addf %89, %83 : vector<4xf16>
    %96 = arith.addf %94, %84 : vector<4xf16>
    %97 = vector.extract_strided_slice %29 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %98 = vector.bitcast %97 : vector<2xf32> to vector<4xf16>
    %99 = vector.extract_strided_slice %27 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %100 = vector.bitcast %99 : vector<2xf32> to vector<4xf16>
    %101 = arith.mulf %98, %100 : vector<4xf16>
    %102 = vector.extract_strided_slice %29 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %103 = vector.bitcast %102 : vector<2xf32> to vector<4xf16>
    %104 = vector.extract_strided_slice %27 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %105 = vector.bitcast %104 : vector<2xf32> to vector<4xf16>
    %106 = arith.mulf %103, %105 : vector<4xf16>
    %107 = arith.addf %101, %95 : vector<4xf16>
    %108 = arith.addf %106, %96 : vector<4xf16>
    %109 = vector.extract_strided_slice %33 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %110 = vector.bitcast %109 : vector<2xf32> to vector<4xf16>
    %111 = vector.extract_strided_slice %31 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %112 = vector.bitcast %111 : vector<2xf32> to vector<4xf16>
    %113 = arith.mulf %110, %112 : vector<4xf16>
    %114 = vector.extract_strided_slice %33 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %115 = vector.bitcast %114 : vector<2xf32> to vector<4xf16>
    %116 = vector.extract_strided_slice %31 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %117 = vector.bitcast %116 : vector<2xf32> to vector<4xf16>
    %118 = arith.mulf %115, %117 : vector<4xf16>
    %119 = arith.addf %113, %107 : vector<4xf16>
    %120 = arith.addf %118, %108 : vector<4xf16>
    %121 = vector.extract_strided_slice %36 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %122 = vector.bitcast %121 : vector<2xf32> to vector<4xf16>
    %123 = vector.extract_strided_slice %35 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %124 = vector.bitcast %123 : vector<2xf32> to vector<4xf16>
    %125 = arith.mulf %122, %124 : vector<4xf16>
    %126 = vector.extract_strided_slice %36 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %127 = vector.bitcast %126 : vector<2xf32> to vector<4xf16>
    %128 = vector.extract_strided_slice %35 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %129 = vector.bitcast %128 : vector<2xf32> to vector<4xf16>
    %130 = arith.mulf %127, %129 : vector<4xf16>
    %131 = arith.addf %125, %119 : vector<4xf16>
    %132 = arith.addf %130, %120 : vector<4xf16>
    %133 = vector.reduction <add>, %131 : vector<4xf16> into f16
    %134 = vector.reduction <add>, %132 : vector<4xf16> into f16
    %135 = arith.addf %133, %134 : f16
    %136 = gpu.subgroup_reduce  add %135 : (f16) -> f16
    %137 = arith.addf %136, %cst_0 : f16
    %138 = arith.cmpi eq, %0, %c0 : index
    scf.if %138 {
      memref.store %137, %3[%workgroup_id_x] : memref<?xf16, #spirv.storage_class<StorageBuffer>>
    }
    return
  }
}

// -----// IR Dump After ConvertBf16ToUInt16Buffers (iree-convert-bf16-to-uint16-buffers) //----- //
module {
  func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
    %c64 = arith.constant 64 : index
    %c128 = arith.constant 128 : index
    %c192 = arith.constant 192 : index
    %c256 = arith.constant 256 : index
    %c320 = arith.constant 320 : index
    %c384 = arith.constant 384 : index
    %c448 = arith.constant 448 : index
    %c32000 = arith.constant 32000 : index
    %c16384000 = arith.constant 16384000 : index
    %c512 = arith.constant 512 : index
    %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f16
    %0 = gpu.thread_id  x
    %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>{%c512}
    %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>{%c16384000}
    %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<?xf16, #spirv.storage_class<StorageBuffer>>{%c32000}
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = arith.muli %workgroup_id_x, %c512 : index
    %5 = arith.addi %0, %4 : index
    %6 = arith.addi %5, %c448 : index
    %7 = memref.load %2[%6] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %8 = arith.addi %0, %c448 : index
    %9 = memref.load %1[%8] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %10 = arith.addi %5, %c384 : index
    %11 = memref.load %2[%10] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %12 = arith.addi %0, %c384 : index
    %13 = memref.load %1[%12] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %14 = arith.addi %5, %c320 : index
    %15 = memref.load %2[%14] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %16 = arith.addi %0, %c320 : index
    %17 = memref.load %1[%16] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %18 = arith.addi %5, %c256 : index
    %19 = memref.load %2[%18] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %20 = arith.addi %0, %c256 : index
    %21 = memref.load %1[%20] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %22 = arith.addi %5, %c192 : index
    %23 = memref.load %2[%22] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %24 = arith.addi %0, %c192 : index
    %25 = memref.load %1[%24] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %26 = arith.addi %5, %c128 : index
    %27 = memref.load %2[%26] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %28 = arith.addi %0, %c128 : index
    %29 = memref.load %1[%28] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %30 = arith.addi %5, %c64 : index
    %31 = memref.load %2[%30] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %32 = arith.addi %0, %c64 : index
    %33 = memref.load %1[%32] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %34 = arith.addi %4, %0 : index
    %35 = memref.load %2[%34] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %36 = memref.load %1[%0] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %37 = vector.extract_strided_slice %9 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %38 = vector.bitcast %37 : vector<2xf32> to vector<4xf16>
    %39 = vector.extract_strided_slice %7 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %40 = vector.bitcast %39 : vector<2xf32> to vector<4xf16>
    %41 = arith.mulf %38, %40 : vector<4xf16>
    %42 = vector.extract_strided_slice %9 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %43 = vector.bitcast %42 : vector<2xf32> to vector<4xf16>
    %44 = vector.extract_strided_slice %7 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %45 = vector.bitcast %44 : vector<2xf32> to vector<4xf16>
    %46 = arith.mulf %43, %45 : vector<4xf16>
    %47 = arith.addf %41, %cst : vector<4xf16>
    %48 = arith.addf %46, %cst : vector<4xf16>
    %49 = vector.extract_strided_slice %13 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %50 = vector.bitcast %49 : vector<2xf32> to vector<4xf16>
    %51 = vector.extract_strided_slice %11 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %52 = vector.bitcast %51 : vector<2xf32> to vector<4xf16>
    %53 = arith.mulf %50, %52 : vector<4xf16>
    %54 = vector.extract_strided_slice %13 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %55 = vector.bitcast %54 : vector<2xf32> to vector<4xf16>
    %56 = vector.extract_strided_slice %11 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %57 = vector.bitcast %56 : vector<2xf32> to vector<4xf16>
    %58 = arith.mulf %55, %57 : vector<4xf16>
    %59 = arith.addf %53, %47 : vector<4xf16>
    %60 = arith.addf %58, %48 : vector<4xf16>
    %61 = vector.extract_strided_slice %17 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %62 = vector.bitcast %61 : vector<2xf32> to vector<4xf16>
    %63 = vector.extract_strided_slice %15 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %64 = vector.bitcast %63 : vector<2xf32> to vector<4xf16>
    %65 = arith.mulf %62, %64 : vector<4xf16>
    %66 = vector.extract_strided_slice %17 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %67 = vector.bitcast %66 : vector<2xf32> to vector<4xf16>
    %68 = vector.extract_strided_slice %15 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %69 = vector.bitcast %68 : vector<2xf32> to vector<4xf16>
    %70 = arith.mulf %67, %69 : vector<4xf16>
    %71 = arith.addf %65, %59 : vector<4xf16>
    %72 = arith.addf %70, %60 : vector<4xf16>
    %73 = vector.extract_strided_slice %21 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %74 = vector.bitcast %73 : vector<2xf32> to vector<4xf16>
    %75 = vector.extract_strided_slice %19 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %76 = vector.bitcast %75 : vector<2xf32> to vector<4xf16>
    %77 = arith.mulf %74, %76 : vector<4xf16>
    %78 = vector.extract_strided_slice %21 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %79 = vector.bitcast %78 : vector<2xf32> to vector<4xf16>
    %80 = vector.extract_strided_slice %19 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %81 = vector.bitcast %80 : vector<2xf32> to vector<4xf16>
    %82 = arith.mulf %79, %81 : vector<4xf16>
    %83 = arith.addf %77, %71 : vector<4xf16>
    %84 = arith.addf %82, %72 : vector<4xf16>
    %85 = vector.extract_strided_slice %25 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %86 = vector.bitcast %85 : vector<2xf32> to vector<4xf16>
    %87 = vector.extract_strided_slice %23 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %88 = vector.bitcast %87 : vector<2xf32> to vector<4xf16>
    %89 = arith.mulf %86, %88 : vector<4xf16>
    %90 = vector.extract_strided_slice %25 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %91 = vector.bitcast %90 : vector<2xf32> to vector<4xf16>
    %92 = vector.extract_strided_slice %23 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %93 = vector.bitcast %92 : vector<2xf32> to vector<4xf16>
    %94 = arith.mulf %91, %93 : vector<4xf16>
    %95 = arith.addf %89, %83 : vector<4xf16>
    %96 = arith.addf %94, %84 : vector<4xf16>
    %97 = vector.extract_strided_slice %29 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %98 = vector.bitcast %97 : vector<2xf32> to vector<4xf16>
    %99 = vector.extract_strided_slice %27 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %100 = vector.bitcast %99 : vector<2xf32> to vector<4xf16>
    %101 = arith.mulf %98, %100 : vector<4xf16>
    %102 = vector.extract_strided_slice %29 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %103 = vector.bitcast %102 : vector<2xf32> to vector<4xf16>
    %104 = vector.extract_strided_slice %27 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %105 = vector.bitcast %104 : vector<2xf32> to vector<4xf16>
    %106 = arith.mulf %103, %105 : vector<4xf16>
    %107 = arith.addf %101, %95 : vector<4xf16>
    %108 = arith.addf %106, %96 : vector<4xf16>
    %109 = vector.extract_strided_slice %33 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %110 = vector.bitcast %109 : vector<2xf32> to vector<4xf16>
    %111 = vector.extract_strided_slice %31 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %112 = vector.bitcast %111 : vector<2xf32> to vector<4xf16>
    %113 = arith.mulf %110, %112 : vector<4xf16>
    %114 = vector.extract_strided_slice %33 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %115 = vector.bitcast %114 : vector<2xf32> to vector<4xf16>
    %116 = vector.extract_strided_slice %31 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %117 = vector.bitcast %116 : vector<2xf32> to vector<4xf16>
    %118 = arith.mulf %115, %117 : vector<4xf16>
    %119 = arith.addf %113, %107 : vector<4xf16>
    %120 = arith.addf %118, %108 : vector<4xf16>
    %121 = vector.extract_strided_slice %36 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %122 = vector.bitcast %121 : vector<2xf32> to vector<4xf16>
    %123 = vector.extract_strided_slice %35 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %124 = vector.bitcast %123 : vector<2xf32> to vector<4xf16>
    %125 = arith.mulf %122, %124 : vector<4xf16>
    %126 = vector.extract_strided_slice %36 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %127 = vector.bitcast %126 : vector<2xf32> to vector<4xf16>
    %128 = vector.extract_strided_slice %35 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %129 = vector.bitcast %128 : vector<2xf32> to vector<4xf16>
    %130 = arith.mulf %127, %129 : vector<4xf16>
    %131 = arith.addf %125, %119 : vector<4xf16>
    %132 = arith.addf %130, %120 : vector<4xf16>
    %133 = vector.reduction <add>, %131 : vector<4xf16> into f16
    %134 = vector.reduction <add>, %132 : vector<4xf16> into f16
    %135 = arith.addf %133, %134 : f16
    %136 = gpu.subgroup_reduce  add %135 : (f16) -> f16
    %137 = arith.addf %136, %cst_0 : f16
    %138 = arith.cmpi eq, %0, %c0 : index
    scf.if %138 {
      memref.store %137, %3[%workgroup_id_x] : memref<?xf16, #spirv.storage_class<StorageBuffer>>
    }
    return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module {
  func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
    %c64 = arith.constant 64 : index
    %c128 = arith.constant 128 : index
    %c192 = arith.constant 192 : index
    %c256 = arith.constant 256 : index
    %c320 = arith.constant 320 : index
    %c384 = arith.constant 384 : index
    %c448 = arith.constant 448 : index
    %c32000 = arith.constant 32000 : index
    %c16384000 = arith.constant 16384000 : index
    %c512 = arith.constant 512 : index
    %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f16
    %0 = gpu.thread_id  x
    %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>{%c512}
    %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>{%c16384000}
    %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<?xf16, #spirv.storage_class<StorageBuffer>>{%c32000}
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = arith.muli %workgroup_id_x, %c512 : index
    %5 = arith.addi %0, %4 : index
    %6 = arith.addi %5, %c448 : index
    %7 = memref.load %2[%6] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %8 = arith.addi %0, %c448 : index
    %9 = memref.load %1[%8] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %10 = arith.addi %5, %c384 : index
    %11 = memref.load %2[%10] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %12 = arith.addi %0, %c384 : index
    %13 = memref.load %1[%12] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %14 = arith.addi %5, %c320 : index
    %15 = memref.load %2[%14] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %16 = arith.addi %0, %c320 : index
    %17 = memref.load %1[%16] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %18 = arith.addi %5, %c256 : index
    %19 = memref.load %2[%18] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %20 = arith.addi %0, %c256 : index
    %21 = memref.load %1[%20] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %22 = arith.addi %5, %c192 : index
    %23 = memref.load %2[%22] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %24 = arith.addi %0, %c192 : index
    %25 = memref.load %1[%24] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %26 = arith.addi %5, %c128 : index
    %27 = memref.load %2[%26] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %28 = arith.addi %0, %c128 : index
    %29 = memref.load %1[%28] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %30 = arith.addi %5, %c64 : index
    %31 = memref.load %2[%30] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %32 = arith.addi %0, %c64 : index
    %33 = memref.load %1[%32] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %34 = arith.addi %4, %0 : index
    %35 = memref.load %2[%34] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %36 = memref.load %1[%0] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %37 = vector.extract_strided_slice %9 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %38 = vector.bitcast %37 : vector<2xf32> to vector<4xf16>
    %39 = vector.extract_strided_slice %7 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %40 = vector.bitcast %39 : vector<2xf32> to vector<4xf16>
    %41 = arith.mulf %38, %40 : vector<4xf16>
    %42 = vector.extract_strided_slice %9 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %43 = vector.bitcast %42 : vector<2xf32> to vector<4xf16>
    %44 = vector.extract_strided_slice %7 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %45 = vector.bitcast %44 : vector<2xf32> to vector<4xf16>
    %46 = arith.mulf %43, %45 : vector<4xf16>
    %47 = arith.addf %41, %cst : vector<4xf16>
    %48 = arith.addf %46, %cst : vector<4xf16>
    %49 = vector.extract_strided_slice %13 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %50 = vector.bitcast %49 : vector<2xf32> to vector<4xf16>
    %51 = vector.extract_strided_slice %11 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %52 = vector.bitcast %51 : vector<2xf32> to vector<4xf16>
    %53 = arith.mulf %50, %52 : vector<4xf16>
    %54 = vector.extract_strided_slice %13 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %55 = vector.bitcast %54 : vector<2xf32> to vector<4xf16>
    %56 = vector.extract_strided_slice %11 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %57 = vector.bitcast %56 : vector<2xf32> to vector<4xf16>
    %58 = arith.mulf %55, %57 : vector<4xf16>
    %59 = arith.addf %53, %47 : vector<4xf16>
    %60 = arith.addf %58, %48 : vector<4xf16>
    %61 = vector.extract_strided_slice %17 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %62 = vector.bitcast %61 : vector<2xf32> to vector<4xf16>
    %63 = vector.extract_strided_slice %15 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %64 = vector.bitcast %63 : vector<2xf32> to vector<4xf16>
    %65 = arith.mulf %62, %64 : vector<4xf16>
    %66 = vector.extract_strided_slice %17 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %67 = vector.bitcast %66 : vector<2xf32> to vector<4xf16>
    %68 = vector.extract_strided_slice %15 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %69 = vector.bitcast %68 : vector<2xf32> to vector<4xf16>
    %70 = arith.mulf %67, %69 : vector<4xf16>
    %71 = arith.addf %65, %59 : vector<4xf16>
    %72 = arith.addf %70, %60 : vector<4xf16>
    %73 = vector.extract_strided_slice %21 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %74 = vector.bitcast %73 : vector<2xf32> to vector<4xf16>
    %75 = vector.extract_strided_slice %19 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %76 = vector.bitcast %75 : vector<2xf32> to vector<4xf16>
    %77 = arith.mulf %74, %76 : vector<4xf16>
    %78 = vector.extract_strided_slice %21 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %79 = vector.bitcast %78 : vector<2xf32> to vector<4xf16>
    %80 = vector.extract_strided_slice %19 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %81 = vector.bitcast %80 : vector<2xf32> to vector<4xf16>
    %82 = arith.mulf %79, %81 : vector<4xf16>
    %83 = arith.addf %77, %71 : vector<4xf16>
    %84 = arith.addf %82, %72 : vector<4xf16>
    %85 = vector.extract_strided_slice %25 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %86 = vector.bitcast %85 : vector<2xf32> to vector<4xf16>
    %87 = vector.extract_strided_slice %23 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %88 = vector.bitcast %87 : vector<2xf32> to vector<4xf16>
    %89 = arith.mulf %86, %88 : vector<4xf16>
    %90 = vector.extract_strided_slice %25 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %91 = vector.bitcast %90 : vector<2xf32> to vector<4xf16>
    %92 = vector.extract_strided_slice %23 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %93 = vector.bitcast %92 : vector<2xf32> to vector<4xf16>
    %94 = arith.mulf %91, %93 : vector<4xf16>
    %95 = arith.addf %89, %83 : vector<4xf16>
    %96 = arith.addf %94, %84 : vector<4xf16>
    %97 = vector.extract_strided_slice %29 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %98 = vector.bitcast %97 : vector<2xf32> to vector<4xf16>
    %99 = vector.extract_strided_slice %27 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %100 = vector.bitcast %99 : vector<2xf32> to vector<4xf16>
    %101 = arith.mulf %98, %100 : vector<4xf16>
    %102 = vector.extract_strided_slice %29 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %103 = vector.bitcast %102 : vector<2xf32> to vector<4xf16>
    %104 = vector.extract_strided_slice %27 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %105 = vector.bitcast %104 : vector<2xf32> to vector<4xf16>
    %106 = arith.mulf %103, %105 : vector<4xf16>
    %107 = arith.addf %101, %95 : vector<4xf16>
    %108 = arith.addf %106, %96 : vector<4xf16>
    %109 = vector.extract_strided_slice %33 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %110 = vector.bitcast %109 : vector<2xf32> to vector<4xf16>
    %111 = vector.extract_strided_slice %31 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %112 = vector.bitcast %111 : vector<2xf32> to vector<4xf16>
    %113 = arith.mulf %110, %112 : vector<4xf16>
    %114 = vector.extract_strided_slice %33 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %115 = vector.bitcast %114 : vector<2xf32> to vector<4xf16>
    %116 = vector.extract_strided_slice %31 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %117 = vector.bitcast %116 : vector<2xf32> to vector<4xf16>
    %118 = arith.mulf %115, %117 : vector<4xf16>
    %119 = arith.addf %113, %107 : vector<4xf16>
    %120 = arith.addf %118, %108 : vector<4xf16>
    %121 = vector.extract_strided_slice %36 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %122 = vector.bitcast %121 : vector<2xf32> to vector<4xf16>
    %123 = vector.extract_strided_slice %35 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %124 = vector.bitcast %123 : vector<2xf32> to vector<4xf16>
    %125 = arith.mulf %122, %124 : vector<4xf16>
    %126 = vector.extract_strided_slice %36 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %127 = vector.bitcast %126 : vector<2xf32> to vector<4xf16>
    %128 = vector.extract_strided_slice %35 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %129 = vector.bitcast %128 : vector<2xf32> to vector<4xf16>
    %130 = arith.mulf %127, %129 : vector<4xf16>
    %131 = arith.addf %125, %119 : vector<4xf16>
    %132 = arith.addf %130, %120 : vector<4xf16>
    %133 = vector.reduction <add>, %131 : vector<4xf16> into f16
    %134 = vector.reduction <add>, %132 : vector<4xf16> into f16
    %135 = arith.addf %133, %134 : f16
    %136 = gpu.subgroup_reduce  add %135 : (f16) -> f16
    %137 = arith.addf %136, %cst_0 : f16
    %138 = arith.cmpi eq, %0, %c0 : index
    scf.if %138 {
      memref.store %137, %3[%workgroup_id_x] : memref<?xf16, #spirv.storage_class<StorageBuffer>>
    }
    return
  }
}

// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() {
    %c64 = arith.constant 64 : index
    %c128 = arith.constant 128 : index
    %c192 = arith.constant 192 : index
    %c256 = arith.constant 256 : index
    %c320 = arith.constant 320 : index
    %c384 = arith.constant 384 : index
    %c448 = arith.constant 448 : index
    %c32000 = arith.constant 32000 : index
    %c16384000 = arith.constant 16384000 : index
    %c512 = arith.constant 512 : index
    %cst = arith.constant dense<0.000000e+00> : vector<4xf16>
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f16
    %0 = gpu.thread_id  x
    %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>{%c512}
    %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>{%c16384000}
    %3 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<?xf16, #spirv.storage_class<StorageBuffer>>{%c32000}
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = arith.muli %workgroup_id_x, %c512 : index
    %5 = arith.addi %0, %4 : index
    %6 = arith.addi %5, %c448 : index
    %7 = memref.load %2[%6] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %8 = arith.addi %0, %c448 : index
    %9 = memref.load %1[%8] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %10 = arith.addi %5, %c384 : index
    %11 = memref.load %2[%10] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %12 = arith.addi %0, %c384 : index
    %13 = memref.load %1[%12] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %14 = arith.addi %5, %c320 : index
    %15 = memref.load %2[%14] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %16 = arith.addi %0, %c320 : index
    %17 = memref.load %1[%16] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %18 = arith.addi %5, %c256 : index
    %19 = memref.load %2[%18] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %20 = arith.addi %0, %c256 : index
    %21 = memref.load %1[%20] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %22 = arith.addi %5, %c192 : index
    %23 = memref.load %2[%22] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %24 = arith.addi %0, %c192 : index
    %25 = memref.load %1[%24] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %26 = arith.addi %5, %c128 : index
    %27 = memref.load %2[%26] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %28 = arith.addi %0, %c128 : index
    %29 = memref.load %1[%28] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %30 = arith.addi %5, %c64 : index
    %31 = memref.load %2[%30] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %32 = arith.addi %0, %c64 : index
    %33 = memref.load %1[%32] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %34 = arith.addi %4, %0 : index
    %35 = memref.load %2[%34] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %36 = memref.load %1[%0] : memref<?xvector<4xf32>, #spirv.storage_class<StorageBuffer>>
    %37 = vector.extract_strided_slice %9 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %38 = vector.bitcast %37 : vector<2xf32> to vector<4xf16>
    %39 = vector.extract_strided_slice %7 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %40 = vector.bitcast %39 : vector<2xf32> to vector<4xf16>
    %41 = arith.mulf %38, %40 : vector<4xf16>
    %42 = vector.extract_strided_slice %9 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %43 = vector.bitcast %42 : vector<2xf32> to vector<4xf16>
    %44 = vector.extract_strided_slice %7 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %45 = vector.bitcast %44 : vector<2xf32> to vector<4xf16>
    %46 = arith.mulf %43, %45 : vector<4xf16>
    %47 = arith.addf %41, %cst : vector<4xf16>
    %48 = arith.addf %46, %cst : vector<4xf16>
    %49 = vector.extract_strided_slice %13 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %50 = vector.bitcast %49 : vector<2xf32> to vector<4xf16>
    %51 = vector.extract_strided_slice %11 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %52 = vector.bitcast %51 : vector<2xf32> to vector<4xf16>
    %53 = arith.mulf %50, %52 : vector<4xf16>
    %54 = vector.extract_strided_slice %13 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %55 = vector.bitcast %54 : vector<2xf32> to vector<4xf16>
    %56 = vector.extract_strided_slice %11 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %57 = vector.bitcast %56 : vector<2xf32> to vector<4xf16>
    %58 = arith.mulf %55, %57 : vector<4xf16>
    %59 = arith.addf %53, %47 : vector<4xf16>
    %60 = arith.addf %58, %48 : vector<4xf16>
    %61 = vector.extract_strided_slice %17 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %62 = vector.bitcast %61 : vector<2xf32> to vector<4xf16>
    %63 = vector.extract_strided_slice %15 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %64 = vector.bitcast %63 : vector<2xf32> to vector<4xf16>
    %65 = arith.mulf %62, %64 : vector<4xf16>
    %66 = vector.extract_strided_slice %17 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %67 = vector.bitcast %66 : vector<2xf32> to vector<4xf16>
    %68 = vector.extract_strided_slice %15 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %69 = vector.bitcast %68 : vector<2xf32> to vector<4xf16>
    %70 = arith.mulf %67, %69 : vector<4xf16>
    %71 = arith.addf %65, %59 : vector<4xf16>
    %72 = arith.addf %70, %60 : vector<4xf16>
    %73 = vector.extract_strided_slice %21 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %74 = vector.bitcast %73 : vector<2xf32> to vector<4xf16>
    %75 = vector.extract_strided_slice %19 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %76 = vector.bitcast %75 : vector<2xf32> to vector<4xf16>
    %77 = arith.mulf %74, %76 : vector<4xf16>
    %78 = vector.extract_strided_slice %21 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %79 = vector.bitcast %78 : vector<2xf32> to vector<4xf16>
    %80 = vector.extract_strided_slice %19 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %81 = vector.bitcast %80 : vector<2xf32> to vector<4xf16>
    %82 = arith.mulf %79, %81 : vector<4xf16>
    %83 = arith.addf %77, %71 : vector<4xf16>
    %84 = arith.addf %82, %72 : vector<4xf16>
    %85 = vector.extract_strided_slice %25 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %86 = vector.bitcast %85 : vector<2xf32> to vector<4xf16>
    %87 = vector.extract_strided_slice %23 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %88 = vector.bitcast %87 : vector<2xf32> to vector<4xf16>
    %89 = arith.mulf %86, %88 : vector<4xf16>
    %90 = vector.extract_strided_slice %25 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %91 = vector.bitcast %90 : vector<2xf32> to vector<4xf16>
    %92 = vector.extract_strided_slice %23 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %93 = vector.bitcast %92 : vector<2xf32> to vector<4xf16>
    %94 = arith.mulf %91, %93 : vector<4xf16>
    %95 = arith.addf %89, %83 : vector<4xf16>
    %96 = arith.addf %94, %84 : vector<4xf16>
    %97 = vector.extract_strided_slice %29 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %98 = vector.bitcast %97 : vector<2xf32> to vector<4xf16>
    %99 = vector.extract_strided_slice %27 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %100 = vector.bitcast %99 : vector<2xf32> to vector<4xf16>
    %101 = arith.mulf %98, %100 : vector<4xf16>
    %102 = vector.extract_strided_slice %29 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %103 = vector.bitcast %102 : vector<2xf32> to vector<4xf16>
    %104 = vector.extract_strided_slice %27 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %105 = vector.bitcast %104 : vector<2xf32> to vector<4xf16>
    %106 = arith.mulf %103, %105 : vector<4xf16>
    %107 = arith.addf %101, %95 : vector<4xf16>
    %108 = arith.addf %106, %96 : vector<4xf16>
    %109 = vector.extract_strided_slice %33 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %110 = vector.bitcast %109 : vector<2xf32> to vector<4xf16>
    %111 = vector.extract_strided_slice %31 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %112 = vector.bitcast %111 : vector<2xf32> to vector<4xf16>
    %113 = arith.mulf %110, %112 : vector<4xf16>
    %114 = vector.extract_strided_slice %33 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %115 = vector.bitcast %114 : vector<2xf32> to vector<4xf16>
    %116 = vector.extract_strided_slice %31 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %117 = vector.bitcast %116 : vector<2xf32> to vector<4xf16>
    %118 = arith.mulf %115, %117 : vector<4xf16>
    %119 = arith.addf %113, %107 : vector<4xf16>
    %120 = arith.addf %118, %108 : vector<4xf16>
    %121 = vector.extract_strided_slice %36 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %122 = vector.bitcast %121 : vector<2xf32> to vector<4xf16>
    %123 = vector.extract_strided_slice %35 {offsets = [0], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %124 = vector.bitcast %123 : vector<2xf32> to vector<4xf16>
    %125 = arith.mulf %122, %124 : vector<4xf16>
    %126 = vector.extract_strided_slice %36 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %127 = vector.bitcast %126 : vector<2xf32> to vector<4xf16>
    %128 = vector.extract_strided_slice %35 {offsets = [2], sizes = [2], strides = [1]} : vector<4xf32> to vector<2xf32>
    %129 = vector.bitcast %128 : vector<2xf32> to vector<4xf16>
    %130 = arith.mulf %127, %129 : vector<4xf16>
    %131 = arith.addf %125, %119 : vector<4xf16>
    %132 = arith.addf %130, %120 : vector<4xf16>
    %133 = vector.reduction <add>, %131 : vector<4xf16> into f16
    %134 = vector.reduction <add>, %132 : vector<4xf16> into f16
    %135 = arith.addf %133, %134 : f16
    %136 = gpu.subgroup_reduce  add %135 : (f16) -> f16
    %137 = arith.addf %136, %cst_0 : f16
    %138 = arith.cmpi eq, %0, %c0 : index
    scf.if %138 {
      memref.store %137, %3[%workgroup_id_x] : memref<?xf16, #spirv.storage_class<StorageBuffer>>
    }
    return
  }
}

// -----// IR Dump After ConvertToSPIRV (iree-convert-to-spirv) //----- //
module attributes {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>} {
  spirv.module Logical GLSL450 {
    spirv.GlobalVariable @__builtin__WorkgroupId__ built_in("WorkgroupId") : !spirv.ptr<vector<3xi32>, Input>
    spirv.GlobalVariable @__builtin__LocalInvocationId__ built_in("LocalInvocationId") : !spirv.ptr<vector<3xi32>, Input>
    spirv.GlobalVariable @__resource_var_0_0_ bind(0, 0) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
    spirv.GlobalVariable @__resource_var_0_1_ bind(0, 1) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
    spirv.GlobalVariable @__resource_var_0_2_ bind(0, 2) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
    spirv.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() "None" attributes {spirv.entry_point_abi = #spirv.entry_point_abi<workgroup_size = [64, 1, 1]>} {
      %cst64_i32 = spirv.Constant 64 : i32
      %cst128_i32 = spirv.Constant 128 : i32
      %cst192_i32 = spirv.Constant 192 : i32
      %cst256_i32 = spirv.Constant 256 : i32
      %cst320_i32 = spirv.Constant 320 : i32
      %cst384_i32 = spirv.Constant 384 : i32
      %cst448_i32 = spirv.Constant 448 : i32
      %cst32000_i32 = spirv.Constant 32000 : i32
      %cst16384000_i32 = spirv.Constant 16384000 : i32
      %cst512_i32 = spirv.Constant 512 : i32
      %cst_vec_4xf16 = spirv.Constant dense<0.000000e+00> : vector<4xf16>
      %cst0_i32 = spirv.Constant 0 : i32
      %cst_f16 = spirv.Constant 0.000000e+00 : f16
      %__builtin__LocalInvocationId___addr = spirv.mlir.addressof @__builtin__LocalInvocationId__ : !spirv.ptr<vector<3xi32>, Input>
      %0 = spirv.Load "Input" %__builtin__LocalInvocationId___addr : vector<3xi32>
      %1 = spirv.CompositeExtract %0[0 : i32] : vector<3xi32>
      %__resource_var_0_0__addr = spirv.mlir.addressof @__resource_var_0_0_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
      %__resource_var_0_1__addr = spirv.mlir.addressof @__resource_var_0_1_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
      %__resource_var_0_2__addr = spirv.mlir.addressof @__resource_var_0_2_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
      %__builtin__WorkgroupId___addr = spirv.mlir.addressof @__builtin__WorkgroupId__ : !spirv.ptr<vector<3xi32>, Input>
      %2 = spirv.Load "Input" %__builtin__WorkgroupId___addr : vector<3xi32>
      %3 = spirv.CompositeExtract %2[0 : i32] : vector<3xi32>
      %4 = spirv.IMul %3, %cst512_i32 : i32
      %5 = spirv.IAdd %1, %4 : i32
      %6 = spirv.IAdd %5, %cst448_i32 : i32
      %cst0_i32_0 = spirv.Constant 0 : i32
      %cst0_i32_1 = spirv.Constant 0 : i32
      %cst1_i32 = spirv.Constant 1 : i32
      %7 = spirv.IMul %cst1_i32, %6 : i32
      %8 = spirv.IAdd %cst0_i32_1, %7 : i32
      %9 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32_0, %8] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
      %10 = spirv.Load "StorageBuffer" %9 : vector<4xf32>
      %11 = spirv.IAdd %1, %cst448_i32 : i32
      %cst0_i32_2 = spirv.Constant 0 : i32
      %cst0_i32_3 = spirv.Constant 0 : i32
      %cst1_i32_4 = spirv.Constant 1 : i32
      %12 = spirv.IMul %cst1_i32_4, %11 : i32
      %13 = spirv.IAdd %cst0_i32_3, %12 : i32
      %14 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32_2, %13] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
      %15 = spirv.Load "StorageBuffer" %14 : vector<4xf32>
      %16 = spirv.IAdd %5, %cst384_i32 : i32
      %cst0_i32_5 = spirv.Constant 0 : i32
      %cst0_i32_6 = spirv.Constant 0 : i32
      %cst1_i32_7 = spirv.Constant 1 : i32
      %17 = spirv.IMul %cst1_i32_7, %16 : i32
      %18 = spirv.IAdd %cst0_i32_6, %17 : i32
      %19 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32_5, %18] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
      %20 = spirv.Load "StorageBuffer" %19 : vector<4xf32>
      %21 = spirv.IAdd %1, %cst384_i32 : i32
      %cst0_i32_8 = spirv.Constant 0 : i32
      %cst0_i32_9 = spirv.Constant 0 : i32
      %cst1_i32_10 = spirv.Constant 1 : i32
      %22 = spirv.IMul %cst1_i32_10, %21 : i32
      %23 = spirv.IAdd %cst0_i32_9, %22 : i32
      %24 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32_8, %23] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
      %25 = spirv.Load "StorageBuffer" %24 : vector<4xf32>
      %26 = spirv.IAdd %5, %cst320_i32 : i32
      %cst0_i32_11 = spirv.Constant 0 : i32
      %cst0_i32_12 = spirv.Constant 0 : i32
      %cst1_i32_13 = spirv.Constant 1 : i32
      %27 = spirv.IMul %cst1_i32_13, %26 : i32
      %28 = spirv.IAdd %cst0_i32_12, %27 : i32
      %29 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32_11, %28] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
      %30 = spirv.Load "StorageBuffer" %29 : vector<4xf32>
      %31 = spirv.IAdd %1, %cst320_i32 : i32
      %cst0_i32_14 = spirv.Constant 0 : i32
      %cst0_i32_15 = spirv.Constant 0 : i32
      %cst1_i32_16 = spirv.Constant 1 : i32
      %32 = spirv.IMul %cst1_i32_16, %31 : i32
      %33 = spirv.IAdd %cst0_i32_15, %32 : i32
      %34 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32_14, %33] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
      %35 = spirv.Load "StorageBuffer" %34 : vector<4xf32>
      %36 = spirv.IAdd %5, %cst256_i32 : i32
      %cst0_i32_17 = spirv.Constant 0 : i32
      %cst0_i32_18 = spirv.Constant 0 : i32
      %cst1_i32_19 = spirv.Constant 1 : i32
      %37 = spirv.IMul %cst1_i32_19, %36 : i32
      %38 = spirv.IAdd %cst0_i32_18, %37 : i32
      %39 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32_17, %38] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
      %40 = spirv.Load "StorageBuffer" %39 : vector<4xf32>
      %41 = spirv.IAdd %1, %cst256_i32 : i32
      %cst0_i32_20 = spirv.Constant 0 : i32
      %cst0_i32_21 = spirv.Constant 0 : i32
      %cst1_i32_22 = spirv.Constant 1 : i32
      %42 = spirv.IMul %cst1_i32_22, %41 : i32
      %43 = spirv.IAdd %cst0_i32_21, %42 : i32
      %44 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32_20, %43] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
      %45 = spirv.Load "StorageBuffer" %44 : vector<4xf32>
      %46 = spirv.IAdd %5, %cst192_i32 : i32
      %cst0_i32_23 = spirv.Constant 0 : i32
      %cst0_i32_24 = spirv.Constant 0 : i32
      %cst1_i32_25 = spirv.Constant 1 : i32
      %47 = spirv.IMul %cst1_i32_25, %46 : i32
      %48 = spirv.IAdd %cst0_i32_24, %47 : i32
      %49 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32_23, %48] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
      %50 = spirv.Load "StorageBuffer" %49 : vector<4xf32>
      %51 = spirv.IAdd %1, %cst192_i32 : i32
      %cst0_i32_26 = spirv.Constant 0 : i32
      %cst0_i32_27 = spirv.Constant 0 : i32
      %cst1_i32_28 = spirv.Constant 1 : i32
      %52 = spirv.IMul %cst1_i32_28, %51 : i32
      %53 = spirv.IAdd %cst0_i32_27, %52 : i32
      %54 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32_26, %53] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
      %55 = spirv.Load "StorageBuffer" %54 : vector<4xf32>
      %56 = spirv.IAdd %5, %cst128_i32 : i32
      %cst0_i32_29 = spirv.Constant 0 : i32
      %cst0_i32_30 = spirv.Constant 0 : i32
      %cst1_i32_31 = spirv.Constant 1 : i32
      %57 = spirv.IMul %cst1_i32_31, %56 : i32
      %58 = spirv.IAdd %cst0_i32_30, %57 : i32
      %59 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32_29, %58] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
      %60 = spirv.Load "StorageBuffer" %59 : vector<4xf32>
      %61 = spirv.IAdd %1, %cst128_i32 : i32
      %cst0_i32_32 = spirv.Constant 0 : i32
      %cst0_i32_33 = spirv.Constant 0 : i32
      %cst1_i32_34 = spirv.Constant 1 : i32
      %62 = spirv.IMul %cst1_i32_34, %61 : i32
      %63 = spirv.IAdd %cst0_i32_33, %62 : i32
      %64 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32_32, %63] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
      %65 = spirv.Load "StorageBuffer" %64 : vector<4xf32>
      %66 = spirv.IAdd %5, %cst64_i32 : i32
      %cst0_i32_35 = spirv.Constant 0 : i32
      %cst0_i32_36 = spirv.Constant 0 : i32
      %cst1_i32_37 = spirv.Constant 1 : i32
      %67 = spirv.IMul %cst1_i32_37, %66 : i32
      %68 = spirv.IAdd %cst0_i32_36, %67 : i32
      %69 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32_35, %68] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
      %70 = spirv.Load "StorageBuffer" %69 : vector<4xf32>
      %71 = spirv.IAdd %1, %cst64_i32 : i32
      %cst0_i32_38 = spirv.Constant 0 : i32
      %cst0_i32_39 = spirv.Constant 0 : i32
      %cst1_i32_40 = spirv.Constant 1 : i32
      %72 = spirv.IMul %cst1_i32_40, %71 : i32
      %73 = spirv.IAdd %cst0_i32_39, %72 : i32
      %74 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32_38, %73] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
      %75 = spirv.Load "StorageBuffer" %74 : vector<4xf32>
      %76 = spirv.IAdd %4, %1 : i32
      %cst0_i32_41 = spirv.Constant 0 : i32
      %cst0_i32_42 = spirv.Constant 0 : i32
      %cst1_i32_43 = spirv.Constant 1 : i32
      %77 = spirv.IMul %cst1_i32_43, %76 : i32
      %78 = spirv.IAdd %cst0_i32_42, %77 : i32
      %79 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32_41, %78] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
      %80 = spirv.Load "StorageBuffer" %79 : vector<4xf32>
      %cst0_i32_44 = spirv.Constant 0 : i32
      %cst0_i32_45 = spirv.Constant 0 : i32
      %cst1_i32_46 = spirv.Constant 1 : i32
      %81 = spirv.IMul %cst1_i32_46, %1 : i32
      %82 = spirv.IAdd %cst0_i32_45, %81 : i32
      %83 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32_44, %82] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
      %84 = spirv.Load "StorageBuffer" %83 : vector<4xf32>
      %85 = spirv.VectorShuffle [0 : i32, 1 : i32] %15 : vector<4xf32>, %15 : vector<4xf32> -> vector<2xf32>
      %86 = spirv.Bitcast %85 : vector<2xf32> to vector<4xf16>
      %87 = spirv.VectorShuffle [0 : i32, 1 : i32] %10 : vector<4xf32>, %10 : vector<4xf32> -> vector<2xf32>
      %88 = spirv.Bitcast %87 : vector<2xf32> to vector<4xf16>
      %89 = spirv.FMul %86, %88 : vector<4xf16>
      %90 = spirv.VectorShuffle [2 : i32, 3 : i32] %15 : vector<4xf32>, %15 : vector<4xf32> -> vector<2xf32>
      %91 = spirv.Bitcast %90 : vector<2xf32> to vector<4xf16>
      %92 = spirv.VectorShuffle [2 : i32, 3 : i32] %10 : vector<4xf32>, %10 : vector<4xf32> -> vector<2xf32>
      %93 = spirv.Bitcast %92 : vector<2xf32> to vector<4xf16>
      %94 = spirv.FMul %91, %93 : vector<4xf16>
      %95 = spirv.FAdd %89, %cst_vec_4xf16 : vector<4xf16>
      %96 = spirv.FAdd %94, %cst_vec_4xf16 : vector<4xf16>
      %97 = spirv.VectorShuffle [0 : i32, 1 : i32] %25 : vector<4xf32>, %25 : vector<4xf32> -> vector<2xf32>
      %98 = spirv.Bitcast %97 : vector<2xf32> to vector<4xf16>
      %99 = spirv.VectorShuffle [0 : i32, 1 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
      %100 = spirv.Bitcast %99 : vector<2xf32> to vector<4xf16>
      %101 = spirv.FMul %98, %100 : vector<4xf16>
      %102 = spirv.VectorShuffle [2 : i32, 3 : i32] %25 : vector<4xf32>, %25 : vector<4xf32> -> vector<2xf32>
      %103 = spirv.Bitcast %102 : vector<2xf32> to vector<4xf16>
      %104 = spirv.VectorShuffle [2 : i32, 3 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
      %105 = spirv.Bitcast %104 : vector<2xf32> to vector<4xf16>
      %106 = spirv.FMul %103, %105 : vector<4xf16>
      %107 = spirv.FAdd %101, %95 : vector<4xf16>
      %108 = spirv.FAdd %106, %96 : vector<4xf16>
      %109 = spirv.VectorShuffle [0 : i32, 1 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
      %110 = spirv.Bitcast %109 : vector<2xf32> to vector<4xf16>
      %111 = spirv.VectorShuffle [0 : i32, 1 : i32] %30 : vector<4xf32>, %30 : vector<4xf32> -> vector<2xf32>
      %112 = spirv.Bitcast %111 : vector<2xf32> to vector<4xf16>
      %113 = spirv.FMul %110, %112 : vector<4xf16>
      %114 = spirv.VectorShuffle [2 : i32, 3 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
      %115 = spirv.Bitcast %114 : vector<2xf32> to vector<4xf16>
      %116 = spirv.VectorShuffle [2 : i32, 3 : i32] %30 : vector<4xf32>, %30 : vector<4xf32> -> vector<2xf32>
      %117 = spirv.Bitcast %116 : vector<2xf32> to vector<4xf16>
      %118 = spirv.FMul %115, %117 : vector<4xf16>
      %119 = spirv.FAdd %113, %107 : vector<4xf16>
      %120 = spirv.FAdd %118, %108 : vector<4xf16>
      %121 = spirv.VectorShuffle [0 : i32, 1 : i32] %45 : vector<4xf32>, %45 : vector<4xf32> -> vector<2xf32>
      %122 = spirv.Bitcast %121 : vector<2xf32> to vector<4xf16>
      %123 = spirv.VectorShuffle [0 : i32, 1 : i32] %40 : vector<4xf32>, %40 : vector<4xf32> -> vector<2xf32>
      %124 = spirv.Bitcast %123 : vector<2xf32> to vector<4xf16>
      %125 = spirv.FMul %122, %124 : vector<4xf16>
      %126 = spirv.VectorShuffle [2 : i32, 3 : i32] %45 : vector<4xf32>, %45 : vector<4xf32> -> vector<2xf32>
      %127 = spirv.Bitcast %126 : vector<2xf32> to vector<4xf16>
      %128 = spirv.VectorShuffle [2 : i32, 3 : i32] %40 : vector<4xf32>, %40 : vector<4xf32> -> vector<2xf32>
      %129 = spirv.Bitcast %128 : vector<2xf32> to vector<4xf16>
      %130 = spirv.FMul %127, %129 : vector<4xf16>
      %131 = spirv.FAdd %125, %119 : vector<4xf16>
      %132 = spirv.FAdd %130, %120 : vector<4xf16>
      %133 = spirv.VectorShuffle [0 : i32, 1 : i32] %55 : vector<4xf32>, %55 : vector<4xf32> -> vector<2xf32>
      %134 = spirv.Bitcast %133 : vector<2xf32> to vector<4xf16>
      %135 = spirv.VectorShuffle [0 : i32, 1 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
      %136 = spirv.Bitcast %135 : vector<2xf32> to vector<4xf16>
      %137 = spirv.FMul %134, %136 : vector<4xf16>
      %138 = spirv.VectorShuffle [2 : i32, 3 : i32] %55 : vector<4xf32>, %55 : vector<4xf32> -> vector<2xf32>
      %139 = spirv.Bitcast %138 : vector<2xf32> to vector<4xf16>
      %140 = spirv.VectorShuffle [2 : i32, 3 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
      %141 = spirv.Bitcast %140 : vector<2xf32> to vector<4xf16>
      %142 = spirv.FMul %139, %141 : vector<4xf16>
      %143 = spirv.FAdd %137, %131 : vector<4xf16>
      %144 = spirv.FAdd %142, %132 : vector<4xf16>
      %145 = spirv.VectorShuffle [0 : i32, 1 : i32] %65 : vector<4xf32>, %65 : vector<4xf32> -> vector<2xf32>
      %146 = spirv.Bitcast %145 : vector<2xf32> to vector<4xf16>
      %147 = spirv.VectorShuffle [0 : i32, 1 : i32] %60 : vector<4xf32>, %60 : vector<4xf32> -> vector<2xf32>
      %148 = spirv.Bitcast %147 : vector<2xf32> to vector<4xf16>
      %149 = spirv.FMul %146, %148 : vector<4xf16>
      %150 = spirv.VectorShuffle [2 : i32, 3 : i32] %65 : vector<4xf32>, %65 : vector<4xf32> -> vector<2xf32>
      %151 = spirv.Bitcast %150 : vector<2xf32> to vector<4xf16>
      %152 = spirv.VectorShuffle [2 : i32, 3 : i32] %60 : vector<4xf32>, %60 : vector<4xf32> -> vector<2xf32>
      %153 = spirv.Bitcast %152 : vector<2xf32> to vector<4xf16>
      %154 = spirv.FMul %151, %153 : vector<4xf16>
      %155 = spirv.FAdd %149, %143 : vector<4xf16>
      %156 = spirv.FAdd %154, %144 : vector<4xf16>
      %157 = spirv.VectorShuffle [0 : i32, 1 : i32] %75 : vector<4xf32>, %75 : vector<4xf32> -> vector<2xf32>
      %158 = spirv.Bitcast %157 : vector<2xf32> to vector<4xf16>
      %159 = spirv.VectorShuffle [0 : i32, 1 : i32] %70 : vector<4xf32>, %70 : vector<4xf32> -> vector<2xf32>
      %160 = spirv.Bitcast %159 : vector<2xf32> to vector<4xf16>
      %161 = spirv.FMul %158, %160 : vector<4xf16>
      %162 = spirv.VectorShuffle [2 : i32, 3 : i32] %75 : vector<4xf32>, %75 : vector<4xf32> -> vector<2xf32>
      %163 = spirv.Bitcast %162 : vector<2xf32> to vector<4xf16>
      %164 = spirv.VectorShuffle [2 : i32, 3 : i32] %70 : vector<4xf32>, %70 : vector<4xf32> -> vector<2xf32>
      %165 = spirv.Bitcast %164 : vector<2xf32> to vector<4xf16>
      %166 = spirv.FMul %163, %165 : vector<4xf16>
      %167 = spirv.FAdd %161, %155 : vector<4xf16>
      %168 = spirv.FAdd %166, %156 : vector<4xf16>
      %169 = spirv.VectorShuffle [0 : i32, 1 : i32] %84 : vector<4xf32>, %84 : vector<4xf32> -> vector<2xf32>
      %170 = spirv.Bitcast %169 : vector<2xf32> to vector<4xf16>
      %171 = spirv.VectorShuffle [0 : i32, 1 : i32] %80 : vector<4xf32>, %80 : vector<4xf32> -> vector<2xf32>
      %172 = spirv.Bitcast %171 : vector<2xf32> to vector<4xf16>
      %173 = spirv.FMul %170, %172 : vector<4xf16>
      %174 = spirv.VectorShuffle [2 : i32, 3 : i32] %84 : vector<4xf32>, %84 : vector<4xf32> -> vector<2xf32>
      %175 = spirv.Bitcast %174 : vector<2xf32> to vector<4xf16>
      %176 = spirv.VectorShuffle [2 : i32, 3 : i32] %80 : vector<4xf32>, %80 : vector<4xf32> -> vector<2xf32>
      %177 = spirv.Bitcast %176 : vector<2xf32> to vector<4xf16>
      %178 = spirv.FMul %175, %177 : vector<4xf16>
      %179 = spirv.FAdd %173, %167 : vector<4xf16>
      %180 = spirv.FAdd %178, %168 : vector<4xf16>
      %181 = spirv.CompositeExtract %179[0 : i32] : vector<4xf16>
      %182 = spirv.CompositeExtract %179[1 : i32] : vector<4xf16>
      %183 = spirv.CompositeExtract %179[2 : i32] : vector<4xf16>
      %184 = spirv.CompositeExtract %179[3 : i32] : vector<4xf16>
      %185 = spirv.FAdd %181, %182 : f16
      %186 = spirv.FAdd %185, %183 : f16
      %187 = spirv.FAdd %186, %184 : f16
      %188 = spirv.CompositeExtract %180[0 : i32] : vector<4xf16>
      %189 = spirv.CompositeExtract %180[1 : i32] : vector<4xf16>
      %190 = spirv.CompositeExtract %180[2 : i32] : vector<4xf16>
      %191 = spirv.CompositeExtract %180[3 : i32] : vector<4xf16>
      %192 = spirv.FAdd %188, %189 : f16
      %193 = spirv.FAdd %192, %190 : f16
      %194 = spirv.FAdd %193, %191 : f16
      %195 = spirv.FAdd %187, %194 : f16
      %196 = spirv.GroupNonUniformFAdd "Subgroup" "Reduce" %195 : f16
      %197 = spirv.FAdd %196, %cst_f16 : f16
      %198 = spirv.IEqual %1, %cst0_i32 : i32
      spirv.mlir.selection {
        spirv.BranchConditional %198, ^bb1, ^bb2
      ^bb1:  // pred: ^bb0
        %cst0_i32_47 = spirv.Constant 0 : i32
        %cst0_i32_48 = spirv.Constant 0 : i32
        %cst1_i32_49 = spirv.Constant 1 : i32
        %199 = spirv.IMul %cst1_i32_49, %3 : i32
        %200 = spirv.IAdd %cst0_i32_48, %199 : i32
        %201 = spirv.AccessChain %__resource_var_0_2__addr[%cst0_i32_47, %200] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>, i32, i32
        spirv.Store "StorageBuffer" %201, %197 : f16
        spirv.Branch ^bb2
      ^bb2:  // 2 preds: ^bb0, ^bb1
        spirv.mlir.merge
      }
      spirv.Return
    }
  }
}

// -----// IR Dump After SPIRVUnifyAliasedResourcePass (spirv-unify-aliased-resource) //----- //
spirv.module Logical GLSL450 {
  spirv.GlobalVariable @__builtin__WorkgroupId__ built_in("WorkgroupId") : !spirv.ptr<vector<3xi32>, Input>
  spirv.GlobalVariable @__builtin__LocalInvocationId__ built_in("LocalInvocationId") : !spirv.ptr<vector<3xi32>, Input>
  spirv.GlobalVariable @__resource_var_0_0_ bind(0, 0) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
  spirv.GlobalVariable @__resource_var_0_1_ bind(0, 1) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
  spirv.GlobalVariable @__resource_var_0_2_ bind(0, 2) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
  spirv.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() "None" attributes {spirv.entry_point_abi = #spirv.entry_point_abi<workgroup_size = [64, 1, 1]>} {
    %cst64_i32 = spirv.Constant 64 : i32
    %cst128_i32 = spirv.Constant 128 : i32
    %cst192_i32 = spirv.Constant 192 : i32
    %cst256_i32 = spirv.Constant 256 : i32
    %cst320_i32 = spirv.Constant 320 : i32
    %cst384_i32 = spirv.Constant 384 : i32
    %cst448_i32 = spirv.Constant 448 : i32
    %cst32000_i32 = spirv.Constant 32000 : i32
    %cst16384000_i32 = spirv.Constant 16384000 : i32
    %cst512_i32 = spirv.Constant 512 : i32
    %cst_vec_4xf16 = spirv.Constant dense<0.000000e+00> : vector<4xf16>
    %cst0_i32 = spirv.Constant 0 : i32
    %cst_f16 = spirv.Constant 0.000000e+00 : f16
    %__builtin__LocalInvocationId___addr = spirv.mlir.addressof @__builtin__LocalInvocationId__ : !spirv.ptr<vector<3xi32>, Input>
    %0 = spirv.Load "Input" %__builtin__LocalInvocationId___addr : vector<3xi32>
    %1 = spirv.CompositeExtract %0[0 : i32] : vector<3xi32>
    %__resource_var_0_0__addr = spirv.mlir.addressof @__resource_var_0_0_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
    %__resource_var_0_1__addr = spirv.mlir.addressof @__resource_var_0_1_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
    %__resource_var_0_2__addr = spirv.mlir.addressof @__resource_var_0_2_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
    %__builtin__WorkgroupId___addr = spirv.mlir.addressof @__builtin__WorkgroupId__ : !spirv.ptr<vector<3xi32>, Input>
    %2 = spirv.Load "Input" %__builtin__WorkgroupId___addr : vector<3xi32>
    %3 = spirv.CompositeExtract %2[0 : i32] : vector<3xi32>
    %4 = spirv.IMul %3, %cst512_i32 : i32
    %5 = spirv.IAdd %1, %4 : i32
    %6 = spirv.IAdd %5, %cst448_i32 : i32
    %cst0_i32_0 = spirv.Constant 0 : i32
    %cst0_i32_1 = spirv.Constant 0 : i32
    %cst1_i32 = spirv.Constant 1 : i32
    %7 = spirv.IMul %cst1_i32, %6 : i32
    %8 = spirv.IAdd %cst0_i32_1, %7 : i32
    %9 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32_0, %8] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %10 = spirv.Load "StorageBuffer" %9 : vector<4xf32>
    %11 = spirv.IAdd %1, %cst448_i32 : i32
    %cst0_i32_2 = spirv.Constant 0 : i32
    %cst0_i32_3 = spirv.Constant 0 : i32
    %cst1_i32_4 = spirv.Constant 1 : i32
    %12 = spirv.IMul %cst1_i32_4, %11 : i32
    %13 = spirv.IAdd %cst0_i32_3, %12 : i32
    %14 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32_2, %13] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %15 = spirv.Load "StorageBuffer" %14 : vector<4xf32>
    %16 = spirv.IAdd %5, %cst384_i32 : i32
    %cst0_i32_5 = spirv.Constant 0 : i32
    %cst0_i32_6 = spirv.Constant 0 : i32
    %cst1_i32_7 = spirv.Constant 1 : i32
    %17 = spirv.IMul %cst1_i32_7, %16 : i32
    %18 = spirv.IAdd %cst0_i32_6, %17 : i32
    %19 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32_5, %18] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %20 = spirv.Load "StorageBuffer" %19 : vector<4xf32>
    %21 = spirv.IAdd %1, %cst384_i32 : i32
    %cst0_i32_8 = spirv.Constant 0 : i32
    %cst0_i32_9 = spirv.Constant 0 : i32
    %cst1_i32_10 = spirv.Constant 1 : i32
    %22 = spirv.IMul %cst1_i32_10, %21 : i32
    %23 = spirv.IAdd %cst0_i32_9, %22 : i32
    %24 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32_8, %23] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %25 = spirv.Load "StorageBuffer" %24 : vector<4xf32>
    %26 = spirv.IAdd %5, %cst320_i32 : i32
    %cst0_i32_11 = spirv.Constant 0 : i32
    %cst0_i32_12 = spirv.Constant 0 : i32
    %cst1_i32_13 = spirv.Constant 1 : i32
    %27 = spirv.IMul %cst1_i32_13, %26 : i32
    %28 = spirv.IAdd %cst0_i32_12, %27 : i32
    %29 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32_11, %28] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %30 = spirv.Load "StorageBuffer" %29 : vector<4xf32>
    %31 = spirv.IAdd %1, %cst320_i32 : i32
    %cst0_i32_14 = spirv.Constant 0 : i32
    %cst0_i32_15 = spirv.Constant 0 : i32
    %cst1_i32_16 = spirv.Constant 1 : i32
    %32 = spirv.IMul %cst1_i32_16, %31 : i32
    %33 = spirv.IAdd %cst0_i32_15, %32 : i32
    %34 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32_14, %33] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %35 = spirv.Load "StorageBuffer" %34 : vector<4xf32>
    %36 = spirv.IAdd %5, %cst256_i32 : i32
    %cst0_i32_17 = spirv.Constant 0 : i32
    %cst0_i32_18 = spirv.Constant 0 : i32
    %cst1_i32_19 = spirv.Constant 1 : i32
    %37 = spirv.IMul %cst1_i32_19, %36 : i32
    %38 = spirv.IAdd %cst0_i32_18, %37 : i32
    %39 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32_17, %38] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %40 = spirv.Load "StorageBuffer" %39 : vector<4xf32>
    %41 = spirv.IAdd %1, %cst256_i32 : i32
    %cst0_i32_20 = spirv.Constant 0 : i32
    %cst0_i32_21 = spirv.Constant 0 : i32
    %cst1_i32_22 = spirv.Constant 1 : i32
    %42 = spirv.IMul %cst1_i32_22, %41 : i32
    %43 = spirv.IAdd %cst0_i32_21, %42 : i32
    %44 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32_20, %43] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %45 = spirv.Load "StorageBuffer" %44 : vector<4xf32>
    %46 = spirv.IAdd %5, %cst192_i32 : i32
    %cst0_i32_23 = spirv.Constant 0 : i32
    %cst0_i32_24 = spirv.Constant 0 : i32
    %cst1_i32_25 = spirv.Constant 1 : i32
    %47 = spirv.IMul %cst1_i32_25, %46 : i32
    %48 = spirv.IAdd %cst0_i32_24, %47 : i32
    %49 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32_23, %48] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %50 = spirv.Load "StorageBuffer" %49 : vector<4xf32>
    %51 = spirv.IAdd %1, %cst192_i32 : i32
    %cst0_i32_26 = spirv.Constant 0 : i32
    %cst0_i32_27 = spirv.Constant 0 : i32
    %cst1_i32_28 = spirv.Constant 1 : i32
    %52 = spirv.IMul %cst1_i32_28, %51 : i32
    %53 = spirv.IAdd %cst0_i32_27, %52 : i32
    %54 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32_26, %53] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %55 = spirv.Load "StorageBuffer" %54 : vector<4xf32>
    %56 = spirv.IAdd %5, %cst128_i32 : i32
    %cst0_i32_29 = spirv.Constant 0 : i32
    %cst0_i32_30 = spirv.Constant 0 : i32
    %cst1_i32_31 = spirv.Constant 1 : i32
    %57 = spirv.IMul %cst1_i32_31, %56 : i32
    %58 = spirv.IAdd %cst0_i32_30, %57 : i32
    %59 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32_29, %58] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %60 = spirv.Load "StorageBuffer" %59 : vector<4xf32>
    %61 = spirv.IAdd %1, %cst128_i32 : i32
    %cst0_i32_32 = spirv.Constant 0 : i32
    %cst0_i32_33 = spirv.Constant 0 : i32
    %cst1_i32_34 = spirv.Constant 1 : i32
    %62 = spirv.IMul %cst1_i32_34, %61 : i32
    %63 = spirv.IAdd %cst0_i32_33, %62 : i32
    %64 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32_32, %63] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %65 = spirv.Load "StorageBuffer" %64 : vector<4xf32>
    %66 = spirv.IAdd %5, %cst64_i32 : i32
    %cst0_i32_35 = spirv.Constant 0 : i32
    %cst0_i32_36 = spirv.Constant 0 : i32
    %cst1_i32_37 = spirv.Constant 1 : i32
    %67 = spirv.IMul %cst1_i32_37, %66 : i32
    %68 = spirv.IAdd %cst0_i32_36, %67 : i32
    %69 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32_35, %68] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %70 = spirv.Load "StorageBuffer" %69 : vector<4xf32>
    %71 = spirv.IAdd %1, %cst64_i32 : i32
    %cst0_i32_38 = spirv.Constant 0 : i32
    %cst0_i32_39 = spirv.Constant 0 : i32
    %cst1_i32_40 = spirv.Constant 1 : i32
    %72 = spirv.IMul %cst1_i32_40, %71 : i32
    %73 = spirv.IAdd %cst0_i32_39, %72 : i32
    %74 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32_38, %73] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %75 = spirv.Load "StorageBuffer" %74 : vector<4xf32>
    %76 = spirv.IAdd %4, %1 : i32
    %cst0_i32_41 = spirv.Constant 0 : i32
    %cst0_i32_42 = spirv.Constant 0 : i32
    %cst1_i32_43 = spirv.Constant 1 : i32
    %77 = spirv.IMul %cst1_i32_43, %76 : i32
    %78 = spirv.IAdd %cst0_i32_42, %77 : i32
    %79 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32_41, %78] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %80 = spirv.Load "StorageBuffer" %79 : vector<4xf32>
    %cst0_i32_44 = spirv.Constant 0 : i32
    %cst0_i32_45 = spirv.Constant 0 : i32
    %cst1_i32_46 = spirv.Constant 1 : i32
    %81 = spirv.IMul %cst1_i32_46, %1 : i32
    %82 = spirv.IAdd %cst0_i32_45, %81 : i32
    %83 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32_44, %82] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %84 = spirv.Load "StorageBuffer" %83 : vector<4xf32>
    %85 = spirv.VectorShuffle [0 : i32, 1 : i32] %15 : vector<4xf32>, %15 : vector<4xf32> -> vector<2xf32>
    %86 = spirv.Bitcast %85 : vector<2xf32> to vector<4xf16>
    %87 = spirv.VectorShuffle [0 : i32, 1 : i32] %10 : vector<4xf32>, %10 : vector<4xf32> -> vector<2xf32>
    %88 = spirv.Bitcast %87 : vector<2xf32> to vector<4xf16>
    %89 = spirv.FMul %86, %88 : vector<4xf16>
    %90 = spirv.VectorShuffle [2 : i32, 3 : i32] %15 : vector<4xf32>, %15 : vector<4xf32> -> vector<2xf32>
    %91 = spirv.Bitcast %90 : vector<2xf32> to vector<4xf16>
    %92 = spirv.VectorShuffle [2 : i32, 3 : i32] %10 : vector<4xf32>, %10 : vector<4xf32> -> vector<2xf32>
    %93 = spirv.Bitcast %92 : vector<2xf32> to vector<4xf16>
    %94 = spirv.FMul %91, %93 : vector<4xf16>
    %95 = spirv.FAdd %89, %cst_vec_4xf16 : vector<4xf16>
    %96 = spirv.FAdd %94, %cst_vec_4xf16 : vector<4xf16>
    %97 = spirv.VectorShuffle [0 : i32, 1 : i32] %25 : vector<4xf32>, %25 : vector<4xf32> -> vector<2xf32>
    %98 = spirv.Bitcast %97 : vector<2xf32> to vector<4xf16>
    %99 = spirv.VectorShuffle [0 : i32, 1 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
    %100 = spirv.Bitcast %99 : vector<2xf32> to vector<4xf16>
    %101 = spirv.FMul %98, %100 : vector<4xf16>
    %102 = spirv.VectorShuffle [2 : i32, 3 : i32] %25 : vector<4xf32>, %25 : vector<4xf32> -> vector<2xf32>
    %103 = spirv.Bitcast %102 : vector<2xf32> to vector<4xf16>
    %104 = spirv.VectorShuffle [2 : i32, 3 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
    %105 = spirv.Bitcast %104 : vector<2xf32> to vector<4xf16>
    %106 = spirv.FMul %103, %105 : vector<4xf16>
    %107 = spirv.FAdd %101, %95 : vector<4xf16>
    %108 = spirv.FAdd %106, %96 : vector<4xf16>
    %109 = spirv.VectorShuffle [0 : i32, 1 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
    %110 = spirv.Bitcast %109 : vector<2xf32> to vector<4xf16>
    %111 = spirv.VectorShuffle [0 : i32, 1 : i32] %30 : vector<4xf32>, %30 : vector<4xf32> -> vector<2xf32>
    %112 = spirv.Bitcast %111 : vector<2xf32> to vector<4xf16>
    %113 = spirv.FMul %110, %112 : vector<4xf16>
    %114 = spirv.VectorShuffle [2 : i32, 3 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
    %115 = spirv.Bitcast %114 : vector<2xf32> to vector<4xf16>
    %116 = spirv.VectorShuffle [2 : i32, 3 : i32] %30 : vector<4xf32>, %30 : vector<4xf32> -> vector<2xf32>
    %117 = spirv.Bitcast %116 : vector<2xf32> to vector<4xf16>
    %118 = spirv.FMul %115, %117 : vector<4xf16>
    %119 = spirv.FAdd %113, %107 : vector<4xf16>
    %120 = spirv.FAdd %118, %108 : vector<4xf16>
    %121 = spirv.VectorShuffle [0 : i32, 1 : i32] %45 : vector<4xf32>, %45 : vector<4xf32> -> vector<2xf32>
    %122 = spirv.Bitcast %121 : vector<2xf32> to vector<4xf16>
    %123 = spirv.VectorShuffle [0 : i32, 1 : i32] %40 : vector<4xf32>, %40 : vector<4xf32> -> vector<2xf32>
    %124 = spirv.Bitcast %123 : vector<2xf32> to vector<4xf16>
    %125 = spirv.FMul %122, %124 : vector<4xf16>
    %126 = spirv.VectorShuffle [2 : i32, 3 : i32] %45 : vector<4xf32>, %45 : vector<4xf32> -> vector<2xf32>
    %127 = spirv.Bitcast %126 : vector<2xf32> to vector<4xf16>
    %128 = spirv.VectorShuffle [2 : i32, 3 : i32] %40 : vector<4xf32>, %40 : vector<4xf32> -> vector<2xf32>
    %129 = spirv.Bitcast %128 : vector<2xf32> to vector<4xf16>
    %130 = spirv.FMul %127, %129 : vector<4xf16>
    %131 = spirv.FAdd %125, %119 : vector<4xf16>
    %132 = spirv.FAdd %130, %120 : vector<4xf16>
    %133 = spirv.VectorShuffle [0 : i32, 1 : i32] %55 : vector<4xf32>, %55 : vector<4xf32> -> vector<2xf32>
    %134 = spirv.Bitcast %133 : vector<2xf32> to vector<4xf16>
    %135 = spirv.VectorShuffle [0 : i32, 1 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
    %136 = spirv.Bitcast %135 : vector<2xf32> to vector<4xf16>
    %137 = spirv.FMul %134, %136 : vector<4xf16>
    %138 = spirv.VectorShuffle [2 : i32, 3 : i32] %55 : vector<4xf32>, %55 : vector<4xf32> -> vector<2xf32>
    %139 = spirv.Bitcast %138 : vector<2xf32> to vector<4xf16>
    %140 = spirv.VectorShuffle [2 : i32, 3 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
    %141 = spirv.Bitcast %140 : vector<2xf32> to vector<4xf16>
    %142 = spirv.FMul %139, %141 : vector<4xf16>
    %143 = spirv.FAdd %137, %131 : vector<4xf16>
    %144 = spirv.FAdd %142, %132 : vector<4xf16>
    %145 = spirv.VectorShuffle [0 : i32, 1 : i32] %65 : vector<4xf32>, %65 : vector<4xf32> -> vector<2xf32>
    %146 = spirv.Bitcast %145 : vector<2xf32> to vector<4xf16>
    %147 = spirv.VectorShuffle [0 : i32, 1 : i32] %60 : vector<4xf32>, %60 : vector<4xf32> -> vector<2xf32>
    %148 = spirv.Bitcast %147 : vector<2xf32> to vector<4xf16>
    %149 = spirv.FMul %146, %148 : vector<4xf16>
    %150 = spirv.VectorShuffle [2 : i32, 3 : i32] %65 : vector<4xf32>, %65 : vector<4xf32> -> vector<2xf32>
    %151 = spirv.Bitcast %150 : vector<2xf32> to vector<4xf16>
    %152 = spirv.VectorShuffle [2 : i32, 3 : i32] %60 : vector<4xf32>, %60 : vector<4xf32> -> vector<2xf32>
    %153 = spirv.Bitcast %152 : vector<2xf32> to vector<4xf16>
    %154 = spirv.FMul %151, %153 : vector<4xf16>
    %155 = spirv.FAdd %149, %143 : vector<4xf16>
    %156 = spirv.FAdd %154, %144 : vector<4xf16>
    %157 = spirv.VectorShuffle [0 : i32, 1 : i32] %75 : vector<4xf32>, %75 : vector<4xf32> -> vector<2xf32>
    %158 = spirv.Bitcast %157 : vector<2xf32> to vector<4xf16>
    %159 = spirv.VectorShuffle [0 : i32, 1 : i32] %70 : vector<4xf32>, %70 : vector<4xf32> -> vector<2xf32>
    %160 = spirv.Bitcast %159 : vector<2xf32> to vector<4xf16>
    %161 = spirv.FMul %158, %160 : vector<4xf16>
    %162 = spirv.VectorShuffle [2 : i32, 3 : i32] %75 : vector<4xf32>, %75 : vector<4xf32> -> vector<2xf32>
    %163 = spirv.Bitcast %162 : vector<2xf32> to vector<4xf16>
    %164 = spirv.VectorShuffle [2 : i32, 3 : i32] %70 : vector<4xf32>, %70 : vector<4xf32> -> vector<2xf32>
    %165 = spirv.Bitcast %164 : vector<2xf32> to vector<4xf16>
    %166 = spirv.FMul %163, %165 : vector<4xf16>
    %167 = spirv.FAdd %161, %155 : vector<4xf16>
    %168 = spirv.FAdd %166, %156 : vector<4xf16>
    %169 = spirv.VectorShuffle [0 : i32, 1 : i32] %84 : vector<4xf32>, %84 : vector<4xf32> -> vector<2xf32>
    %170 = spirv.Bitcast %169 : vector<2xf32> to vector<4xf16>
    %171 = spirv.VectorShuffle [0 : i32, 1 : i32] %80 : vector<4xf32>, %80 : vector<4xf32> -> vector<2xf32>
    %172 = spirv.Bitcast %171 : vector<2xf32> to vector<4xf16>
    %173 = spirv.FMul %170, %172 : vector<4xf16>
    %174 = spirv.VectorShuffle [2 : i32, 3 : i32] %84 : vector<4xf32>, %84 : vector<4xf32> -> vector<2xf32>
    %175 = spirv.Bitcast %174 : vector<2xf32> to vector<4xf16>
    %176 = spirv.VectorShuffle [2 : i32, 3 : i32] %80 : vector<4xf32>, %80 : vector<4xf32> -> vector<2xf32>
    %177 = spirv.Bitcast %176 : vector<2xf32> to vector<4xf16>
    %178 = spirv.FMul %175, %177 : vector<4xf16>
    %179 = spirv.FAdd %173, %167 : vector<4xf16>
    %180 = spirv.FAdd %178, %168 : vector<4xf16>
    %181 = spirv.CompositeExtract %179[0 : i32] : vector<4xf16>
    %182 = spirv.CompositeExtract %179[1 : i32] : vector<4xf16>
    %183 = spirv.CompositeExtract %179[2 : i32] : vector<4xf16>
    %184 = spirv.CompositeExtract %179[3 : i32] : vector<4xf16>
    %185 = spirv.FAdd %181, %182 : f16
    %186 = spirv.FAdd %185, %183 : f16
    %187 = spirv.FAdd %186, %184 : f16
    %188 = spirv.CompositeExtract %180[0 : i32] : vector<4xf16>
    %189 = spirv.CompositeExtract %180[1 : i32] : vector<4xf16>
    %190 = spirv.CompositeExtract %180[2 : i32] : vector<4xf16>
    %191 = spirv.CompositeExtract %180[3 : i32] : vector<4xf16>
    %192 = spirv.FAdd %188, %189 : f16
    %193 = spirv.FAdd %192, %190 : f16
    %194 = spirv.FAdd %193, %191 : f16
    %195 = spirv.FAdd %187, %194 : f16
    %196 = spirv.GroupNonUniformFAdd "Subgroup" "Reduce" %195 : f16
    %197 = spirv.FAdd %196, %cst_f16 : f16
    %198 = spirv.IEqual %1, %cst0_i32 : i32
    spirv.mlir.selection {
      spirv.BranchConditional %198, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %cst0_i32_47 = spirv.Constant 0 : i32
      %cst0_i32_48 = spirv.Constant 0 : i32
      %cst1_i32_49 = spirv.Constant 1 : i32
      %199 = spirv.IMul %cst1_i32_49, %3 : i32
      %200 = spirv.IAdd %cst0_i32_48, %199 : i32
      %201 = spirv.AccessChain %__resource_var_0_2__addr[%cst0_i32_47, %200] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>, i32, i32
      spirv.Store "StorageBuffer" %201, %197 : f16
      spirv.Branch ^bb2
    ^bb2:  // 2 preds: ^bb0, ^bb1
      spirv.mlir.merge
    }
    spirv.Return
  }
}

// -----// IR Dump After SPIRVLowerABIAttributesPass (spirv-lower-abi-attrs) //----- //
spirv.module Logical GLSL450 {
  spirv.GlobalVariable @__builtin__WorkgroupId__ built_in("WorkgroupId") : !spirv.ptr<vector<3xi32>, Input>
  spirv.GlobalVariable @__builtin__LocalInvocationId__ built_in("LocalInvocationId") : !spirv.ptr<vector<3xi32>, Input>
  spirv.GlobalVariable @__resource_var_0_0_ bind(0, 0) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
  spirv.GlobalVariable @__resource_var_0_1_ bind(0, 1) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
  spirv.GlobalVariable @__resource_var_0_2_ bind(0, 2) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
  spirv.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() "None" {
    %cst64_i32 = spirv.Constant 64 : i32
    %cst128_i32 = spirv.Constant 128 : i32
    %cst192_i32 = spirv.Constant 192 : i32
    %cst256_i32 = spirv.Constant 256 : i32
    %cst320_i32 = spirv.Constant 320 : i32
    %cst384_i32 = spirv.Constant 384 : i32
    %cst448_i32 = spirv.Constant 448 : i32
    %cst32000_i32 = spirv.Constant 32000 : i32
    %cst16384000_i32 = spirv.Constant 16384000 : i32
    %cst512_i32 = spirv.Constant 512 : i32
    %cst_vec_4xf16 = spirv.Constant dense<0.000000e+00> : vector<4xf16>
    %cst0_i32 = spirv.Constant 0 : i32
    %cst_f16 = spirv.Constant 0.000000e+00 : f16
    %__builtin__LocalInvocationId___addr = spirv.mlir.addressof @__builtin__LocalInvocationId__ : !spirv.ptr<vector<3xi32>, Input>
    %0 = spirv.Load "Input" %__builtin__LocalInvocationId___addr : vector<3xi32>
    %1 = spirv.CompositeExtract %0[0 : i32] : vector<3xi32>
    %__resource_var_0_0__addr = spirv.mlir.addressof @__resource_var_0_0_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
    %__resource_var_0_1__addr = spirv.mlir.addressof @__resource_var_0_1_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
    %__resource_var_0_2__addr = spirv.mlir.addressof @__resource_var_0_2_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
    %__builtin__WorkgroupId___addr = spirv.mlir.addressof @__builtin__WorkgroupId__ : !spirv.ptr<vector<3xi32>, Input>
    %2 = spirv.Load "Input" %__builtin__WorkgroupId___addr : vector<3xi32>
    %3 = spirv.CompositeExtract %2[0 : i32] : vector<3xi32>
    %4 = spirv.IMul %3, %cst512_i32 : i32
    %5 = spirv.IAdd %1, %4 : i32
    %6 = spirv.IAdd %5, %cst448_i32 : i32
    %cst0_i32_0 = spirv.Constant 0 : i32
    %cst0_i32_1 = spirv.Constant 0 : i32
    %cst1_i32 = spirv.Constant 1 : i32
    %7 = spirv.IMul %cst1_i32, %6 : i32
    %8 = spirv.IAdd %cst0_i32_1, %7 : i32
    %9 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32_0, %8] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %10 = spirv.Load "StorageBuffer" %9 : vector<4xf32>
    %11 = spirv.IAdd %1, %cst448_i32 : i32
    %cst0_i32_2 = spirv.Constant 0 : i32
    %cst0_i32_3 = spirv.Constant 0 : i32
    %cst1_i32_4 = spirv.Constant 1 : i32
    %12 = spirv.IMul %cst1_i32_4, %11 : i32
    %13 = spirv.IAdd %cst0_i32_3, %12 : i32
    %14 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32_2, %13] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %15 = spirv.Load "StorageBuffer" %14 : vector<4xf32>
    %16 = spirv.IAdd %5, %cst384_i32 : i32
    %cst0_i32_5 = spirv.Constant 0 : i32
    %cst0_i32_6 = spirv.Constant 0 : i32
    %cst1_i32_7 = spirv.Constant 1 : i32
    %17 = spirv.IMul %cst1_i32_7, %16 : i32
    %18 = spirv.IAdd %cst0_i32_6, %17 : i32
    %19 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32_5, %18] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %20 = spirv.Load "StorageBuffer" %19 : vector<4xf32>
    %21 = spirv.IAdd %1, %cst384_i32 : i32
    %cst0_i32_8 = spirv.Constant 0 : i32
    %cst0_i32_9 = spirv.Constant 0 : i32
    %cst1_i32_10 = spirv.Constant 1 : i32
    %22 = spirv.IMul %cst1_i32_10, %21 : i32
    %23 = spirv.IAdd %cst0_i32_9, %22 : i32
    %24 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32_8, %23] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %25 = spirv.Load "StorageBuffer" %24 : vector<4xf32>
    %26 = spirv.IAdd %5, %cst320_i32 : i32
    %cst0_i32_11 = spirv.Constant 0 : i32
    %cst0_i32_12 = spirv.Constant 0 : i32
    %cst1_i32_13 = spirv.Constant 1 : i32
    %27 = spirv.IMul %cst1_i32_13, %26 : i32
    %28 = spirv.IAdd %cst0_i32_12, %27 : i32
    %29 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32_11, %28] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %30 = spirv.Load "StorageBuffer" %29 : vector<4xf32>
    %31 = spirv.IAdd %1, %cst320_i32 : i32
    %cst0_i32_14 = spirv.Constant 0 : i32
    %cst0_i32_15 = spirv.Constant 0 : i32
    %cst1_i32_16 = spirv.Constant 1 : i32
    %32 = spirv.IMul %cst1_i32_16, %31 : i32
    %33 = spirv.IAdd %cst0_i32_15, %32 : i32
    %34 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32_14, %33] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %35 = spirv.Load "StorageBuffer" %34 : vector<4xf32>
    %36 = spirv.IAdd %5, %cst256_i32 : i32
    %cst0_i32_17 = spirv.Constant 0 : i32
    %cst0_i32_18 = spirv.Constant 0 : i32
    %cst1_i32_19 = spirv.Constant 1 : i32
    %37 = spirv.IMul %cst1_i32_19, %36 : i32
    %38 = spirv.IAdd %cst0_i32_18, %37 : i32
    %39 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32_17, %38] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %40 = spirv.Load "StorageBuffer" %39 : vector<4xf32>
    %41 = spirv.IAdd %1, %cst256_i32 : i32
    %cst0_i32_20 = spirv.Constant 0 : i32
    %cst0_i32_21 = spirv.Constant 0 : i32
    %cst1_i32_22 = spirv.Constant 1 : i32
    %42 = spirv.IMul %cst1_i32_22, %41 : i32
    %43 = spirv.IAdd %cst0_i32_21, %42 : i32
    %44 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32_20, %43] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %45 = spirv.Load "StorageBuffer" %44 : vector<4xf32>
    %46 = spirv.IAdd %5, %cst192_i32 : i32
    %cst0_i32_23 = spirv.Constant 0 : i32
    %cst0_i32_24 = spirv.Constant 0 : i32
    %cst1_i32_25 = spirv.Constant 1 : i32
    %47 = spirv.IMul %cst1_i32_25, %46 : i32
    %48 = spirv.IAdd %cst0_i32_24, %47 : i32
    %49 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32_23, %48] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %50 = spirv.Load "StorageBuffer" %49 : vector<4xf32>
    %51 = spirv.IAdd %1, %cst192_i32 : i32
    %cst0_i32_26 = spirv.Constant 0 : i32
    %cst0_i32_27 = spirv.Constant 0 : i32
    %cst1_i32_28 = spirv.Constant 1 : i32
    %52 = spirv.IMul %cst1_i32_28, %51 : i32
    %53 = spirv.IAdd %cst0_i32_27, %52 : i32
    %54 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32_26, %53] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %55 = spirv.Load "StorageBuffer" %54 : vector<4xf32>
    %56 = spirv.IAdd %5, %cst128_i32 : i32
    %cst0_i32_29 = spirv.Constant 0 : i32
    %cst0_i32_30 = spirv.Constant 0 : i32
    %cst1_i32_31 = spirv.Constant 1 : i32
    %57 = spirv.IMul %cst1_i32_31, %56 : i32
    %58 = spirv.IAdd %cst0_i32_30, %57 : i32
    %59 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32_29, %58] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %60 = spirv.Load "StorageBuffer" %59 : vector<4xf32>
    %61 = spirv.IAdd %1, %cst128_i32 : i32
    %cst0_i32_32 = spirv.Constant 0 : i32
    %cst0_i32_33 = spirv.Constant 0 : i32
    %cst1_i32_34 = spirv.Constant 1 : i32
    %62 = spirv.IMul %cst1_i32_34, %61 : i32
    %63 = spirv.IAdd %cst0_i32_33, %62 : i32
    %64 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32_32, %63] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %65 = spirv.Load "StorageBuffer" %64 : vector<4xf32>
    %66 = spirv.IAdd %5, %cst64_i32 : i32
    %cst0_i32_35 = spirv.Constant 0 : i32
    %cst0_i32_36 = spirv.Constant 0 : i32
    %cst1_i32_37 = spirv.Constant 1 : i32
    %67 = spirv.IMul %cst1_i32_37, %66 : i32
    %68 = spirv.IAdd %cst0_i32_36, %67 : i32
    %69 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32_35, %68] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %70 = spirv.Load "StorageBuffer" %69 : vector<4xf32>
    %71 = spirv.IAdd %1, %cst64_i32 : i32
    %cst0_i32_38 = spirv.Constant 0 : i32
    %cst0_i32_39 = spirv.Constant 0 : i32
    %cst1_i32_40 = spirv.Constant 1 : i32
    %72 = spirv.IMul %cst1_i32_40, %71 : i32
    %73 = spirv.IAdd %cst0_i32_39, %72 : i32
    %74 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32_38, %73] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %75 = spirv.Load "StorageBuffer" %74 : vector<4xf32>
    %76 = spirv.IAdd %4, %1 : i32
    %cst0_i32_41 = spirv.Constant 0 : i32
    %cst0_i32_42 = spirv.Constant 0 : i32
    %cst1_i32_43 = spirv.Constant 1 : i32
    %77 = spirv.IMul %cst1_i32_43, %76 : i32
    %78 = spirv.IAdd %cst0_i32_42, %77 : i32
    %79 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32_41, %78] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %80 = spirv.Load "StorageBuffer" %79 : vector<4xf32>
    %cst0_i32_44 = spirv.Constant 0 : i32
    %cst0_i32_45 = spirv.Constant 0 : i32
    %cst1_i32_46 = spirv.Constant 1 : i32
    %81 = spirv.IMul %cst1_i32_46, %1 : i32
    %82 = spirv.IAdd %cst0_i32_45, %81 : i32
    %83 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32_44, %82] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %84 = spirv.Load "StorageBuffer" %83 : vector<4xf32>
    %85 = spirv.VectorShuffle [0 : i32, 1 : i32] %15 : vector<4xf32>, %15 : vector<4xf32> -> vector<2xf32>
    %86 = spirv.Bitcast %85 : vector<2xf32> to vector<4xf16>
    %87 = spirv.VectorShuffle [0 : i32, 1 : i32] %10 : vector<4xf32>, %10 : vector<4xf32> -> vector<2xf32>
    %88 = spirv.Bitcast %87 : vector<2xf32> to vector<4xf16>
    %89 = spirv.FMul %86, %88 : vector<4xf16>
    %90 = spirv.VectorShuffle [2 : i32, 3 : i32] %15 : vector<4xf32>, %15 : vector<4xf32> -> vector<2xf32>
    %91 = spirv.Bitcast %90 : vector<2xf32> to vector<4xf16>
    %92 = spirv.VectorShuffle [2 : i32, 3 : i32] %10 : vector<4xf32>, %10 : vector<4xf32> -> vector<2xf32>
    %93 = spirv.Bitcast %92 : vector<2xf32> to vector<4xf16>
    %94 = spirv.FMul %91, %93 : vector<4xf16>
    %95 = spirv.FAdd %89, %cst_vec_4xf16 : vector<4xf16>
    %96 = spirv.FAdd %94, %cst_vec_4xf16 : vector<4xf16>
    %97 = spirv.VectorShuffle [0 : i32, 1 : i32] %25 : vector<4xf32>, %25 : vector<4xf32> -> vector<2xf32>
    %98 = spirv.Bitcast %97 : vector<2xf32> to vector<4xf16>
    %99 = spirv.VectorShuffle [0 : i32, 1 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
    %100 = spirv.Bitcast %99 : vector<2xf32> to vector<4xf16>
    %101 = spirv.FMul %98, %100 : vector<4xf16>
    %102 = spirv.VectorShuffle [2 : i32, 3 : i32] %25 : vector<4xf32>, %25 : vector<4xf32> -> vector<2xf32>
    %103 = spirv.Bitcast %102 : vector<2xf32> to vector<4xf16>
    %104 = spirv.VectorShuffle [2 : i32, 3 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
    %105 = spirv.Bitcast %104 : vector<2xf32> to vector<4xf16>
    %106 = spirv.FMul %103, %105 : vector<4xf16>
    %107 = spirv.FAdd %101, %95 : vector<4xf16>
    %108 = spirv.FAdd %106, %96 : vector<4xf16>
    %109 = spirv.VectorShuffle [0 : i32, 1 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
    %110 = spirv.Bitcast %109 : vector<2xf32> to vector<4xf16>
    %111 = spirv.VectorShuffle [0 : i32, 1 : i32] %30 : vector<4xf32>, %30 : vector<4xf32> -> vector<2xf32>
    %112 = spirv.Bitcast %111 : vector<2xf32> to vector<4xf16>
    %113 = spirv.FMul %110, %112 : vector<4xf16>
    %114 = spirv.VectorShuffle [2 : i32, 3 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
    %115 = spirv.Bitcast %114 : vector<2xf32> to vector<4xf16>
    %116 = spirv.VectorShuffle [2 : i32, 3 : i32] %30 : vector<4xf32>, %30 : vector<4xf32> -> vector<2xf32>
    %117 = spirv.Bitcast %116 : vector<2xf32> to vector<4xf16>
    %118 = spirv.FMul %115, %117 : vector<4xf16>
    %119 = spirv.FAdd %113, %107 : vector<4xf16>
    %120 = spirv.FAdd %118, %108 : vector<4xf16>
    %121 = spirv.VectorShuffle [0 : i32, 1 : i32] %45 : vector<4xf32>, %45 : vector<4xf32> -> vector<2xf32>
    %122 = spirv.Bitcast %121 : vector<2xf32> to vector<4xf16>
    %123 = spirv.VectorShuffle [0 : i32, 1 : i32] %40 : vector<4xf32>, %40 : vector<4xf32> -> vector<2xf32>
    %124 = spirv.Bitcast %123 : vector<2xf32> to vector<4xf16>
    %125 = spirv.FMul %122, %124 : vector<4xf16>
    %126 = spirv.VectorShuffle [2 : i32, 3 : i32] %45 : vector<4xf32>, %45 : vector<4xf32> -> vector<2xf32>
    %127 = spirv.Bitcast %126 : vector<2xf32> to vector<4xf16>
    %128 = spirv.VectorShuffle [2 : i32, 3 : i32] %40 : vector<4xf32>, %40 : vector<4xf32> -> vector<2xf32>
    %129 = spirv.Bitcast %128 : vector<2xf32> to vector<4xf16>
    %130 = spirv.FMul %127, %129 : vector<4xf16>
    %131 = spirv.FAdd %125, %119 : vector<4xf16>
    %132 = spirv.FAdd %130, %120 : vector<4xf16>
    %133 = spirv.VectorShuffle [0 : i32, 1 : i32] %55 : vector<4xf32>, %55 : vector<4xf32> -> vector<2xf32>
    %134 = spirv.Bitcast %133 : vector<2xf32> to vector<4xf16>
    %135 = spirv.VectorShuffle [0 : i32, 1 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
    %136 = spirv.Bitcast %135 : vector<2xf32> to vector<4xf16>
    %137 = spirv.FMul %134, %136 : vector<4xf16>
    %138 = spirv.VectorShuffle [2 : i32, 3 : i32] %55 : vector<4xf32>, %55 : vector<4xf32> -> vector<2xf32>
    %139 = spirv.Bitcast %138 : vector<2xf32> to vector<4xf16>
    %140 = spirv.VectorShuffle [2 : i32, 3 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
    %141 = spirv.Bitcast %140 : vector<2xf32> to vector<4xf16>
    %142 = spirv.FMul %139, %141 : vector<4xf16>
    %143 = spirv.FAdd %137, %131 : vector<4xf16>
    %144 = spirv.FAdd %142, %132 : vector<4xf16>
    %145 = spirv.VectorShuffle [0 : i32, 1 : i32] %65 : vector<4xf32>, %65 : vector<4xf32> -> vector<2xf32>
    %146 = spirv.Bitcast %145 : vector<2xf32> to vector<4xf16>
    %147 = spirv.VectorShuffle [0 : i32, 1 : i32] %60 : vector<4xf32>, %60 : vector<4xf32> -> vector<2xf32>
    %148 = spirv.Bitcast %147 : vector<2xf32> to vector<4xf16>
    %149 = spirv.FMul %146, %148 : vector<4xf16>
    %150 = spirv.VectorShuffle [2 : i32, 3 : i32] %65 : vector<4xf32>, %65 : vector<4xf32> -> vector<2xf32>
    %151 = spirv.Bitcast %150 : vector<2xf32> to vector<4xf16>
    %152 = spirv.VectorShuffle [2 : i32, 3 : i32] %60 : vector<4xf32>, %60 : vector<4xf32> -> vector<2xf32>
    %153 = spirv.Bitcast %152 : vector<2xf32> to vector<4xf16>
    %154 = spirv.FMul %151, %153 : vector<4xf16>
    %155 = spirv.FAdd %149, %143 : vector<4xf16>
    %156 = spirv.FAdd %154, %144 : vector<4xf16>
    %157 = spirv.VectorShuffle [0 : i32, 1 : i32] %75 : vector<4xf32>, %75 : vector<4xf32> -> vector<2xf32>
    %158 = spirv.Bitcast %157 : vector<2xf32> to vector<4xf16>
    %159 = spirv.VectorShuffle [0 : i32, 1 : i32] %70 : vector<4xf32>, %70 : vector<4xf32> -> vector<2xf32>
    %160 = spirv.Bitcast %159 : vector<2xf32> to vector<4xf16>
    %161 = spirv.FMul %158, %160 : vector<4xf16>
    %162 = spirv.VectorShuffle [2 : i32, 3 : i32] %75 : vector<4xf32>, %75 : vector<4xf32> -> vector<2xf32>
    %163 = spirv.Bitcast %162 : vector<2xf32> to vector<4xf16>
    %164 = spirv.VectorShuffle [2 : i32, 3 : i32] %70 : vector<4xf32>, %70 : vector<4xf32> -> vector<2xf32>
    %165 = spirv.Bitcast %164 : vector<2xf32> to vector<4xf16>
    %166 = spirv.FMul %163, %165 : vector<4xf16>
    %167 = spirv.FAdd %161, %155 : vector<4xf16>
    %168 = spirv.FAdd %166, %156 : vector<4xf16>
    %169 = spirv.VectorShuffle [0 : i32, 1 : i32] %84 : vector<4xf32>, %84 : vector<4xf32> -> vector<2xf32>
    %170 = spirv.Bitcast %169 : vector<2xf32> to vector<4xf16>
    %171 = spirv.VectorShuffle [0 : i32, 1 : i32] %80 : vector<4xf32>, %80 : vector<4xf32> -> vector<2xf32>
    %172 = spirv.Bitcast %171 : vector<2xf32> to vector<4xf16>
    %173 = spirv.FMul %170, %172 : vector<4xf16>
    %174 = spirv.VectorShuffle [2 : i32, 3 : i32] %84 : vector<4xf32>, %84 : vector<4xf32> -> vector<2xf32>
    %175 = spirv.Bitcast %174 : vector<2xf32> to vector<4xf16>
    %176 = spirv.VectorShuffle [2 : i32, 3 : i32] %80 : vector<4xf32>, %80 : vector<4xf32> -> vector<2xf32>
    %177 = spirv.Bitcast %176 : vector<2xf32> to vector<4xf16>
    %178 = spirv.FMul %175, %177 : vector<4xf16>
    %179 = spirv.FAdd %173, %167 : vector<4xf16>
    %180 = spirv.FAdd %178, %168 : vector<4xf16>
    %181 = spirv.CompositeExtract %179[0 : i32] : vector<4xf16>
    %182 = spirv.CompositeExtract %179[1 : i32] : vector<4xf16>
    %183 = spirv.CompositeExtract %179[2 : i32] : vector<4xf16>
    %184 = spirv.CompositeExtract %179[3 : i32] : vector<4xf16>
    %185 = spirv.FAdd %181, %182 : f16
    %186 = spirv.FAdd %185, %183 : f16
    %187 = spirv.FAdd %186, %184 : f16
    %188 = spirv.CompositeExtract %180[0 : i32] : vector<4xf16>
    %189 = spirv.CompositeExtract %180[1 : i32] : vector<4xf16>
    %190 = spirv.CompositeExtract %180[2 : i32] : vector<4xf16>
    %191 = spirv.CompositeExtract %180[3 : i32] : vector<4xf16>
    %192 = spirv.FAdd %188, %189 : f16
    %193 = spirv.FAdd %192, %190 : f16
    %194 = spirv.FAdd %193, %191 : f16
    %195 = spirv.FAdd %187, %194 : f16
    %196 = spirv.GroupNonUniformFAdd "Subgroup" "Reduce" %195 : f16
    %197 = spirv.FAdd %196, %cst_f16 : f16
    %198 = spirv.IEqual %1, %cst0_i32 : i32
    spirv.mlir.selection {
      spirv.BranchConditional %198, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %cst0_i32_47 = spirv.Constant 0 : i32
      %cst0_i32_48 = spirv.Constant 0 : i32
      %cst1_i32_49 = spirv.Constant 1 : i32
      %199 = spirv.IMul %cst1_i32_49, %3 : i32
      %200 = spirv.IAdd %cst0_i32_48, %199 : i32
      %201 = spirv.AccessChain %__resource_var_0_2__addr[%cst0_i32_47, %200] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>, i32, i32
      spirv.Store "StorageBuffer" %201, %197 : f16
      spirv.Branch ^bb2
    ^bb2:  // 2 preds: ^bb0, ^bb1
      spirv.mlir.merge
    }
    spirv.Return
  }
  spirv.EntryPoint "GLCompute" @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16, @__builtin__LocalInvocationId__, @__builtin__WorkgroupId__
  spirv.ExecutionMode @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 "LocalSize", 64, 1, 1
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
spirv.module Logical GLSL450 {
  spirv.GlobalVariable @__builtin__WorkgroupId__ built_in("WorkgroupId") : !spirv.ptr<vector<3xi32>, Input>
  spirv.GlobalVariable @__builtin__LocalInvocationId__ built_in("LocalInvocationId") : !spirv.ptr<vector<3xi32>, Input>
  spirv.GlobalVariable @__resource_var_0_0_ bind(0, 0) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
  spirv.GlobalVariable @__resource_var_0_1_ bind(0, 1) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
  spirv.GlobalVariable @__resource_var_0_2_ bind(0, 2) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
  spirv.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() "None" {
    %cst64_i32 = spirv.Constant 64 : i32
    %cst128_i32 = spirv.Constant 128 : i32
    %cst192_i32 = spirv.Constant 192 : i32
    %cst256_i32 = spirv.Constant 256 : i32
    %cst320_i32 = spirv.Constant 320 : i32
    %cst384_i32 = spirv.Constant 384 : i32
    %cst448_i32 = spirv.Constant 448 : i32
    %cst512_i32 = spirv.Constant 512 : i32
    %cst_vec_4xf16 = spirv.Constant dense<0.000000e+00> : vector<4xf16>
    %cst0_i32 = spirv.Constant 0 : i32
    %cst_f16 = spirv.Constant 0.000000e+00 : f16
    %__builtin__LocalInvocationId___addr = spirv.mlir.addressof @__builtin__LocalInvocationId__ : !spirv.ptr<vector<3xi32>, Input>
    %0 = spirv.Load "Input" %__builtin__LocalInvocationId___addr : vector<3xi32>
    %1 = spirv.CompositeExtract %0[0 : i32] : vector<3xi32>
    %__resource_var_0_0__addr = spirv.mlir.addressof @__resource_var_0_0_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
    %__resource_var_0_1__addr = spirv.mlir.addressof @__resource_var_0_1_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
    %__resource_var_0_2__addr = spirv.mlir.addressof @__resource_var_0_2_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
    %__builtin__WorkgroupId___addr = spirv.mlir.addressof @__builtin__WorkgroupId__ : !spirv.ptr<vector<3xi32>, Input>
    %2 = spirv.Load "Input" %__builtin__WorkgroupId___addr : vector<3xi32>
    %3 = spirv.CompositeExtract %2[0 : i32] : vector<3xi32>
    %4 = spirv.IMul %3, %cst512_i32 : i32
    %5 = spirv.IAdd %1, %4 : i32
    %6 = spirv.IAdd %5, %cst448_i32 : i32
    %7 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %6] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %8 = spirv.Load "StorageBuffer" %7 : vector<4xf32>
    %9 = spirv.IAdd %1, %cst448_i32 : i32
    %10 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %9] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %11 = spirv.Load "StorageBuffer" %10 : vector<4xf32>
    %12 = spirv.IAdd %5, %cst384_i32 : i32
    %13 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %12] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %14 = spirv.Load "StorageBuffer" %13 : vector<4xf32>
    %15 = spirv.IAdd %1, %cst384_i32 : i32
    %16 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %15] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %17 = spirv.Load "StorageBuffer" %16 : vector<4xf32>
    %18 = spirv.IAdd %5, %cst320_i32 : i32
    %19 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %18] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %20 = spirv.Load "StorageBuffer" %19 : vector<4xf32>
    %21 = spirv.IAdd %1, %cst320_i32 : i32
    %22 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %21] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %23 = spirv.Load "StorageBuffer" %22 : vector<4xf32>
    %24 = spirv.IAdd %5, %cst256_i32 : i32
    %25 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %24] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %26 = spirv.Load "StorageBuffer" %25 : vector<4xf32>
    %27 = spirv.IAdd %1, %cst256_i32 : i32
    %28 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %27] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %29 = spirv.Load "StorageBuffer" %28 : vector<4xf32>
    %30 = spirv.IAdd %5, %cst192_i32 : i32
    %31 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %30] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %32 = spirv.Load "StorageBuffer" %31 : vector<4xf32>
    %33 = spirv.IAdd %1, %cst192_i32 : i32
    %34 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %33] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %35 = spirv.Load "StorageBuffer" %34 : vector<4xf32>
    %36 = spirv.IAdd %5, %cst128_i32 : i32
    %37 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %36] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %38 = spirv.Load "StorageBuffer" %37 : vector<4xf32>
    %39 = spirv.IAdd %1, %cst128_i32 : i32
    %40 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %39] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %41 = spirv.Load "StorageBuffer" %40 : vector<4xf32>
    %42 = spirv.IAdd %5, %cst64_i32 : i32
    %43 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %42] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %44 = spirv.Load "StorageBuffer" %43 : vector<4xf32>
    %45 = spirv.IAdd %1, %cst64_i32 : i32
    %46 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %45] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %47 = spirv.Load "StorageBuffer" %46 : vector<4xf32>
    %48 = spirv.IAdd %4, %1 : i32
    %49 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %48] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %50 = spirv.Load "StorageBuffer" %49 : vector<4xf32>
    %51 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %1] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %52 = spirv.Load "StorageBuffer" %51 : vector<4xf32>
    %53 = spirv.VectorShuffle [0 : i32, 1 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
    %54 = spirv.Bitcast %53 : vector<2xf32> to vector<4xf16>
    %55 = spirv.VectorShuffle [0 : i32, 1 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
    %56 = spirv.Bitcast %55 : vector<2xf32> to vector<4xf16>
    %57 = spirv.FMul %54, %56 : vector<4xf16>
    %58 = spirv.VectorShuffle [2 : i32, 3 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
    %59 = spirv.Bitcast %58 : vector<2xf32> to vector<4xf16>
    %60 = spirv.VectorShuffle [2 : i32, 3 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
    %61 = spirv.Bitcast %60 : vector<2xf32> to vector<4xf16>
    %62 = spirv.FMul %59, %61 : vector<4xf16>
    %63 = spirv.FAdd %57, %cst_vec_4xf16 : vector<4xf16>
    %64 = spirv.FAdd %62, %cst_vec_4xf16 : vector<4xf16>
    %65 = spirv.VectorShuffle [0 : i32, 1 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
    %66 = spirv.Bitcast %65 : vector<2xf32> to vector<4xf16>
    %67 = spirv.VectorShuffle [0 : i32, 1 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
    %68 = spirv.Bitcast %67 : vector<2xf32> to vector<4xf16>
    %69 = spirv.FMul %66, %68 : vector<4xf16>
    %70 = spirv.VectorShuffle [2 : i32, 3 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
    %71 = spirv.Bitcast %70 : vector<2xf32> to vector<4xf16>
    %72 = spirv.VectorShuffle [2 : i32, 3 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
    %73 = spirv.Bitcast %72 : vector<2xf32> to vector<4xf16>
    %74 = spirv.FMul %71, %73 : vector<4xf16>
    %75 = spirv.FAdd %69, %63 : vector<4xf16>
    %76 = spirv.FAdd %74, %64 : vector<4xf16>
    %77 = spirv.VectorShuffle [0 : i32, 1 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
    %78 = spirv.Bitcast %77 : vector<2xf32> to vector<4xf16>
    %79 = spirv.VectorShuffle [0 : i32, 1 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
    %80 = spirv.Bitcast %79 : vector<2xf32> to vector<4xf16>
    %81 = spirv.FMul %78, %80 : vector<4xf16>
    %82 = spirv.VectorShuffle [2 : i32, 3 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
    %83 = spirv.Bitcast %82 : vector<2xf32> to vector<4xf16>
    %84 = spirv.VectorShuffle [2 : i32, 3 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
    %85 = spirv.Bitcast %84 : vector<2xf32> to vector<4xf16>
    %86 = spirv.FMul %83, %85 : vector<4xf16>
    %87 = spirv.FAdd %81, %75 : vector<4xf16>
    %88 = spirv.FAdd %86, %76 : vector<4xf16>
    %89 = spirv.VectorShuffle [0 : i32, 1 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
    %90 = spirv.Bitcast %89 : vector<2xf32> to vector<4xf16>
    %91 = spirv.VectorShuffle [0 : i32, 1 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
    %92 = spirv.Bitcast %91 : vector<2xf32> to vector<4xf16>
    %93 = spirv.FMul %90, %92 : vector<4xf16>
    %94 = spirv.VectorShuffle [2 : i32, 3 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
    %95 = spirv.Bitcast %94 : vector<2xf32> to vector<4xf16>
    %96 = spirv.VectorShuffle [2 : i32, 3 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
    %97 = spirv.Bitcast %96 : vector<2xf32> to vector<4xf16>
    %98 = spirv.FMul %95, %97 : vector<4xf16>
    %99 = spirv.FAdd %93, %87 : vector<4xf16>
    %100 = spirv.FAdd %98, %88 : vector<4xf16>
    %101 = spirv.VectorShuffle [0 : i32, 1 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
    %102 = spirv.Bitcast %101 : vector<2xf32> to vector<4xf16>
    %103 = spirv.VectorShuffle [0 : i32, 1 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
    %104 = spirv.Bitcast %103 : vector<2xf32> to vector<4xf16>
    %105 = spirv.FMul %102, %104 : vector<4xf16>
    %106 = spirv.VectorShuffle [2 : i32, 3 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
    %107 = spirv.Bitcast %106 : vector<2xf32> to vector<4xf16>
    %108 = spirv.VectorShuffle [2 : i32, 3 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
    %109 = spirv.Bitcast %108 : vector<2xf32> to vector<4xf16>
    %110 = spirv.FMul %107, %109 : vector<4xf16>
    %111 = spirv.FAdd %105, %99 : vector<4xf16>
    %112 = spirv.FAdd %110, %100 : vector<4xf16>
    %113 = spirv.VectorShuffle [0 : i32, 1 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
    %114 = spirv.Bitcast %113 : vector<2xf32> to vector<4xf16>
    %115 = spirv.VectorShuffle [0 : i32, 1 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
    %116 = spirv.Bitcast %115 : vector<2xf32> to vector<4xf16>
    %117 = spirv.FMul %114, %116 : vector<4xf16>
    %118 = spirv.VectorShuffle [2 : i32, 3 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
    %119 = spirv.Bitcast %118 : vector<2xf32> to vector<4xf16>
    %120 = spirv.VectorShuffle [2 : i32, 3 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
    %121 = spirv.Bitcast %120 : vector<2xf32> to vector<4xf16>
    %122 = spirv.FMul %119, %121 : vector<4xf16>
    %123 = spirv.FAdd %117, %111 : vector<4xf16>
    %124 = spirv.FAdd %122, %112 : vector<4xf16>
    %125 = spirv.VectorShuffle [0 : i32, 1 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
    %126 = spirv.Bitcast %125 : vector<2xf32> to vector<4xf16>
    %127 = spirv.VectorShuffle [0 : i32, 1 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
    %128 = spirv.Bitcast %127 : vector<2xf32> to vector<4xf16>
    %129 = spirv.FMul %126, %128 : vector<4xf16>
    %130 = spirv.VectorShuffle [2 : i32, 3 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
    %131 = spirv.Bitcast %130 : vector<2xf32> to vector<4xf16>
    %132 = spirv.VectorShuffle [2 : i32, 3 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
    %133 = spirv.Bitcast %132 : vector<2xf32> to vector<4xf16>
    %134 = spirv.FMul %131, %133 : vector<4xf16>
    %135 = spirv.FAdd %129, %123 : vector<4xf16>
    %136 = spirv.FAdd %134, %124 : vector<4xf16>
    %137 = spirv.VectorShuffle [0 : i32, 1 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
    %138 = spirv.Bitcast %137 : vector<2xf32> to vector<4xf16>
    %139 = spirv.VectorShuffle [0 : i32, 1 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
    %140 = spirv.Bitcast %139 : vector<2xf32> to vector<4xf16>
    %141 = spirv.FMul %138, %140 : vector<4xf16>
    %142 = spirv.VectorShuffle [2 : i32, 3 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
    %143 = spirv.Bitcast %142 : vector<2xf32> to vector<4xf16>
    %144 = spirv.VectorShuffle [2 : i32, 3 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
    %145 = spirv.Bitcast %144 : vector<2xf32> to vector<4xf16>
    %146 = spirv.FMul %143, %145 : vector<4xf16>
    %147 = spirv.FAdd %141, %135 : vector<4xf16>
    %148 = spirv.FAdd %146, %136 : vector<4xf16>
    %149 = spirv.CompositeExtract %147[0 : i32] : vector<4xf16>
    %150 = spirv.CompositeExtract %147[1 : i32] : vector<4xf16>
    %151 = spirv.CompositeExtract %147[2 : i32] : vector<4xf16>
    %152 = spirv.CompositeExtract %147[3 : i32] : vector<4xf16>
    %153 = spirv.FAdd %149, %150 : f16
    %154 = spirv.FAdd %153, %151 : f16
    %155 = spirv.FAdd %154, %152 : f16
    %156 = spirv.CompositeExtract %148[0 : i32] : vector<4xf16>
    %157 = spirv.CompositeExtract %148[1 : i32] : vector<4xf16>
    %158 = spirv.CompositeExtract %148[2 : i32] : vector<4xf16>
    %159 = spirv.CompositeExtract %148[3 : i32] : vector<4xf16>
    %160 = spirv.FAdd %156, %157 : f16
    %161 = spirv.FAdd %160, %158 : f16
    %162 = spirv.FAdd %161, %159 : f16
    %163 = spirv.FAdd %155, %162 : f16
    %164 = spirv.GroupNonUniformFAdd "Subgroup" "Reduce" %163 : f16
    %165 = spirv.FAdd %164, %cst_f16 : f16
    %166 = spirv.IEqual %1, %cst0_i32 : i32
    spirv.mlir.selection {
      spirv.BranchConditional %166, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %167 = spirv.AccessChain %__resource_var_0_2__addr[%cst0_i32, %3] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>, i32, i32
      spirv.Store "StorageBuffer" %167, %165 : f16
      spirv.Branch ^bb2
    ^bb2:  // 2 preds: ^bb0, ^bb1
      spirv.mlir.merge
    }
    spirv.Return
  }
  spirv.EntryPoint "GLCompute" @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16, @__builtin__LocalInvocationId__, @__builtin__WorkgroupId__
  spirv.ExecutionMode @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 "LocalSize", 64, 1, 1
}

// -----// IR Dump After CSE (cse) //----- //
spirv.module Logical GLSL450 {
  spirv.GlobalVariable @__builtin__WorkgroupId__ built_in("WorkgroupId") : !spirv.ptr<vector<3xi32>, Input>
  spirv.GlobalVariable @__builtin__LocalInvocationId__ built_in("LocalInvocationId") : !spirv.ptr<vector<3xi32>, Input>
  spirv.GlobalVariable @__resource_var_0_0_ bind(0, 0) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
  spirv.GlobalVariable @__resource_var_0_1_ bind(0, 1) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
  spirv.GlobalVariable @__resource_var_0_2_ bind(0, 2) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
  spirv.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() "None" {
    %cst64_i32 = spirv.Constant 64 : i32
    %cst128_i32 = spirv.Constant 128 : i32
    %cst192_i32 = spirv.Constant 192 : i32
    %cst256_i32 = spirv.Constant 256 : i32
    %cst320_i32 = spirv.Constant 320 : i32
    %cst384_i32 = spirv.Constant 384 : i32
    %cst448_i32 = spirv.Constant 448 : i32
    %cst512_i32 = spirv.Constant 512 : i32
    %cst_vec_4xf16 = spirv.Constant dense<0.000000e+00> : vector<4xf16>
    %cst0_i32 = spirv.Constant 0 : i32
    %cst_f16 = spirv.Constant 0.000000e+00 : f16
    %__builtin__LocalInvocationId___addr = spirv.mlir.addressof @__builtin__LocalInvocationId__ : !spirv.ptr<vector<3xi32>, Input>
    %0 = spirv.Load "Input" %__builtin__LocalInvocationId___addr : vector<3xi32>
    %1 = spirv.CompositeExtract %0[0 : i32] : vector<3xi32>
    %__resource_var_0_0__addr = spirv.mlir.addressof @__resource_var_0_0_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
    %__resource_var_0_1__addr = spirv.mlir.addressof @__resource_var_0_1_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
    %__resource_var_0_2__addr = spirv.mlir.addressof @__resource_var_0_2_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
    %__builtin__WorkgroupId___addr = spirv.mlir.addressof @__builtin__WorkgroupId__ : !spirv.ptr<vector<3xi32>, Input>
    %2 = spirv.Load "Input" %__builtin__WorkgroupId___addr : vector<3xi32>
    %3 = spirv.CompositeExtract %2[0 : i32] : vector<3xi32>
    %4 = spirv.IMul %3, %cst512_i32 : i32
    %5 = spirv.IAdd %1, %4 : i32
    %6 = spirv.IAdd %5, %cst448_i32 : i32
    %7 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %6] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %8 = spirv.Load "StorageBuffer" %7 : vector<4xf32>
    %9 = spirv.IAdd %1, %cst448_i32 : i32
    %10 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %9] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %11 = spirv.Load "StorageBuffer" %10 : vector<4xf32>
    %12 = spirv.IAdd %5, %cst384_i32 : i32
    %13 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %12] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %14 = spirv.Load "StorageBuffer" %13 : vector<4xf32>
    %15 = spirv.IAdd %1, %cst384_i32 : i32
    %16 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %15] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %17 = spirv.Load "StorageBuffer" %16 : vector<4xf32>
    %18 = spirv.IAdd %5, %cst320_i32 : i32
    %19 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %18] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %20 = spirv.Load "StorageBuffer" %19 : vector<4xf32>
    %21 = spirv.IAdd %1, %cst320_i32 : i32
    %22 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %21] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %23 = spirv.Load "StorageBuffer" %22 : vector<4xf32>
    %24 = spirv.IAdd %5, %cst256_i32 : i32
    %25 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %24] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %26 = spirv.Load "StorageBuffer" %25 : vector<4xf32>
    %27 = spirv.IAdd %1, %cst256_i32 : i32
    %28 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %27] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %29 = spirv.Load "StorageBuffer" %28 : vector<4xf32>
    %30 = spirv.IAdd %5, %cst192_i32 : i32
    %31 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %30] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %32 = spirv.Load "StorageBuffer" %31 : vector<4xf32>
    %33 = spirv.IAdd %1, %cst192_i32 : i32
    %34 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %33] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %35 = spirv.Load "StorageBuffer" %34 : vector<4xf32>
    %36 = spirv.IAdd %5, %cst128_i32 : i32
    %37 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %36] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %38 = spirv.Load "StorageBuffer" %37 : vector<4xf32>
    %39 = spirv.IAdd %1, %cst128_i32 : i32
    %40 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %39] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %41 = spirv.Load "StorageBuffer" %40 : vector<4xf32>
    %42 = spirv.IAdd %5, %cst64_i32 : i32
    %43 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %42] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %44 = spirv.Load "StorageBuffer" %43 : vector<4xf32>
    %45 = spirv.IAdd %1, %cst64_i32 : i32
    %46 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %45] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %47 = spirv.Load "StorageBuffer" %46 : vector<4xf32>
    %48 = spirv.IAdd %4, %1 : i32
    %49 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %48] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %50 = spirv.Load "StorageBuffer" %49 : vector<4xf32>
    %51 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %1] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %52 = spirv.Load "StorageBuffer" %51 : vector<4xf32>
    %53 = spirv.VectorShuffle [0 : i32, 1 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
    %54 = spirv.Bitcast %53 : vector<2xf32> to vector<4xf16>
    %55 = spirv.VectorShuffle [0 : i32, 1 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
    %56 = spirv.Bitcast %55 : vector<2xf32> to vector<4xf16>
    %57 = spirv.FMul %54, %56 : vector<4xf16>
    %58 = spirv.VectorShuffle [2 : i32, 3 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
    %59 = spirv.Bitcast %58 : vector<2xf32> to vector<4xf16>
    %60 = spirv.VectorShuffle [2 : i32, 3 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
    %61 = spirv.Bitcast %60 : vector<2xf32> to vector<4xf16>
    %62 = spirv.FMul %59, %61 : vector<4xf16>
    %63 = spirv.FAdd %57, %cst_vec_4xf16 : vector<4xf16>
    %64 = spirv.FAdd %62, %cst_vec_4xf16 : vector<4xf16>
    %65 = spirv.VectorShuffle [0 : i32, 1 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
    %66 = spirv.Bitcast %65 : vector<2xf32> to vector<4xf16>
    %67 = spirv.VectorShuffle [0 : i32, 1 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
    %68 = spirv.Bitcast %67 : vector<2xf32> to vector<4xf16>
    %69 = spirv.FMul %66, %68 : vector<4xf16>
    %70 = spirv.VectorShuffle [2 : i32, 3 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
    %71 = spirv.Bitcast %70 : vector<2xf32> to vector<4xf16>
    %72 = spirv.VectorShuffle [2 : i32, 3 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
    %73 = spirv.Bitcast %72 : vector<2xf32> to vector<4xf16>
    %74 = spirv.FMul %71, %73 : vector<4xf16>
    %75 = spirv.FAdd %69, %63 : vector<4xf16>
    %76 = spirv.FAdd %74, %64 : vector<4xf16>
    %77 = spirv.VectorShuffle [0 : i32, 1 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
    %78 = spirv.Bitcast %77 : vector<2xf32> to vector<4xf16>
    %79 = spirv.VectorShuffle [0 : i32, 1 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
    %80 = spirv.Bitcast %79 : vector<2xf32> to vector<4xf16>
    %81 = spirv.FMul %78, %80 : vector<4xf16>
    %82 = spirv.VectorShuffle [2 : i32, 3 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
    %83 = spirv.Bitcast %82 : vector<2xf32> to vector<4xf16>
    %84 = spirv.VectorShuffle [2 : i32, 3 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
    %85 = spirv.Bitcast %84 : vector<2xf32> to vector<4xf16>
    %86 = spirv.FMul %83, %85 : vector<4xf16>
    %87 = spirv.FAdd %81, %75 : vector<4xf16>
    %88 = spirv.FAdd %86, %76 : vector<4xf16>
    %89 = spirv.VectorShuffle [0 : i32, 1 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
    %90 = spirv.Bitcast %89 : vector<2xf32> to vector<4xf16>
    %91 = spirv.VectorShuffle [0 : i32, 1 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
    %92 = spirv.Bitcast %91 : vector<2xf32> to vector<4xf16>
    %93 = spirv.FMul %90, %92 : vector<4xf16>
    %94 = spirv.VectorShuffle [2 : i32, 3 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
    %95 = spirv.Bitcast %94 : vector<2xf32> to vector<4xf16>
    %96 = spirv.VectorShuffle [2 : i32, 3 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
    %97 = spirv.Bitcast %96 : vector<2xf32> to vector<4xf16>
    %98 = spirv.FMul %95, %97 : vector<4xf16>
    %99 = spirv.FAdd %93, %87 : vector<4xf16>
    %100 = spirv.FAdd %98, %88 : vector<4xf16>
    %101 = spirv.VectorShuffle [0 : i32, 1 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
    %102 = spirv.Bitcast %101 : vector<2xf32> to vector<4xf16>
    %103 = spirv.VectorShuffle [0 : i32, 1 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
    %104 = spirv.Bitcast %103 : vector<2xf32> to vector<4xf16>
    %105 = spirv.FMul %102, %104 : vector<4xf16>
    %106 = spirv.VectorShuffle [2 : i32, 3 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
    %107 = spirv.Bitcast %106 : vector<2xf32> to vector<4xf16>
    %108 = spirv.VectorShuffle [2 : i32, 3 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
    %109 = spirv.Bitcast %108 : vector<2xf32> to vector<4xf16>
    %110 = spirv.FMul %107, %109 : vector<4xf16>
    %111 = spirv.FAdd %105, %99 : vector<4xf16>
    %112 = spirv.FAdd %110, %100 : vector<4xf16>
    %113 = spirv.VectorShuffle [0 : i32, 1 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
    %114 = spirv.Bitcast %113 : vector<2xf32> to vector<4xf16>
    %115 = spirv.VectorShuffle [0 : i32, 1 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
    %116 = spirv.Bitcast %115 : vector<2xf32> to vector<4xf16>
    %117 = spirv.FMul %114, %116 : vector<4xf16>
    %118 = spirv.VectorShuffle [2 : i32, 3 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
    %119 = spirv.Bitcast %118 : vector<2xf32> to vector<4xf16>
    %120 = spirv.VectorShuffle [2 : i32, 3 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
    %121 = spirv.Bitcast %120 : vector<2xf32> to vector<4xf16>
    %122 = spirv.FMul %119, %121 : vector<4xf16>
    %123 = spirv.FAdd %117, %111 : vector<4xf16>
    %124 = spirv.FAdd %122, %112 : vector<4xf16>
    %125 = spirv.VectorShuffle [0 : i32, 1 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
    %126 = spirv.Bitcast %125 : vector<2xf32> to vector<4xf16>
    %127 = spirv.VectorShuffle [0 : i32, 1 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
    %128 = spirv.Bitcast %127 : vector<2xf32> to vector<4xf16>
    %129 = spirv.FMul %126, %128 : vector<4xf16>
    %130 = spirv.VectorShuffle [2 : i32, 3 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
    %131 = spirv.Bitcast %130 : vector<2xf32> to vector<4xf16>
    %132 = spirv.VectorShuffle [2 : i32, 3 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
    %133 = spirv.Bitcast %132 : vector<2xf32> to vector<4xf16>
    %134 = spirv.FMul %131, %133 : vector<4xf16>
    %135 = spirv.FAdd %129, %123 : vector<4xf16>
    %136 = spirv.FAdd %134, %124 : vector<4xf16>
    %137 = spirv.VectorShuffle [0 : i32, 1 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
    %138 = spirv.Bitcast %137 : vector<2xf32> to vector<4xf16>
    %139 = spirv.VectorShuffle [0 : i32, 1 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
    %140 = spirv.Bitcast %139 : vector<2xf32> to vector<4xf16>
    %141 = spirv.FMul %138, %140 : vector<4xf16>
    %142 = spirv.VectorShuffle [2 : i32, 3 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
    %143 = spirv.Bitcast %142 : vector<2xf32> to vector<4xf16>
    %144 = spirv.VectorShuffle [2 : i32, 3 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
    %145 = spirv.Bitcast %144 : vector<2xf32> to vector<4xf16>
    %146 = spirv.FMul %143, %145 : vector<4xf16>
    %147 = spirv.FAdd %141, %135 : vector<4xf16>
    %148 = spirv.FAdd %146, %136 : vector<4xf16>
    %149 = spirv.CompositeExtract %147[0 : i32] : vector<4xf16>
    %150 = spirv.CompositeExtract %147[1 : i32] : vector<4xf16>
    %151 = spirv.CompositeExtract %147[2 : i32] : vector<4xf16>
    %152 = spirv.CompositeExtract %147[3 : i32] : vector<4xf16>
    %153 = spirv.FAdd %149, %150 : f16
    %154 = spirv.FAdd %153, %151 : f16
    %155 = spirv.FAdd %154, %152 : f16
    %156 = spirv.CompositeExtract %148[0 : i32] : vector<4xf16>
    %157 = spirv.CompositeExtract %148[1 : i32] : vector<4xf16>
    %158 = spirv.CompositeExtract %148[2 : i32] : vector<4xf16>
    %159 = spirv.CompositeExtract %148[3 : i32] : vector<4xf16>
    %160 = spirv.FAdd %156, %157 : f16
    %161 = spirv.FAdd %160, %158 : f16
    %162 = spirv.FAdd %161, %159 : f16
    %163 = spirv.FAdd %155, %162 : f16
    %164 = spirv.GroupNonUniformFAdd "Subgroup" "Reduce" %163 : f16
    %165 = spirv.FAdd %164, %cst_f16 : f16
    %166 = spirv.IEqual %1, %cst0_i32 : i32
    spirv.mlir.selection {
      spirv.BranchConditional %166, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %167 = spirv.AccessChain %__resource_var_0_2__addr[%cst0_i32, %3] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>, i32, i32
      spirv.Store "StorageBuffer" %167, %165 : f16
      spirv.Branch ^bb2
    ^bb2:  // 2 preds: ^bb0, ^bb1
      spirv.mlir.merge
    }
    spirv.Return
  }
  spirv.EntryPoint "GLCompute" @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16, @__builtin__LocalInvocationId__, @__builtin__WorkgroupId__
  spirv.ExecutionMode @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 "LocalSize", 64, 1, 1
}

// -----// IR Dump After SPIRVRewriteInsertsPass (spirv-rewrite-inserts) //----- //
spirv.module Logical GLSL450 {
  spirv.GlobalVariable @__builtin__WorkgroupId__ built_in("WorkgroupId") : !spirv.ptr<vector<3xi32>, Input>
  spirv.GlobalVariable @__builtin__LocalInvocationId__ built_in("LocalInvocationId") : !spirv.ptr<vector<3xi32>, Input>
  spirv.GlobalVariable @__resource_var_0_0_ bind(0, 0) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
  spirv.GlobalVariable @__resource_var_0_1_ bind(0, 1) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
  spirv.GlobalVariable @__resource_var_0_2_ bind(0, 2) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
  spirv.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() "None" {
    %cst64_i32 = spirv.Constant 64 : i32
    %cst128_i32 = spirv.Constant 128 : i32
    %cst192_i32 = spirv.Constant 192 : i32
    %cst256_i32 = spirv.Constant 256 : i32
    %cst320_i32 = spirv.Constant 320 : i32
    %cst384_i32 = spirv.Constant 384 : i32
    %cst448_i32 = spirv.Constant 448 : i32
    %cst512_i32 = spirv.Constant 512 : i32
    %cst_vec_4xf16 = spirv.Constant dense<0.000000e+00> : vector<4xf16>
    %cst0_i32 = spirv.Constant 0 : i32
    %cst_f16 = spirv.Constant 0.000000e+00 : f16
    %__builtin__LocalInvocationId___addr = spirv.mlir.addressof @__builtin__LocalInvocationId__ : !spirv.ptr<vector<3xi32>, Input>
    %0 = spirv.Load "Input" %__builtin__LocalInvocationId___addr : vector<3xi32>
    %1 = spirv.CompositeExtract %0[0 : i32] : vector<3xi32>
    %__resource_var_0_0__addr = spirv.mlir.addressof @__resource_var_0_0_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
    %__resource_var_0_1__addr = spirv.mlir.addressof @__resource_var_0_1_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
    %__resource_var_0_2__addr = spirv.mlir.addressof @__resource_var_0_2_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
    %__builtin__WorkgroupId___addr = spirv.mlir.addressof @__builtin__WorkgroupId__ : !spirv.ptr<vector<3xi32>, Input>
    %2 = spirv.Load "Input" %__builtin__WorkgroupId___addr : vector<3xi32>
    %3 = spirv.CompositeExtract %2[0 : i32] : vector<3xi32>
    %4 = spirv.IMul %3, %cst512_i32 : i32
    %5 = spirv.IAdd %1, %4 : i32
    %6 = spirv.IAdd %5, %cst448_i32 : i32
    %7 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %6] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %8 = spirv.Load "StorageBuffer" %7 : vector<4xf32>
    %9 = spirv.IAdd %1, %cst448_i32 : i32
    %10 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %9] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %11 = spirv.Load "StorageBuffer" %10 : vector<4xf32>
    %12 = spirv.IAdd %5, %cst384_i32 : i32
    %13 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %12] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %14 = spirv.Load "StorageBuffer" %13 : vector<4xf32>
    %15 = spirv.IAdd %1, %cst384_i32 : i32
    %16 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %15] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %17 = spirv.Load "StorageBuffer" %16 : vector<4xf32>
    %18 = spirv.IAdd %5, %cst320_i32 : i32
    %19 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %18] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %20 = spirv.Load "StorageBuffer" %19 : vector<4xf32>
    %21 = spirv.IAdd %1, %cst320_i32 : i32
    %22 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %21] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %23 = spirv.Load "StorageBuffer" %22 : vector<4xf32>
    %24 = spirv.IAdd %5, %cst256_i32 : i32
    %25 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %24] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %26 = spirv.Load "StorageBuffer" %25 : vector<4xf32>
    %27 = spirv.IAdd %1, %cst256_i32 : i32
    %28 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %27] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %29 = spirv.Load "StorageBuffer" %28 : vector<4xf32>
    %30 = spirv.IAdd %5, %cst192_i32 : i32
    %31 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %30] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %32 = spirv.Load "StorageBuffer" %31 : vector<4xf32>
    %33 = spirv.IAdd %1, %cst192_i32 : i32
    %34 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %33] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %35 = spirv.Load "StorageBuffer" %34 : vector<4xf32>
    %36 = spirv.IAdd %5, %cst128_i32 : i32
    %37 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %36] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %38 = spirv.Load "StorageBuffer" %37 : vector<4xf32>
    %39 = spirv.IAdd %1, %cst128_i32 : i32
    %40 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %39] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %41 = spirv.Load "StorageBuffer" %40 : vector<4xf32>
    %42 = spirv.IAdd %5, %cst64_i32 : i32
    %43 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %42] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %44 = spirv.Load "StorageBuffer" %43 : vector<4xf32>
    %45 = spirv.IAdd %1, %cst64_i32 : i32
    %46 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %45] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %47 = spirv.Load "StorageBuffer" %46 : vector<4xf32>
    %48 = spirv.IAdd %4, %1 : i32
    %49 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %48] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %50 = spirv.Load "StorageBuffer" %49 : vector<4xf32>
    %51 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %1] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %52 = spirv.Load "StorageBuffer" %51 : vector<4xf32>
    %53 = spirv.VectorShuffle [0 : i32, 1 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
    %54 = spirv.Bitcast %53 : vector<2xf32> to vector<4xf16>
    %55 = spirv.VectorShuffle [0 : i32, 1 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
    %56 = spirv.Bitcast %55 : vector<2xf32> to vector<4xf16>
    %57 = spirv.FMul %54, %56 : vector<4xf16>
    %58 = spirv.VectorShuffle [2 : i32, 3 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
    %59 = spirv.Bitcast %58 : vector<2xf32> to vector<4xf16>
    %60 = spirv.VectorShuffle [2 : i32, 3 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
    %61 = spirv.Bitcast %60 : vector<2xf32> to vector<4xf16>
    %62 = spirv.FMul %59, %61 : vector<4xf16>
    %63 = spirv.FAdd %57, %cst_vec_4xf16 : vector<4xf16>
    %64 = spirv.FAdd %62, %cst_vec_4xf16 : vector<4xf16>
    %65 = spirv.VectorShuffle [0 : i32, 1 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
    %66 = spirv.Bitcast %65 : vector<2xf32> to vector<4xf16>
    %67 = spirv.VectorShuffle [0 : i32, 1 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
    %68 = spirv.Bitcast %67 : vector<2xf32> to vector<4xf16>
    %69 = spirv.FMul %66, %68 : vector<4xf16>
    %70 = spirv.VectorShuffle [2 : i32, 3 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
    %71 = spirv.Bitcast %70 : vector<2xf32> to vector<4xf16>
    %72 = spirv.VectorShuffle [2 : i32, 3 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
    %73 = spirv.Bitcast %72 : vector<2xf32> to vector<4xf16>
    %74 = spirv.FMul %71, %73 : vector<4xf16>
    %75 = spirv.FAdd %69, %63 : vector<4xf16>
    %76 = spirv.FAdd %74, %64 : vector<4xf16>
    %77 = spirv.VectorShuffle [0 : i32, 1 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
    %78 = spirv.Bitcast %77 : vector<2xf32> to vector<4xf16>
    %79 = spirv.VectorShuffle [0 : i32, 1 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
    %80 = spirv.Bitcast %79 : vector<2xf32> to vector<4xf16>
    %81 = spirv.FMul %78, %80 : vector<4xf16>
    %82 = spirv.VectorShuffle [2 : i32, 3 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
    %83 = spirv.Bitcast %82 : vector<2xf32> to vector<4xf16>
    %84 = spirv.VectorShuffle [2 : i32, 3 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
    %85 = spirv.Bitcast %84 : vector<2xf32> to vector<4xf16>
    %86 = spirv.FMul %83, %85 : vector<4xf16>
    %87 = spirv.FAdd %81, %75 : vector<4xf16>
    %88 = spirv.FAdd %86, %76 : vector<4xf16>
    %89 = spirv.VectorShuffle [0 : i32, 1 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
    %90 = spirv.Bitcast %89 : vector<2xf32> to vector<4xf16>
    %91 = spirv.VectorShuffle [0 : i32, 1 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
    %92 = spirv.Bitcast %91 : vector<2xf32> to vector<4xf16>
    %93 = spirv.FMul %90, %92 : vector<4xf16>
    %94 = spirv.VectorShuffle [2 : i32, 3 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
    %95 = spirv.Bitcast %94 : vector<2xf32> to vector<4xf16>
    %96 = spirv.VectorShuffle [2 : i32, 3 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
    %97 = spirv.Bitcast %96 : vector<2xf32> to vector<4xf16>
    %98 = spirv.FMul %95, %97 : vector<4xf16>
    %99 = spirv.FAdd %93, %87 : vector<4xf16>
    %100 = spirv.FAdd %98, %88 : vector<4xf16>
    %101 = spirv.VectorShuffle [0 : i32, 1 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
    %102 = spirv.Bitcast %101 : vector<2xf32> to vector<4xf16>
    %103 = spirv.VectorShuffle [0 : i32, 1 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
    %104 = spirv.Bitcast %103 : vector<2xf32> to vector<4xf16>
    %105 = spirv.FMul %102, %104 : vector<4xf16>
    %106 = spirv.VectorShuffle [2 : i32, 3 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
    %107 = spirv.Bitcast %106 : vector<2xf32> to vector<4xf16>
    %108 = spirv.VectorShuffle [2 : i32, 3 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
    %109 = spirv.Bitcast %108 : vector<2xf32> to vector<4xf16>
    %110 = spirv.FMul %107, %109 : vector<4xf16>
    %111 = spirv.FAdd %105, %99 : vector<4xf16>
    %112 = spirv.FAdd %110, %100 : vector<4xf16>
    %113 = spirv.VectorShuffle [0 : i32, 1 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
    %114 = spirv.Bitcast %113 : vector<2xf32> to vector<4xf16>
    %115 = spirv.VectorShuffle [0 : i32, 1 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
    %116 = spirv.Bitcast %115 : vector<2xf32> to vector<4xf16>
    %117 = spirv.FMul %114, %116 : vector<4xf16>
    %118 = spirv.VectorShuffle [2 : i32, 3 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
    %119 = spirv.Bitcast %118 : vector<2xf32> to vector<4xf16>
    %120 = spirv.VectorShuffle [2 : i32, 3 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
    %121 = spirv.Bitcast %120 : vector<2xf32> to vector<4xf16>
    %122 = spirv.FMul %119, %121 : vector<4xf16>
    %123 = spirv.FAdd %117, %111 : vector<4xf16>
    %124 = spirv.FAdd %122, %112 : vector<4xf16>
    %125 = spirv.VectorShuffle [0 : i32, 1 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
    %126 = spirv.Bitcast %125 : vector<2xf32> to vector<4xf16>
    %127 = spirv.VectorShuffle [0 : i32, 1 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
    %128 = spirv.Bitcast %127 : vector<2xf32> to vector<4xf16>
    %129 = spirv.FMul %126, %128 : vector<4xf16>
    %130 = spirv.VectorShuffle [2 : i32, 3 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
    %131 = spirv.Bitcast %130 : vector<2xf32> to vector<4xf16>
    %132 = spirv.VectorShuffle [2 : i32, 3 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
    %133 = spirv.Bitcast %132 : vector<2xf32> to vector<4xf16>
    %134 = spirv.FMul %131, %133 : vector<4xf16>
    %135 = spirv.FAdd %129, %123 : vector<4xf16>
    %136 = spirv.FAdd %134, %124 : vector<4xf16>
    %137 = spirv.VectorShuffle [0 : i32, 1 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
    %138 = spirv.Bitcast %137 : vector<2xf32> to vector<4xf16>
    %139 = spirv.VectorShuffle [0 : i32, 1 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
    %140 = spirv.Bitcast %139 : vector<2xf32> to vector<4xf16>
    %141 = spirv.FMul %138, %140 : vector<4xf16>
    %142 = spirv.VectorShuffle [2 : i32, 3 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
    %143 = spirv.Bitcast %142 : vector<2xf32> to vector<4xf16>
    %144 = spirv.VectorShuffle [2 : i32, 3 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
    %145 = spirv.Bitcast %144 : vector<2xf32> to vector<4xf16>
    %146 = spirv.FMul %143, %145 : vector<4xf16>
    %147 = spirv.FAdd %141, %135 : vector<4xf16>
    %148 = spirv.FAdd %146, %136 : vector<4xf16>
    %149 = spirv.CompositeExtract %147[0 : i32] : vector<4xf16>
    %150 = spirv.CompositeExtract %147[1 : i32] : vector<4xf16>
    %151 = spirv.CompositeExtract %147[2 : i32] : vector<4xf16>
    %152 = spirv.CompositeExtract %147[3 : i32] : vector<4xf16>
    %153 = spirv.FAdd %149, %150 : f16
    %154 = spirv.FAdd %153, %151 : f16
    %155 = spirv.FAdd %154, %152 : f16
    %156 = spirv.CompositeExtract %148[0 : i32] : vector<4xf16>
    %157 = spirv.CompositeExtract %148[1 : i32] : vector<4xf16>
    %158 = spirv.CompositeExtract %148[2 : i32] : vector<4xf16>
    %159 = spirv.CompositeExtract %148[3 : i32] : vector<4xf16>
    %160 = spirv.FAdd %156, %157 : f16
    %161 = spirv.FAdd %160, %158 : f16
    %162 = spirv.FAdd %161, %159 : f16
    %163 = spirv.FAdd %155, %162 : f16
    %164 = spirv.GroupNonUniformFAdd "Subgroup" "Reduce" %163 : f16
    %165 = spirv.FAdd %164, %cst_f16 : f16
    %166 = spirv.IEqual %1, %cst0_i32 : i32
    spirv.mlir.selection {
      spirv.BranchConditional %166, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %167 = spirv.AccessChain %__resource_var_0_2__addr[%cst0_i32, %3] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>, i32, i32
      spirv.Store "StorageBuffer" %167, %165 : f16
      spirv.Branch ^bb2
    ^bb2:  // 2 preds: ^bb0, ^bb1
      spirv.mlir.merge
    }
    spirv.Return
  }
  spirv.EntryPoint "GLCompute" @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16, @__builtin__LocalInvocationId__, @__builtin__WorkgroupId__
  spirv.ExecutionMode @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 "LocalSize", 64, 1, 1
}

// -----// IR Dump After SPIRVCanonicalizeGLPass (spirv-canonicalize-gl) //----- //
spirv.module Logical GLSL450 {
  spirv.GlobalVariable @__builtin__WorkgroupId__ built_in("WorkgroupId") : !spirv.ptr<vector<3xi32>, Input>
  spirv.GlobalVariable @__builtin__LocalInvocationId__ built_in("LocalInvocationId") : !spirv.ptr<vector<3xi32>, Input>
  spirv.GlobalVariable @__resource_var_0_0_ bind(0, 0) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
  spirv.GlobalVariable @__resource_var_0_1_ bind(0, 1) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
  spirv.GlobalVariable @__resource_var_0_2_ bind(0, 2) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
  spirv.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() "None" {
    %cst64_i32 = spirv.Constant 64 : i32
    %cst128_i32 = spirv.Constant 128 : i32
    %cst192_i32 = spirv.Constant 192 : i32
    %cst256_i32 = spirv.Constant 256 : i32
    %cst320_i32 = spirv.Constant 320 : i32
    %cst384_i32 = spirv.Constant 384 : i32
    %cst448_i32 = spirv.Constant 448 : i32
    %cst512_i32 = spirv.Constant 512 : i32
    %cst_vec_4xf16 = spirv.Constant dense<0.000000e+00> : vector<4xf16>
    %cst0_i32 = spirv.Constant 0 : i32
    %cst_f16 = spirv.Constant 0.000000e+00 : f16
    %__builtin__LocalInvocationId___addr = spirv.mlir.addressof @__builtin__LocalInvocationId__ : !spirv.ptr<vector<3xi32>, Input>
    %0 = spirv.Load "Input" %__builtin__LocalInvocationId___addr : vector<3xi32>
    %1 = spirv.CompositeExtract %0[0 : i32] : vector<3xi32>
    %__resource_var_0_0__addr = spirv.mlir.addressof @__resource_var_0_0_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
    %__resource_var_0_1__addr = spirv.mlir.addressof @__resource_var_0_1_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
    %__resource_var_0_2__addr = spirv.mlir.addressof @__resource_var_0_2_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
    %__builtin__WorkgroupId___addr = spirv.mlir.addressof @__builtin__WorkgroupId__ : !spirv.ptr<vector<3xi32>, Input>
    %2 = spirv.Load "Input" %__builtin__WorkgroupId___addr : vector<3xi32>
    %3 = spirv.CompositeExtract %2[0 : i32] : vector<3xi32>
    %4 = spirv.IMul %3, %cst512_i32 : i32
    %5 = spirv.IAdd %1, %4 : i32
    %6 = spirv.IAdd %5, %cst448_i32 : i32
    %7 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %6] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %8 = spirv.Load "StorageBuffer" %7 : vector<4xf32>
    %9 = spirv.IAdd %1, %cst448_i32 : i32
    %10 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %9] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %11 = spirv.Load "StorageBuffer" %10 : vector<4xf32>
    %12 = spirv.IAdd %5, %cst384_i32 : i32
    %13 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %12] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %14 = spirv.Load "StorageBuffer" %13 : vector<4xf32>
    %15 = spirv.IAdd %1, %cst384_i32 : i32
    %16 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %15] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %17 = spirv.Load "StorageBuffer" %16 : vector<4xf32>
    %18 = spirv.IAdd %5, %cst320_i32 : i32
    %19 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %18] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %20 = spirv.Load "StorageBuffer" %19 : vector<4xf32>
    %21 = spirv.IAdd %1, %cst320_i32 : i32
    %22 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %21] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %23 = spirv.Load "StorageBuffer" %22 : vector<4xf32>
    %24 = spirv.IAdd %5, %cst256_i32 : i32
    %25 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %24] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %26 = spirv.Load "StorageBuffer" %25 : vector<4xf32>
    %27 = spirv.IAdd %1, %cst256_i32 : i32
    %28 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %27] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %29 = spirv.Load "StorageBuffer" %28 : vector<4xf32>
    %30 = spirv.IAdd %5, %cst192_i32 : i32
    %31 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %30] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %32 = spirv.Load "StorageBuffer" %31 : vector<4xf32>
    %33 = spirv.IAdd %1, %cst192_i32 : i32
    %34 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %33] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %35 = spirv.Load "StorageBuffer" %34 : vector<4xf32>
    %36 = spirv.IAdd %5, %cst128_i32 : i32
    %37 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %36] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %38 = spirv.Load "StorageBuffer" %37 : vector<4xf32>
    %39 = spirv.IAdd %1, %cst128_i32 : i32
    %40 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %39] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %41 = spirv.Load "StorageBuffer" %40 : vector<4xf32>
    %42 = spirv.IAdd %5, %cst64_i32 : i32
    %43 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %42] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %44 = spirv.Load "StorageBuffer" %43 : vector<4xf32>
    %45 = spirv.IAdd %1, %cst64_i32 : i32
    %46 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %45] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %47 = spirv.Load "StorageBuffer" %46 : vector<4xf32>
    %48 = spirv.IAdd %4, %1 : i32
    %49 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %48] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %50 = spirv.Load "StorageBuffer" %49 : vector<4xf32>
    %51 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %1] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %52 = spirv.Load "StorageBuffer" %51 : vector<4xf32>
    %53 = spirv.VectorShuffle [0 : i32, 1 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
    %54 = spirv.Bitcast %53 : vector<2xf32> to vector<4xf16>
    %55 = spirv.VectorShuffle [0 : i32, 1 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
    %56 = spirv.Bitcast %55 : vector<2xf32> to vector<4xf16>
    %57 = spirv.FMul %54, %56 : vector<4xf16>
    %58 = spirv.VectorShuffle [2 : i32, 3 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
    %59 = spirv.Bitcast %58 : vector<2xf32> to vector<4xf16>
    %60 = spirv.VectorShuffle [2 : i32, 3 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
    %61 = spirv.Bitcast %60 : vector<2xf32> to vector<4xf16>
    %62 = spirv.FMul %59, %61 : vector<4xf16>
    %63 = spirv.FAdd %57, %cst_vec_4xf16 : vector<4xf16>
    %64 = spirv.FAdd %62, %cst_vec_4xf16 : vector<4xf16>
    %65 = spirv.VectorShuffle [0 : i32, 1 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
    %66 = spirv.Bitcast %65 : vector<2xf32> to vector<4xf16>
    %67 = spirv.VectorShuffle [0 : i32, 1 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
    %68 = spirv.Bitcast %67 : vector<2xf32> to vector<4xf16>
    %69 = spirv.FMul %66, %68 : vector<4xf16>
    %70 = spirv.VectorShuffle [2 : i32, 3 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
    %71 = spirv.Bitcast %70 : vector<2xf32> to vector<4xf16>
    %72 = spirv.VectorShuffle [2 : i32, 3 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
    %73 = spirv.Bitcast %72 : vector<2xf32> to vector<4xf16>
    %74 = spirv.FMul %71, %73 : vector<4xf16>
    %75 = spirv.FAdd %69, %63 : vector<4xf16>
    %76 = spirv.FAdd %74, %64 : vector<4xf16>
    %77 = spirv.VectorShuffle [0 : i32, 1 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
    %78 = spirv.Bitcast %77 : vector<2xf32> to vector<4xf16>
    %79 = spirv.VectorShuffle [0 : i32, 1 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
    %80 = spirv.Bitcast %79 : vector<2xf32> to vector<4xf16>
    %81 = spirv.FMul %78, %80 : vector<4xf16>
    %82 = spirv.VectorShuffle [2 : i32, 3 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
    %83 = spirv.Bitcast %82 : vector<2xf32> to vector<4xf16>
    %84 = spirv.VectorShuffle [2 : i32, 3 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
    %85 = spirv.Bitcast %84 : vector<2xf32> to vector<4xf16>
    %86 = spirv.FMul %83, %85 : vector<4xf16>
    %87 = spirv.FAdd %81, %75 : vector<4xf16>
    %88 = spirv.FAdd %86, %76 : vector<4xf16>
    %89 = spirv.VectorShuffle [0 : i32, 1 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
    %90 = spirv.Bitcast %89 : vector<2xf32> to vector<4xf16>
    %91 = spirv.VectorShuffle [0 : i32, 1 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
    %92 = spirv.Bitcast %91 : vector<2xf32> to vector<4xf16>
    %93 = spirv.FMul %90, %92 : vector<4xf16>
    %94 = spirv.VectorShuffle [2 : i32, 3 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
    %95 = spirv.Bitcast %94 : vector<2xf32> to vector<4xf16>
    %96 = spirv.VectorShuffle [2 : i32, 3 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
    %97 = spirv.Bitcast %96 : vector<2xf32> to vector<4xf16>
    %98 = spirv.FMul %95, %97 : vector<4xf16>
    %99 = spirv.FAdd %93, %87 : vector<4xf16>
    %100 = spirv.FAdd %98, %88 : vector<4xf16>
    %101 = spirv.VectorShuffle [0 : i32, 1 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
    %102 = spirv.Bitcast %101 : vector<2xf32> to vector<4xf16>
    %103 = spirv.VectorShuffle [0 : i32, 1 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
    %104 = spirv.Bitcast %103 : vector<2xf32> to vector<4xf16>
    %105 = spirv.FMul %102, %104 : vector<4xf16>
    %106 = spirv.VectorShuffle [2 : i32, 3 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
    %107 = spirv.Bitcast %106 : vector<2xf32> to vector<4xf16>
    %108 = spirv.VectorShuffle [2 : i32, 3 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
    %109 = spirv.Bitcast %108 : vector<2xf32> to vector<4xf16>
    %110 = spirv.FMul %107, %109 : vector<4xf16>
    %111 = spirv.FAdd %105, %99 : vector<4xf16>
    %112 = spirv.FAdd %110, %100 : vector<4xf16>
    %113 = spirv.VectorShuffle [0 : i32, 1 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
    %114 = spirv.Bitcast %113 : vector<2xf32> to vector<4xf16>
    %115 = spirv.VectorShuffle [0 : i32, 1 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
    %116 = spirv.Bitcast %115 : vector<2xf32> to vector<4xf16>
    %117 = spirv.FMul %114, %116 : vector<4xf16>
    %118 = spirv.VectorShuffle [2 : i32, 3 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
    %119 = spirv.Bitcast %118 : vector<2xf32> to vector<4xf16>
    %120 = spirv.VectorShuffle [2 : i32, 3 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
    %121 = spirv.Bitcast %120 : vector<2xf32> to vector<4xf16>
    %122 = spirv.FMul %119, %121 : vector<4xf16>
    %123 = spirv.FAdd %117, %111 : vector<4xf16>
    %124 = spirv.FAdd %122, %112 : vector<4xf16>
    %125 = spirv.VectorShuffle [0 : i32, 1 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
    %126 = spirv.Bitcast %125 : vector<2xf32> to vector<4xf16>
    %127 = spirv.VectorShuffle [0 : i32, 1 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
    %128 = spirv.Bitcast %127 : vector<2xf32> to vector<4xf16>
    %129 = spirv.FMul %126, %128 : vector<4xf16>
    %130 = spirv.VectorShuffle [2 : i32, 3 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
    %131 = spirv.Bitcast %130 : vector<2xf32> to vector<4xf16>
    %132 = spirv.VectorShuffle [2 : i32, 3 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
    %133 = spirv.Bitcast %132 : vector<2xf32> to vector<4xf16>
    %134 = spirv.FMul %131, %133 : vector<4xf16>
    %135 = spirv.FAdd %129, %123 : vector<4xf16>
    %136 = spirv.FAdd %134, %124 : vector<4xf16>
    %137 = spirv.VectorShuffle [0 : i32, 1 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
    %138 = spirv.Bitcast %137 : vector<2xf32> to vector<4xf16>
    %139 = spirv.VectorShuffle [0 : i32, 1 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
    %140 = spirv.Bitcast %139 : vector<2xf32> to vector<4xf16>
    %141 = spirv.FMul %138, %140 : vector<4xf16>
    %142 = spirv.VectorShuffle [2 : i32, 3 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
    %143 = spirv.Bitcast %142 : vector<2xf32> to vector<4xf16>
    %144 = spirv.VectorShuffle [2 : i32, 3 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
    %145 = spirv.Bitcast %144 : vector<2xf32> to vector<4xf16>
    %146 = spirv.FMul %143, %145 : vector<4xf16>
    %147 = spirv.FAdd %141, %135 : vector<4xf16>
    %148 = spirv.FAdd %146, %136 : vector<4xf16>
    %149 = spirv.CompositeExtract %147[0 : i32] : vector<4xf16>
    %150 = spirv.CompositeExtract %147[1 : i32] : vector<4xf16>
    %151 = spirv.CompositeExtract %147[2 : i32] : vector<4xf16>
    %152 = spirv.CompositeExtract %147[3 : i32] : vector<4xf16>
    %153 = spirv.FAdd %149, %150 : f16
    %154 = spirv.FAdd %153, %151 : f16
    %155 = spirv.FAdd %154, %152 : f16
    %156 = spirv.CompositeExtract %148[0 : i32] : vector<4xf16>
    %157 = spirv.CompositeExtract %148[1 : i32] : vector<4xf16>
    %158 = spirv.CompositeExtract %148[2 : i32] : vector<4xf16>
    %159 = spirv.CompositeExtract %148[3 : i32] : vector<4xf16>
    %160 = spirv.FAdd %156, %157 : f16
    %161 = spirv.FAdd %160, %158 : f16
    %162 = spirv.FAdd %161, %159 : f16
    %163 = spirv.FAdd %155, %162 : f16
    %164 = spirv.GroupNonUniformFAdd "Subgroup" "Reduce" %163 : f16
    %165 = spirv.FAdd %164, %cst_f16 : f16
    %166 = spirv.IEqual %1, %cst0_i32 : i32
    spirv.mlir.selection {
      spirv.BranchConditional %166, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %167 = spirv.AccessChain %__resource_var_0_2__addr[%cst0_i32, %3] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>, i32, i32
      spirv.Store "StorageBuffer" %167, %165 : f16
      spirv.Branch ^bb2
    ^bb2:  // 2 preds: ^bb0, ^bb1
      spirv.mlir.merge
    }
    spirv.Return
  }
  spirv.EntryPoint "GLCompute" @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16, @__builtin__LocalInvocationId__, @__builtin__WorkgroupId__
  spirv.ExecutionMode @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 "LocalSize", 64, 1, 1
}

// -----// IR Dump After SPIRVUpdateVCEPass (spirv-update-vce) //----- //
spirv.module Logical GLSL450 requires #spirv.vce<v1.3, [Shader, StorageBuffer16BitAccess, Float16, GroupNonUniformArithmetic], [SPV_KHR_storage_buffer_storage_class, SPV_KHR_16bit_storage]> {
  spirv.GlobalVariable @__builtin__WorkgroupId__ built_in("WorkgroupId") : !spirv.ptr<vector<3xi32>, Input>
  spirv.GlobalVariable @__builtin__LocalInvocationId__ built_in("LocalInvocationId") : !spirv.ptr<vector<3xi32>, Input>
  spirv.GlobalVariable @__resource_var_0_0_ bind(0, 0) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
  spirv.GlobalVariable @__resource_var_0_1_ bind(0, 1) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
  spirv.GlobalVariable @__resource_var_0_2_ bind(0, 2) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
  spirv.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() "None" {
    %cst64_i32 = spirv.Constant 64 : i32
    %cst128_i32 = spirv.Constant 128 : i32
    %cst192_i32 = spirv.Constant 192 : i32
    %cst256_i32 = spirv.Constant 256 : i32
    %cst320_i32 = spirv.Constant 320 : i32
    %cst384_i32 = spirv.Constant 384 : i32
    %cst448_i32 = spirv.Constant 448 : i32
    %cst512_i32 = spirv.Constant 512 : i32
    %cst_vec_4xf16 = spirv.Constant dense<0.000000e+00> : vector<4xf16>
    %cst0_i32 = spirv.Constant 0 : i32
    %cst_f16 = spirv.Constant 0.000000e+00 : f16
    %__builtin__LocalInvocationId___addr = spirv.mlir.addressof @__builtin__LocalInvocationId__ : !spirv.ptr<vector<3xi32>, Input>
    %0 = spirv.Load "Input" %__builtin__LocalInvocationId___addr : vector<3xi32>
    %1 = spirv.CompositeExtract %0[0 : i32] : vector<3xi32>
    %__resource_var_0_0__addr = spirv.mlir.addressof @__resource_var_0_0_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
    %__resource_var_0_1__addr = spirv.mlir.addressof @__resource_var_0_1_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
    %__resource_var_0_2__addr = spirv.mlir.addressof @__resource_var_0_2_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
    %__builtin__WorkgroupId___addr = spirv.mlir.addressof @__builtin__WorkgroupId__ : !spirv.ptr<vector<3xi32>, Input>
    %2 = spirv.Load "Input" %__builtin__WorkgroupId___addr : vector<3xi32>
    %3 = spirv.CompositeExtract %2[0 : i32] : vector<3xi32>
    %4 = spirv.IMul %3, %cst512_i32 : i32
    %5 = spirv.IAdd %1, %4 : i32
    %6 = spirv.IAdd %5, %cst448_i32 : i32
    %7 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %6] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %8 = spirv.Load "StorageBuffer" %7 : vector<4xf32>
    %9 = spirv.IAdd %1, %cst448_i32 : i32
    %10 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %9] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %11 = spirv.Load "StorageBuffer" %10 : vector<4xf32>
    %12 = spirv.IAdd %5, %cst384_i32 : i32
    %13 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %12] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %14 = spirv.Load "StorageBuffer" %13 : vector<4xf32>
    %15 = spirv.IAdd %1, %cst384_i32 : i32
    %16 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %15] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %17 = spirv.Load "StorageBuffer" %16 : vector<4xf32>
    %18 = spirv.IAdd %5, %cst320_i32 : i32
    %19 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %18] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %20 = spirv.Load "StorageBuffer" %19 : vector<4xf32>
    %21 = spirv.IAdd %1, %cst320_i32 : i32
    %22 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %21] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %23 = spirv.Load "StorageBuffer" %22 : vector<4xf32>
    %24 = spirv.IAdd %5, %cst256_i32 : i32
    %25 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %24] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %26 = spirv.Load "StorageBuffer" %25 : vector<4xf32>
    %27 = spirv.IAdd %1, %cst256_i32 : i32
    %28 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %27] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %29 = spirv.Load "StorageBuffer" %28 : vector<4xf32>
    %30 = spirv.IAdd %5, %cst192_i32 : i32
    %31 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %30] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %32 = spirv.Load "StorageBuffer" %31 : vector<4xf32>
    %33 = spirv.IAdd %1, %cst192_i32 : i32
    %34 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %33] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %35 = spirv.Load "StorageBuffer" %34 : vector<4xf32>
    %36 = spirv.IAdd %5, %cst128_i32 : i32
    %37 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %36] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %38 = spirv.Load "StorageBuffer" %37 : vector<4xf32>
    %39 = spirv.IAdd %1, %cst128_i32 : i32
    %40 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %39] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %41 = spirv.Load "StorageBuffer" %40 : vector<4xf32>
    %42 = spirv.IAdd %5, %cst64_i32 : i32
    %43 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %42] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %44 = spirv.Load "StorageBuffer" %43 : vector<4xf32>
    %45 = spirv.IAdd %1, %cst64_i32 : i32
    %46 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %45] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %47 = spirv.Load "StorageBuffer" %46 : vector<4xf32>
    %48 = spirv.IAdd %4, %1 : i32
    %49 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %48] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %50 = spirv.Load "StorageBuffer" %49 : vector<4xf32>
    %51 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %1] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
    %52 = spirv.Load "StorageBuffer" %51 : vector<4xf32>
    %53 = spirv.VectorShuffle [0 : i32, 1 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
    %54 = spirv.Bitcast %53 : vector<2xf32> to vector<4xf16>
    %55 = spirv.VectorShuffle [0 : i32, 1 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
    %56 = spirv.Bitcast %55 : vector<2xf32> to vector<4xf16>
    %57 = spirv.FMul %54, %56 : vector<4xf16>
    %58 = spirv.VectorShuffle [2 : i32, 3 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
    %59 = spirv.Bitcast %58 : vector<2xf32> to vector<4xf16>
    %60 = spirv.VectorShuffle [2 : i32, 3 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
    %61 = spirv.Bitcast %60 : vector<2xf32> to vector<4xf16>
    %62 = spirv.FMul %59, %61 : vector<4xf16>
    %63 = spirv.FAdd %57, %cst_vec_4xf16 : vector<4xf16>
    %64 = spirv.FAdd %62, %cst_vec_4xf16 : vector<4xf16>
    %65 = spirv.VectorShuffle [0 : i32, 1 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
    %66 = spirv.Bitcast %65 : vector<2xf32> to vector<4xf16>
    %67 = spirv.VectorShuffle [0 : i32, 1 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
    %68 = spirv.Bitcast %67 : vector<2xf32> to vector<4xf16>
    %69 = spirv.FMul %66, %68 : vector<4xf16>
    %70 = spirv.VectorShuffle [2 : i32, 3 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
    %71 = spirv.Bitcast %70 : vector<2xf32> to vector<4xf16>
    %72 = spirv.VectorShuffle [2 : i32, 3 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
    %73 = spirv.Bitcast %72 : vector<2xf32> to vector<4xf16>
    %74 = spirv.FMul %71, %73 : vector<4xf16>
    %75 = spirv.FAdd %69, %63 : vector<4xf16>
    %76 = spirv.FAdd %74, %64 : vector<4xf16>
    %77 = spirv.VectorShuffle [0 : i32, 1 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
    %78 = spirv.Bitcast %77 : vector<2xf32> to vector<4xf16>
    %79 = spirv.VectorShuffle [0 : i32, 1 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
    %80 = spirv.Bitcast %79 : vector<2xf32> to vector<4xf16>
    %81 = spirv.FMul %78, %80 : vector<4xf16>
    %82 = spirv.VectorShuffle [2 : i32, 3 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
    %83 = spirv.Bitcast %82 : vector<2xf32> to vector<4xf16>
    %84 = spirv.VectorShuffle [2 : i32, 3 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
    %85 = spirv.Bitcast %84 : vector<2xf32> to vector<4xf16>
    %86 = spirv.FMul %83, %85 : vector<4xf16>
    %87 = spirv.FAdd %81, %75 : vector<4xf16>
    %88 = spirv.FAdd %86, %76 : vector<4xf16>
    %89 = spirv.VectorShuffle [0 : i32, 1 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
    %90 = spirv.Bitcast %89 : vector<2xf32> to vector<4xf16>
    %91 = spirv.VectorShuffle [0 : i32, 1 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
    %92 = spirv.Bitcast %91 : vector<2xf32> to vector<4xf16>
    %93 = spirv.FMul %90, %92 : vector<4xf16>
    %94 = spirv.VectorShuffle [2 : i32, 3 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
    %95 = spirv.Bitcast %94 : vector<2xf32> to vector<4xf16>
    %96 = spirv.VectorShuffle [2 : i32, 3 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
    %97 = spirv.Bitcast %96 : vector<2xf32> to vector<4xf16>
    %98 = spirv.FMul %95, %97 : vector<4xf16>
    %99 = spirv.FAdd %93, %87 : vector<4xf16>
    %100 = spirv.FAdd %98, %88 : vector<4xf16>
    %101 = spirv.VectorShuffle [0 : i32, 1 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
    %102 = spirv.Bitcast %101 : vector<2xf32> to vector<4xf16>
    %103 = spirv.VectorShuffle [0 : i32, 1 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
    %104 = spirv.Bitcast %103 : vector<2xf32> to vector<4xf16>
    %105 = spirv.FMul %102, %104 : vector<4xf16>
    %106 = spirv.VectorShuffle [2 : i32, 3 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
    %107 = spirv.Bitcast %106 : vector<2xf32> to vector<4xf16>
    %108 = spirv.VectorShuffle [2 : i32, 3 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
    %109 = spirv.Bitcast %108 : vector<2xf32> to vector<4xf16>
    %110 = spirv.FMul %107, %109 : vector<4xf16>
    %111 = spirv.FAdd %105, %99 : vector<4xf16>
    %112 = spirv.FAdd %110, %100 : vector<4xf16>
    %113 = spirv.VectorShuffle [0 : i32, 1 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
    %114 = spirv.Bitcast %113 : vector<2xf32> to vector<4xf16>
    %115 = spirv.VectorShuffle [0 : i32, 1 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
    %116 = spirv.Bitcast %115 : vector<2xf32> to vector<4xf16>
    %117 = spirv.FMul %114, %116 : vector<4xf16>
    %118 = spirv.VectorShuffle [2 : i32, 3 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
    %119 = spirv.Bitcast %118 : vector<2xf32> to vector<4xf16>
    %120 = spirv.VectorShuffle [2 : i32, 3 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
    %121 = spirv.Bitcast %120 : vector<2xf32> to vector<4xf16>
    %122 = spirv.FMul %119, %121 : vector<4xf16>
    %123 = spirv.FAdd %117, %111 : vector<4xf16>
    %124 = spirv.FAdd %122, %112 : vector<4xf16>
    %125 = spirv.VectorShuffle [0 : i32, 1 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
    %126 = spirv.Bitcast %125 : vector<2xf32> to vector<4xf16>
    %127 = spirv.VectorShuffle [0 : i32, 1 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
    %128 = spirv.Bitcast %127 : vector<2xf32> to vector<4xf16>
    %129 = spirv.FMul %126, %128 : vector<4xf16>
    %130 = spirv.VectorShuffle [2 : i32, 3 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
    %131 = spirv.Bitcast %130 : vector<2xf32> to vector<4xf16>
    %132 = spirv.VectorShuffle [2 : i32, 3 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
    %133 = spirv.Bitcast %132 : vector<2xf32> to vector<4xf16>
    %134 = spirv.FMul %131, %133 : vector<4xf16>
    %135 = spirv.FAdd %129, %123 : vector<4xf16>
    %136 = spirv.FAdd %134, %124 : vector<4xf16>
    %137 = spirv.VectorShuffle [0 : i32, 1 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
    %138 = spirv.Bitcast %137 : vector<2xf32> to vector<4xf16>
    %139 = spirv.VectorShuffle [0 : i32, 1 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
    %140 = spirv.Bitcast %139 : vector<2xf32> to vector<4xf16>
    %141 = spirv.FMul %138, %140 : vector<4xf16>
    %142 = spirv.VectorShuffle [2 : i32, 3 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
    %143 = spirv.Bitcast %142 : vector<2xf32> to vector<4xf16>
    %144 = spirv.VectorShuffle [2 : i32, 3 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
    %145 = spirv.Bitcast %144 : vector<2xf32> to vector<4xf16>
    %146 = spirv.FMul %143, %145 : vector<4xf16>
    %147 = spirv.FAdd %141, %135 : vector<4xf16>
    %148 = spirv.FAdd %146, %136 : vector<4xf16>
    %149 = spirv.CompositeExtract %147[0 : i32] : vector<4xf16>
    %150 = spirv.CompositeExtract %147[1 : i32] : vector<4xf16>
    %151 = spirv.CompositeExtract %147[2 : i32] : vector<4xf16>
    %152 = spirv.CompositeExtract %147[3 : i32] : vector<4xf16>
    %153 = spirv.FAdd %149, %150 : f16
    %154 = spirv.FAdd %153, %151 : f16
    %155 = spirv.FAdd %154, %152 : f16
    %156 = spirv.CompositeExtract %148[0 : i32] : vector<4xf16>
    %157 = spirv.CompositeExtract %148[1 : i32] : vector<4xf16>
    %158 = spirv.CompositeExtract %148[2 : i32] : vector<4xf16>
    %159 = spirv.CompositeExtract %148[3 : i32] : vector<4xf16>
    %160 = spirv.FAdd %156, %157 : f16
    %161 = spirv.FAdd %160, %158 : f16
    %162 = spirv.FAdd %161, %159 : f16
    %163 = spirv.FAdd %155, %162 : f16
    %164 = spirv.GroupNonUniformFAdd "Subgroup" "Reduce" %163 : f16
    %165 = spirv.FAdd %164, %cst_f16 : f16
    %166 = spirv.IEqual %1, %cst0_i32 : i32
    spirv.mlir.selection {
      spirv.BranchConditional %166, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %167 = spirv.AccessChain %__resource_var_0_2__addr[%cst0_i32, %3] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>, i32, i32
      spirv.Store "StorageBuffer" %167, %165 : f16
      spirv.Branch ^bb2
    ^bb2:  // 2 preds: ^bb0, ^bb1
      spirv.mlir.merge
    }
    spirv.Return
  }
  spirv.EntryPoint "GLCompute" @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16, @__builtin__LocalInvocationId__, @__builtin__WorkgroupId__
  spirv.ExecutionMode @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 "LocalSize", 64, 1, 1
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::TranslateTargetExecutableVariantsPass (iree-hal-translate-target-executable-variants) //----- //
hal.executable.variant public @vulkan_spirv_fb target(<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>) {
  hal.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>) attributes {translation_info = #iree_codegen.translation_info<SPIRVSubgroupReduce>, workgroup_size = [64 : index, 1 : index, 1 : index]} {
  ^bb0(%arg0: !hal.device):
    %c32000 = arith.constant 32000 : index
    %c1 = arith.constant 1 : index
    hal.return %c32000, %c1, %c1 : index, index, index
  }
  builtin.module attributes {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>} {
    spirv.module Logical GLSL450 requires #spirv.vce<v1.3, [Shader, StorageBuffer16BitAccess, Float16, GroupNonUniformArithmetic], [SPV_KHR_storage_buffer_storage_class, SPV_KHR_16bit_storage]> {
      spirv.GlobalVariable @__builtin__WorkgroupId__ built_in("WorkgroupId") : !spirv.ptr<vector<3xi32>, Input>
      spirv.GlobalVariable @__builtin__LocalInvocationId__ built_in("LocalInvocationId") : !spirv.ptr<vector<3xi32>, Input>
      spirv.GlobalVariable @__resource_var_0_0_ bind(0, 0) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
      spirv.GlobalVariable @__resource_var_0_1_ bind(0, 1) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
      spirv.GlobalVariable @__resource_var_0_2_ bind(0, 2) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
      spirv.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() "None" {
        %cst64_i32 = spirv.Constant 64 : i32
        %cst128_i32 = spirv.Constant 128 : i32
        %cst192_i32 = spirv.Constant 192 : i32
        %cst256_i32 = spirv.Constant 256 : i32
        %cst320_i32 = spirv.Constant 320 : i32
        %cst384_i32 = spirv.Constant 384 : i32
        %cst448_i32 = spirv.Constant 448 : i32
        %cst512_i32 = spirv.Constant 512 : i32
        %cst_vec_4xf16 = spirv.Constant dense<0.000000e+00> : vector<4xf16>
        %cst0_i32 = spirv.Constant 0 : i32
        %cst_f16 = spirv.Constant 0.000000e+00 : f16
        %__builtin__LocalInvocationId___addr = spirv.mlir.addressof @__builtin__LocalInvocationId__ : !spirv.ptr<vector<3xi32>, Input>
        %0 = spirv.Load "Input" %__builtin__LocalInvocationId___addr : vector<3xi32>
        %1 = spirv.CompositeExtract %0[0 : i32] : vector<3xi32>
        %__resource_var_0_0__addr = spirv.mlir.addressof @__resource_var_0_0_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
        %__resource_var_0_1__addr = spirv.mlir.addressof @__resource_var_0_1_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
        %__resource_var_0_2__addr = spirv.mlir.addressof @__resource_var_0_2_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
        %__builtin__WorkgroupId___addr = spirv.mlir.addressof @__builtin__WorkgroupId__ : !spirv.ptr<vector<3xi32>, Input>
        %2 = spirv.Load "Input" %__builtin__WorkgroupId___addr : vector<3xi32>
        %3 = spirv.CompositeExtract %2[0 : i32] : vector<3xi32>
        %4 = spirv.IMul %3, %cst512_i32 : i32
        %5 = spirv.IAdd %1, %4 : i32
        %6 = spirv.IAdd %5, %cst448_i32 : i32
        %7 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %6] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
        %8 = spirv.Load "StorageBuffer" %7 : vector<4xf32>
        %9 = spirv.IAdd %1, %cst448_i32 : i32
        %10 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %9] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
        %11 = spirv.Load "StorageBuffer" %10 : vector<4xf32>
        %12 = spirv.IAdd %5, %cst384_i32 : i32
        %13 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %12] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
        %14 = spirv.Load "StorageBuffer" %13 : vector<4xf32>
        %15 = spirv.IAdd %1, %cst384_i32 : i32
        %16 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %15] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
        %17 = spirv.Load "StorageBuffer" %16 : vector<4xf32>
        %18 = spirv.IAdd %5, %cst320_i32 : i32
        %19 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %18] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
        %20 = spirv.Load "StorageBuffer" %19 : vector<4xf32>
        %21 = spirv.IAdd %1, %cst320_i32 : i32
        %22 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %21] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
        %23 = spirv.Load "StorageBuffer" %22 : vector<4xf32>
        %24 = spirv.IAdd %5, %cst256_i32 : i32
        %25 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %24] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
        %26 = spirv.Load "StorageBuffer" %25 : vector<4xf32>
        %27 = spirv.IAdd %1, %cst256_i32 : i32
        %28 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %27] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
        %29 = spirv.Load "StorageBuffer" %28 : vector<4xf32>
        %30 = spirv.IAdd %5, %cst192_i32 : i32
        %31 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %30] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
        %32 = spirv.Load "StorageBuffer" %31 : vector<4xf32>
        %33 = spirv.IAdd %1, %cst192_i32 : i32
        %34 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %33] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
        %35 = spirv.Load "StorageBuffer" %34 : vector<4xf32>
        %36 = spirv.IAdd %5, %cst128_i32 : i32
        %37 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %36] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
        %38 = spirv.Load "StorageBuffer" %37 : vector<4xf32>
        %39 = spirv.IAdd %1, %cst128_i32 : i32
        %40 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %39] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
        %41 = spirv.Load "StorageBuffer" %40 : vector<4xf32>
        %42 = spirv.IAdd %5, %cst64_i32 : i32
        %43 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %42] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
        %44 = spirv.Load "StorageBuffer" %43 : vector<4xf32>
        %45 = spirv.IAdd %1, %cst64_i32 : i32
        %46 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %45] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
        %47 = spirv.Load "StorageBuffer" %46 : vector<4xf32>
        %48 = spirv.IAdd %4, %1 : i32
        %49 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %48] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
        %50 = spirv.Load "StorageBuffer" %49 : vector<4xf32>
        %51 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %1] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
        %52 = spirv.Load "StorageBuffer" %51 : vector<4xf32>
        %53 = spirv.VectorShuffle [0 : i32, 1 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
        %54 = spirv.Bitcast %53 : vector<2xf32> to vector<4xf16>
        %55 = spirv.VectorShuffle [0 : i32, 1 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
        %56 = spirv.Bitcast %55 : vector<2xf32> to vector<4xf16>
        %57 = spirv.FMul %54, %56 : vector<4xf16>
        %58 = spirv.VectorShuffle [2 : i32, 3 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
        %59 = spirv.Bitcast %58 : vector<2xf32> to vector<4xf16>
        %60 = spirv.VectorShuffle [2 : i32, 3 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
        %61 = spirv.Bitcast %60 : vector<2xf32> to vector<4xf16>
        %62 = spirv.FMul %59, %61 : vector<4xf16>
        %63 = spirv.FAdd %57, %cst_vec_4xf16 : vector<4xf16>
        %64 = spirv.FAdd %62, %cst_vec_4xf16 : vector<4xf16>
        %65 = spirv.VectorShuffle [0 : i32, 1 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
        %66 = spirv.Bitcast %65 : vector<2xf32> to vector<4xf16>
        %67 = spirv.VectorShuffle [0 : i32, 1 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
        %68 = spirv.Bitcast %67 : vector<2xf32> to vector<4xf16>
        %69 = spirv.FMul %66, %68 : vector<4xf16>
        %70 = spirv.VectorShuffle [2 : i32, 3 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
        %71 = spirv.Bitcast %70 : vector<2xf32> to vector<4xf16>
        %72 = spirv.VectorShuffle [2 : i32, 3 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
        %73 = spirv.Bitcast %72 : vector<2xf32> to vector<4xf16>
        %74 = spirv.FMul %71, %73 : vector<4xf16>
        %75 = spirv.FAdd %69, %63 : vector<4xf16>
        %76 = spirv.FAdd %74, %64 : vector<4xf16>
        %77 = spirv.VectorShuffle [0 : i32, 1 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
        %78 = spirv.Bitcast %77 : vector<2xf32> to vector<4xf16>
        %79 = spirv.VectorShuffle [0 : i32, 1 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
        %80 = spirv.Bitcast %79 : vector<2xf32> to vector<4xf16>
        %81 = spirv.FMul %78, %80 : vector<4xf16>
        %82 = spirv.VectorShuffle [2 : i32, 3 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
        %83 = spirv.Bitcast %82 : vector<2xf32> to vector<4xf16>
        %84 = spirv.VectorShuffle [2 : i32, 3 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
        %85 = spirv.Bitcast %84 : vector<2xf32> to vector<4xf16>
        %86 = spirv.FMul %83, %85 : vector<4xf16>
        %87 = spirv.FAdd %81, %75 : vector<4xf16>
        %88 = spirv.FAdd %86, %76 : vector<4xf16>
        %89 = spirv.VectorShuffle [0 : i32, 1 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
        %90 = spirv.Bitcast %89 : vector<2xf32> to vector<4xf16>
        %91 = spirv.VectorShuffle [0 : i32, 1 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
        %92 = spirv.Bitcast %91 : vector<2xf32> to vector<4xf16>
        %93 = spirv.FMul %90, %92 : vector<4xf16>
        %94 = spirv.VectorShuffle [2 : i32, 3 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
        %95 = spirv.Bitcast %94 : vector<2xf32> to vector<4xf16>
        %96 = spirv.VectorShuffle [2 : i32, 3 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
        %97 = spirv.Bitcast %96 : vector<2xf32> to vector<4xf16>
        %98 = spirv.FMul %95, %97 : vector<4xf16>
        %99 = spirv.FAdd %93, %87 : vector<4xf16>
        %100 = spirv.FAdd %98, %88 : vector<4xf16>
        %101 = spirv.VectorShuffle [0 : i32, 1 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
        %102 = spirv.Bitcast %101 : vector<2xf32> to vector<4xf16>
        %103 = spirv.VectorShuffle [0 : i32, 1 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
        %104 = spirv.Bitcast %103 : vector<2xf32> to vector<4xf16>
        %105 = spirv.FMul %102, %104 : vector<4xf16>
        %106 = spirv.VectorShuffle [2 : i32, 3 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
        %107 = spirv.Bitcast %106 : vector<2xf32> to vector<4xf16>
        %108 = spirv.VectorShuffle [2 : i32, 3 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
        %109 = spirv.Bitcast %108 : vector<2xf32> to vector<4xf16>
        %110 = spirv.FMul %107, %109 : vector<4xf16>
        %111 = spirv.FAdd %105, %99 : vector<4xf16>
        %112 = spirv.FAdd %110, %100 : vector<4xf16>
        %113 = spirv.VectorShuffle [0 : i32, 1 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
        %114 = spirv.Bitcast %113 : vector<2xf32> to vector<4xf16>
        %115 = spirv.VectorShuffle [0 : i32, 1 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
        %116 = spirv.Bitcast %115 : vector<2xf32> to vector<4xf16>
        %117 = spirv.FMul %114, %116 : vector<4xf16>
        %118 = spirv.VectorShuffle [2 : i32, 3 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
        %119 = spirv.Bitcast %118 : vector<2xf32> to vector<4xf16>
        %120 = spirv.VectorShuffle [2 : i32, 3 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
        %121 = spirv.Bitcast %120 : vector<2xf32> to vector<4xf16>
        %122 = spirv.FMul %119, %121 : vector<4xf16>
        %123 = spirv.FAdd %117, %111 : vector<4xf16>
        %124 = spirv.FAdd %122, %112 : vector<4xf16>
        %125 = spirv.VectorShuffle [0 : i32, 1 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
        %126 = spirv.Bitcast %125 : vector<2xf32> to vector<4xf16>
        %127 = spirv.VectorShuffle [0 : i32, 1 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
        %128 = spirv.Bitcast %127 : vector<2xf32> to vector<4xf16>
        %129 = spirv.FMul %126, %128 : vector<4xf16>
        %130 = spirv.VectorShuffle [2 : i32, 3 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
        %131 = spirv.Bitcast %130 : vector<2xf32> to vector<4xf16>
        %132 = spirv.VectorShuffle [2 : i32, 3 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
        %133 = spirv.Bitcast %132 : vector<2xf32> to vector<4xf16>
        %134 = spirv.FMul %131, %133 : vector<4xf16>
        %135 = spirv.FAdd %129, %123 : vector<4xf16>
        %136 = spirv.FAdd %134, %124 : vector<4xf16>
        %137 = spirv.VectorShuffle [0 : i32, 1 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
        %138 = spirv.Bitcast %137 : vector<2xf32> to vector<4xf16>
        %139 = spirv.VectorShuffle [0 : i32, 1 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
        %140 = spirv.Bitcast %139 : vector<2xf32> to vector<4xf16>
        %141 = spirv.FMul %138, %140 : vector<4xf16>
        %142 = spirv.VectorShuffle [2 : i32, 3 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
        %143 = spirv.Bitcast %142 : vector<2xf32> to vector<4xf16>
        %144 = spirv.VectorShuffle [2 : i32, 3 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
        %145 = spirv.Bitcast %144 : vector<2xf32> to vector<4xf16>
        %146 = spirv.FMul %143, %145 : vector<4xf16>
        %147 = spirv.FAdd %141, %135 : vector<4xf16>
        %148 = spirv.FAdd %146, %136 : vector<4xf16>
        %149 = spirv.CompositeExtract %147[0 : i32] : vector<4xf16>
        %150 = spirv.CompositeExtract %147[1 : i32] : vector<4xf16>
        %151 = spirv.CompositeExtract %147[2 : i32] : vector<4xf16>
        %152 = spirv.CompositeExtract %147[3 : i32] : vector<4xf16>
        %153 = spirv.FAdd %149, %150 : f16
        %154 = spirv.FAdd %153, %151 : f16
        %155 = spirv.FAdd %154, %152 : f16
        %156 = spirv.CompositeExtract %148[0 : i32] : vector<4xf16>
        %157 = spirv.CompositeExtract %148[1 : i32] : vector<4xf16>
        %158 = spirv.CompositeExtract %148[2 : i32] : vector<4xf16>
        %159 = spirv.CompositeExtract %148[3 : i32] : vector<4xf16>
        %160 = spirv.FAdd %156, %157 : f16
        %161 = spirv.FAdd %160, %158 : f16
        %162 = spirv.FAdd %161, %159 : f16
        %163 = spirv.FAdd %155, %162 : f16
        %164 = spirv.GroupNonUniformFAdd "Subgroup" "Reduce" %163 : f16
        %165 = spirv.FAdd %164, %cst_f16 : f16
        %166 = spirv.IEqual %1, %cst0_i32 : i32
        spirv.mlir.selection {
          spirv.BranchConditional %166, ^bb1, ^bb2
        ^bb1:  // pred: ^bb0
          %167 = spirv.AccessChain %__resource_var_0_2__addr[%cst0_i32, %3] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>, i32, i32
          spirv.Store "StorageBuffer" %167, %165 : f16
          spirv.Branch ^bb2
        ^bb2:  // 2 preds: ^bb0, ^bb1
          spirv.mlir.merge
        }
        spirv.Return
      }
      spirv.EntryPoint "GLCompute" @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16, @__builtin__LocalInvocationId__, @__builtin__WorkgroupId__
      spirv.ExecutionMode @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 "LocalSize", 64, 1, 1
    }
  }
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::TranslateExecutablesPass (iree-hal-translate-executables) //----- //
hal.executable private @_main_dispatch_0 {
  hal.executable.variant public @vulkan_spirv_fb target(<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>) {
    hal.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>) attributes {translation_info = #iree_codegen.translation_info<SPIRVSubgroupReduce>, workgroup_size = [64 : index, 1 : index, 1 : index]} {
    ^bb0(%arg0: !hal.device):
      %c32000 = arith.constant 32000 : index
      %c1 = arith.constant 1 : index
      hal.return %c32000, %c1, %c1 : index, index, index
    }
    builtin.module attributes {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>} {
      spirv.module Logical GLSL450 requires #spirv.vce<v1.3, [Shader, StorageBuffer16BitAccess, Float16, GroupNonUniformArithmetic], [SPV_KHR_storage_buffer_storage_class, SPV_KHR_16bit_storage]> {
        spirv.GlobalVariable @__builtin__WorkgroupId__ built_in("WorkgroupId") : !spirv.ptr<vector<3xi32>, Input>
        spirv.GlobalVariable @__builtin__LocalInvocationId__ built_in("LocalInvocationId") : !spirv.ptr<vector<3xi32>, Input>
        spirv.GlobalVariable @__resource_var_0_0_ bind(0, 0) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
        spirv.GlobalVariable @__resource_var_0_1_ bind(0, 1) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
        spirv.GlobalVariable @__resource_var_0_2_ bind(0, 2) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
        spirv.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() "None" {
          %cst64_i32 = spirv.Constant 64 : i32
          %cst128_i32 = spirv.Constant 128 : i32
          %cst192_i32 = spirv.Constant 192 : i32
          %cst256_i32 = spirv.Constant 256 : i32
          %cst320_i32 = spirv.Constant 320 : i32
          %cst384_i32 = spirv.Constant 384 : i32
          %cst448_i32 = spirv.Constant 448 : i32
          %cst512_i32 = spirv.Constant 512 : i32
          %cst_vec_4xf16 = spirv.Constant dense<0.000000e+00> : vector<4xf16>
          %cst0_i32 = spirv.Constant 0 : i32
          %cst_f16 = spirv.Constant 0.000000e+00 : f16
          %__builtin__LocalInvocationId___addr = spirv.mlir.addressof @__builtin__LocalInvocationId__ : !spirv.ptr<vector<3xi32>, Input>
          %0 = spirv.Load "Input" %__builtin__LocalInvocationId___addr : vector<3xi32>
          %1 = spirv.CompositeExtract %0[0 : i32] : vector<3xi32>
          %__resource_var_0_0__addr = spirv.mlir.addressof @__resource_var_0_0_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          %__resource_var_0_1__addr = spirv.mlir.addressof @__resource_var_0_1_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          %__resource_var_0_2__addr = spirv.mlir.addressof @__resource_var_0_2_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
          %__builtin__WorkgroupId___addr = spirv.mlir.addressof @__builtin__WorkgroupId__ : !spirv.ptr<vector<3xi32>, Input>
          %2 = spirv.Load "Input" %__builtin__WorkgroupId___addr : vector<3xi32>
          %3 = spirv.CompositeExtract %2[0 : i32] : vector<3xi32>
          %4 = spirv.IMul %3, %cst512_i32 : i32
          %5 = spirv.IAdd %1, %4 : i32
          %6 = spirv.IAdd %5, %cst448_i32 : i32
          %7 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %6] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
          %8 = spirv.Load "StorageBuffer" %7 : vector<4xf32>
          %9 = spirv.IAdd %1, %cst448_i32 : i32
          %10 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %9] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
          %11 = spirv.Load "StorageBuffer" %10 : vector<4xf32>
          %12 = spirv.IAdd %5, %cst384_i32 : i32
          %13 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %12] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
          %14 = spirv.Load "StorageBuffer" %13 : vector<4xf32>
          %15 = spirv.IAdd %1, %cst384_i32 : i32
          %16 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %15] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
          %17 = spirv.Load "StorageBuffer" %16 : vector<4xf32>
          %18 = spirv.IAdd %5, %cst320_i32 : i32
          %19 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %18] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
          %20 = spirv.Load "StorageBuffer" %19 : vector<4xf32>
          %21 = spirv.IAdd %1, %cst320_i32 : i32
          %22 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %21] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
          %23 = spirv.Load "StorageBuffer" %22 : vector<4xf32>
          %24 = spirv.IAdd %5, %cst256_i32 : i32
          %25 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %24] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
          %26 = spirv.Load "StorageBuffer" %25 : vector<4xf32>
          %27 = spirv.IAdd %1, %cst256_i32 : i32
          %28 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %27] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
          %29 = spirv.Load "StorageBuffer" %28 : vector<4xf32>
          %30 = spirv.IAdd %5, %cst192_i32 : i32
          %31 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %30] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
          %32 = spirv.Load "StorageBuffer" %31 : vector<4xf32>
          %33 = spirv.IAdd %1, %cst192_i32 : i32
          %34 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %33] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
          %35 = spirv.Load "StorageBuffer" %34 : vector<4xf32>
          %36 = spirv.IAdd %5, %cst128_i32 : i32
          %37 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %36] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
          %38 = spirv.Load "StorageBuffer" %37 : vector<4xf32>
          %39 = spirv.IAdd %1, %cst128_i32 : i32
          %40 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %39] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
          %41 = spirv.Load "StorageBuffer" %40 : vector<4xf32>
          %42 = spirv.IAdd %5, %cst64_i32 : i32
          %43 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %42] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
          %44 = spirv.Load "StorageBuffer" %43 : vector<4xf32>
          %45 = spirv.IAdd %1, %cst64_i32 : i32
          %46 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %45] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
          %47 = spirv.Load "StorageBuffer" %46 : vector<4xf32>
          %48 = spirv.IAdd %4, %1 : i32
          %49 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %48] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
          %50 = spirv.Load "StorageBuffer" %49 : vector<4xf32>
          %51 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %1] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
          %52 = spirv.Load "StorageBuffer" %51 : vector<4xf32>
          %53 = spirv.VectorShuffle [0 : i32, 1 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
          %54 = spirv.Bitcast %53 : vector<2xf32> to vector<4xf16>
          %55 = spirv.VectorShuffle [0 : i32, 1 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
          %56 = spirv.Bitcast %55 : vector<2xf32> to vector<4xf16>
          %57 = spirv.FMul %54, %56 : vector<4xf16>
          %58 = spirv.VectorShuffle [2 : i32, 3 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
          %59 = spirv.Bitcast %58 : vector<2xf32> to vector<4xf16>
          %60 = spirv.VectorShuffle [2 : i32, 3 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
          %61 = spirv.Bitcast %60 : vector<2xf32> to vector<4xf16>
          %62 = spirv.FMul %59, %61 : vector<4xf16>
          %63 = spirv.FAdd %57, %cst_vec_4xf16 : vector<4xf16>
          %64 = spirv.FAdd %62, %cst_vec_4xf16 : vector<4xf16>
          %65 = spirv.VectorShuffle [0 : i32, 1 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
          %66 = spirv.Bitcast %65 : vector<2xf32> to vector<4xf16>
          %67 = spirv.VectorShuffle [0 : i32, 1 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
          %68 = spirv.Bitcast %67 : vector<2xf32> to vector<4xf16>
          %69 = spirv.FMul %66, %68 : vector<4xf16>
          %70 = spirv.VectorShuffle [2 : i32, 3 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
          %71 = spirv.Bitcast %70 : vector<2xf32> to vector<4xf16>
          %72 = spirv.VectorShuffle [2 : i32, 3 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
          %73 = spirv.Bitcast %72 : vector<2xf32> to vector<4xf16>
          %74 = spirv.FMul %71, %73 : vector<4xf16>
          %75 = spirv.FAdd %69, %63 : vector<4xf16>
          %76 = spirv.FAdd %74, %64 : vector<4xf16>
          %77 = spirv.VectorShuffle [0 : i32, 1 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
          %78 = spirv.Bitcast %77 : vector<2xf32> to vector<4xf16>
          %79 = spirv.VectorShuffle [0 : i32, 1 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
          %80 = spirv.Bitcast %79 : vector<2xf32> to vector<4xf16>
          %81 = spirv.FMul %78, %80 : vector<4xf16>
          %82 = spirv.VectorShuffle [2 : i32, 3 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
          %83 = spirv.Bitcast %82 : vector<2xf32> to vector<4xf16>
          %84 = spirv.VectorShuffle [2 : i32, 3 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
          %85 = spirv.Bitcast %84 : vector<2xf32> to vector<4xf16>
          %86 = spirv.FMul %83, %85 : vector<4xf16>
          %87 = spirv.FAdd %81, %75 : vector<4xf16>
          %88 = spirv.FAdd %86, %76 : vector<4xf16>
          %89 = spirv.VectorShuffle [0 : i32, 1 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
          %90 = spirv.Bitcast %89 : vector<2xf32> to vector<4xf16>
          %91 = spirv.VectorShuffle [0 : i32, 1 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
          %92 = spirv.Bitcast %91 : vector<2xf32> to vector<4xf16>
          %93 = spirv.FMul %90, %92 : vector<4xf16>
          %94 = spirv.VectorShuffle [2 : i32, 3 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
          %95 = spirv.Bitcast %94 : vector<2xf32> to vector<4xf16>
          %96 = spirv.VectorShuffle [2 : i32, 3 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
          %97 = spirv.Bitcast %96 : vector<2xf32> to vector<4xf16>
          %98 = spirv.FMul %95, %97 : vector<4xf16>
          %99 = spirv.FAdd %93, %87 : vector<4xf16>
          %100 = spirv.FAdd %98, %88 : vector<4xf16>
          %101 = spirv.VectorShuffle [0 : i32, 1 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
          %102 = spirv.Bitcast %101 : vector<2xf32> to vector<4xf16>
          %103 = spirv.VectorShuffle [0 : i32, 1 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
          %104 = spirv.Bitcast %103 : vector<2xf32> to vector<4xf16>
          %105 = spirv.FMul %102, %104 : vector<4xf16>
          %106 = spirv.VectorShuffle [2 : i32, 3 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
          %107 = spirv.Bitcast %106 : vector<2xf32> to vector<4xf16>
          %108 = spirv.VectorShuffle [2 : i32, 3 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
          %109 = spirv.Bitcast %108 : vector<2xf32> to vector<4xf16>
          %110 = spirv.FMul %107, %109 : vector<4xf16>
          %111 = spirv.FAdd %105, %99 : vector<4xf16>
          %112 = spirv.FAdd %110, %100 : vector<4xf16>
          %113 = spirv.VectorShuffle [0 : i32, 1 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
          %114 = spirv.Bitcast %113 : vector<2xf32> to vector<4xf16>
          %115 = spirv.VectorShuffle [0 : i32, 1 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
          %116 = spirv.Bitcast %115 : vector<2xf32> to vector<4xf16>
          %117 = spirv.FMul %114, %116 : vector<4xf16>
          %118 = spirv.VectorShuffle [2 : i32, 3 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
          %119 = spirv.Bitcast %118 : vector<2xf32> to vector<4xf16>
          %120 = spirv.VectorShuffle [2 : i32, 3 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
          %121 = spirv.Bitcast %120 : vector<2xf32> to vector<4xf16>
          %122 = spirv.FMul %119, %121 : vector<4xf16>
          %123 = spirv.FAdd %117, %111 : vector<4xf16>
          %124 = spirv.FAdd %122, %112 : vector<4xf16>
          %125 = spirv.VectorShuffle [0 : i32, 1 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
          %126 = spirv.Bitcast %125 : vector<2xf32> to vector<4xf16>
          %127 = spirv.VectorShuffle [0 : i32, 1 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
          %128 = spirv.Bitcast %127 : vector<2xf32> to vector<4xf16>
          %129 = spirv.FMul %126, %128 : vector<4xf16>
          %130 = spirv.VectorShuffle [2 : i32, 3 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
          %131 = spirv.Bitcast %130 : vector<2xf32> to vector<4xf16>
          %132 = spirv.VectorShuffle [2 : i32, 3 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
          %133 = spirv.Bitcast %132 : vector<2xf32> to vector<4xf16>
          %134 = spirv.FMul %131, %133 : vector<4xf16>
          %135 = spirv.FAdd %129, %123 : vector<4xf16>
          %136 = spirv.FAdd %134, %124 : vector<4xf16>
          %137 = spirv.VectorShuffle [0 : i32, 1 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
          %138 = spirv.Bitcast %137 : vector<2xf32> to vector<4xf16>
          %139 = spirv.VectorShuffle [0 : i32, 1 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
          %140 = spirv.Bitcast %139 : vector<2xf32> to vector<4xf16>
          %141 = spirv.FMul %138, %140 : vector<4xf16>
          %142 = spirv.VectorShuffle [2 : i32, 3 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
          %143 = spirv.Bitcast %142 : vector<2xf32> to vector<4xf16>
          %144 = spirv.VectorShuffle [2 : i32, 3 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
          %145 = spirv.Bitcast %144 : vector<2xf32> to vector<4xf16>
          %146 = spirv.FMul %143, %145 : vector<4xf16>
          %147 = spirv.FAdd %141, %135 : vector<4xf16>
          %148 = spirv.FAdd %146, %136 : vector<4xf16>
          %149 = spirv.CompositeExtract %147[0 : i32] : vector<4xf16>
          %150 = spirv.CompositeExtract %147[1 : i32] : vector<4xf16>
          %151 = spirv.CompositeExtract %147[2 : i32] : vector<4xf16>
          %152 = spirv.CompositeExtract %147[3 : i32] : vector<4xf16>
          %153 = spirv.FAdd %149, %150 : f16
          %154 = spirv.FAdd %153, %151 : f16
          %155 = spirv.FAdd %154, %152 : f16
          %156 = spirv.CompositeExtract %148[0 : i32] : vector<4xf16>
          %157 = spirv.CompositeExtract %148[1 : i32] : vector<4xf16>
          %158 = spirv.CompositeExtract %148[2 : i32] : vector<4xf16>
          %159 = spirv.CompositeExtract %148[3 : i32] : vector<4xf16>
          %160 = spirv.FAdd %156, %157 : f16
          %161 = spirv.FAdd %160, %158 : f16
          %162 = spirv.FAdd %161, %159 : f16
          %163 = spirv.FAdd %155, %162 : f16
          %164 = spirv.GroupNonUniformFAdd "Subgroup" "Reduce" %163 : f16
          %165 = spirv.FAdd %164, %cst_f16 : f16
          %166 = spirv.IEqual %1, %cst0_i32 : i32
          spirv.mlir.selection {
            spirv.BranchConditional %166, ^bb1, ^bb2
          ^bb1:  // pred: ^bb0
            %167 = spirv.AccessChain %__resource_var_0_2__addr[%cst0_i32, %3] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>, i32, i32
            spirv.Store "StorageBuffer" %167, %165 : f16
            spirv.Branch ^bb2
          ^bb2:  // 2 preds: ^bb0, ^bb1
            spirv.mlir.merge
          }
          spirv.Return
        }
        spirv.EntryPoint "GLCompute" @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16, @__builtin__LocalInvocationId__, @__builtin__WorkgroupId__
        spirv.ExecutionMode @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 "LocalSize", 64, 1, 1
      }
    }
  }
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::(anonymous namespace)::ConvertToHALPass (iree-hal-conversion) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<SPIRVSubgroupReduce>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.variant public @vulkan_spirv_fb target(#executable_target_vulkan_spirv_fb) {
      hal.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c32000 = arith.constant 32000 : index
        %c1 = arith.constant 1 : index
        hal.return %c32000, %c1, %c1 : index, index, index
      }
      builtin.module attributes {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>} {
        spirv.module Logical GLSL450 requires #spirv.vce<v1.3, [Shader, StorageBuffer16BitAccess, Float16, GroupNonUniformArithmetic], [SPV_KHR_storage_buffer_storage_class, SPV_KHR_16bit_storage]> {
          spirv.GlobalVariable @__builtin__WorkgroupId__ built_in("WorkgroupId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__builtin__LocalInvocationId__ built_in("LocalInvocationId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__resource_var_0_0_ bind(0, 0) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_1_ bind(0, 1) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_2_ bind(0, 2) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
          spirv.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() "None" {
            %cst64_i32 = spirv.Constant 64 : i32
            %cst128_i32 = spirv.Constant 128 : i32
            %cst192_i32 = spirv.Constant 192 : i32
            %cst256_i32 = spirv.Constant 256 : i32
            %cst320_i32 = spirv.Constant 320 : i32
            %cst384_i32 = spirv.Constant 384 : i32
            %cst448_i32 = spirv.Constant 448 : i32
            %cst512_i32 = spirv.Constant 512 : i32
            %cst_vec_4xf16 = spirv.Constant dense<0.000000e+00> : vector<4xf16>
            %cst0_i32 = spirv.Constant 0 : i32
            %cst_f16 = spirv.Constant 0.000000e+00 : f16
            %__builtin__LocalInvocationId___addr = spirv.mlir.addressof @__builtin__LocalInvocationId__ : !spirv.ptr<vector<3xi32>, Input>
            %0 = spirv.Load "Input" %__builtin__LocalInvocationId___addr : vector<3xi32>
            %1 = spirv.CompositeExtract %0[0 : i32] : vector<3xi32>
            %__resource_var_0_0__addr = spirv.mlir.addressof @__resource_var_0_0_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_1__addr = spirv.mlir.addressof @__resource_var_0_1_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_2__addr = spirv.mlir.addressof @__resource_var_0_2_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
            %__builtin__WorkgroupId___addr = spirv.mlir.addressof @__builtin__WorkgroupId__ : !spirv.ptr<vector<3xi32>, Input>
            %2 = spirv.Load "Input" %__builtin__WorkgroupId___addr : vector<3xi32>
            %3 = spirv.CompositeExtract %2[0 : i32] : vector<3xi32>
            %4 = spirv.IMul %3, %cst512_i32 : i32
            %5 = spirv.IAdd %1, %4 : i32
            %6 = spirv.IAdd %5, %cst448_i32 : i32
            %7 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %6] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %8 = spirv.Load "StorageBuffer" %7 : vector<4xf32>
            %9 = spirv.IAdd %1, %cst448_i32 : i32
            %10 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %9] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %11 = spirv.Load "StorageBuffer" %10 : vector<4xf32>
            %12 = spirv.IAdd %5, %cst384_i32 : i32
            %13 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %12] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %14 = spirv.Load "StorageBuffer" %13 : vector<4xf32>
            %15 = spirv.IAdd %1, %cst384_i32 : i32
            %16 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %15] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %17 = spirv.Load "StorageBuffer" %16 : vector<4xf32>
            %18 = spirv.IAdd %5, %cst320_i32 : i32
            %19 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %18] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %20 = spirv.Load "StorageBuffer" %19 : vector<4xf32>
            %21 = spirv.IAdd %1, %cst320_i32 : i32
            %22 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %21] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %23 = spirv.Load "StorageBuffer" %22 : vector<4xf32>
            %24 = spirv.IAdd %5, %cst256_i32 : i32
            %25 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %24] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %26 = spirv.Load "StorageBuffer" %25 : vector<4xf32>
            %27 = spirv.IAdd %1, %cst256_i32 : i32
            %28 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %27] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %29 = spirv.Load "StorageBuffer" %28 : vector<4xf32>
            %30 = spirv.IAdd %5, %cst192_i32 : i32
            %31 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %30] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %32 = spirv.Load "StorageBuffer" %31 : vector<4xf32>
            %33 = spirv.IAdd %1, %cst192_i32 : i32
            %34 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %33] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %35 = spirv.Load "StorageBuffer" %34 : vector<4xf32>
            %36 = spirv.IAdd %5, %cst128_i32 : i32
            %37 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %36] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %38 = spirv.Load "StorageBuffer" %37 : vector<4xf32>
            %39 = spirv.IAdd %1, %cst128_i32 : i32
            %40 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %39] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %41 = spirv.Load "StorageBuffer" %40 : vector<4xf32>
            %42 = spirv.IAdd %5, %cst64_i32 : i32
            %43 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %42] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %44 = spirv.Load "StorageBuffer" %43 : vector<4xf32>
            %45 = spirv.IAdd %1, %cst64_i32 : i32
            %46 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %45] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %47 = spirv.Load "StorageBuffer" %46 : vector<4xf32>
            %48 = spirv.IAdd %4, %1 : i32
            %49 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %48] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %50 = spirv.Load "StorageBuffer" %49 : vector<4xf32>
            %51 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %1] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %52 = spirv.Load "StorageBuffer" %51 : vector<4xf32>
            %53 = spirv.VectorShuffle [0 : i32, 1 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %54 = spirv.Bitcast %53 : vector<2xf32> to vector<4xf16>
            %55 = spirv.VectorShuffle [0 : i32, 1 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %56 = spirv.Bitcast %55 : vector<2xf32> to vector<4xf16>
            %57 = spirv.FMul %54, %56 : vector<4xf16>
            %58 = spirv.VectorShuffle [2 : i32, 3 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %59 = spirv.Bitcast %58 : vector<2xf32> to vector<4xf16>
            %60 = spirv.VectorShuffle [2 : i32, 3 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %61 = spirv.Bitcast %60 : vector<2xf32> to vector<4xf16>
            %62 = spirv.FMul %59, %61 : vector<4xf16>
            %63 = spirv.FAdd %57, %cst_vec_4xf16 : vector<4xf16>
            %64 = spirv.FAdd %62, %cst_vec_4xf16 : vector<4xf16>
            %65 = spirv.VectorShuffle [0 : i32, 1 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %66 = spirv.Bitcast %65 : vector<2xf32> to vector<4xf16>
            %67 = spirv.VectorShuffle [0 : i32, 1 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %68 = spirv.Bitcast %67 : vector<2xf32> to vector<4xf16>
            %69 = spirv.FMul %66, %68 : vector<4xf16>
            %70 = spirv.VectorShuffle [2 : i32, 3 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %71 = spirv.Bitcast %70 : vector<2xf32> to vector<4xf16>
            %72 = spirv.VectorShuffle [2 : i32, 3 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %73 = spirv.Bitcast %72 : vector<2xf32> to vector<4xf16>
            %74 = spirv.FMul %71, %73 : vector<4xf16>
            %75 = spirv.FAdd %69, %63 : vector<4xf16>
            %76 = spirv.FAdd %74, %64 : vector<4xf16>
            %77 = spirv.VectorShuffle [0 : i32, 1 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %78 = spirv.Bitcast %77 : vector<2xf32> to vector<4xf16>
            %79 = spirv.VectorShuffle [0 : i32, 1 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %80 = spirv.Bitcast %79 : vector<2xf32> to vector<4xf16>
            %81 = spirv.FMul %78, %80 : vector<4xf16>
            %82 = spirv.VectorShuffle [2 : i32, 3 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %83 = spirv.Bitcast %82 : vector<2xf32> to vector<4xf16>
            %84 = spirv.VectorShuffle [2 : i32, 3 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %85 = spirv.Bitcast %84 : vector<2xf32> to vector<4xf16>
            %86 = spirv.FMul %83, %85 : vector<4xf16>
            %87 = spirv.FAdd %81, %75 : vector<4xf16>
            %88 = spirv.FAdd %86, %76 : vector<4xf16>
            %89 = spirv.VectorShuffle [0 : i32, 1 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %90 = spirv.Bitcast %89 : vector<2xf32> to vector<4xf16>
            %91 = spirv.VectorShuffle [0 : i32, 1 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %92 = spirv.Bitcast %91 : vector<2xf32> to vector<4xf16>
            %93 = spirv.FMul %90, %92 : vector<4xf16>
            %94 = spirv.VectorShuffle [2 : i32, 3 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %95 = spirv.Bitcast %94 : vector<2xf32> to vector<4xf16>
            %96 = spirv.VectorShuffle [2 : i32, 3 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %97 = spirv.Bitcast %96 : vector<2xf32> to vector<4xf16>
            %98 = spirv.FMul %95, %97 : vector<4xf16>
            %99 = spirv.FAdd %93, %87 : vector<4xf16>
            %100 = spirv.FAdd %98, %88 : vector<4xf16>
            %101 = spirv.VectorShuffle [0 : i32, 1 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %102 = spirv.Bitcast %101 : vector<2xf32> to vector<4xf16>
            %103 = spirv.VectorShuffle [0 : i32, 1 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %104 = spirv.Bitcast %103 : vector<2xf32> to vector<4xf16>
            %105 = spirv.FMul %102, %104 : vector<4xf16>
            %106 = spirv.VectorShuffle [2 : i32, 3 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %107 = spirv.Bitcast %106 : vector<2xf32> to vector<4xf16>
            %108 = spirv.VectorShuffle [2 : i32, 3 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %109 = spirv.Bitcast %108 : vector<2xf32> to vector<4xf16>
            %110 = spirv.FMul %107, %109 : vector<4xf16>
            %111 = spirv.FAdd %105, %99 : vector<4xf16>
            %112 = spirv.FAdd %110, %100 : vector<4xf16>
            %113 = spirv.VectorShuffle [0 : i32, 1 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %114 = spirv.Bitcast %113 : vector<2xf32> to vector<4xf16>
            %115 = spirv.VectorShuffle [0 : i32, 1 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %116 = spirv.Bitcast %115 : vector<2xf32> to vector<4xf16>
            %117 = spirv.FMul %114, %116 : vector<4xf16>
            %118 = spirv.VectorShuffle [2 : i32, 3 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %119 = spirv.Bitcast %118 : vector<2xf32> to vector<4xf16>
            %120 = spirv.VectorShuffle [2 : i32, 3 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %121 = spirv.Bitcast %120 : vector<2xf32> to vector<4xf16>
            %122 = spirv.FMul %119, %121 : vector<4xf16>
            %123 = spirv.FAdd %117, %111 : vector<4xf16>
            %124 = spirv.FAdd %122, %112 : vector<4xf16>
            %125 = spirv.VectorShuffle [0 : i32, 1 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %126 = spirv.Bitcast %125 : vector<2xf32> to vector<4xf16>
            %127 = spirv.VectorShuffle [0 : i32, 1 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %128 = spirv.Bitcast %127 : vector<2xf32> to vector<4xf16>
            %129 = spirv.FMul %126, %128 : vector<4xf16>
            %130 = spirv.VectorShuffle [2 : i32, 3 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %131 = spirv.Bitcast %130 : vector<2xf32> to vector<4xf16>
            %132 = spirv.VectorShuffle [2 : i32, 3 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %133 = spirv.Bitcast %132 : vector<2xf32> to vector<4xf16>
            %134 = spirv.FMul %131, %133 : vector<4xf16>
            %135 = spirv.FAdd %129, %123 : vector<4xf16>
            %136 = spirv.FAdd %134, %124 : vector<4xf16>
            %137 = spirv.VectorShuffle [0 : i32, 1 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %138 = spirv.Bitcast %137 : vector<2xf32> to vector<4xf16>
            %139 = spirv.VectorShuffle [0 : i32, 1 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %140 = spirv.Bitcast %139 : vector<2xf32> to vector<4xf16>
            %141 = spirv.FMul %138, %140 : vector<4xf16>
            %142 = spirv.VectorShuffle [2 : i32, 3 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %143 = spirv.Bitcast %142 : vector<2xf32> to vector<4xf16>
            %144 = spirv.VectorShuffle [2 : i32, 3 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %145 = spirv.Bitcast %144 : vector<2xf32> to vector<4xf16>
            %146 = spirv.FMul %143, %145 : vector<4xf16>
            %147 = spirv.FAdd %141, %135 : vector<4xf16>
            %148 = spirv.FAdd %146, %136 : vector<4xf16>
            %149 = spirv.CompositeExtract %147[0 : i32] : vector<4xf16>
            %150 = spirv.CompositeExtract %147[1 : i32] : vector<4xf16>
            %151 = spirv.CompositeExtract %147[2 : i32] : vector<4xf16>
            %152 = spirv.CompositeExtract %147[3 : i32] : vector<4xf16>
            %153 = spirv.FAdd %149, %150 : f16
            %154 = spirv.FAdd %153, %151 : f16
            %155 = spirv.FAdd %154, %152 : f16
            %156 = spirv.CompositeExtract %148[0 : i32] : vector<4xf16>
            %157 = spirv.CompositeExtract %148[1 : i32] : vector<4xf16>
            %158 = spirv.CompositeExtract %148[2 : i32] : vector<4xf16>
            %159 = spirv.CompositeExtract %148[3 : i32] : vector<4xf16>
            %160 = spirv.FAdd %156, %157 : f16
            %161 = spirv.FAdd %160, %158 : f16
            %162 = spirv.FAdd %161, %159 : f16
            %163 = spirv.FAdd %155, %162 : f16
            %164 = spirv.GroupNonUniformFAdd "Subgroup" "Reduce" %163 : f16
            %165 = spirv.FAdd %164, %cst_f16 : f16
            %166 = spirv.IEqual %1, %cst0_i32 : i32
            spirv.mlir.selection {
              spirv.BranchConditional %166, ^bb1, ^bb2
            ^bb1:  // pred: ^bb0
              %167 = spirv.AccessChain %__resource_var_0_2__addr[%cst0_i32, %3] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>, i32, i32
              spirv.Store "StorageBuffer" %167, %165 : f16
              spirv.Branch ^bb2
            ^bb2:  // 2 preds: ^bb0, ^bb1
              spirv.mlir.merge
            }
            spirv.Return
          }
          spirv.EntryPoint "GLCompute" @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16, @__builtin__LocalInvocationId__, @__builtin__WorkgroupId__
          spirv.ExecutionMode @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 "LocalSize", 64, 1, 1
        }
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %device = hal.ex.shared_device : !hal.device
    %c-1_i64 = arith.constant -1 : i64
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %c0_i64 = arith.constant 0 : i64
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %device_0 = hal.ex.shared_device : !hal.device
    %c-1_i64_1 = arith.constant -1 : i64
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_2 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64_1) wait(%fence) signal(%fence_2) commands([%cmd])
    %c-1_i32 = arith.constant -1 : i32
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %device_3 = hal.ex.shared_device : !hal.device
    %c-1_i64_4 = arith.constant -1 : i64
    %2 = util.null : !hal.fence
    %fence_5 = hal.fence.create device(%device_3 : !hal.device) flags("None") : !hal.fence
    %c0_i64_6 = arith.constant 0 : i64
    %transient_buffer_7 = hal.device.queue.alloca<%device_3 : !hal.device> affinity(%c-1_i64_4) wait(%2) signal(%fence_5) pool(%c0_i64_6) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %device_8 = hal.ex.shared_device : !hal.device
    %c-1_i64_9 = arith.constant -1 : i64
    %cmd_10 = hal.command_buffer.create device(%device_8 : !hal.device) mode(OneShot) categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_10 : !hal.command_buffer> target(%transient_buffer_7 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_10 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_10 : !hal.command_buffer>
    %fence_11 = hal.fence.create device(%device_8 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_8 : !hal.device> affinity(%c-1_i64_9) wait(%fence_5) signal(%fence_11) commands([%cmd_10])
    %c-1_i32_12 = arith.constant -1 : i32
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32_12) : i32
    util.status.check_ok %status_13, "failed to wait on timepoint"
    %3 = util.optimization_barrier %transient_buffer_7 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_14 = hal.buffer.length<%3 : !hal.buffer> : index
    %device_15 = hal.ex.shared_device : !hal.device
    %c-1_i64_16 = arith.constant -1 : i64
    %4 = util.null : !hal.fence
    %fence_17 = hal.fence.create device(%device_15 : !hal.device) flags("None") : !hal.fence
    %c0_i64_18 = arith.constant 0 : i64
    %transient_buffer_19 = hal.device.queue.alloca<%device_15 : !hal.device> affinity(%c-1_i64_16) wait(%4) signal(%fence_17) pool(%c0_i64_18) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %device_20 = hal.ex.shared_device : !hal.device
    %c-1_i64_21 = arith.constant -1 : i64
    %cmd_22 = hal.command_buffer.create device(%device_20 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %5 = hal.command_buffer.device<%cmd_22 : !hal.command_buffer> : !hal.device
    %ok, %value = hal.device.query<%5 : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    %c-1 = arith.constant -1 : index
    %c0_23 = arith.constant 0 : index
    %6 = arith.select %value, %c0_23, %c-1 : index
    scf.index_switch %6 
    case 0 {
      %pipeline_layout = hal.pipeline_layout.lookup device(%5 : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      %c0_36 = arith.constant 0 : index
      %c1_37 = arith.constant 1 : index
      %c2 = arith.constant 2 : index
      %c0_38 = arith.constant 0 : index
      hal.command_buffer.push_descriptor_set<%cmd_22 : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0_38] bindings([
        %c0_36 = (%1 : !hal.buffer)[%c0, %len], 
        %c1_37 = (%3 : !hal.buffer)[%c0, %len_14], 
        %c2 = (%transient_buffer_19 : !hal.buffer)[%c0, %c128000]
      ])
      %c32000_39 = arith.constant 32000 : index
      %c1_40 = arith.constant 1 : index
      hal.command_buffer.dispatch.symbol<%cmd_22 : !hal.command_buffer> target(@_main_dispatch_0::@vulkan_spirv_fb::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16) workgroups([%c32000_39, %c1_40, %c1_40])
      scf.yield
    }
    default {
    }
    hal.command_buffer.fill_buffer<%cmd_22 : !hal.command_buffer> target(%transient_buffer_19 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_22 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_22 : !hal.command_buffer>
    %fence_24 = hal.fence.create device(%device_20 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_20 : !hal.device> affinity(%c-1_i64_21) wait(%fence_17) signal(%fence_24) commands([%cmd_22])
    %c-1_i32_25 = arith.constant -1 : i32
    %status_26 = hal.fence.await until([%fence_24]) timeout_millis(%c-1_i32_25) : i32
    util.status.check_ok %status_26, "failed to wait on timepoint"
    %buffer = hal.buffer.subspan<%transient_buffer_19 : !hal.buffer>[%c64000, %c64000] : !hal.buffer
    %buffer_27 = hal.buffer.subspan<%transient_buffer_19 : !hal.buffer>[%c0, %c64000] : !hal.buffer
    %c1 = arith.constant 1 : index
    %c32000 = arith.constant 32000 : index
    %c0_28 = arith.constant 0 : index
    %c553648144_i32 = arith.constant 553648144 : i32
    %c1_i32 = arith.constant 1 : i32
    %view = hal.buffer_view.create buffer(%buffer : !hal.buffer)[%c0_28, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %c1_29 = arith.constant 1 : index
    %c32000_30 = arith.constant 32000 : index
    %c0_31 = arith.constant 0 : index
    %c553648144_i32_32 = arith.constant 553648144 : i32
    %c1_i32_33 = arith.constant 1 : i32
    %view_34 = hal.buffer_view.create buffer(%buffer_27 : !hal.buffer)[%c0_31, %c64000] shape([%c1_29, %c32000_30]) type(%c553648144_i32_32) encoding(%c1_i32_33) : !hal.buffer_view
    %device_35 = hal.ex.shared_device : !hal.device
    check.expect_eq<%device_35>(%view_34, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::FixupLegacySyncPass (iree-hal-fixup-legacy-sync) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<SPIRVSubgroupReduce>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.variant public @vulkan_spirv_fb target(#executable_target_vulkan_spirv_fb) {
      hal.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c32000 = arith.constant 32000 : index
        %c1 = arith.constant 1 : index
        hal.return %c32000, %c1, %c1 : index, index, index
      }
      builtin.module attributes {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>} {
        spirv.module Logical GLSL450 requires #spirv.vce<v1.3, [Shader, StorageBuffer16BitAccess, Float16, GroupNonUniformArithmetic], [SPV_KHR_storage_buffer_storage_class, SPV_KHR_16bit_storage]> {
          spirv.GlobalVariable @__builtin__WorkgroupId__ built_in("WorkgroupId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__builtin__LocalInvocationId__ built_in("LocalInvocationId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__resource_var_0_0_ bind(0, 0) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_1_ bind(0, 1) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_2_ bind(0, 2) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
          spirv.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() "None" {
            %cst64_i32 = spirv.Constant 64 : i32
            %cst128_i32 = spirv.Constant 128 : i32
            %cst192_i32 = spirv.Constant 192 : i32
            %cst256_i32 = spirv.Constant 256 : i32
            %cst320_i32 = spirv.Constant 320 : i32
            %cst384_i32 = spirv.Constant 384 : i32
            %cst448_i32 = spirv.Constant 448 : i32
            %cst512_i32 = spirv.Constant 512 : i32
            %cst_vec_4xf16 = spirv.Constant dense<0.000000e+00> : vector<4xf16>
            %cst0_i32 = spirv.Constant 0 : i32
            %cst_f16 = spirv.Constant 0.000000e+00 : f16
            %__builtin__LocalInvocationId___addr = spirv.mlir.addressof @__builtin__LocalInvocationId__ : !spirv.ptr<vector<3xi32>, Input>
            %0 = spirv.Load "Input" %__builtin__LocalInvocationId___addr : vector<3xi32>
            %1 = spirv.CompositeExtract %0[0 : i32] : vector<3xi32>
            %__resource_var_0_0__addr = spirv.mlir.addressof @__resource_var_0_0_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_1__addr = spirv.mlir.addressof @__resource_var_0_1_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_2__addr = spirv.mlir.addressof @__resource_var_0_2_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
            %__builtin__WorkgroupId___addr = spirv.mlir.addressof @__builtin__WorkgroupId__ : !spirv.ptr<vector<3xi32>, Input>
            %2 = spirv.Load "Input" %__builtin__WorkgroupId___addr : vector<3xi32>
            %3 = spirv.CompositeExtract %2[0 : i32] : vector<3xi32>
            %4 = spirv.IMul %3, %cst512_i32 : i32
            %5 = spirv.IAdd %1, %4 : i32
            %6 = spirv.IAdd %5, %cst448_i32 : i32
            %7 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %6] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %8 = spirv.Load "StorageBuffer" %7 : vector<4xf32>
            %9 = spirv.IAdd %1, %cst448_i32 : i32
            %10 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %9] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %11 = spirv.Load "StorageBuffer" %10 : vector<4xf32>
            %12 = spirv.IAdd %5, %cst384_i32 : i32
            %13 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %12] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %14 = spirv.Load "StorageBuffer" %13 : vector<4xf32>
            %15 = spirv.IAdd %1, %cst384_i32 : i32
            %16 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %15] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %17 = spirv.Load "StorageBuffer" %16 : vector<4xf32>
            %18 = spirv.IAdd %5, %cst320_i32 : i32
            %19 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %18] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %20 = spirv.Load "StorageBuffer" %19 : vector<4xf32>
            %21 = spirv.IAdd %1, %cst320_i32 : i32
            %22 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %21] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %23 = spirv.Load "StorageBuffer" %22 : vector<4xf32>
            %24 = spirv.IAdd %5, %cst256_i32 : i32
            %25 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %24] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %26 = spirv.Load "StorageBuffer" %25 : vector<4xf32>
            %27 = spirv.IAdd %1, %cst256_i32 : i32
            %28 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %27] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %29 = spirv.Load "StorageBuffer" %28 : vector<4xf32>
            %30 = spirv.IAdd %5, %cst192_i32 : i32
            %31 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %30] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %32 = spirv.Load "StorageBuffer" %31 : vector<4xf32>
            %33 = spirv.IAdd %1, %cst192_i32 : i32
            %34 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %33] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %35 = spirv.Load "StorageBuffer" %34 : vector<4xf32>
            %36 = spirv.IAdd %5, %cst128_i32 : i32
            %37 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %36] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %38 = spirv.Load "StorageBuffer" %37 : vector<4xf32>
            %39 = spirv.IAdd %1, %cst128_i32 : i32
            %40 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %39] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %41 = spirv.Load "StorageBuffer" %40 : vector<4xf32>
            %42 = spirv.IAdd %5, %cst64_i32 : i32
            %43 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %42] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %44 = spirv.Load "StorageBuffer" %43 : vector<4xf32>
            %45 = spirv.IAdd %1, %cst64_i32 : i32
            %46 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %45] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %47 = spirv.Load "StorageBuffer" %46 : vector<4xf32>
            %48 = spirv.IAdd %4, %1 : i32
            %49 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %48] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %50 = spirv.Load "StorageBuffer" %49 : vector<4xf32>
            %51 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %1] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %52 = spirv.Load "StorageBuffer" %51 : vector<4xf32>
            %53 = spirv.VectorShuffle [0 : i32, 1 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %54 = spirv.Bitcast %53 : vector<2xf32> to vector<4xf16>
            %55 = spirv.VectorShuffle [0 : i32, 1 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %56 = spirv.Bitcast %55 : vector<2xf32> to vector<4xf16>
            %57 = spirv.FMul %54, %56 : vector<4xf16>
            %58 = spirv.VectorShuffle [2 : i32, 3 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %59 = spirv.Bitcast %58 : vector<2xf32> to vector<4xf16>
            %60 = spirv.VectorShuffle [2 : i32, 3 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %61 = spirv.Bitcast %60 : vector<2xf32> to vector<4xf16>
            %62 = spirv.FMul %59, %61 : vector<4xf16>
            %63 = spirv.FAdd %57, %cst_vec_4xf16 : vector<4xf16>
            %64 = spirv.FAdd %62, %cst_vec_4xf16 : vector<4xf16>
            %65 = spirv.VectorShuffle [0 : i32, 1 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %66 = spirv.Bitcast %65 : vector<2xf32> to vector<4xf16>
            %67 = spirv.VectorShuffle [0 : i32, 1 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %68 = spirv.Bitcast %67 : vector<2xf32> to vector<4xf16>
            %69 = spirv.FMul %66, %68 : vector<4xf16>
            %70 = spirv.VectorShuffle [2 : i32, 3 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %71 = spirv.Bitcast %70 : vector<2xf32> to vector<4xf16>
            %72 = spirv.VectorShuffle [2 : i32, 3 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %73 = spirv.Bitcast %72 : vector<2xf32> to vector<4xf16>
            %74 = spirv.FMul %71, %73 : vector<4xf16>
            %75 = spirv.FAdd %69, %63 : vector<4xf16>
            %76 = spirv.FAdd %74, %64 : vector<4xf16>
            %77 = spirv.VectorShuffle [0 : i32, 1 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %78 = spirv.Bitcast %77 : vector<2xf32> to vector<4xf16>
            %79 = spirv.VectorShuffle [0 : i32, 1 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %80 = spirv.Bitcast %79 : vector<2xf32> to vector<4xf16>
            %81 = spirv.FMul %78, %80 : vector<4xf16>
            %82 = spirv.VectorShuffle [2 : i32, 3 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %83 = spirv.Bitcast %82 : vector<2xf32> to vector<4xf16>
            %84 = spirv.VectorShuffle [2 : i32, 3 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %85 = spirv.Bitcast %84 : vector<2xf32> to vector<4xf16>
            %86 = spirv.FMul %83, %85 : vector<4xf16>
            %87 = spirv.FAdd %81, %75 : vector<4xf16>
            %88 = spirv.FAdd %86, %76 : vector<4xf16>
            %89 = spirv.VectorShuffle [0 : i32, 1 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %90 = spirv.Bitcast %89 : vector<2xf32> to vector<4xf16>
            %91 = spirv.VectorShuffle [0 : i32, 1 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %92 = spirv.Bitcast %91 : vector<2xf32> to vector<4xf16>
            %93 = spirv.FMul %90, %92 : vector<4xf16>
            %94 = spirv.VectorShuffle [2 : i32, 3 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %95 = spirv.Bitcast %94 : vector<2xf32> to vector<4xf16>
            %96 = spirv.VectorShuffle [2 : i32, 3 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %97 = spirv.Bitcast %96 : vector<2xf32> to vector<4xf16>
            %98 = spirv.FMul %95, %97 : vector<4xf16>
            %99 = spirv.FAdd %93, %87 : vector<4xf16>
            %100 = spirv.FAdd %98, %88 : vector<4xf16>
            %101 = spirv.VectorShuffle [0 : i32, 1 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %102 = spirv.Bitcast %101 : vector<2xf32> to vector<4xf16>
            %103 = spirv.VectorShuffle [0 : i32, 1 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %104 = spirv.Bitcast %103 : vector<2xf32> to vector<4xf16>
            %105 = spirv.FMul %102, %104 : vector<4xf16>
            %106 = spirv.VectorShuffle [2 : i32, 3 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %107 = spirv.Bitcast %106 : vector<2xf32> to vector<4xf16>
            %108 = spirv.VectorShuffle [2 : i32, 3 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %109 = spirv.Bitcast %108 : vector<2xf32> to vector<4xf16>
            %110 = spirv.FMul %107, %109 : vector<4xf16>
            %111 = spirv.FAdd %105, %99 : vector<4xf16>
            %112 = spirv.FAdd %110, %100 : vector<4xf16>
            %113 = spirv.VectorShuffle [0 : i32, 1 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %114 = spirv.Bitcast %113 : vector<2xf32> to vector<4xf16>
            %115 = spirv.VectorShuffle [0 : i32, 1 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %116 = spirv.Bitcast %115 : vector<2xf32> to vector<4xf16>
            %117 = spirv.FMul %114, %116 : vector<4xf16>
            %118 = spirv.VectorShuffle [2 : i32, 3 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %119 = spirv.Bitcast %118 : vector<2xf32> to vector<4xf16>
            %120 = spirv.VectorShuffle [2 : i32, 3 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %121 = spirv.Bitcast %120 : vector<2xf32> to vector<4xf16>
            %122 = spirv.FMul %119, %121 : vector<4xf16>
            %123 = spirv.FAdd %117, %111 : vector<4xf16>
            %124 = spirv.FAdd %122, %112 : vector<4xf16>
            %125 = spirv.VectorShuffle [0 : i32, 1 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %126 = spirv.Bitcast %125 : vector<2xf32> to vector<4xf16>
            %127 = spirv.VectorShuffle [0 : i32, 1 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %128 = spirv.Bitcast %127 : vector<2xf32> to vector<4xf16>
            %129 = spirv.FMul %126, %128 : vector<4xf16>
            %130 = spirv.VectorShuffle [2 : i32, 3 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %131 = spirv.Bitcast %130 : vector<2xf32> to vector<4xf16>
            %132 = spirv.VectorShuffle [2 : i32, 3 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %133 = spirv.Bitcast %132 : vector<2xf32> to vector<4xf16>
            %134 = spirv.FMul %131, %133 : vector<4xf16>
            %135 = spirv.FAdd %129, %123 : vector<4xf16>
            %136 = spirv.FAdd %134, %124 : vector<4xf16>
            %137 = spirv.VectorShuffle [0 : i32, 1 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %138 = spirv.Bitcast %137 : vector<2xf32> to vector<4xf16>
            %139 = spirv.VectorShuffle [0 : i32, 1 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %140 = spirv.Bitcast %139 : vector<2xf32> to vector<4xf16>
            %141 = spirv.FMul %138, %140 : vector<4xf16>
            %142 = spirv.VectorShuffle [2 : i32, 3 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %143 = spirv.Bitcast %142 : vector<2xf32> to vector<4xf16>
            %144 = spirv.VectorShuffle [2 : i32, 3 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %145 = spirv.Bitcast %144 : vector<2xf32> to vector<4xf16>
            %146 = spirv.FMul %143, %145 : vector<4xf16>
            %147 = spirv.FAdd %141, %135 : vector<4xf16>
            %148 = spirv.FAdd %146, %136 : vector<4xf16>
            %149 = spirv.CompositeExtract %147[0 : i32] : vector<4xf16>
            %150 = spirv.CompositeExtract %147[1 : i32] : vector<4xf16>
            %151 = spirv.CompositeExtract %147[2 : i32] : vector<4xf16>
            %152 = spirv.CompositeExtract %147[3 : i32] : vector<4xf16>
            %153 = spirv.FAdd %149, %150 : f16
            %154 = spirv.FAdd %153, %151 : f16
            %155 = spirv.FAdd %154, %152 : f16
            %156 = spirv.CompositeExtract %148[0 : i32] : vector<4xf16>
            %157 = spirv.CompositeExtract %148[1 : i32] : vector<4xf16>
            %158 = spirv.CompositeExtract %148[2 : i32] : vector<4xf16>
            %159 = spirv.CompositeExtract %148[3 : i32] : vector<4xf16>
            %160 = spirv.FAdd %156, %157 : f16
            %161 = spirv.FAdd %160, %158 : f16
            %162 = spirv.FAdd %161, %159 : f16
            %163 = spirv.FAdd %155, %162 : f16
            %164 = spirv.GroupNonUniformFAdd "Subgroup" "Reduce" %163 : f16
            %165 = spirv.FAdd %164, %cst_f16 : f16
            %166 = spirv.IEqual %1, %cst0_i32 : i32
            spirv.mlir.selection {
              spirv.BranchConditional %166, ^bb1, ^bb2
            ^bb1:  // pred: ^bb0
              %167 = spirv.AccessChain %__resource_var_0_2__addr[%cst0_i32, %3] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>, i32, i32
              spirv.Store "StorageBuffer" %167, %165 : f16
              spirv.Branch ^bb2
            ^bb2:  // 2 preds: ^bb0, ^bb1
              spirv.mlir.merge
            }
            spirv.Return
          }
          spirv.EntryPoint "GLCompute" @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16, @__builtin__LocalInvocationId__, @__builtin__WorkgroupId__
          spirv.ExecutionMode @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 "LocalSize", 64, 1, 1
        }
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %device = hal.ex.shared_device : !hal.device
    %c-1_i64 = arith.constant -1 : i64
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %c0_i64 = arith.constant 0 : i64
    %c-1_i32 = arith.constant -1 : i32
    %status = hal.fence.await until([%0]) timeout_millis(%c-1_i32) : i32
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status_0 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %device_1 = hal.ex.shared_device : !hal.device
    %c-1_i64_2 = arith.constant -1 : i64
    %cmd = hal.command_buffer.create device(%device_1 : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_3 = hal.fence.create device(%device_1 : !hal.device) flags("None") : !hal.fence
    %c-1_i32_4 = arith.constant -1 : i32
    %status_5 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32_4) : i32
    %1 = util.null : !hal.fence
    hal.device.queue.execute<%device_1 : !hal.device> affinity(%c-1_i64_2) wait(%1) signal(%fence_3) commands([%cmd])
    %c-1_i32_6 = arith.constant -1 : i32
    %status_7 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32_6) : i32
    util.status.check_ok %status_7, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer : !hal.buffer
    %device_8 = hal.ex.shared_device : !hal.device
    %c-1_i64_9 = arith.constant -1 : i64
    %3 = util.null : !hal.fence
    %fence_10 = hal.fence.create device(%device_8 : !hal.device) flags("None") : !hal.fence
    %c0_i64_11 = arith.constant 0 : i64
    %c-1_i32_12 = arith.constant -1 : i32
    %status_13 = hal.fence.await until([%3]) timeout_millis(%c-1_i32_12) : i32
    %transient_buffer_14 = hal.device.queue.alloca<%device_8 : !hal.device> affinity(%c-1_i64_9) wait(%3) signal(%fence_10) pool(%c0_i64_11) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_15 = hal.fence.await until([%fence_10]) timeout_millis(%c-1_i32_12) : i32
    %device_16 = hal.ex.shared_device : !hal.device
    %c-1_i64_17 = arith.constant -1 : i64
    %cmd_18 = hal.command_buffer.create device(%device_16 : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_18 : !hal.command_buffer> target(%transient_buffer_14 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_18 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_18 : !hal.command_buffer>
    %fence_19 = hal.fence.create device(%device_16 : !hal.device) flags("None") : !hal.fence
    %c-1_i32_20 = arith.constant -1 : i32
    %status_21 = hal.fence.await until([%fence_10]) timeout_millis(%c-1_i32_20) : i32
    %4 = util.null : !hal.fence
    hal.device.queue.execute<%device_16 : !hal.device> affinity(%c-1_i64_17) wait(%4) signal(%fence_19) commands([%cmd_18])
    %c-1_i32_22 = arith.constant -1 : i32
    %status_23 = hal.fence.await until([%fence_19]) timeout_millis(%c-1_i32_22) : i32
    util.status.check_ok %status_23, "failed to wait on timepoint"
    %5 = util.optimization_barrier %transient_buffer_14 : !hal.buffer
    %len = hal.buffer.length<%2 : !hal.buffer> : index
    %len_24 = hal.buffer.length<%5 : !hal.buffer> : index
    %device_25 = hal.ex.shared_device : !hal.device
    %c-1_i64_26 = arith.constant -1 : i64
    %6 = util.null : !hal.fence
    %fence_27 = hal.fence.create device(%device_25 : !hal.device) flags("None") : !hal.fence
    %c0_i64_28 = arith.constant 0 : i64
    %c-1_i32_29 = arith.constant -1 : i32
    %status_30 = hal.fence.await until([%6]) timeout_millis(%c-1_i32_29) : i32
    %transient_buffer_31 = hal.device.queue.alloca<%device_25 : !hal.device> affinity(%c-1_i64_26) wait(%6) signal(%fence_27) pool(%c0_i64_28) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_32 = hal.fence.await until([%fence_27]) timeout_millis(%c-1_i32_29) : i32
    %device_33 = hal.ex.shared_device : !hal.device
    %c-1_i64_34 = arith.constant -1 : i64
    %cmd_35 = hal.command_buffer.create device(%device_33 : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %7 = hal.command_buffer.device<%cmd_35 : !hal.command_buffer> : !hal.device
    %ok, %value = hal.device.query<%7 : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    %c-1 = arith.constant -1 : index
    %c0_36 = arith.constant 0 : index
    %8 = arith.select %value, %c0_36, %c-1 : index
    scf.index_switch %8 
    case 0 {
      %pipeline_layout = hal.pipeline_layout.lookup device(%7 : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      %c0_51 = arith.constant 0 : index
      %c1_52 = arith.constant 1 : index
      %c2 = arith.constant 2 : index
      %c0_53 = arith.constant 0 : index
      hal.command_buffer.push_descriptor_set<%cmd_35 : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0_53] bindings([
        %c0_51 = (%2 : !hal.buffer)[%c0, %len], 
        %c1_52 = (%5 : !hal.buffer)[%c0, %len_24], 
        %c2 = (%transient_buffer_31 : !hal.buffer)[%c0, %c128000]
      ])
      %c32000_54 = arith.constant 32000 : index
      %c1_55 = arith.constant 1 : index
      hal.command_buffer.dispatch.symbol<%cmd_35 : !hal.command_buffer> target(@_main_dispatch_0::@vulkan_spirv_fb::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16) workgroups([%c32000_54, %c1_55, %c1_55])
      scf.yield
    }
    default {
    }
    hal.command_buffer.fill_buffer<%cmd_35 : !hal.command_buffer> target(%transient_buffer_31 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_35 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_35 : !hal.command_buffer>
    %fence_37 = hal.fence.create device(%device_33 : !hal.device) flags("None") : !hal.fence
    %c-1_i32_38 = arith.constant -1 : i32
    %status_39 = hal.fence.await until([%fence_27]) timeout_millis(%c-1_i32_38) : i32
    %9 = util.null : !hal.fence
    hal.device.queue.execute<%device_33 : !hal.device> affinity(%c-1_i64_34) wait(%9) signal(%fence_37) commands([%cmd_35])
    %c-1_i32_40 = arith.constant -1 : i32
    %status_41 = hal.fence.await until([%fence_37]) timeout_millis(%c-1_i32_40) : i32
    util.status.check_ok %status_41, "failed to wait on timepoint"
    %buffer = hal.buffer.subspan<%transient_buffer_31 : !hal.buffer>[%c64000, %c64000] : !hal.buffer
    %buffer_42 = hal.buffer.subspan<%transient_buffer_31 : !hal.buffer>[%c0, %c64000] : !hal.buffer
    %c1 = arith.constant 1 : index
    %c32000 = arith.constant 32000 : index
    %c0_43 = arith.constant 0 : index
    %c553648144_i32 = arith.constant 553648144 : i32
    %c1_i32 = arith.constant 1 : i32
    %view = hal.buffer_view.create buffer(%buffer : !hal.buffer)[%c0_43, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %c1_44 = arith.constant 1 : index
    %c32000_45 = arith.constant 32000 : index
    %c0_46 = arith.constant 0 : index
    %c553648144_i32_47 = arith.constant 553648144 : i32
    %c1_i32_48 = arith.constant 1 : i32
    %view_49 = hal.buffer_view.create buffer(%buffer_42 : !hal.buffer)[%c0_46, %c64000] shape([%c1_44, %c32000_45]) type(%c553648144_i32_47) encoding(%c1_i32_48) : !hal.buffer_view
    %device_50 = hal.ex.shared_device : !hal.device
    check.expect_eq<%device_50>(%view_49, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<SPIRVSubgroupReduce>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.variant public @vulkan_spirv_fb target(#executable_target_vulkan_spirv_fb) {
      hal.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c32000 = arith.constant 32000 : index
        %c1 = arith.constant 1 : index
        hal.return %c32000, %c1, %c1 : index, index, index
      }
      builtin.module attributes {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>} {
        spirv.module Logical GLSL450 requires #spirv.vce<v1.3, [Shader, StorageBuffer16BitAccess, Float16, GroupNonUniformArithmetic], [SPV_KHR_storage_buffer_storage_class, SPV_KHR_16bit_storage]> {
          spirv.GlobalVariable @__builtin__WorkgroupId__ built_in("WorkgroupId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__builtin__LocalInvocationId__ built_in("LocalInvocationId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__resource_var_0_0_ bind(0, 0) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_1_ bind(0, 1) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_2_ bind(0, 2) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
          spirv.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() "None" {
            %cst64_i32 = spirv.Constant 64 : i32
            %cst128_i32 = spirv.Constant 128 : i32
            %cst192_i32 = spirv.Constant 192 : i32
            %cst256_i32 = spirv.Constant 256 : i32
            %cst320_i32 = spirv.Constant 320 : i32
            %cst384_i32 = spirv.Constant 384 : i32
            %cst448_i32 = spirv.Constant 448 : i32
            %cst512_i32 = spirv.Constant 512 : i32
            %cst_vec_4xf16 = spirv.Constant dense<0.000000e+00> : vector<4xf16>
            %cst0_i32 = spirv.Constant 0 : i32
            %cst_f16 = spirv.Constant 0.000000e+00 : f16
            %__builtin__LocalInvocationId___addr = spirv.mlir.addressof @__builtin__LocalInvocationId__ : !spirv.ptr<vector<3xi32>, Input>
            %0 = spirv.Load "Input" %__builtin__LocalInvocationId___addr : vector<3xi32>
            %1 = spirv.CompositeExtract %0[0 : i32] : vector<3xi32>
            %__resource_var_0_0__addr = spirv.mlir.addressof @__resource_var_0_0_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_1__addr = spirv.mlir.addressof @__resource_var_0_1_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_2__addr = spirv.mlir.addressof @__resource_var_0_2_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
            %__builtin__WorkgroupId___addr = spirv.mlir.addressof @__builtin__WorkgroupId__ : !spirv.ptr<vector<3xi32>, Input>
            %2 = spirv.Load "Input" %__builtin__WorkgroupId___addr : vector<3xi32>
            %3 = spirv.CompositeExtract %2[0 : i32] : vector<3xi32>
            %4 = spirv.IMul %3, %cst512_i32 : i32
            %5 = spirv.IAdd %1, %4 : i32
            %6 = spirv.IAdd %5, %cst448_i32 : i32
            %7 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %6] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %8 = spirv.Load "StorageBuffer" %7 : vector<4xf32>
            %9 = spirv.IAdd %1, %cst448_i32 : i32
            %10 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %9] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %11 = spirv.Load "StorageBuffer" %10 : vector<4xf32>
            %12 = spirv.IAdd %5, %cst384_i32 : i32
            %13 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %12] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %14 = spirv.Load "StorageBuffer" %13 : vector<4xf32>
            %15 = spirv.IAdd %1, %cst384_i32 : i32
            %16 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %15] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %17 = spirv.Load "StorageBuffer" %16 : vector<4xf32>
            %18 = spirv.IAdd %5, %cst320_i32 : i32
            %19 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %18] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %20 = spirv.Load "StorageBuffer" %19 : vector<4xf32>
            %21 = spirv.IAdd %1, %cst320_i32 : i32
            %22 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %21] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %23 = spirv.Load "StorageBuffer" %22 : vector<4xf32>
            %24 = spirv.IAdd %5, %cst256_i32 : i32
            %25 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %24] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %26 = spirv.Load "StorageBuffer" %25 : vector<4xf32>
            %27 = spirv.IAdd %1, %cst256_i32 : i32
            %28 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %27] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %29 = spirv.Load "StorageBuffer" %28 : vector<4xf32>
            %30 = spirv.IAdd %5, %cst192_i32 : i32
            %31 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %30] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %32 = spirv.Load "StorageBuffer" %31 : vector<4xf32>
            %33 = spirv.IAdd %1, %cst192_i32 : i32
            %34 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %33] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %35 = spirv.Load "StorageBuffer" %34 : vector<4xf32>
            %36 = spirv.IAdd %5, %cst128_i32 : i32
            %37 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %36] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %38 = spirv.Load "StorageBuffer" %37 : vector<4xf32>
            %39 = spirv.IAdd %1, %cst128_i32 : i32
            %40 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %39] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %41 = spirv.Load "StorageBuffer" %40 : vector<4xf32>
            %42 = spirv.IAdd %5, %cst64_i32 : i32
            %43 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %42] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %44 = spirv.Load "StorageBuffer" %43 : vector<4xf32>
            %45 = spirv.IAdd %1, %cst64_i32 : i32
            %46 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %45] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %47 = spirv.Load "StorageBuffer" %46 : vector<4xf32>
            %48 = spirv.IAdd %4, %1 : i32
            %49 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %48] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %50 = spirv.Load "StorageBuffer" %49 : vector<4xf32>
            %51 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %1] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %52 = spirv.Load "StorageBuffer" %51 : vector<4xf32>
            %53 = spirv.VectorShuffle [0 : i32, 1 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %54 = spirv.Bitcast %53 : vector<2xf32> to vector<4xf16>
            %55 = spirv.VectorShuffle [0 : i32, 1 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %56 = spirv.Bitcast %55 : vector<2xf32> to vector<4xf16>
            %57 = spirv.FMul %54, %56 : vector<4xf16>
            %58 = spirv.VectorShuffle [2 : i32, 3 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %59 = spirv.Bitcast %58 : vector<2xf32> to vector<4xf16>
            %60 = spirv.VectorShuffle [2 : i32, 3 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %61 = spirv.Bitcast %60 : vector<2xf32> to vector<4xf16>
            %62 = spirv.FMul %59, %61 : vector<4xf16>
            %63 = spirv.FAdd %57, %cst_vec_4xf16 : vector<4xf16>
            %64 = spirv.FAdd %62, %cst_vec_4xf16 : vector<4xf16>
            %65 = spirv.VectorShuffle [0 : i32, 1 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %66 = spirv.Bitcast %65 : vector<2xf32> to vector<4xf16>
            %67 = spirv.VectorShuffle [0 : i32, 1 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %68 = spirv.Bitcast %67 : vector<2xf32> to vector<4xf16>
            %69 = spirv.FMul %66, %68 : vector<4xf16>
            %70 = spirv.VectorShuffle [2 : i32, 3 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %71 = spirv.Bitcast %70 : vector<2xf32> to vector<4xf16>
            %72 = spirv.VectorShuffle [2 : i32, 3 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %73 = spirv.Bitcast %72 : vector<2xf32> to vector<4xf16>
            %74 = spirv.FMul %71, %73 : vector<4xf16>
            %75 = spirv.FAdd %69, %63 : vector<4xf16>
            %76 = spirv.FAdd %74, %64 : vector<4xf16>
            %77 = spirv.VectorShuffle [0 : i32, 1 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %78 = spirv.Bitcast %77 : vector<2xf32> to vector<4xf16>
            %79 = spirv.VectorShuffle [0 : i32, 1 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %80 = spirv.Bitcast %79 : vector<2xf32> to vector<4xf16>
            %81 = spirv.FMul %78, %80 : vector<4xf16>
            %82 = spirv.VectorShuffle [2 : i32, 3 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %83 = spirv.Bitcast %82 : vector<2xf32> to vector<4xf16>
            %84 = spirv.VectorShuffle [2 : i32, 3 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %85 = spirv.Bitcast %84 : vector<2xf32> to vector<4xf16>
            %86 = spirv.FMul %83, %85 : vector<4xf16>
            %87 = spirv.FAdd %81, %75 : vector<4xf16>
            %88 = spirv.FAdd %86, %76 : vector<4xf16>
            %89 = spirv.VectorShuffle [0 : i32, 1 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %90 = spirv.Bitcast %89 : vector<2xf32> to vector<4xf16>
            %91 = spirv.VectorShuffle [0 : i32, 1 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %92 = spirv.Bitcast %91 : vector<2xf32> to vector<4xf16>
            %93 = spirv.FMul %90, %92 : vector<4xf16>
            %94 = spirv.VectorShuffle [2 : i32, 3 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %95 = spirv.Bitcast %94 : vector<2xf32> to vector<4xf16>
            %96 = spirv.VectorShuffle [2 : i32, 3 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %97 = spirv.Bitcast %96 : vector<2xf32> to vector<4xf16>
            %98 = spirv.FMul %95, %97 : vector<4xf16>
            %99 = spirv.FAdd %93, %87 : vector<4xf16>
            %100 = spirv.FAdd %98, %88 : vector<4xf16>
            %101 = spirv.VectorShuffle [0 : i32, 1 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %102 = spirv.Bitcast %101 : vector<2xf32> to vector<4xf16>
            %103 = spirv.VectorShuffle [0 : i32, 1 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %104 = spirv.Bitcast %103 : vector<2xf32> to vector<4xf16>
            %105 = spirv.FMul %102, %104 : vector<4xf16>
            %106 = spirv.VectorShuffle [2 : i32, 3 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %107 = spirv.Bitcast %106 : vector<2xf32> to vector<4xf16>
            %108 = spirv.VectorShuffle [2 : i32, 3 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %109 = spirv.Bitcast %108 : vector<2xf32> to vector<4xf16>
            %110 = spirv.FMul %107, %109 : vector<4xf16>
            %111 = spirv.FAdd %105, %99 : vector<4xf16>
            %112 = spirv.FAdd %110, %100 : vector<4xf16>
            %113 = spirv.VectorShuffle [0 : i32, 1 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %114 = spirv.Bitcast %113 : vector<2xf32> to vector<4xf16>
            %115 = spirv.VectorShuffle [0 : i32, 1 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %116 = spirv.Bitcast %115 : vector<2xf32> to vector<4xf16>
            %117 = spirv.FMul %114, %116 : vector<4xf16>
            %118 = spirv.VectorShuffle [2 : i32, 3 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %119 = spirv.Bitcast %118 : vector<2xf32> to vector<4xf16>
            %120 = spirv.VectorShuffle [2 : i32, 3 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %121 = spirv.Bitcast %120 : vector<2xf32> to vector<4xf16>
            %122 = spirv.FMul %119, %121 : vector<4xf16>
            %123 = spirv.FAdd %117, %111 : vector<4xf16>
            %124 = spirv.FAdd %122, %112 : vector<4xf16>
            %125 = spirv.VectorShuffle [0 : i32, 1 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %126 = spirv.Bitcast %125 : vector<2xf32> to vector<4xf16>
            %127 = spirv.VectorShuffle [0 : i32, 1 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %128 = spirv.Bitcast %127 : vector<2xf32> to vector<4xf16>
            %129 = spirv.FMul %126, %128 : vector<4xf16>
            %130 = spirv.VectorShuffle [2 : i32, 3 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %131 = spirv.Bitcast %130 : vector<2xf32> to vector<4xf16>
            %132 = spirv.VectorShuffle [2 : i32, 3 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %133 = spirv.Bitcast %132 : vector<2xf32> to vector<4xf16>
            %134 = spirv.FMul %131, %133 : vector<4xf16>
            %135 = spirv.FAdd %129, %123 : vector<4xf16>
            %136 = spirv.FAdd %134, %124 : vector<4xf16>
            %137 = spirv.VectorShuffle [0 : i32, 1 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %138 = spirv.Bitcast %137 : vector<2xf32> to vector<4xf16>
            %139 = spirv.VectorShuffle [0 : i32, 1 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %140 = spirv.Bitcast %139 : vector<2xf32> to vector<4xf16>
            %141 = spirv.FMul %138, %140 : vector<4xf16>
            %142 = spirv.VectorShuffle [2 : i32, 3 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %143 = spirv.Bitcast %142 : vector<2xf32> to vector<4xf16>
            %144 = spirv.VectorShuffle [2 : i32, 3 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %145 = spirv.Bitcast %144 : vector<2xf32> to vector<4xf16>
            %146 = spirv.FMul %143, %145 : vector<4xf16>
            %147 = spirv.FAdd %141, %135 : vector<4xf16>
            %148 = spirv.FAdd %146, %136 : vector<4xf16>
            %149 = spirv.CompositeExtract %147[0 : i32] : vector<4xf16>
            %150 = spirv.CompositeExtract %147[1 : i32] : vector<4xf16>
            %151 = spirv.CompositeExtract %147[2 : i32] : vector<4xf16>
            %152 = spirv.CompositeExtract %147[3 : i32] : vector<4xf16>
            %153 = spirv.FAdd %149, %150 : f16
            %154 = spirv.FAdd %153, %151 : f16
            %155 = spirv.FAdd %154, %152 : f16
            %156 = spirv.CompositeExtract %148[0 : i32] : vector<4xf16>
            %157 = spirv.CompositeExtract %148[1 : i32] : vector<4xf16>
            %158 = spirv.CompositeExtract %148[2 : i32] : vector<4xf16>
            %159 = spirv.CompositeExtract %148[3 : i32] : vector<4xf16>
            %160 = spirv.FAdd %156, %157 : f16
            %161 = spirv.FAdd %160, %158 : f16
            %162 = spirv.FAdd %161, %159 : f16
            %163 = spirv.FAdd %155, %162 : f16
            %164 = spirv.GroupNonUniformFAdd "Subgroup" "Reduce" %163 : f16
            %165 = spirv.FAdd %164, %cst_f16 : f16
            %166 = spirv.IEqual %1, %cst0_i32 : i32
            spirv.mlir.selection {
              spirv.BranchConditional %166, ^bb1, ^bb2
            ^bb1:  // pred: ^bb0
              %167 = spirv.AccessChain %__resource_var_0_2__addr[%cst0_i32, %3] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>, i32, i32
              spirv.Store "StorageBuffer" %167, %165 : f16
              spirv.Branch ^bb2
            ^bb2:  // 2 preds: ^bb0, ^bb1
              spirv.mlir.merge
            }
            spirv.Return
          }
          spirv.EntryPoint "GLCompute" @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16, @__builtin__LocalInvocationId__, @__builtin__WorkgroupId__
          spirv.ExecutionMode @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 "LocalSize", 64, 1, 1
        }
      }
    }
  }
  func.func private @_main() {
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %device = hal.ex.shared_device : !hal.device
    %c-1_i64 = arith.constant -1 : i64
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %c0_i64 = arith.constant 0 : i64
    %c-1_i32 = arith.constant -1 : i32
    %status = hal.fence.await until([%0]) timeout_millis(%c-1_i32) : i32
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status_0 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_2 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_1) commands([%cmd])
    %status_3 = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_3, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_4 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_5 = hal.fence.await until([%0]) timeout_millis(%c-1_i32) : i32
    %transient_buffer_6 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_4) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_7 = hal.fence.await until([%fence_4]) timeout_millis(%c-1_i32) : i32
    %cmd_8 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_8 : !hal.command_buffer> target(%transient_buffer_6 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_8 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_8 : !hal.command_buffer>
    %fence_9 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_10 = hal.fence.await until([%fence_4]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_9) commands([%cmd_8])
    %status_11 = hal.fence.await until([%fence_9]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_11, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_6 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_12 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_13 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_14 = hal.fence.await until([%0]) timeout_millis(%c-1_i32) : i32
    %transient_buffer_15 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_13) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_16 = hal.fence.await until([%fence_13]) timeout_millis(%c-1_i32) : i32
    %cmd_17 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %3 = hal.command_buffer.device<%cmd_17 : !hal.command_buffer> : !hal.device
    %ok, %value = hal.device.query<%3 : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    %c-1 = arith.constant -1 : index
    %4 = arith.select %value, %c0, %c-1 : index
    scf.index_switch %4 
    case 0 {
      %pipeline_layout = hal.pipeline_layout.lookup device(%3 : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      %c1_23 = arith.constant 1 : index
      %c2 = arith.constant 2 : index
      hal.command_buffer.push_descriptor_set<%cmd_17 : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%1 : !hal.buffer)[%c0, %len], 
        %c1_23 = (%2 : !hal.buffer)[%c0, %len_12], 
        %c2 = (%transient_buffer_15 : !hal.buffer)[%c0, %c128000]
      ])
      %c32000_24 = arith.constant 32000 : index
      hal.command_buffer.dispatch.symbol<%cmd_17 : !hal.command_buffer> target(@_main_dispatch_0::@vulkan_spirv_fb::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16) workgroups([%c32000_24, %c1_23, %c1_23])
      scf.yield
    }
    default {
    }
    hal.command_buffer.fill_buffer<%cmd_17 : !hal.command_buffer> target(%transient_buffer_15 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_17 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_17 : !hal.command_buffer>
    %fence_18 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_19 = hal.fence.await until([%fence_13]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_18) commands([%cmd_17])
    %status_20 = hal.fence.await until([%fence_18]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_20, "failed to wait on timepoint"
    %buffer = hal.buffer.subspan<%transient_buffer_15 : !hal.buffer>[%c64000, %c64000] : !hal.buffer
    %buffer_21 = hal.buffer.subspan<%transient_buffer_15 : !hal.buffer>[%c0, %c64000] : !hal.buffer
    %c1 = arith.constant 1 : index
    %c32000 = arith.constant 32000 : index
    %c553648144_i32 = arith.constant 553648144 : i32
    %c1_i32 = arith.constant 1 : i32
    %view = hal.buffer_view.create buffer(%buffer : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_22 = hal.buffer_view.create buffer(%buffer_21 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_22, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<SPIRVSubgroupReduce>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.variant public @vulkan_spirv_fb target(#executable_target_vulkan_spirv_fb) {
      hal.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c32000 = arith.constant 32000 : index
        %c1 = arith.constant 1 : index
        hal.return %c32000, %c1, %c1 : index, index, index
      }
      builtin.module attributes {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>} {
        spirv.module Logical GLSL450 requires #spirv.vce<v1.3, [Shader, StorageBuffer16BitAccess, Float16, GroupNonUniformArithmetic], [SPV_KHR_storage_buffer_storage_class, SPV_KHR_16bit_storage]> {
          spirv.GlobalVariable @__builtin__WorkgroupId__ built_in("WorkgroupId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__builtin__LocalInvocationId__ built_in("LocalInvocationId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__resource_var_0_0_ bind(0, 0) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_1_ bind(0, 1) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_2_ bind(0, 2) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
          spirv.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() "None" {
            %cst64_i32 = spirv.Constant 64 : i32
            %cst128_i32 = spirv.Constant 128 : i32
            %cst192_i32 = spirv.Constant 192 : i32
            %cst256_i32 = spirv.Constant 256 : i32
            %cst320_i32 = spirv.Constant 320 : i32
            %cst384_i32 = spirv.Constant 384 : i32
            %cst448_i32 = spirv.Constant 448 : i32
            %cst512_i32 = spirv.Constant 512 : i32
            %cst_vec_4xf16 = spirv.Constant dense<0.000000e+00> : vector<4xf16>
            %cst0_i32 = spirv.Constant 0 : i32
            %cst_f16 = spirv.Constant 0.000000e+00 : f16
            %__builtin__LocalInvocationId___addr = spirv.mlir.addressof @__builtin__LocalInvocationId__ : !spirv.ptr<vector<3xi32>, Input>
            %0 = spirv.Load "Input" %__builtin__LocalInvocationId___addr : vector<3xi32>
            %1 = spirv.CompositeExtract %0[0 : i32] : vector<3xi32>
            %__resource_var_0_0__addr = spirv.mlir.addressof @__resource_var_0_0_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_1__addr = spirv.mlir.addressof @__resource_var_0_1_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_2__addr = spirv.mlir.addressof @__resource_var_0_2_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
            %__builtin__WorkgroupId___addr = spirv.mlir.addressof @__builtin__WorkgroupId__ : !spirv.ptr<vector<3xi32>, Input>
            %2 = spirv.Load "Input" %__builtin__WorkgroupId___addr : vector<3xi32>
            %3 = spirv.CompositeExtract %2[0 : i32] : vector<3xi32>
            %4 = spirv.IMul %3, %cst512_i32 : i32
            %5 = spirv.IAdd %1, %4 : i32
            %6 = spirv.IAdd %5, %cst448_i32 : i32
            %7 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %6] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %8 = spirv.Load "StorageBuffer" %7 : vector<4xf32>
            %9 = spirv.IAdd %1, %cst448_i32 : i32
            %10 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %9] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %11 = spirv.Load "StorageBuffer" %10 : vector<4xf32>
            %12 = spirv.IAdd %5, %cst384_i32 : i32
            %13 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %12] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %14 = spirv.Load "StorageBuffer" %13 : vector<4xf32>
            %15 = spirv.IAdd %1, %cst384_i32 : i32
            %16 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %15] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %17 = spirv.Load "StorageBuffer" %16 : vector<4xf32>
            %18 = spirv.IAdd %5, %cst320_i32 : i32
            %19 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %18] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %20 = spirv.Load "StorageBuffer" %19 : vector<4xf32>
            %21 = spirv.IAdd %1, %cst320_i32 : i32
            %22 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %21] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %23 = spirv.Load "StorageBuffer" %22 : vector<4xf32>
            %24 = spirv.IAdd %5, %cst256_i32 : i32
            %25 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %24] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %26 = spirv.Load "StorageBuffer" %25 : vector<4xf32>
            %27 = spirv.IAdd %1, %cst256_i32 : i32
            %28 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %27] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %29 = spirv.Load "StorageBuffer" %28 : vector<4xf32>
            %30 = spirv.IAdd %5, %cst192_i32 : i32
            %31 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %30] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %32 = spirv.Load "StorageBuffer" %31 : vector<4xf32>
            %33 = spirv.IAdd %1, %cst192_i32 : i32
            %34 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %33] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %35 = spirv.Load "StorageBuffer" %34 : vector<4xf32>
            %36 = spirv.IAdd %5, %cst128_i32 : i32
            %37 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %36] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %38 = spirv.Load "StorageBuffer" %37 : vector<4xf32>
            %39 = spirv.IAdd %1, %cst128_i32 : i32
            %40 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %39] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %41 = spirv.Load "StorageBuffer" %40 : vector<4xf32>
            %42 = spirv.IAdd %5, %cst64_i32 : i32
            %43 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %42] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %44 = spirv.Load "StorageBuffer" %43 : vector<4xf32>
            %45 = spirv.IAdd %1, %cst64_i32 : i32
            %46 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %45] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %47 = spirv.Load "StorageBuffer" %46 : vector<4xf32>
            %48 = spirv.IAdd %4, %1 : i32
            %49 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %48] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %50 = spirv.Load "StorageBuffer" %49 : vector<4xf32>
            %51 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %1] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %52 = spirv.Load "StorageBuffer" %51 : vector<4xf32>
            %53 = spirv.VectorShuffle [0 : i32, 1 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %54 = spirv.Bitcast %53 : vector<2xf32> to vector<4xf16>
            %55 = spirv.VectorShuffle [0 : i32, 1 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %56 = spirv.Bitcast %55 : vector<2xf32> to vector<4xf16>
            %57 = spirv.FMul %54, %56 : vector<4xf16>
            %58 = spirv.VectorShuffle [2 : i32, 3 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %59 = spirv.Bitcast %58 : vector<2xf32> to vector<4xf16>
            %60 = spirv.VectorShuffle [2 : i32, 3 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %61 = spirv.Bitcast %60 : vector<2xf32> to vector<4xf16>
            %62 = spirv.FMul %59, %61 : vector<4xf16>
            %63 = spirv.FAdd %57, %cst_vec_4xf16 : vector<4xf16>
            %64 = spirv.FAdd %62, %cst_vec_4xf16 : vector<4xf16>
            %65 = spirv.VectorShuffle [0 : i32, 1 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %66 = spirv.Bitcast %65 : vector<2xf32> to vector<4xf16>
            %67 = spirv.VectorShuffle [0 : i32, 1 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %68 = spirv.Bitcast %67 : vector<2xf32> to vector<4xf16>
            %69 = spirv.FMul %66, %68 : vector<4xf16>
            %70 = spirv.VectorShuffle [2 : i32, 3 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %71 = spirv.Bitcast %70 : vector<2xf32> to vector<4xf16>
            %72 = spirv.VectorShuffle [2 : i32, 3 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %73 = spirv.Bitcast %72 : vector<2xf32> to vector<4xf16>
            %74 = spirv.FMul %71, %73 : vector<4xf16>
            %75 = spirv.FAdd %69, %63 : vector<4xf16>
            %76 = spirv.FAdd %74, %64 : vector<4xf16>
            %77 = spirv.VectorShuffle [0 : i32, 1 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %78 = spirv.Bitcast %77 : vector<2xf32> to vector<4xf16>
            %79 = spirv.VectorShuffle [0 : i32, 1 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %80 = spirv.Bitcast %79 : vector<2xf32> to vector<4xf16>
            %81 = spirv.FMul %78, %80 : vector<4xf16>
            %82 = spirv.VectorShuffle [2 : i32, 3 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %83 = spirv.Bitcast %82 : vector<2xf32> to vector<4xf16>
            %84 = spirv.VectorShuffle [2 : i32, 3 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %85 = spirv.Bitcast %84 : vector<2xf32> to vector<4xf16>
            %86 = spirv.FMul %83, %85 : vector<4xf16>
            %87 = spirv.FAdd %81, %75 : vector<4xf16>
            %88 = spirv.FAdd %86, %76 : vector<4xf16>
            %89 = spirv.VectorShuffle [0 : i32, 1 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %90 = spirv.Bitcast %89 : vector<2xf32> to vector<4xf16>
            %91 = spirv.VectorShuffle [0 : i32, 1 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %92 = spirv.Bitcast %91 : vector<2xf32> to vector<4xf16>
            %93 = spirv.FMul %90, %92 : vector<4xf16>
            %94 = spirv.VectorShuffle [2 : i32, 3 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %95 = spirv.Bitcast %94 : vector<2xf32> to vector<4xf16>
            %96 = spirv.VectorShuffle [2 : i32, 3 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %97 = spirv.Bitcast %96 : vector<2xf32> to vector<4xf16>
            %98 = spirv.FMul %95, %97 : vector<4xf16>
            %99 = spirv.FAdd %93, %87 : vector<4xf16>
            %100 = spirv.FAdd %98, %88 : vector<4xf16>
            %101 = spirv.VectorShuffle [0 : i32, 1 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %102 = spirv.Bitcast %101 : vector<2xf32> to vector<4xf16>
            %103 = spirv.VectorShuffle [0 : i32, 1 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %104 = spirv.Bitcast %103 : vector<2xf32> to vector<4xf16>
            %105 = spirv.FMul %102, %104 : vector<4xf16>
            %106 = spirv.VectorShuffle [2 : i32, 3 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %107 = spirv.Bitcast %106 : vector<2xf32> to vector<4xf16>
            %108 = spirv.VectorShuffle [2 : i32, 3 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %109 = spirv.Bitcast %108 : vector<2xf32> to vector<4xf16>
            %110 = spirv.FMul %107, %109 : vector<4xf16>
            %111 = spirv.FAdd %105, %99 : vector<4xf16>
            %112 = spirv.FAdd %110, %100 : vector<4xf16>
            %113 = spirv.VectorShuffle [0 : i32, 1 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %114 = spirv.Bitcast %113 : vector<2xf32> to vector<4xf16>
            %115 = spirv.VectorShuffle [0 : i32, 1 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %116 = spirv.Bitcast %115 : vector<2xf32> to vector<4xf16>
            %117 = spirv.FMul %114, %116 : vector<4xf16>
            %118 = spirv.VectorShuffle [2 : i32, 3 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %119 = spirv.Bitcast %118 : vector<2xf32> to vector<4xf16>
            %120 = spirv.VectorShuffle [2 : i32, 3 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %121 = spirv.Bitcast %120 : vector<2xf32> to vector<4xf16>
            %122 = spirv.FMul %119, %121 : vector<4xf16>
            %123 = spirv.FAdd %117, %111 : vector<4xf16>
            %124 = spirv.FAdd %122, %112 : vector<4xf16>
            %125 = spirv.VectorShuffle [0 : i32, 1 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %126 = spirv.Bitcast %125 : vector<2xf32> to vector<4xf16>
            %127 = spirv.VectorShuffle [0 : i32, 1 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %128 = spirv.Bitcast %127 : vector<2xf32> to vector<4xf16>
            %129 = spirv.FMul %126, %128 : vector<4xf16>
            %130 = spirv.VectorShuffle [2 : i32, 3 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %131 = spirv.Bitcast %130 : vector<2xf32> to vector<4xf16>
            %132 = spirv.VectorShuffle [2 : i32, 3 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %133 = spirv.Bitcast %132 : vector<2xf32> to vector<4xf16>
            %134 = spirv.FMul %131, %133 : vector<4xf16>
            %135 = spirv.FAdd %129, %123 : vector<4xf16>
            %136 = spirv.FAdd %134, %124 : vector<4xf16>
            %137 = spirv.VectorShuffle [0 : i32, 1 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %138 = spirv.Bitcast %137 : vector<2xf32> to vector<4xf16>
            %139 = spirv.VectorShuffle [0 : i32, 1 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %140 = spirv.Bitcast %139 : vector<2xf32> to vector<4xf16>
            %141 = spirv.FMul %138, %140 : vector<4xf16>
            %142 = spirv.VectorShuffle [2 : i32, 3 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %143 = spirv.Bitcast %142 : vector<2xf32> to vector<4xf16>
            %144 = spirv.VectorShuffle [2 : i32, 3 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %145 = spirv.Bitcast %144 : vector<2xf32> to vector<4xf16>
            %146 = spirv.FMul %143, %145 : vector<4xf16>
            %147 = spirv.FAdd %141, %135 : vector<4xf16>
            %148 = spirv.FAdd %146, %136 : vector<4xf16>
            %149 = spirv.CompositeExtract %147[0 : i32] : vector<4xf16>
            %150 = spirv.CompositeExtract %147[1 : i32] : vector<4xf16>
            %151 = spirv.CompositeExtract %147[2 : i32] : vector<4xf16>
            %152 = spirv.CompositeExtract %147[3 : i32] : vector<4xf16>
            %153 = spirv.FAdd %149, %150 : f16
            %154 = spirv.FAdd %153, %151 : f16
            %155 = spirv.FAdd %154, %152 : f16
            %156 = spirv.CompositeExtract %148[0 : i32] : vector<4xf16>
            %157 = spirv.CompositeExtract %148[1 : i32] : vector<4xf16>
            %158 = spirv.CompositeExtract %148[2 : i32] : vector<4xf16>
            %159 = spirv.CompositeExtract %148[3 : i32] : vector<4xf16>
            %160 = spirv.FAdd %156, %157 : f16
            %161 = spirv.FAdd %160, %158 : f16
            %162 = spirv.FAdd %161, %159 : f16
            %163 = spirv.FAdd %155, %162 : f16
            %164 = spirv.GroupNonUniformFAdd "Subgroup" "Reduce" %163 : f16
            %165 = spirv.FAdd %164, %cst_f16 : f16
            %166 = spirv.IEqual %1, %cst0_i32 : i32
            spirv.mlir.selection {
              spirv.BranchConditional %166, ^bb1, ^bb2
            ^bb1:  // pred: ^bb0
              %167 = spirv.AccessChain %__resource_var_0_2__addr[%cst0_i32, %3] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>, i32, i32
              spirv.Store "StorageBuffer" %167, %165 : f16
              spirv.Branch ^bb2
            ^bb2:  // 2 preds: ^bb0, ^bb1
              spirv.mlir.merge
            }
            spirv.Return
          }
          spirv.EntryPoint "GLCompute" @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16, @__builtin__LocalInvocationId__, @__builtin__WorkgroupId__
          spirv.ExecutionMode @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 "LocalSize", 64, 1, 1
        }
      }
    }
  }
  func.func private @_main() {
    %c1_i32 = arith.constant 1 : i32
    %c553648144_i32 = arith.constant 553648144 : i32
    %c32000 = arith.constant 32000 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %device = hal.ex.shared_device : !hal.device
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
    %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_2, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
    %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
    %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_9, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    %3 = arith.select %value, %c0, %c-1 : index
    scf.index_switch %3 
    case 0 {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%1 : !hal.buffer)[%c0, %len], 
        %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
        %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
      ])
      hal.command_buffer.dispatch.symbol<%cmd_14 : !hal.command_buffer> target(@_main_dispatch_0::@vulkan_spirv_fb::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16) workgroups([%c32000, %c1, %c1])
      scf.yield
    }
    default {
    }
    hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
    %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
    %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_17, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<SPIRVSubgroupReduce>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.variant public @vulkan_spirv_fb target(#executable_target_vulkan_spirv_fb) {
      hal.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c32000 = arith.constant 32000 : index
        %c1 = arith.constant 1 : index
        hal.return %c32000, %c1, %c1 : index, index, index
      }
      builtin.module attributes {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>} {
        spirv.module Logical GLSL450 requires #spirv.vce<v1.3, [Shader, StorageBuffer16BitAccess, Float16, GroupNonUniformArithmetic], [SPV_KHR_storage_buffer_storage_class, SPV_KHR_16bit_storage]> {
          spirv.GlobalVariable @__builtin__WorkgroupId__ built_in("WorkgroupId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__builtin__LocalInvocationId__ built_in("LocalInvocationId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__resource_var_0_0_ bind(0, 0) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_1_ bind(0, 1) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_2_ bind(0, 2) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
          spirv.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() "None" {
            %cst64_i32 = spirv.Constant 64 : i32
            %cst128_i32 = spirv.Constant 128 : i32
            %cst192_i32 = spirv.Constant 192 : i32
            %cst256_i32 = spirv.Constant 256 : i32
            %cst320_i32 = spirv.Constant 320 : i32
            %cst384_i32 = spirv.Constant 384 : i32
            %cst448_i32 = spirv.Constant 448 : i32
            %cst512_i32 = spirv.Constant 512 : i32
            %cst_vec_4xf16 = spirv.Constant dense<0.000000e+00> : vector<4xf16>
            %cst0_i32 = spirv.Constant 0 : i32
            %cst_f16 = spirv.Constant 0.000000e+00 : f16
            %__builtin__LocalInvocationId___addr = spirv.mlir.addressof @__builtin__LocalInvocationId__ : !spirv.ptr<vector<3xi32>, Input>
            %0 = spirv.Load "Input" %__builtin__LocalInvocationId___addr : vector<3xi32>
            %1 = spirv.CompositeExtract %0[0 : i32] : vector<3xi32>
            %__resource_var_0_0__addr = spirv.mlir.addressof @__resource_var_0_0_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_1__addr = spirv.mlir.addressof @__resource_var_0_1_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_2__addr = spirv.mlir.addressof @__resource_var_0_2_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
            %__builtin__WorkgroupId___addr = spirv.mlir.addressof @__builtin__WorkgroupId__ : !spirv.ptr<vector<3xi32>, Input>
            %2 = spirv.Load "Input" %__builtin__WorkgroupId___addr : vector<3xi32>
            %3 = spirv.CompositeExtract %2[0 : i32] : vector<3xi32>
            %4 = spirv.IMul %3, %cst512_i32 : i32
            %5 = spirv.IAdd %1, %4 : i32
            %6 = spirv.IAdd %5, %cst448_i32 : i32
            %7 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %6] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %8 = spirv.Load "StorageBuffer" %7 : vector<4xf32>
            %9 = spirv.IAdd %1, %cst448_i32 : i32
            %10 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %9] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %11 = spirv.Load "StorageBuffer" %10 : vector<4xf32>
            %12 = spirv.IAdd %5, %cst384_i32 : i32
            %13 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %12] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %14 = spirv.Load "StorageBuffer" %13 : vector<4xf32>
            %15 = spirv.IAdd %1, %cst384_i32 : i32
            %16 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %15] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %17 = spirv.Load "StorageBuffer" %16 : vector<4xf32>
            %18 = spirv.IAdd %5, %cst320_i32 : i32
            %19 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %18] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %20 = spirv.Load "StorageBuffer" %19 : vector<4xf32>
            %21 = spirv.IAdd %1, %cst320_i32 : i32
            %22 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %21] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %23 = spirv.Load "StorageBuffer" %22 : vector<4xf32>
            %24 = spirv.IAdd %5, %cst256_i32 : i32
            %25 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %24] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %26 = spirv.Load "StorageBuffer" %25 : vector<4xf32>
            %27 = spirv.IAdd %1, %cst256_i32 : i32
            %28 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %27] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %29 = spirv.Load "StorageBuffer" %28 : vector<4xf32>
            %30 = spirv.IAdd %5, %cst192_i32 : i32
            %31 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %30] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %32 = spirv.Load "StorageBuffer" %31 : vector<4xf32>
            %33 = spirv.IAdd %1, %cst192_i32 : i32
            %34 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %33] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %35 = spirv.Load "StorageBuffer" %34 : vector<4xf32>
            %36 = spirv.IAdd %5, %cst128_i32 : i32
            %37 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %36] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %38 = spirv.Load "StorageBuffer" %37 : vector<4xf32>
            %39 = spirv.IAdd %1, %cst128_i32 : i32
            %40 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %39] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %41 = spirv.Load "StorageBuffer" %40 : vector<4xf32>
            %42 = spirv.IAdd %5, %cst64_i32 : i32
            %43 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %42] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %44 = spirv.Load "StorageBuffer" %43 : vector<4xf32>
            %45 = spirv.IAdd %1, %cst64_i32 : i32
            %46 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %45] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %47 = spirv.Load "StorageBuffer" %46 : vector<4xf32>
            %48 = spirv.IAdd %4, %1 : i32
            %49 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %48] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %50 = spirv.Load "StorageBuffer" %49 : vector<4xf32>
            %51 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %1] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %52 = spirv.Load "StorageBuffer" %51 : vector<4xf32>
            %53 = spirv.VectorShuffle [0 : i32, 1 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %54 = spirv.Bitcast %53 : vector<2xf32> to vector<4xf16>
            %55 = spirv.VectorShuffle [0 : i32, 1 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %56 = spirv.Bitcast %55 : vector<2xf32> to vector<4xf16>
            %57 = spirv.FMul %54, %56 : vector<4xf16>
            %58 = spirv.VectorShuffle [2 : i32, 3 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %59 = spirv.Bitcast %58 : vector<2xf32> to vector<4xf16>
            %60 = spirv.VectorShuffle [2 : i32, 3 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %61 = spirv.Bitcast %60 : vector<2xf32> to vector<4xf16>
            %62 = spirv.FMul %59, %61 : vector<4xf16>
            %63 = spirv.FAdd %57, %cst_vec_4xf16 : vector<4xf16>
            %64 = spirv.FAdd %62, %cst_vec_4xf16 : vector<4xf16>
            %65 = spirv.VectorShuffle [0 : i32, 1 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %66 = spirv.Bitcast %65 : vector<2xf32> to vector<4xf16>
            %67 = spirv.VectorShuffle [0 : i32, 1 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %68 = spirv.Bitcast %67 : vector<2xf32> to vector<4xf16>
            %69 = spirv.FMul %66, %68 : vector<4xf16>
            %70 = spirv.VectorShuffle [2 : i32, 3 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %71 = spirv.Bitcast %70 : vector<2xf32> to vector<4xf16>
            %72 = spirv.VectorShuffle [2 : i32, 3 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %73 = spirv.Bitcast %72 : vector<2xf32> to vector<4xf16>
            %74 = spirv.FMul %71, %73 : vector<4xf16>
            %75 = spirv.FAdd %69, %63 : vector<4xf16>
            %76 = spirv.FAdd %74, %64 : vector<4xf16>
            %77 = spirv.VectorShuffle [0 : i32, 1 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %78 = spirv.Bitcast %77 : vector<2xf32> to vector<4xf16>
            %79 = spirv.VectorShuffle [0 : i32, 1 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %80 = spirv.Bitcast %79 : vector<2xf32> to vector<4xf16>
            %81 = spirv.FMul %78, %80 : vector<4xf16>
            %82 = spirv.VectorShuffle [2 : i32, 3 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %83 = spirv.Bitcast %82 : vector<2xf32> to vector<4xf16>
            %84 = spirv.VectorShuffle [2 : i32, 3 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %85 = spirv.Bitcast %84 : vector<2xf32> to vector<4xf16>
            %86 = spirv.FMul %83, %85 : vector<4xf16>
            %87 = spirv.FAdd %81, %75 : vector<4xf16>
            %88 = spirv.FAdd %86, %76 : vector<4xf16>
            %89 = spirv.VectorShuffle [0 : i32, 1 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %90 = spirv.Bitcast %89 : vector<2xf32> to vector<4xf16>
            %91 = spirv.VectorShuffle [0 : i32, 1 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %92 = spirv.Bitcast %91 : vector<2xf32> to vector<4xf16>
            %93 = spirv.FMul %90, %92 : vector<4xf16>
            %94 = spirv.VectorShuffle [2 : i32, 3 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %95 = spirv.Bitcast %94 : vector<2xf32> to vector<4xf16>
            %96 = spirv.VectorShuffle [2 : i32, 3 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %97 = spirv.Bitcast %96 : vector<2xf32> to vector<4xf16>
            %98 = spirv.FMul %95, %97 : vector<4xf16>
            %99 = spirv.FAdd %93, %87 : vector<4xf16>
            %100 = spirv.FAdd %98, %88 : vector<4xf16>
            %101 = spirv.VectorShuffle [0 : i32, 1 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %102 = spirv.Bitcast %101 : vector<2xf32> to vector<4xf16>
            %103 = spirv.VectorShuffle [0 : i32, 1 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %104 = spirv.Bitcast %103 : vector<2xf32> to vector<4xf16>
            %105 = spirv.FMul %102, %104 : vector<4xf16>
            %106 = spirv.VectorShuffle [2 : i32, 3 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %107 = spirv.Bitcast %106 : vector<2xf32> to vector<4xf16>
            %108 = spirv.VectorShuffle [2 : i32, 3 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %109 = spirv.Bitcast %108 : vector<2xf32> to vector<4xf16>
            %110 = spirv.FMul %107, %109 : vector<4xf16>
            %111 = spirv.FAdd %105, %99 : vector<4xf16>
            %112 = spirv.FAdd %110, %100 : vector<4xf16>
            %113 = spirv.VectorShuffle [0 : i32, 1 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %114 = spirv.Bitcast %113 : vector<2xf32> to vector<4xf16>
            %115 = spirv.VectorShuffle [0 : i32, 1 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %116 = spirv.Bitcast %115 : vector<2xf32> to vector<4xf16>
            %117 = spirv.FMul %114, %116 : vector<4xf16>
            %118 = spirv.VectorShuffle [2 : i32, 3 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %119 = spirv.Bitcast %118 : vector<2xf32> to vector<4xf16>
            %120 = spirv.VectorShuffle [2 : i32, 3 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %121 = spirv.Bitcast %120 : vector<2xf32> to vector<4xf16>
            %122 = spirv.FMul %119, %121 : vector<4xf16>
            %123 = spirv.FAdd %117, %111 : vector<4xf16>
            %124 = spirv.FAdd %122, %112 : vector<4xf16>
            %125 = spirv.VectorShuffle [0 : i32, 1 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %126 = spirv.Bitcast %125 : vector<2xf32> to vector<4xf16>
            %127 = spirv.VectorShuffle [0 : i32, 1 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %128 = spirv.Bitcast %127 : vector<2xf32> to vector<4xf16>
            %129 = spirv.FMul %126, %128 : vector<4xf16>
            %130 = spirv.VectorShuffle [2 : i32, 3 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %131 = spirv.Bitcast %130 : vector<2xf32> to vector<4xf16>
            %132 = spirv.VectorShuffle [2 : i32, 3 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %133 = spirv.Bitcast %132 : vector<2xf32> to vector<4xf16>
            %134 = spirv.FMul %131, %133 : vector<4xf16>
            %135 = spirv.FAdd %129, %123 : vector<4xf16>
            %136 = spirv.FAdd %134, %124 : vector<4xf16>
            %137 = spirv.VectorShuffle [0 : i32, 1 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %138 = spirv.Bitcast %137 : vector<2xf32> to vector<4xf16>
            %139 = spirv.VectorShuffle [0 : i32, 1 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %140 = spirv.Bitcast %139 : vector<2xf32> to vector<4xf16>
            %141 = spirv.FMul %138, %140 : vector<4xf16>
            %142 = spirv.VectorShuffle [2 : i32, 3 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %143 = spirv.Bitcast %142 : vector<2xf32> to vector<4xf16>
            %144 = spirv.VectorShuffle [2 : i32, 3 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %145 = spirv.Bitcast %144 : vector<2xf32> to vector<4xf16>
            %146 = spirv.FMul %143, %145 : vector<4xf16>
            %147 = spirv.FAdd %141, %135 : vector<4xf16>
            %148 = spirv.FAdd %146, %136 : vector<4xf16>
            %149 = spirv.CompositeExtract %147[0 : i32] : vector<4xf16>
            %150 = spirv.CompositeExtract %147[1 : i32] : vector<4xf16>
            %151 = spirv.CompositeExtract %147[2 : i32] : vector<4xf16>
            %152 = spirv.CompositeExtract %147[3 : i32] : vector<4xf16>
            %153 = spirv.FAdd %149, %150 : f16
            %154 = spirv.FAdd %153, %151 : f16
            %155 = spirv.FAdd %154, %152 : f16
            %156 = spirv.CompositeExtract %148[0 : i32] : vector<4xf16>
            %157 = spirv.CompositeExtract %148[1 : i32] : vector<4xf16>
            %158 = spirv.CompositeExtract %148[2 : i32] : vector<4xf16>
            %159 = spirv.CompositeExtract %148[3 : i32] : vector<4xf16>
            %160 = spirv.FAdd %156, %157 : f16
            %161 = spirv.FAdd %160, %158 : f16
            %162 = spirv.FAdd %161, %159 : f16
            %163 = spirv.FAdd %155, %162 : f16
            %164 = spirv.GroupNonUniformFAdd "Subgroup" "Reduce" %163 : f16
            %165 = spirv.FAdd %164, %cst_f16 : f16
            %166 = spirv.IEqual %1, %cst0_i32 : i32
            spirv.mlir.selection {
              spirv.BranchConditional %166, ^bb1, ^bb2
            ^bb1:  // pred: ^bb0
              %167 = spirv.AccessChain %__resource_var_0_2__addr[%cst0_i32, %3] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>, i32, i32
              spirv.Store "StorageBuffer" %167, %165 : f16
              spirv.Branch ^bb2
            ^bb2:  // 2 preds: ^bb0, ^bb1
              spirv.mlir.merge
            }
            spirv.Return
          }
          spirv.EntryPoint "GLCompute" @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16, @__builtin__LocalInvocationId__, @__builtin__WorkgroupId__
          spirv.ExecutionMode @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 "LocalSize", 64, 1, 1
        }
      }
    }
  }
  func.func private @_main() {
    %c1_i32 = arith.constant 1 : i32
    %c553648144_i32 = arith.constant 553648144 : i32
    %c32000 = arith.constant 32000 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %device = hal.ex.shared_device : !hal.device
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
    %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_2, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
    %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
    %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_9, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    %3 = arith.select %value, %c0, %c-1 : index
    scf.index_switch %3 
    case 0 {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%1 : !hal.buffer)[%c0, %len], 
        %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
        %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
      ])
      hal.command_buffer.dispatch.symbol<%cmd_14 : !hal.command_buffer> target(@_main_dispatch_0::@vulkan_spirv_fb::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16) workgroups([%c32000, %c1, %c1])
      scf.yield
    }
    default {
    }
    hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
    %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
    %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_17, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func private @_main() {
  %c1_i32 = arith.constant 1 : i32
  %c553648144_i32 = arith.constant 553648144 : i32
  %c32000 = arith.constant 32000 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c-1 = arith.constant -1 : index
  %c-1_i32 = arith.constant -1 : i32
  %c0_i64 = arith.constant 0 : i64
  %c-1_i64 = arith.constant -1 : i64
  %c128000 = arith.constant 128000 : index
  %c0 = arith.constant 0 : index
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %device = hal.ex.shared_device : !hal.device
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
  hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
  %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_2, "failed to wait on timepoint"
  %1 = util.optimization_barrier %transient_buffer : !hal.buffer
  %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
  %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
  %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
  hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
  %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
  %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_9, "failed to wait on timepoint"
  %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
  %len = hal.buffer.length<%1 : !hal.buffer> : index
  %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
  %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
  %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
  %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
  %3 = arith.select %value, %c0, %c-1 : index
  scf.index_switch %3 
  case 0 {
    %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>) : !hal.pipeline_layout
    hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%1 : !hal.buffer)[%c0, %len], 
      %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
      %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
    ])
    hal.command_buffer.dispatch.symbol<%cmd_14 : !hal.command_buffer> target(@_main_dispatch_0::@vulkan_spirv_fb::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16) workgroups([%c32000, %c1, %c1])
    scf.yield
  }
  default {
  }
  hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
  %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
  %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_17, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
  %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
  check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
  return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<SPIRVSubgroupReduce>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.variant public @vulkan_spirv_fb target(#executable_target_vulkan_spirv_fb) {
      hal.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c32000 = arith.constant 32000 : index
        %c1 = arith.constant 1 : index
        hal.return %c32000, %c1, %c1 : index, index, index
      }
      builtin.module attributes {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>} {
        spirv.module Logical GLSL450 requires #spirv.vce<v1.3, [Shader, StorageBuffer16BitAccess, Float16, GroupNonUniformArithmetic], [SPV_KHR_storage_buffer_storage_class, SPV_KHR_16bit_storage]> {
          spirv.GlobalVariable @__builtin__WorkgroupId__ built_in("WorkgroupId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__builtin__LocalInvocationId__ built_in("LocalInvocationId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__resource_var_0_0_ bind(0, 0) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_1_ bind(0, 1) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_2_ bind(0, 2) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
          spirv.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() "None" {
            %cst64_i32 = spirv.Constant 64 : i32
            %cst128_i32 = spirv.Constant 128 : i32
            %cst192_i32 = spirv.Constant 192 : i32
            %cst256_i32 = spirv.Constant 256 : i32
            %cst320_i32 = spirv.Constant 320 : i32
            %cst384_i32 = spirv.Constant 384 : i32
            %cst448_i32 = spirv.Constant 448 : i32
            %cst512_i32 = spirv.Constant 512 : i32
            %cst_vec_4xf16 = spirv.Constant dense<0.000000e+00> : vector<4xf16>
            %cst0_i32 = spirv.Constant 0 : i32
            %cst_f16 = spirv.Constant 0.000000e+00 : f16
            %__builtin__LocalInvocationId___addr = spirv.mlir.addressof @__builtin__LocalInvocationId__ : !spirv.ptr<vector<3xi32>, Input>
            %0 = spirv.Load "Input" %__builtin__LocalInvocationId___addr : vector<3xi32>
            %1 = spirv.CompositeExtract %0[0 : i32] : vector<3xi32>
            %__resource_var_0_0__addr = spirv.mlir.addressof @__resource_var_0_0_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_1__addr = spirv.mlir.addressof @__resource_var_0_1_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_2__addr = spirv.mlir.addressof @__resource_var_0_2_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
            %__builtin__WorkgroupId___addr = spirv.mlir.addressof @__builtin__WorkgroupId__ : !spirv.ptr<vector<3xi32>, Input>
            %2 = spirv.Load "Input" %__builtin__WorkgroupId___addr : vector<3xi32>
            %3 = spirv.CompositeExtract %2[0 : i32] : vector<3xi32>
            %4 = spirv.IMul %3, %cst512_i32 : i32
            %5 = spirv.IAdd %1, %4 : i32
            %6 = spirv.IAdd %5, %cst448_i32 : i32
            %7 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %6] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %8 = spirv.Load "StorageBuffer" %7 : vector<4xf32>
            %9 = spirv.IAdd %1, %cst448_i32 : i32
            %10 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %9] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %11 = spirv.Load "StorageBuffer" %10 : vector<4xf32>
            %12 = spirv.IAdd %5, %cst384_i32 : i32
            %13 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %12] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %14 = spirv.Load "StorageBuffer" %13 : vector<4xf32>
            %15 = spirv.IAdd %1, %cst384_i32 : i32
            %16 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %15] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %17 = spirv.Load "StorageBuffer" %16 : vector<4xf32>
            %18 = spirv.IAdd %5, %cst320_i32 : i32
            %19 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %18] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %20 = spirv.Load "StorageBuffer" %19 : vector<4xf32>
            %21 = spirv.IAdd %1, %cst320_i32 : i32
            %22 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %21] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %23 = spirv.Load "StorageBuffer" %22 : vector<4xf32>
            %24 = spirv.IAdd %5, %cst256_i32 : i32
            %25 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %24] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %26 = spirv.Load "StorageBuffer" %25 : vector<4xf32>
            %27 = spirv.IAdd %1, %cst256_i32 : i32
            %28 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %27] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %29 = spirv.Load "StorageBuffer" %28 : vector<4xf32>
            %30 = spirv.IAdd %5, %cst192_i32 : i32
            %31 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %30] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %32 = spirv.Load "StorageBuffer" %31 : vector<4xf32>
            %33 = spirv.IAdd %1, %cst192_i32 : i32
            %34 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %33] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %35 = spirv.Load "StorageBuffer" %34 : vector<4xf32>
            %36 = spirv.IAdd %5, %cst128_i32 : i32
            %37 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %36] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %38 = spirv.Load "StorageBuffer" %37 : vector<4xf32>
            %39 = spirv.IAdd %1, %cst128_i32 : i32
            %40 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %39] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %41 = spirv.Load "StorageBuffer" %40 : vector<4xf32>
            %42 = spirv.IAdd %5, %cst64_i32 : i32
            %43 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %42] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %44 = spirv.Load "StorageBuffer" %43 : vector<4xf32>
            %45 = spirv.IAdd %1, %cst64_i32 : i32
            %46 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %45] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %47 = spirv.Load "StorageBuffer" %46 : vector<4xf32>
            %48 = spirv.IAdd %4, %1 : i32
            %49 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %48] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %50 = spirv.Load "StorageBuffer" %49 : vector<4xf32>
            %51 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %1] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %52 = spirv.Load "StorageBuffer" %51 : vector<4xf32>
            %53 = spirv.VectorShuffle [0 : i32, 1 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %54 = spirv.Bitcast %53 : vector<2xf32> to vector<4xf16>
            %55 = spirv.VectorShuffle [0 : i32, 1 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %56 = spirv.Bitcast %55 : vector<2xf32> to vector<4xf16>
            %57 = spirv.FMul %54, %56 : vector<4xf16>
            %58 = spirv.VectorShuffle [2 : i32, 3 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %59 = spirv.Bitcast %58 : vector<2xf32> to vector<4xf16>
            %60 = spirv.VectorShuffle [2 : i32, 3 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %61 = spirv.Bitcast %60 : vector<2xf32> to vector<4xf16>
            %62 = spirv.FMul %59, %61 : vector<4xf16>
            %63 = spirv.FAdd %57, %cst_vec_4xf16 : vector<4xf16>
            %64 = spirv.FAdd %62, %cst_vec_4xf16 : vector<4xf16>
            %65 = spirv.VectorShuffle [0 : i32, 1 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %66 = spirv.Bitcast %65 : vector<2xf32> to vector<4xf16>
            %67 = spirv.VectorShuffle [0 : i32, 1 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %68 = spirv.Bitcast %67 : vector<2xf32> to vector<4xf16>
            %69 = spirv.FMul %66, %68 : vector<4xf16>
            %70 = spirv.VectorShuffle [2 : i32, 3 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %71 = spirv.Bitcast %70 : vector<2xf32> to vector<4xf16>
            %72 = spirv.VectorShuffle [2 : i32, 3 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %73 = spirv.Bitcast %72 : vector<2xf32> to vector<4xf16>
            %74 = spirv.FMul %71, %73 : vector<4xf16>
            %75 = spirv.FAdd %69, %63 : vector<4xf16>
            %76 = spirv.FAdd %74, %64 : vector<4xf16>
            %77 = spirv.VectorShuffle [0 : i32, 1 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %78 = spirv.Bitcast %77 : vector<2xf32> to vector<4xf16>
            %79 = spirv.VectorShuffle [0 : i32, 1 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %80 = spirv.Bitcast %79 : vector<2xf32> to vector<4xf16>
            %81 = spirv.FMul %78, %80 : vector<4xf16>
            %82 = spirv.VectorShuffle [2 : i32, 3 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %83 = spirv.Bitcast %82 : vector<2xf32> to vector<4xf16>
            %84 = spirv.VectorShuffle [2 : i32, 3 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %85 = spirv.Bitcast %84 : vector<2xf32> to vector<4xf16>
            %86 = spirv.FMul %83, %85 : vector<4xf16>
            %87 = spirv.FAdd %81, %75 : vector<4xf16>
            %88 = spirv.FAdd %86, %76 : vector<4xf16>
            %89 = spirv.VectorShuffle [0 : i32, 1 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %90 = spirv.Bitcast %89 : vector<2xf32> to vector<4xf16>
            %91 = spirv.VectorShuffle [0 : i32, 1 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %92 = spirv.Bitcast %91 : vector<2xf32> to vector<4xf16>
            %93 = spirv.FMul %90, %92 : vector<4xf16>
            %94 = spirv.VectorShuffle [2 : i32, 3 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %95 = spirv.Bitcast %94 : vector<2xf32> to vector<4xf16>
            %96 = spirv.VectorShuffle [2 : i32, 3 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %97 = spirv.Bitcast %96 : vector<2xf32> to vector<4xf16>
            %98 = spirv.FMul %95, %97 : vector<4xf16>
            %99 = spirv.FAdd %93, %87 : vector<4xf16>
            %100 = spirv.FAdd %98, %88 : vector<4xf16>
            %101 = spirv.VectorShuffle [0 : i32, 1 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %102 = spirv.Bitcast %101 : vector<2xf32> to vector<4xf16>
            %103 = spirv.VectorShuffle [0 : i32, 1 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %104 = spirv.Bitcast %103 : vector<2xf32> to vector<4xf16>
            %105 = spirv.FMul %102, %104 : vector<4xf16>
            %106 = spirv.VectorShuffle [2 : i32, 3 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %107 = spirv.Bitcast %106 : vector<2xf32> to vector<4xf16>
            %108 = spirv.VectorShuffle [2 : i32, 3 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %109 = spirv.Bitcast %108 : vector<2xf32> to vector<4xf16>
            %110 = spirv.FMul %107, %109 : vector<4xf16>
            %111 = spirv.FAdd %105, %99 : vector<4xf16>
            %112 = spirv.FAdd %110, %100 : vector<4xf16>
            %113 = spirv.VectorShuffle [0 : i32, 1 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %114 = spirv.Bitcast %113 : vector<2xf32> to vector<4xf16>
            %115 = spirv.VectorShuffle [0 : i32, 1 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %116 = spirv.Bitcast %115 : vector<2xf32> to vector<4xf16>
            %117 = spirv.FMul %114, %116 : vector<4xf16>
            %118 = spirv.VectorShuffle [2 : i32, 3 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %119 = spirv.Bitcast %118 : vector<2xf32> to vector<4xf16>
            %120 = spirv.VectorShuffle [2 : i32, 3 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %121 = spirv.Bitcast %120 : vector<2xf32> to vector<4xf16>
            %122 = spirv.FMul %119, %121 : vector<4xf16>
            %123 = spirv.FAdd %117, %111 : vector<4xf16>
            %124 = spirv.FAdd %122, %112 : vector<4xf16>
            %125 = spirv.VectorShuffle [0 : i32, 1 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %126 = spirv.Bitcast %125 : vector<2xf32> to vector<4xf16>
            %127 = spirv.VectorShuffle [0 : i32, 1 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %128 = spirv.Bitcast %127 : vector<2xf32> to vector<4xf16>
            %129 = spirv.FMul %126, %128 : vector<4xf16>
            %130 = spirv.VectorShuffle [2 : i32, 3 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %131 = spirv.Bitcast %130 : vector<2xf32> to vector<4xf16>
            %132 = spirv.VectorShuffle [2 : i32, 3 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %133 = spirv.Bitcast %132 : vector<2xf32> to vector<4xf16>
            %134 = spirv.FMul %131, %133 : vector<4xf16>
            %135 = spirv.FAdd %129, %123 : vector<4xf16>
            %136 = spirv.FAdd %134, %124 : vector<4xf16>
            %137 = spirv.VectorShuffle [0 : i32, 1 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %138 = spirv.Bitcast %137 : vector<2xf32> to vector<4xf16>
            %139 = spirv.VectorShuffle [0 : i32, 1 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %140 = spirv.Bitcast %139 : vector<2xf32> to vector<4xf16>
            %141 = spirv.FMul %138, %140 : vector<4xf16>
            %142 = spirv.VectorShuffle [2 : i32, 3 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %143 = spirv.Bitcast %142 : vector<2xf32> to vector<4xf16>
            %144 = spirv.VectorShuffle [2 : i32, 3 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %145 = spirv.Bitcast %144 : vector<2xf32> to vector<4xf16>
            %146 = spirv.FMul %143, %145 : vector<4xf16>
            %147 = spirv.FAdd %141, %135 : vector<4xf16>
            %148 = spirv.FAdd %146, %136 : vector<4xf16>
            %149 = spirv.CompositeExtract %147[0 : i32] : vector<4xf16>
            %150 = spirv.CompositeExtract %147[1 : i32] : vector<4xf16>
            %151 = spirv.CompositeExtract %147[2 : i32] : vector<4xf16>
            %152 = spirv.CompositeExtract %147[3 : i32] : vector<4xf16>
            %153 = spirv.FAdd %149, %150 : f16
            %154 = spirv.FAdd %153, %151 : f16
            %155 = spirv.FAdd %154, %152 : f16
            %156 = spirv.CompositeExtract %148[0 : i32] : vector<4xf16>
            %157 = spirv.CompositeExtract %148[1 : i32] : vector<4xf16>
            %158 = spirv.CompositeExtract %148[2 : i32] : vector<4xf16>
            %159 = spirv.CompositeExtract %148[3 : i32] : vector<4xf16>
            %160 = spirv.FAdd %156, %157 : f16
            %161 = spirv.FAdd %160, %158 : f16
            %162 = spirv.FAdd %161, %159 : f16
            %163 = spirv.FAdd %155, %162 : f16
            %164 = spirv.GroupNonUniformFAdd "Subgroup" "Reduce" %163 : f16
            %165 = spirv.FAdd %164, %cst_f16 : f16
            %166 = spirv.IEqual %1, %cst0_i32 : i32
            spirv.mlir.selection {
              spirv.BranchConditional %166, ^bb1, ^bb2
            ^bb1:  // pred: ^bb0
              %167 = spirv.AccessChain %__resource_var_0_2__addr[%cst0_i32, %3] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>, i32, i32
              spirv.Store "StorageBuffer" %167, %165 : f16
              spirv.Branch ^bb2
            ^bb2:  // 2 preds: ^bb0, ^bb1
              spirv.mlir.merge
            }
            spirv.Return
          }
          spirv.EntryPoint "GLCompute" @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16, @__builtin__LocalInvocationId__, @__builtin__WorkgroupId__
          spirv.ExecutionMode @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 "LocalSize", 64, 1, 1
        }
      }
    }
  }
  func.func private @_main() {
    %c1_i32 = arith.constant 1 : i32
    %c553648144_i32 = arith.constant 553648144 : i32
    %c32000 = arith.constant 32000 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %device = hal.ex.shared_device : !hal.device
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
    %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_2, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
    %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
    %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_9, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    %3 = arith.select %value, %c0, %c-1 : index
    scf.index_switch %3 
    case 0 {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%1 : !hal.buffer)[%c0, %len], 
        %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
        %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
      ])
      hal.command_buffer.dispatch.symbol<%cmd_14 : !hal.command_buffer> target(@_main_dispatch_0::@vulkan_spirv_fb::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16) workgroups([%c32000, %c1, %c1])
      scf.yield
    }
    default {
    }
    hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
    %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
    %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_17, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<SPIRVSubgroupReduce>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.variant public @vulkan_spirv_fb target(#executable_target_vulkan_spirv_fb) {
      hal.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c32000 = arith.constant 32000 : index
        %c1 = arith.constant 1 : index
        hal.return %c32000, %c1, %c1 : index, index, index
      }
      builtin.module attributes {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>} {
        spirv.module Logical GLSL450 requires #spirv.vce<v1.3, [Shader, StorageBuffer16BitAccess, Float16, GroupNonUniformArithmetic], [SPV_KHR_storage_buffer_storage_class, SPV_KHR_16bit_storage]> {
          spirv.GlobalVariable @__builtin__WorkgroupId__ built_in("WorkgroupId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__builtin__LocalInvocationId__ built_in("LocalInvocationId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__resource_var_0_0_ bind(0, 0) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_1_ bind(0, 1) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_2_ bind(0, 2) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
          spirv.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() "None" {
            %cst64_i32 = spirv.Constant 64 : i32
            %cst128_i32 = spirv.Constant 128 : i32
            %cst192_i32 = spirv.Constant 192 : i32
            %cst256_i32 = spirv.Constant 256 : i32
            %cst320_i32 = spirv.Constant 320 : i32
            %cst384_i32 = spirv.Constant 384 : i32
            %cst448_i32 = spirv.Constant 448 : i32
            %cst512_i32 = spirv.Constant 512 : i32
            %cst_vec_4xf16 = spirv.Constant dense<0.000000e+00> : vector<4xf16>
            %cst0_i32 = spirv.Constant 0 : i32
            %cst_f16 = spirv.Constant 0.000000e+00 : f16
            %__builtin__LocalInvocationId___addr = spirv.mlir.addressof @__builtin__LocalInvocationId__ : !spirv.ptr<vector<3xi32>, Input>
            %0 = spirv.Load "Input" %__builtin__LocalInvocationId___addr : vector<3xi32>
            %1 = spirv.CompositeExtract %0[0 : i32] : vector<3xi32>
            %__resource_var_0_0__addr = spirv.mlir.addressof @__resource_var_0_0_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_1__addr = spirv.mlir.addressof @__resource_var_0_1_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_2__addr = spirv.mlir.addressof @__resource_var_0_2_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
            %__builtin__WorkgroupId___addr = spirv.mlir.addressof @__builtin__WorkgroupId__ : !spirv.ptr<vector<3xi32>, Input>
            %2 = spirv.Load "Input" %__builtin__WorkgroupId___addr : vector<3xi32>
            %3 = spirv.CompositeExtract %2[0 : i32] : vector<3xi32>
            %4 = spirv.IMul %3, %cst512_i32 : i32
            %5 = spirv.IAdd %1, %4 : i32
            %6 = spirv.IAdd %5, %cst448_i32 : i32
            %7 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %6] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %8 = spirv.Load "StorageBuffer" %7 : vector<4xf32>
            %9 = spirv.IAdd %1, %cst448_i32 : i32
            %10 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %9] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %11 = spirv.Load "StorageBuffer" %10 : vector<4xf32>
            %12 = spirv.IAdd %5, %cst384_i32 : i32
            %13 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %12] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %14 = spirv.Load "StorageBuffer" %13 : vector<4xf32>
            %15 = spirv.IAdd %1, %cst384_i32 : i32
            %16 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %15] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %17 = spirv.Load "StorageBuffer" %16 : vector<4xf32>
            %18 = spirv.IAdd %5, %cst320_i32 : i32
            %19 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %18] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %20 = spirv.Load "StorageBuffer" %19 : vector<4xf32>
            %21 = spirv.IAdd %1, %cst320_i32 : i32
            %22 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %21] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %23 = spirv.Load "StorageBuffer" %22 : vector<4xf32>
            %24 = spirv.IAdd %5, %cst256_i32 : i32
            %25 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %24] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %26 = spirv.Load "StorageBuffer" %25 : vector<4xf32>
            %27 = spirv.IAdd %1, %cst256_i32 : i32
            %28 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %27] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %29 = spirv.Load "StorageBuffer" %28 : vector<4xf32>
            %30 = spirv.IAdd %5, %cst192_i32 : i32
            %31 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %30] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %32 = spirv.Load "StorageBuffer" %31 : vector<4xf32>
            %33 = spirv.IAdd %1, %cst192_i32 : i32
            %34 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %33] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %35 = spirv.Load "StorageBuffer" %34 : vector<4xf32>
            %36 = spirv.IAdd %5, %cst128_i32 : i32
            %37 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %36] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %38 = spirv.Load "StorageBuffer" %37 : vector<4xf32>
            %39 = spirv.IAdd %1, %cst128_i32 : i32
            %40 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %39] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %41 = spirv.Load "StorageBuffer" %40 : vector<4xf32>
            %42 = spirv.IAdd %5, %cst64_i32 : i32
            %43 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %42] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %44 = spirv.Load "StorageBuffer" %43 : vector<4xf32>
            %45 = spirv.IAdd %1, %cst64_i32 : i32
            %46 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %45] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %47 = spirv.Load "StorageBuffer" %46 : vector<4xf32>
            %48 = spirv.IAdd %4, %1 : i32
            %49 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %48] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %50 = spirv.Load "StorageBuffer" %49 : vector<4xf32>
            %51 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %1] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %52 = spirv.Load "StorageBuffer" %51 : vector<4xf32>
            %53 = spirv.VectorShuffle [0 : i32, 1 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %54 = spirv.Bitcast %53 : vector<2xf32> to vector<4xf16>
            %55 = spirv.VectorShuffle [0 : i32, 1 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %56 = spirv.Bitcast %55 : vector<2xf32> to vector<4xf16>
            %57 = spirv.FMul %54, %56 : vector<4xf16>
            %58 = spirv.VectorShuffle [2 : i32, 3 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %59 = spirv.Bitcast %58 : vector<2xf32> to vector<4xf16>
            %60 = spirv.VectorShuffle [2 : i32, 3 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %61 = spirv.Bitcast %60 : vector<2xf32> to vector<4xf16>
            %62 = spirv.FMul %59, %61 : vector<4xf16>
            %63 = spirv.FAdd %57, %cst_vec_4xf16 : vector<4xf16>
            %64 = spirv.FAdd %62, %cst_vec_4xf16 : vector<4xf16>
            %65 = spirv.VectorShuffle [0 : i32, 1 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %66 = spirv.Bitcast %65 : vector<2xf32> to vector<4xf16>
            %67 = spirv.VectorShuffle [0 : i32, 1 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %68 = spirv.Bitcast %67 : vector<2xf32> to vector<4xf16>
            %69 = spirv.FMul %66, %68 : vector<4xf16>
            %70 = spirv.VectorShuffle [2 : i32, 3 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %71 = spirv.Bitcast %70 : vector<2xf32> to vector<4xf16>
            %72 = spirv.VectorShuffle [2 : i32, 3 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %73 = spirv.Bitcast %72 : vector<2xf32> to vector<4xf16>
            %74 = spirv.FMul %71, %73 : vector<4xf16>
            %75 = spirv.FAdd %69, %63 : vector<4xf16>
            %76 = spirv.FAdd %74, %64 : vector<4xf16>
            %77 = spirv.VectorShuffle [0 : i32, 1 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %78 = spirv.Bitcast %77 : vector<2xf32> to vector<4xf16>
            %79 = spirv.VectorShuffle [0 : i32, 1 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %80 = spirv.Bitcast %79 : vector<2xf32> to vector<4xf16>
            %81 = spirv.FMul %78, %80 : vector<4xf16>
            %82 = spirv.VectorShuffle [2 : i32, 3 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %83 = spirv.Bitcast %82 : vector<2xf32> to vector<4xf16>
            %84 = spirv.VectorShuffle [2 : i32, 3 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %85 = spirv.Bitcast %84 : vector<2xf32> to vector<4xf16>
            %86 = spirv.FMul %83, %85 : vector<4xf16>
            %87 = spirv.FAdd %81, %75 : vector<4xf16>
            %88 = spirv.FAdd %86, %76 : vector<4xf16>
            %89 = spirv.VectorShuffle [0 : i32, 1 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %90 = spirv.Bitcast %89 : vector<2xf32> to vector<4xf16>
            %91 = spirv.VectorShuffle [0 : i32, 1 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %92 = spirv.Bitcast %91 : vector<2xf32> to vector<4xf16>
            %93 = spirv.FMul %90, %92 : vector<4xf16>
            %94 = spirv.VectorShuffle [2 : i32, 3 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %95 = spirv.Bitcast %94 : vector<2xf32> to vector<4xf16>
            %96 = spirv.VectorShuffle [2 : i32, 3 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %97 = spirv.Bitcast %96 : vector<2xf32> to vector<4xf16>
            %98 = spirv.FMul %95, %97 : vector<4xf16>
            %99 = spirv.FAdd %93, %87 : vector<4xf16>
            %100 = spirv.FAdd %98, %88 : vector<4xf16>
            %101 = spirv.VectorShuffle [0 : i32, 1 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %102 = spirv.Bitcast %101 : vector<2xf32> to vector<4xf16>
            %103 = spirv.VectorShuffle [0 : i32, 1 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %104 = spirv.Bitcast %103 : vector<2xf32> to vector<4xf16>
            %105 = spirv.FMul %102, %104 : vector<4xf16>
            %106 = spirv.VectorShuffle [2 : i32, 3 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %107 = spirv.Bitcast %106 : vector<2xf32> to vector<4xf16>
            %108 = spirv.VectorShuffle [2 : i32, 3 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %109 = spirv.Bitcast %108 : vector<2xf32> to vector<4xf16>
            %110 = spirv.FMul %107, %109 : vector<4xf16>
            %111 = spirv.FAdd %105, %99 : vector<4xf16>
            %112 = spirv.FAdd %110, %100 : vector<4xf16>
            %113 = spirv.VectorShuffle [0 : i32, 1 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %114 = spirv.Bitcast %113 : vector<2xf32> to vector<4xf16>
            %115 = spirv.VectorShuffle [0 : i32, 1 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %116 = spirv.Bitcast %115 : vector<2xf32> to vector<4xf16>
            %117 = spirv.FMul %114, %116 : vector<4xf16>
            %118 = spirv.VectorShuffle [2 : i32, 3 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %119 = spirv.Bitcast %118 : vector<2xf32> to vector<4xf16>
            %120 = spirv.VectorShuffle [2 : i32, 3 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %121 = spirv.Bitcast %120 : vector<2xf32> to vector<4xf16>
            %122 = spirv.FMul %119, %121 : vector<4xf16>
            %123 = spirv.FAdd %117, %111 : vector<4xf16>
            %124 = spirv.FAdd %122, %112 : vector<4xf16>
            %125 = spirv.VectorShuffle [0 : i32, 1 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %126 = spirv.Bitcast %125 : vector<2xf32> to vector<4xf16>
            %127 = spirv.VectorShuffle [0 : i32, 1 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %128 = spirv.Bitcast %127 : vector<2xf32> to vector<4xf16>
            %129 = spirv.FMul %126, %128 : vector<4xf16>
            %130 = spirv.VectorShuffle [2 : i32, 3 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %131 = spirv.Bitcast %130 : vector<2xf32> to vector<4xf16>
            %132 = spirv.VectorShuffle [2 : i32, 3 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %133 = spirv.Bitcast %132 : vector<2xf32> to vector<4xf16>
            %134 = spirv.FMul %131, %133 : vector<4xf16>
            %135 = spirv.FAdd %129, %123 : vector<4xf16>
            %136 = spirv.FAdd %134, %124 : vector<4xf16>
            %137 = spirv.VectorShuffle [0 : i32, 1 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %138 = spirv.Bitcast %137 : vector<2xf32> to vector<4xf16>
            %139 = spirv.VectorShuffle [0 : i32, 1 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %140 = spirv.Bitcast %139 : vector<2xf32> to vector<4xf16>
            %141 = spirv.FMul %138, %140 : vector<4xf16>
            %142 = spirv.VectorShuffle [2 : i32, 3 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %143 = spirv.Bitcast %142 : vector<2xf32> to vector<4xf16>
            %144 = spirv.VectorShuffle [2 : i32, 3 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %145 = spirv.Bitcast %144 : vector<2xf32> to vector<4xf16>
            %146 = spirv.FMul %143, %145 : vector<4xf16>
            %147 = spirv.FAdd %141, %135 : vector<4xf16>
            %148 = spirv.FAdd %146, %136 : vector<4xf16>
            %149 = spirv.CompositeExtract %147[0 : i32] : vector<4xf16>
            %150 = spirv.CompositeExtract %147[1 : i32] : vector<4xf16>
            %151 = spirv.CompositeExtract %147[2 : i32] : vector<4xf16>
            %152 = spirv.CompositeExtract %147[3 : i32] : vector<4xf16>
            %153 = spirv.FAdd %149, %150 : f16
            %154 = spirv.FAdd %153, %151 : f16
            %155 = spirv.FAdd %154, %152 : f16
            %156 = spirv.CompositeExtract %148[0 : i32] : vector<4xf16>
            %157 = spirv.CompositeExtract %148[1 : i32] : vector<4xf16>
            %158 = spirv.CompositeExtract %148[2 : i32] : vector<4xf16>
            %159 = spirv.CompositeExtract %148[3 : i32] : vector<4xf16>
            %160 = spirv.FAdd %156, %157 : f16
            %161 = spirv.FAdd %160, %158 : f16
            %162 = spirv.FAdd %161, %159 : f16
            %163 = spirv.FAdd %155, %162 : f16
            %164 = spirv.GroupNonUniformFAdd "Subgroup" "Reduce" %163 : f16
            %165 = spirv.FAdd %164, %cst_f16 : f16
            %166 = spirv.IEqual %1, %cst0_i32 : i32
            spirv.mlir.selection {
              spirv.BranchConditional %166, ^bb1, ^bb2
            ^bb1:  // pred: ^bb0
              %167 = spirv.AccessChain %__resource_var_0_2__addr[%cst0_i32, %3] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>, i32, i32
              spirv.Store "StorageBuffer" %167, %165 : f16
              spirv.Branch ^bb2
            ^bb2:  // 2 preds: ^bb0, ^bb1
              spirv.mlir.merge
            }
            spirv.Return
          }
          spirv.EntryPoint "GLCompute" @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16, @__builtin__LocalInvocationId__, @__builtin__WorkgroupId__
          spirv.ExecutionMode @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 "LocalSize", 64, 1, 1
        }
      }
    }
  }
  func.func private @_main() {
    %c1_i32 = arith.constant 1 : i32
    %c553648144_i32 = arith.constant 553648144 : i32
    %c32000 = arith.constant 32000 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %device = hal.ex.shared_device : !hal.device
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
    %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_2, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
    %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
    %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_9, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    %3 = arith.select %value, %c0, %c-1 : index
    scf.index_switch %3 
    case 0 {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%1 : !hal.buffer)[%c0, %len], 
        %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
        %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
      ])
      hal.command_buffer.dispatch.symbol<%cmd_14 : !hal.command_buffer> target(@_main_dispatch_0::@vulkan_spirv_fb::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16) workgroups([%c32000, %c1, %c1])
      scf.yield
    }
    default {
    }
    hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
    %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
    %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_17, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<SPIRVSubgroupReduce>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.variant public @vulkan_spirv_fb target(#executable_target_vulkan_spirv_fb) {
      hal.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c32000 = arith.constant 32000 : index
        %c1 = arith.constant 1 : index
        hal.return %c32000, %c1, %c1 : index, index, index
      }
      builtin.module attributes {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>} {
        spirv.module Logical GLSL450 requires #spirv.vce<v1.3, [Shader, StorageBuffer16BitAccess, Float16, GroupNonUniformArithmetic], [SPV_KHR_storage_buffer_storage_class, SPV_KHR_16bit_storage]> {
          spirv.GlobalVariable @__builtin__WorkgroupId__ built_in("WorkgroupId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__builtin__LocalInvocationId__ built_in("LocalInvocationId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__resource_var_0_0_ bind(0, 0) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_1_ bind(0, 1) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_2_ bind(0, 2) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
          spirv.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() "None" {
            %cst64_i32 = spirv.Constant 64 : i32
            %cst128_i32 = spirv.Constant 128 : i32
            %cst192_i32 = spirv.Constant 192 : i32
            %cst256_i32 = spirv.Constant 256 : i32
            %cst320_i32 = spirv.Constant 320 : i32
            %cst384_i32 = spirv.Constant 384 : i32
            %cst448_i32 = spirv.Constant 448 : i32
            %cst512_i32 = spirv.Constant 512 : i32
            %cst_vec_4xf16 = spirv.Constant dense<0.000000e+00> : vector<4xf16>
            %cst0_i32 = spirv.Constant 0 : i32
            %cst_f16 = spirv.Constant 0.000000e+00 : f16
            %__builtin__LocalInvocationId___addr = spirv.mlir.addressof @__builtin__LocalInvocationId__ : !spirv.ptr<vector<3xi32>, Input>
            %0 = spirv.Load "Input" %__builtin__LocalInvocationId___addr : vector<3xi32>
            %1 = spirv.CompositeExtract %0[0 : i32] : vector<3xi32>
            %__resource_var_0_0__addr = spirv.mlir.addressof @__resource_var_0_0_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_1__addr = spirv.mlir.addressof @__resource_var_0_1_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_2__addr = spirv.mlir.addressof @__resource_var_0_2_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
            %__builtin__WorkgroupId___addr = spirv.mlir.addressof @__builtin__WorkgroupId__ : !spirv.ptr<vector<3xi32>, Input>
            %2 = spirv.Load "Input" %__builtin__WorkgroupId___addr : vector<3xi32>
            %3 = spirv.CompositeExtract %2[0 : i32] : vector<3xi32>
            %4 = spirv.IMul %3, %cst512_i32 : i32
            %5 = spirv.IAdd %1, %4 : i32
            %6 = spirv.IAdd %5, %cst448_i32 : i32
            %7 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %6] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %8 = spirv.Load "StorageBuffer" %7 : vector<4xf32>
            %9 = spirv.IAdd %1, %cst448_i32 : i32
            %10 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %9] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %11 = spirv.Load "StorageBuffer" %10 : vector<4xf32>
            %12 = spirv.IAdd %5, %cst384_i32 : i32
            %13 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %12] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %14 = spirv.Load "StorageBuffer" %13 : vector<4xf32>
            %15 = spirv.IAdd %1, %cst384_i32 : i32
            %16 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %15] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %17 = spirv.Load "StorageBuffer" %16 : vector<4xf32>
            %18 = spirv.IAdd %5, %cst320_i32 : i32
            %19 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %18] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %20 = spirv.Load "StorageBuffer" %19 : vector<4xf32>
            %21 = spirv.IAdd %1, %cst320_i32 : i32
            %22 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %21] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %23 = spirv.Load "StorageBuffer" %22 : vector<4xf32>
            %24 = spirv.IAdd %5, %cst256_i32 : i32
            %25 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %24] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %26 = spirv.Load "StorageBuffer" %25 : vector<4xf32>
            %27 = spirv.IAdd %1, %cst256_i32 : i32
            %28 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %27] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %29 = spirv.Load "StorageBuffer" %28 : vector<4xf32>
            %30 = spirv.IAdd %5, %cst192_i32 : i32
            %31 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %30] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %32 = spirv.Load "StorageBuffer" %31 : vector<4xf32>
            %33 = spirv.IAdd %1, %cst192_i32 : i32
            %34 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %33] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %35 = spirv.Load "StorageBuffer" %34 : vector<4xf32>
            %36 = spirv.IAdd %5, %cst128_i32 : i32
            %37 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %36] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %38 = spirv.Load "StorageBuffer" %37 : vector<4xf32>
            %39 = spirv.IAdd %1, %cst128_i32 : i32
            %40 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %39] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %41 = spirv.Load "StorageBuffer" %40 : vector<4xf32>
            %42 = spirv.IAdd %5, %cst64_i32 : i32
            %43 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %42] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %44 = spirv.Load "StorageBuffer" %43 : vector<4xf32>
            %45 = spirv.IAdd %1, %cst64_i32 : i32
            %46 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %45] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %47 = spirv.Load "StorageBuffer" %46 : vector<4xf32>
            %48 = spirv.IAdd %4, %1 : i32
            %49 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %48] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %50 = spirv.Load "StorageBuffer" %49 : vector<4xf32>
            %51 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %1] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %52 = spirv.Load "StorageBuffer" %51 : vector<4xf32>
            %53 = spirv.VectorShuffle [0 : i32, 1 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %54 = spirv.Bitcast %53 : vector<2xf32> to vector<4xf16>
            %55 = spirv.VectorShuffle [0 : i32, 1 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %56 = spirv.Bitcast %55 : vector<2xf32> to vector<4xf16>
            %57 = spirv.FMul %54, %56 : vector<4xf16>
            %58 = spirv.VectorShuffle [2 : i32, 3 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %59 = spirv.Bitcast %58 : vector<2xf32> to vector<4xf16>
            %60 = spirv.VectorShuffle [2 : i32, 3 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %61 = spirv.Bitcast %60 : vector<2xf32> to vector<4xf16>
            %62 = spirv.FMul %59, %61 : vector<4xf16>
            %63 = spirv.FAdd %57, %cst_vec_4xf16 : vector<4xf16>
            %64 = spirv.FAdd %62, %cst_vec_4xf16 : vector<4xf16>
            %65 = spirv.VectorShuffle [0 : i32, 1 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %66 = spirv.Bitcast %65 : vector<2xf32> to vector<4xf16>
            %67 = spirv.VectorShuffle [0 : i32, 1 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %68 = spirv.Bitcast %67 : vector<2xf32> to vector<4xf16>
            %69 = spirv.FMul %66, %68 : vector<4xf16>
            %70 = spirv.VectorShuffle [2 : i32, 3 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %71 = spirv.Bitcast %70 : vector<2xf32> to vector<4xf16>
            %72 = spirv.VectorShuffle [2 : i32, 3 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %73 = spirv.Bitcast %72 : vector<2xf32> to vector<4xf16>
            %74 = spirv.FMul %71, %73 : vector<4xf16>
            %75 = spirv.FAdd %69, %63 : vector<4xf16>
            %76 = spirv.FAdd %74, %64 : vector<4xf16>
            %77 = spirv.VectorShuffle [0 : i32, 1 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %78 = spirv.Bitcast %77 : vector<2xf32> to vector<4xf16>
            %79 = spirv.VectorShuffle [0 : i32, 1 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %80 = spirv.Bitcast %79 : vector<2xf32> to vector<4xf16>
            %81 = spirv.FMul %78, %80 : vector<4xf16>
            %82 = spirv.VectorShuffle [2 : i32, 3 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %83 = spirv.Bitcast %82 : vector<2xf32> to vector<4xf16>
            %84 = spirv.VectorShuffle [2 : i32, 3 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %85 = spirv.Bitcast %84 : vector<2xf32> to vector<4xf16>
            %86 = spirv.FMul %83, %85 : vector<4xf16>
            %87 = spirv.FAdd %81, %75 : vector<4xf16>
            %88 = spirv.FAdd %86, %76 : vector<4xf16>
            %89 = spirv.VectorShuffle [0 : i32, 1 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %90 = spirv.Bitcast %89 : vector<2xf32> to vector<4xf16>
            %91 = spirv.VectorShuffle [0 : i32, 1 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %92 = spirv.Bitcast %91 : vector<2xf32> to vector<4xf16>
            %93 = spirv.FMul %90, %92 : vector<4xf16>
            %94 = spirv.VectorShuffle [2 : i32, 3 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %95 = spirv.Bitcast %94 : vector<2xf32> to vector<4xf16>
            %96 = spirv.VectorShuffle [2 : i32, 3 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %97 = spirv.Bitcast %96 : vector<2xf32> to vector<4xf16>
            %98 = spirv.FMul %95, %97 : vector<4xf16>
            %99 = spirv.FAdd %93, %87 : vector<4xf16>
            %100 = spirv.FAdd %98, %88 : vector<4xf16>
            %101 = spirv.VectorShuffle [0 : i32, 1 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %102 = spirv.Bitcast %101 : vector<2xf32> to vector<4xf16>
            %103 = spirv.VectorShuffle [0 : i32, 1 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %104 = spirv.Bitcast %103 : vector<2xf32> to vector<4xf16>
            %105 = spirv.FMul %102, %104 : vector<4xf16>
            %106 = spirv.VectorShuffle [2 : i32, 3 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %107 = spirv.Bitcast %106 : vector<2xf32> to vector<4xf16>
            %108 = spirv.VectorShuffle [2 : i32, 3 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %109 = spirv.Bitcast %108 : vector<2xf32> to vector<4xf16>
            %110 = spirv.FMul %107, %109 : vector<4xf16>
            %111 = spirv.FAdd %105, %99 : vector<4xf16>
            %112 = spirv.FAdd %110, %100 : vector<4xf16>
            %113 = spirv.VectorShuffle [0 : i32, 1 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %114 = spirv.Bitcast %113 : vector<2xf32> to vector<4xf16>
            %115 = spirv.VectorShuffle [0 : i32, 1 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %116 = spirv.Bitcast %115 : vector<2xf32> to vector<4xf16>
            %117 = spirv.FMul %114, %116 : vector<4xf16>
            %118 = spirv.VectorShuffle [2 : i32, 3 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %119 = spirv.Bitcast %118 : vector<2xf32> to vector<4xf16>
            %120 = spirv.VectorShuffle [2 : i32, 3 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %121 = spirv.Bitcast %120 : vector<2xf32> to vector<4xf16>
            %122 = spirv.FMul %119, %121 : vector<4xf16>
            %123 = spirv.FAdd %117, %111 : vector<4xf16>
            %124 = spirv.FAdd %122, %112 : vector<4xf16>
            %125 = spirv.VectorShuffle [0 : i32, 1 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %126 = spirv.Bitcast %125 : vector<2xf32> to vector<4xf16>
            %127 = spirv.VectorShuffle [0 : i32, 1 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %128 = spirv.Bitcast %127 : vector<2xf32> to vector<4xf16>
            %129 = spirv.FMul %126, %128 : vector<4xf16>
            %130 = spirv.VectorShuffle [2 : i32, 3 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %131 = spirv.Bitcast %130 : vector<2xf32> to vector<4xf16>
            %132 = spirv.VectorShuffle [2 : i32, 3 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %133 = spirv.Bitcast %132 : vector<2xf32> to vector<4xf16>
            %134 = spirv.FMul %131, %133 : vector<4xf16>
            %135 = spirv.FAdd %129, %123 : vector<4xf16>
            %136 = spirv.FAdd %134, %124 : vector<4xf16>
            %137 = spirv.VectorShuffle [0 : i32, 1 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %138 = spirv.Bitcast %137 : vector<2xf32> to vector<4xf16>
            %139 = spirv.VectorShuffle [0 : i32, 1 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %140 = spirv.Bitcast %139 : vector<2xf32> to vector<4xf16>
            %141 = spirv.FMul %138, %140 : vector<4xf16>
            %142 = spirv.VectorShuffle [2 : i32, 3 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %143 = spirv.Bitcast %142 : vector<2xf32> to vector<4xf16>
            %144 = spirv.VectorShuffle [2 : i32, 3 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %145 = spirv.Bitcast %144 : vector<2xf32> to vector<4xf16>
            %146 = spirv.FMul %143, %145 : vector<4xf16>
            %147 = spirv.FAdd %141, %135 : vector<4xf16>
            %148 = spirv.FAdd %146, %136 : vector<4xf16>
            %149 = spirv.CompositeExtract %147[0 : i32] : vector<4xf16>
            %150 = spirv.CompositeExtract %147[1 : i32] : vector<4xf16>
            %151 = spirv.CompositeExtract %147[2 : i32] : vector<4xf16>
            %152 = spirv.CompositeExtract %147[3 : i32] : vector<4xf16>
            %153 = spirv.FAdd %149, %150 : f16
            %154 = spirv.FAdd %153, %151 : f16
            %155 = spirv.FAdd %154, %152 : f16
            %156 = spirv.CompositeExtract %148[0 : i32] : vector<4xf16>
            %157 = spirv.CompositeExtract %148[1 : i32] : vector<4xf16>
            %158 = spirv.CompositeExtract %148[2 : i32] : vector<4xf16>
            %159 = spirv.CompositeExtract %148[3 : i32] : vector<4xf16>
            %160 = spirv.FAdd %156, %157 : f16
            %161 = spirv.FAdd %160, %158 : f16
            %162 = spirv.FAdd %161, %159 : f16
            %163 = spirv.FAdd %155, %162 : f16
            %164 = spirv.GroupNonUniformFAdd "Subgroup" "Reduce" %163 : f16
            %165 = spirv.FAdd %164, %cst_f16 : f16
            %166 = spirv.IEqual %1, %cst0_i32 : i32
            spirv.mlir.selection {
              spirv.BranchConditional %166, ^bb1, ^bb2
            ^bb1:  // pred: ^bb0
              %167 = spirv.AccessChain %__resource_var_0_2__addr[%cst0_i32, %3] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>, i32, i32
              spirv.Store "StorageBuffer" %167, %165 : f16
              spirv.Branch ^bb2
            ^bb2:  // 2 preds: ^bb0, ^bb1
              spirv.mlir.merge
            }
            spirv.Return
          }
          spirv.EntryPoint "GLCompute" @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16, @__builtin__LocalInvocationId__, @__builtin__WorkgroupId__
          spirv.ExecutionMode @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 "LocalSize", 64, 1, 1
        }
      }
    }
  }
  func.func private @_main() {
    %c1_i32 = arith.constant 1 : i32
    %c553648144_i32 = arith.constant 553648144 : i32
    %c32000 = arith.constant 32000 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %device = hal.ex.shared_device : !hal.device
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
    %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_2, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
    %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
    %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_9, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    %3 = arith.select %value, %c0, %c-1 : index
    scf.index_switch %3 
    case 0 {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%1 : !hal.buffer)[%c0, %len], 
        %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
        %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
      ])
      hal.command_buffer.dispatch.symbol<%cmd_14 : !hal.command_buffer> target(@_main_dispatch_0::@vulkan_spirv_fb::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16) workgroups([%c32000, %c1, %c1])
      scf.yield
    }
    default {
    }
    hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
    %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
    %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_17, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::LinkTargetExecutablesPass (iree-hal-link-target-executables) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<SPIRVSubgroupReduce>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.variant public @vulkan_spirv_fb target(#executable_target_vulkan_spirv_fb) {
      hal.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c32000 = arith.constant 32000 : index
        %c1 = arith.constant 1 : index
        hal.return %c32000, %c1, %c1 : index, index, index
      }
      builtin.module attributes {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>} {
        spirv.module Logical GLSL450 requires #spirv.vce<v1.3, [Shader, StorageBuffer16BitAccess, Float16, GroupNonUniformArithmetic], [SPV_KHR_storage_buffer_storage_class, SPV_KHR_16bit_storage]> {
          spirv.GlobalVariable @__builtin__WorkgroupId__ built_in("WorkgroupId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__builtin__LocalInvocationId__ built_in("LocalInvocationId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__resource_var_0_0_ bind(0, 0) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_1_ bind(0, 1) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_2_ bind(0, 2) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
          spirv.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() "None" {
            %cst64_i32 = spirv.Constant 64 : i32
            %cst128_i32 = spirv.Constant 128 : i32
            %cst192_i32 = spirv.Constant 192 : i32
            %cst256_i32 = spirv.Constant 256 : i32
            %cst320_i32 = spirv.Constant 320 : i32
            %cst384_i32 = spirv.Constant 384 : i32
            %cst448_i32 = spirv.Constant 448 : i32
            %cst512_i32 = spirv.Constant 512 : i32
            %cst_vec_4xf16 = spirv.Constant dense<0.000000e+00> : vector<4xf16>
            %cst0_i32 = spirv.Constant 0 : i32
            %cst_f16 = spirv.Constant 0.000000e+00 : f16
            %__builtin__LocalInvocationId___addr = spirv.mlir.addressof @__builtin__LocalInvocationId__ : !spirv.ptr<vector<3xi32>, Input>
            %0 = spirv.Load "Input" %__builtin__LocalInvocationId___addr : vector<3xi32>
            %1 = spirv.CompositeExtract %0[0 : i32] : vector<3xi32>
            %__resource_var_0_0__addr = spirv.mlir.addressof @__resource_var_0_0_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_1__addr = spirv.mlir.addressof @__resource_var_0_1_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_2__addr = spirv.mlir.addressof @__resource_var_0_2_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
            %__builtin__WorkgroupId___addr = spirv.mlir.addressof @__builtin__WorkgroupId__ : !spirv.ptr<vector<3xi32>, Input>
            %2 = spirv.Load "Input" %__builtin__WorkgroupId___addr : vector<3xi32>
            %3 = spirv.CompositeExtract %2[0 : i32] : vector<3xi32>
            %4 = spirv.IMul %3, %cst512_i32 : i32
            %5 = spirv.IAdd %1, %4 : i32
            %6 = spirv.IAdd %5, %cst448_i32 : i32
            %7 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %6] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %8 = spirv.Load "StorageBuffer" %7 : vector<4xf32>
            %9 = spirv.IAdd %1, %cst448_i32 : i32
            %10 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %9] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %11 = spirv.Load "StorageBuffer" %10 : vector<4xf32>
            %12 = spirv.IAdd %5, %cst384_i32 : i32
            %13 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %12] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %14 = spirv.Load "StorageBuffer" %13 : vector<4xf32>
            %15 = spirv.IAdd %1, %cst384_i32 : i32
            %16 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %15] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %17 = spirv.Load "StorageBuffer" %16 : vector<4xf32>
            %18 = spirv.IAdd %5, %cst320_i32 : i32
            %19 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %18] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %20 = spirv.Load "StorageBuffer" %19 : vector<4xf32>
            %21 = spirv.IAdd %1, %cst320_i32 : i32
            %22 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %21] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %23 = spirv.Load "StorageBuffer" %22 : vector<4xf32>
            %24 = spirv.IAdd %5, %cst256_i32 : i32
            %25 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %24] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %26 = spirv.Load "StorageBuffer" %25 : vector<4xf32>
            %27 = spirv.IAdd %1, %cst256_i32 : i32
            %28 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %27] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %29 = spirv.Load "StorageBuffer" %28 : vector<4xf32>
            %30 = spirv.IAdd %5, %cst192_i32 : i32
            %31 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %30] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %32 = spirv.Load "StorageBuffer" %31 : vector<4xf32>
            %33 = spirv.IAdd %1, %cst192_i32 : i32
            %34 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %33] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %35 = spirv.Load "StorageBuffer" %34 : vector<4xf32>
            %36 = spirv.IAdd %5, %cst128_i32 : i32
            %37 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %36] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %38 = spirv.Load "StorageBuffer" %37 : vector<4xf32>
            %39 = spirv.IAdd %1, %cst128_i32 : i32
            %40 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %39] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %41 = spirv.Load "StorageBuffer" %40 : vector<4xf32>
            %42 = spirv.IAdd %5, %cst64_i32 : i32
            %43 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %42] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %44 = spirv.Load "StorageBuffer" %43 : vector<4xf32>
            %45 = spirv.IAdd %1, %cst64_i32 : i32
            %46 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %45] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %47 = spirv.Load "StorageBuffer" %46 : vector<4xf32>
            %48 = spirv.IAdd %4, %1 : i32
            %49 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %48] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %50 = spirv.Load "StorageBuffer" %49 : vector<4xf32>
            %51 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %1] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %52 = spirv.Load "StorageBuffer" %51 : vector<4xf32>
            %53 = spirv.VectorShuffle [0 : i32, 1 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %54 = spirv.Bitcast %53 : vector<2xf32> to vector<4xf16>
            %55 = spirv.VectorShuffle [0 : i32, 1 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %56 = spirv.Bitcast %55 : vector<2xf32> to vector<4xf16>
            %57 = spirv.FMul %54, %56 : vector<4xf16>
            %58 = spirv.VectorShuffle [2 : i32, 3 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %59 = spirv.Bitcast %58 : vector<2xf32> to vector<4xf16>
            %60 = spirv.VectorShuffle [2 : i32, 3 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %61 = spirv.Bitcast %60 : vector<2xf32> to vector<4xf16>
            %62 = spirv.FMul %59, %61 : vector<4xf16>
            %63 = spirv.FAdd %57, %cst_vec_4xf16 : vector<4xf16>
            %64 = spirv.FAdd %62, %cst_vec_4xf16 : vector<4xf16>
            %65 = spirv.VectorShuffle [0 : i32, 1 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %66 = spirv.Bitcast %65 : vector<2xf32> to vector<4xf16>
            %67 = spirv.VectorShuffle [0 : i32, 1 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %68 = spirv.Bitcast %67 : vector<2xf32> to vector<4xf16>
            %69 = spirv.FMul %66, %68 : vector<4xf16>
            %70 = spirv.VectorShuffle [2 : i32, 3 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %71 = spirv.Bitcast %70 : vector<2xf32> to vector<4xf16>
            %72 = spirv.VectorShuffle [2 : i32, 3 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %73 = spirv.Bitcast %72 : vector<2xf32> to vector<4xf16>
            %74 = spirv.FMul %71, %73 : vector<4xf16>
            %75 = spirv.FAdd %69, %63 : vector<4xf16>
            %76 = spirv.FAdd %74, %64 : vector<4xf16>
            %77 = spirv.VectorShuffle [0 : i32, 1 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %78 = spirv.Bitcast %77 : vector<2xf32> to vector<4xf16>
            %79 = spirv.VectorShuffle [0 : i32, 1 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %80 = spirv.Bitcast %79 : vector<2xf32> to vector<4xf16>
            %81 = spirv.FMul %78, %80 : vector<4xf16>
            %82 = spirv.VectorShuffle [2 : i32, 3 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %83 = spirv.Bitcast %82 : vector<2xf32> to vector<4xf16>
            %84 = spirv.VectorShuffle [2 : i32, 3 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %85 = spirv.Bitcast %84 : vector<2xf32> to vector<4xf16>
            %86 = spirv.FMul %83, %85 : vector<4xf16>
            %87 = spirv.FAdd %81, %75 : vector<4xf16>
            %88 = spirv.FAdd %86, %76 : vector<4xf16>
            %89 = spirv.VectorShuffle [0 : i32, 1 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %90 = spirv.Bitcast %89 : vector<2xf32> to vector<4xf16>
            %91 = spirv.VectorShuffle [0 : i32, 1 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %92 = spirv.Bitcast %91 : vector<2xf32> to vector<4xf16>
            %93 = spirv.FMul %90, %92 : vector<4xf16>
            %94 = spirv.VectorShuffle [2 : i32, 3 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %95 = spirv.Bitcast %94 : vector<2xf32> to vector<4xf16>
            %96 = spirv.VectorShuffle [2 : i32, 3 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %97 = spirv.Bitcast %96 : vector<2xf32> to vector<4xf16>
            %98 = spirv.FMul %95, %97 : vector<4xf16>
            %99 = spirv.FAdd %93, %87 : vector<4xf16>
            %100 = spirv.FAdd %98, %88 : vector<4xf16>
            %101 = spirv.VectorShuffle [0 : i32, 1 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %102 = spirv.Bitcast %101 : vector<2xf32> to vector<4xf16>
            %103 = spirv.VectorShuffle [0 : i32, 1 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %104 = spirv.Bitcast %103 : vector<2xf32> to vector<4xf16>
            %105 = spirv.FMul %102, %104 : vector<4xf16>
            %106 = spirv.VectorShuffle [2 : i32, 3 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %107 = spirv.Bitcast %106 : vector<2xf32> to vector<4xf16>
            %108 = spirv.VectorShuffle [2 : i32, 3 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %109 = spirv.Bitcast %108 : vector<2xf32> to vector<4xf16>
            %110 = spirv.FMul %107, %109 : vector<4xf16>
            %111 = spirv.FAdd %105, %99 : vector<4xf16>
            %112 = spirv.FAdd %110, %100 : vector<4xf16>
            %113 = spirv.VectorShuffle [0 : i32, 1 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %114 = spirv.Bitcast %113 : vector<2xf32> to vector<4xf16>
            %115 = spirv.VectorShuffle [0 : i32, 1 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %116 = spirv.Bitcast %115 : vector<2xf32> to vector<4xf16>
            %117 = spirv.FMul %114, %116 : vector<4xf16>
            %118 = spirv.VectorShuffle [2 : i32, 3 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %119 = spirv.Bitcast %118 : vector<2xf32> to vector<4xf16>
            %120 = spirv.VectorShuffle [2 : i32, 3 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %121 = spirv.Bitcast %120 : vector<2xf32> to vector<4xf16>
            %122 = spirv.FMul %119, %121 : vector<4xf16>
            %123 = spirv.FAdd %117, %111 : vector<4xf16>
            %124 = spirv.FAdd %122, %112 : vector<4xf16>
            %125 = spirv.VectorShuffle [0 : i32, 1 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %126 = spirv.Bitcast %125 : vector<2xf32> to vector<4xf16>
            %127 = spirv.VectorShuffle [0 : i32, 1 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %128 = spirv.Bitcast %127 : vector<2xf32> to vector<4xf16>
            %129 = spirv.FMul %126, %128 : vector<4xf16>
            %130 = spirv.VectorShuffle [2 : i32, 3 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %131 = spirv.Bitcast %130 : vector<2xf32> to vector<4xf16>
            %132 = spirv.VectorShuffle [2 : i32, 3 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %133 = spirv.Bitcast %132 : vector<2xf32> to vector<4xf16>
            %134 = spirv.FMul %131, %133 : vector<4xf16>
            %135 = spirv.FAdd %129, %123 : vector<4xf16>
            %136 = spirv.FAdd %134, %124 : vector<4xf16>
            %137 = spirv.VectorShuffle [0 : i32, 1 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %138 = spirv.Bitcast %137 : vector<2xf32> to vector<4xf16>
            %139 = spirv.VectorShuffle [0 : i32, 1 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %140 = spirv.Bitcast %139 : vector<2xf32> to vector<4xf16>
            %141 = spirv.FMul %138, %140 : vector<4xf16>
            %142 = spirv.VectorShuffle [2 : i32, 3 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %143 = spirv.Bitcast %142 : vector<2xf32> to vector<4xf16>
            %144 = spirv.VectorShuffle [2 : i32, 3 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %145 = spirv.Bitcast %144 : vector<2xf32> to vector<4xf16>
            %146 = spirv.FMul %143, %145 : vector<4xf16>
            %147 = spirv.FAdd %141, %135 : vector<4xf16>
            %148 = spirv.FAdd %146, %136 : vector<4xf16>
            %149 = spirv.CompositeExtract %147[0 : i32] : vector<4xf16>
            %150 = spirv.CompositeExtract %147[1 : i32] : vector<4xf16>
            %151 = spirv.CompositeExtract %147[2 : i32] : vector<4xf16>
            %152 = spirv.CompositeExtract %147[3 : i32] : vector<4xf16>
            %153 = spirv.FAdd %149, %150 : f16
            %154 = spirv.FAdd %153, %151 : f16
            %155 = spirv.FAdd %154, %152 : f16
            %156 = spirv.CompositeExtract %148[0 : i32] : vector<4xf16>
            %157 = spirv.CompositeExtract %148[1 : i32] : vector<4xf16>
            %158 = spirv.CompositeExtract %148[2 : i32] : vector<4xf16>
            %159 = spirv.CompositeExtract %148[3 : i32] : vector<4xf16>
            %160 = spirv.FAdd %156, %157 : f16
            %161 = spirv.FAdd %160, %158 : f16
            %162 = spirv.FAdd %161, %159 : f16
            %163 = spirv.FAdd %155, %162 : f16
            %164 = spirv.GroupNonUniformFAdd "Subgroup" "Reduce" %163 : f16
            %165 = spirv.FAdd %164, %cst_f16 : f16
            %166 = spirv.IEqual %1, %cst0_i32 : i32
            spirv.mlir.selection {
              spirv.BranchConditional %166, ^bb1, ^bb2
            ^bb1:  // pred: ^bb0
              %167 = spirv.AccessChain %__resource_var_0_2__addr[%cst0_i32, %3] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>, i32, i32
              spirv.Store "StorageBuffer" %167, %165 : f16
              spirv.Branch ^bb2
            ^bb2:  // 2 preds: ^bb0, ^bb1
              spirv.mlir.merge
            }
            spirv.Return
          }
          spirv.EntryPoint "GLCompute" @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16, @__builtin__LocalInvocationId__, @__builtin__WorkgroupId__
          spirv.ExecutionMode @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 "LocalSize", 64, 1, 1
        }
      }
    }
  }
  func.func private @_main() {
    %c1_i32 = arith.constant 1 : i32
    %c553648144_i32 = arith.constant 553648144 : i32
    %c32000 = arith.constant 32000 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %device = hal.ex.shared_device : !hal.device
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
    %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_2, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
    %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
    %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_9, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    %3 = arith.select %value, %c0, %c-1 : index
    scf.index_switch %3 
    case 0 {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%1 : !hal.buffer)[%c0, %len], 
        %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
        %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
      ])
      hal.command_buffer.dispatch.symbol<%cmd_14 : !hal.command_buffer> target(@_main_dispatch_0::@vulkan_spirv_fb::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16) workgroups([%c32000, %c1, %c1])
      scf.yield
    }
    default {
    }
    hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
    %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
    %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_17, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<SPIRVSubgroupReduce>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.variant public @vulkan_spirv_fb target(#executable_target_vulkan_spirv_fb) {
      hal.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c32000 = arith.constant 32000 : index
        %c1 = arith.constant 1 : index
        hal.return %c32000, %c1, %c1 : index, index, index
      }
      builtin.module attributes {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>} {
        spirv.module Logical GLSL450 requires #spirv.vce<v1.3, [Shader, StorageBuffer16BitAccess, Float16, GroupNonUniformArithmetic], [SPV_KHR_storage_buffer_storage_class, SPV_KHR_16bit_storage]> {
          spirv.GlobalVariable @__builtin__WorkgroupId__ built_in("WorkgroupId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__builtin__LocalInvocationId__ built_in("LocalInvocationId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__resource_var_0_0_ bind(0, 0) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_1_ bind(0, 1) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_2_ bind(0, 2) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
          spirv.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() "None" {
            %cst64_i32 = spirv.Constant 64 : i32
            %cst128_i32 = spirv.Constant 128 : i32
            %cst192_i32 = spirv.Constant 192 : i32
            %cst256_i32 = spirv.Constant 256 : i32
            %cst320_i32 = spirv.Constant 320 : i32
            %cst384_i32 = spirv.Constant 384 : i32
            %cst448_i32 = spirv.Constant 448 : i32
            %cst512_i32 = spirv.Constant 512 : i32
            %cst_vec_4xf16 = spirv.Constant dense<0.000000e+00> : vector<4xf16>
            %cst0_i32 = spirv.Constant 0 : i32
            %cst_f16 = spirv.Constant 0.000000e+00 : f16
            %__builtin__LocalInvocationId___addr = spirv.mlir.addressof @__builtin__LocalInvocationId__ : !spirv.ptr<vector<3xi32>, Input>
            %0 = spirv.Load "Input" %__builtin__LocalInvocationId___addr : vector<3xi32>
            %1 = spirv.CompositeExtract %0[0 : i32] : vector<3xi32>
            %__resource_var_0_0__addr = spirv.mlir.addressof @__resource_var_0_0_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_1__addr = spirv.mlir.addressof @__resource_var_0_1_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_2__addr = spirv.mlir.addressof @__resource_var_0_2_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
            %__builtin__WorkgroupId___addr = spirv.mlir.addressof @__builtin__WorkgroupId__ : !spirv.ptr<vector<3xi32>, Input>
            %2 = spirv.Load "Input" %__builtin__WorkgroupId___addr : vector<3xi32>
            %3 = spirv.CompositeExtract %2[0 : i32] : vector<3xi32>
            %4 = spirv.IMul %3, %cst512_i32 : i32
            %5 = spirv.IAdd %1, %4 : i32
            %6 = spirv.IAdd %5, %cst448_i32 : i32
            %7 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %6] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %8 = spirv.Load "StorageBuffer" %7 : vector<4xf32>
            %9 = spirv.IAdd %1, %cst448_i32 : i32
            %10 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %9] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %11 = spirv.Load "StorageBuffer" %10 : vector<4xf32>
            %12 = spirv.IAdd %5, %cst384_i32 : i32
            %13 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %12] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %14 = spirv.Load "StorageBuffer" %13 : vector<4xf32>
            %15 = spirv.IAdd %1, %cst384_i32 : i32
            %16 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %15] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %17 = spirv.Load "StorageBuffer" %16 : vector<4xf32>
            %18 = spirv.IAdd %5, %cst320_i32 : i32
            %19 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %18] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %20 = spirv.Load "StorageBuffer" %19 : vector<4xf32>
            %21 = spirv.IAdd %1, %cst320_i32 : i32
            %22 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %21] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %23 = spirv.Load "StorageBuffer" %22 : vector<4xf32>
            %24 = spirv.IAdd %5, %cst256_i32 : i32
            %25 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %24] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %26 = spirv.Load "StorageBuffer" %25 : vector<4xf32>
            %27 = spirv.IAdd %1, %cst256_i32 : i32
            %28 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %27] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %29 = spirv.Load "StorageBuffer" %28 : vector<4xf32>
            %30 = spirv.IAdd %5, %cst192_i32 : i32
            %31 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %30] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %32 = spirv.Load "StorageBuffer" %31 : vector<4xf32>
            %33 = spirv.IAdd %1, %cst192_i32 : i32
            %34 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %33] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %35 = spirv.Load "StorageBuffer" %34 : vector<4xf32>
            %36 = spirv.IAdd %5, %cst128_i32 : i32
            %37 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %36] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %38 = spirv.Load "StorageBuffer" %37 : vector<4xf32>
            %39 = spirv.IAdd %1, %cst128_i32 : i32
            %40 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %39] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %41 = spirv.Load "StorageBuffer" %40 : vector<4xf32>
            %42 = spirv.IAdd %5, %cst64_i32 : i32
            %43 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %42] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %44 = spirv.Load "StorageBuffer" %43 : vector<4xf32>
            %45 = spirv.IAdd %1, %cst64_i32 : i32
            %46 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %45] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %47 = spirv.Load "StorageBuffer" %46 : vector<4xf32>
            %48 = spirv.IAdd %4, %1 : i32
            %49 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %48] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %50 = spirv.Load "StorageBuffer" %49 : vector<4xf32>
            %51 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %1] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %52 = spirv.Load "StorageBuffer" %51 : vector<4xf32>
            %53 = spirv.VectorShuffle [0 : i32, 1 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %54 = spirv.Bitcast %53 : vector<2xf32> to vector<4xf16>
            %55 = spirv.VectorShuffle [0 : i32, 1 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %56 = spirv.Bitcast %55 : vector<2xf32> to vector<4xf16>
            %57 = spirv.FMul %54, %56 : vector<4xf16>
            %58 = spirv.VectorShuffle [2 : i32, 3 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %59 = spirv.Bitcast %58 : vector<2xf32> to vector<4xf16>
            %60 = spirv.VectorShuffle [2 : i32, 3 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %61 = spirv.Bitcast %60 : vector<2xf32> to vector<4xf16>
            %62 = spirv.FMul %59, %61 : vector<4xf16>
            %63 = spirv.FAdd %57, %cst_vec_4xf16 : vector<4xf16>
            %64 = spirv.FAdd %62, %cst_vec_4xf16 : vector<4xf16>
            %65 = spirv.VectorShuffle [0 : i32, 1 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %66 = spirv.Bitcast %65 : vector<2xf32> to vector<4xf16>
            %67 = spirv.VectorShuffle [0 : i32, 1 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %68 = spirv.Bitcast %67 : vector<2xf32> to vector<4xf16>
            %69 = spirv.FMul %66, %68 : vector<4xf16>
            %70 = spirv.VectorShuffle [2 : i32, 3 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %71 = spirv.Bitcast %70 : vector<2xf32> to vector<4xf16>
            %72 = spirv.VectorShuffle [2 : i32, 3 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %73 = spirv.Bitcast %72 : vector<2xf32> to vector<4xf16>
            %74 = spirv.FMul %71, %73 : vector<4xf16>
            %75 = spirv.FAdd %69, %63 : vector<4xf16>
            %76 = spirv.FAdd %74, %64 : vector<4xf16>
            %77 = spirv.VectorShuffle [0 : i32, 1 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %78 = spirv.Bitcast %77 : vector<2xf32> to vector<4xf16>
            %79 = spirv.VectorShuffle [0 : i32, 1 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %80 = spirv.Bitcast %79 : vector<2xf32> to vector<4xf16>
            %81 = spirv.FMul %78, %80 : vector<4xf16>
            %82 = spirv.VectorShuffle [2 : i32, 3 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %83 = spirv.Bitcast %82 : vector<2xf32> to vector<4xf16>
            %84 = spirv.VectorShuffle [2 : i32, 3 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %85 = spirv.Bitcast %84 : vector<2xf32> to vector<4xf16>
            %86 = spirv.FMul %83, %85 : vector<4xf16>
            %87 = spirv.FAdd %81, %75 : vector<4xf16>
            %88 = spirv.FAdd %86, %76 : vector<4xf16>
            %89 = spirv.VectorShuffle [0 : i32, 1 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %90 = spirv.Bitcast %89 : vector<2xf32> to vector<4xf16>
            %91 = spirv.VectorShuffle [0 : i32, 1 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %92 = spirv.Bitcast %91 : vector<2xf32> to vector<4xf16>
            %93 = spirv.FMul %90, %92 : vector<4xf16>
            %94 = spirv.VectorShuffle [2 : i32, 3 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %95 = spirv.Bitcast %94 : vector<2xf32> to vector<4xf16>
            %96 = spirv.VectorShuffle [2 : i32, 3 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %97 = spirv.Bitcast %96 : vector<2xf32> to vector<4xf16>
            %98 = spirv.FMul %95, %97 : vector<4xf16>
            %99 = spirv.FAdd %93, %87 : vector<4xf16>
            %100 = spirv.FAdd %98, %88 : vector<4xf16>
            %101 = spirv.VectorShuffle [0 : i32, 1 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %102 = spirv.Bitcast %101 : vector<2xf32> to vector<4xf16>
            %103 = spirv.VectorShuffle [0 : i32, 1 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %104 = spirv.Bitcast %103 : vector<2xf32> to vector<4xf16>
            %105 = spirv.FMul %102, %104 : vector<4xf16>
            %106 = spirv.VectorShuffle [2 : i32, 3 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %107 = spirv.Bitcast %106 : vector<2xf32> to vector<4xf16>
            %108 = spirv.VectorShuffle [2 : i32, 3 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %109 = spirv.Bitcast %108 : vector<2xf32> to vector<4xf16>
            %110 = spirv.FMul %107, %109 : vector<4xf16>
            %111 = spirv.FAdd %105, %99 : vector<4xf16>
            %112 = spirv.FAdd %110, %100 : vector<4xf16>
            %113 = spirv.VectorShuffle [0 : i32, 1 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %114 = spirv.Bitcast %113 : vector<2xf32> to vector<4xf16>
            %115 = spirv.VectorShuffle [0 : i32, 1 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %116 = spirv.Bitcast %115 : vector<2xf32> to vector<4xf16>
            %117 = spirv.FMul %114, %116 : vector<4xf16>
            %118 = spirv.VectorShuffle [2 : i32, 3 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %119 = spirv.Bitcast %118 : vector<2xf32> to vector<4xf16>
            %120 = spirv.VectorShuffle [2 : i32, 3 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %121 = spirv.Bitcast %120 : vector<2xf32> to vector<4xf16>
            %122 = spirv.FMul %119, %121 : vector<4xf16>
            %123 = spirv.FAdd %117, %111 : vector<4xf16>
            %124 = spirv.FAdd %122, %112 : vector<4xf16>
            %125 = spirv.VectorShuffle [0 : i32, 1 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %126 = spirv.Bitcast %125 : vector<2xf32> to vector<4xf16>
            %127 = spirv.VectorShuffle [0 : i32, 1 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %128 = spirv.Bitcast %127 : vector<2xf32> to vector<4xf16>
            %129 = spirv.FMul %126, %128 : vector<4xf16>
            %130 = spirv.VectorShuffle [2 : i32, 3 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %131 = spirv.Bitcast %130 : vector<2xf32> to vector<4xf16>
            %132 = spirv.VectorShuffle [2 : i32, 3 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %133 = spirv.Bitcast %132 : vector<2xf32> to vector<4xf16>
            %134 = spirv.FMul %131, %133 : vector<4xf16>
            %135 = spirv.FAdd %129, %123 : vector<4xf16>
            %136 = spirv.FAdd %134, %124 : vector<4xf16>
            %137 = spirv.VectorShuffle [0 : i32, 1 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %138 = spirv.Bitcast %137 : vector<2xf32> to vector<4xf16>
            %139 = spirv.VectorShuffle [0 : i32, 1 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %140 = spirv.Bitcast %139 : vector<2xf32> to vector<4xf16>
            %141 = spirv.FMul %138, %140 : vector<4xf16>
            %142 = spirv.VectorShuffle [2 : i32, 3 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %143 = spirv.Bitcast %142 : vector<2xf32> to vector<4xf16>
            %144 = spirv.VectorShuffle [2 : i32, 3 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %145 = spirv.Bitcast %144 : vector<2xf32> to vector<4xf16>
            %146 = spirv.FMul %143, %145 : vector<4xf16>
            %147 = spirv.FAdd %141, %135 : vector<4xf16>
            %148 = spirv.FAdd %146, %136 : vector<4xf16>
            %149 = spirv.CompositeExtract %147[0 : i32] : vector<4xf16>
            %150 = spirv.CompositeExtract %147[1 : i32] : vector<4xf16>
            %151 = spirv.CompositeExtract %147[2 : i32] : vector<4xf16>
            %152 = spirv.CompositeExtract %147[3 : i32] : vector<4xf16>
            %153 = spirv.FAdd %149, %150 : f16
            %154 = spirv.FAdd %153, %151 : f16
            %155 = spirv.FAdd %154, %152 : f16
            %156 = spirv.CompositeExtract %148[0 : i32] : vector<4xf16>
            %157 = spirv.CompositeExtract %148[1 : i32] : vector<4xf16>
            %158 = spirv.CompositeExtract %148[2 : i32] : vector<4xf16>
            %159 = spirv.CompositeExtract %148[3 : i32] : vector<4xf16>
            %160 = spirv.FAdd %156, %157 : f16
            %161 = spirv.FAdd %160, %158 : f16
            %162 = spirv.FAdd %161, %159 : f16
            %163 = spirv.FAdd %155, %162 : f16
            %164 = spirv.GroupNonUniformFAdd "Subgroup" "Reduce" %163 : f16
            %165 = spirv.FAdd %164, %cst_f16 : f16
            %166 = spirv.IEqual %1, %cst0_i32 : i32
            spirv.mlir.selection {
              spirv.BranchConditional %166, ^bb1, ^bb2
            ^bb1:  // pred: ^bb0
              %167 = spirv.AccessChain %__resource_var_0_2__addr[%cst0_i32, %3] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>, i32, i32
              spirv.Store "StorageBuffer" %167, %165 : f16
              spirv.Branch ^bb2
            ^bb2:  // 2 preds: ^bb0, ^bb1
              spirv.mlir.merge
            }
            spirv.Return
          }
          spirv.EntryPoint "GLCompute" @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16, @__builtin__LocalInvocationId__, @__builtin__WorkgroupId__
          spirv.ExecutionMode @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 "LocalSize", 64, 1, 1
        }
      }
    }
  }
  func.func private @_main() {
    %c1_i32 = arith.constant 1 : i32
    %c553648144_i32 = arith.constant 553648144 : i32
    %c32000 = arith.constant 32000 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %device = hal.ex.shared_device : !hal.device
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
    %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_2, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
    %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
    %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_9, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    %3 = arith.select %value, %c0, %c-1 : index
    scf.index_switch %3 
    case 0 {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%1 : !hal.buffer)[%c0, %len], 
        %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
        %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
      ])
      hal.command_buffer.dispatch.symbol<%cmd_14 : !hal.command_buffer> target(@_main_dispatch_0::@vulkan_spirv_fb::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16) workgroups([%c32000, %c1, %c1])
      scf.yield
    }
    default {
    }
    hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
    %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
    %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_17, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::LinkExecutablesPass (iree-hal-link-executables) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<SPIRVSubgroupReduce>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.variant public @vulkan_spirv_fb target(#executable_target_vulkan_spirv_fb) {
      hal.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c32000 = arith.constant 32000 : index
        %c1 = arith.constant 1 : index
        hal.return %c32000, %c1, %c1 : index, index, index
      }
      builtin.module attributes {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>} {
        spirv.module Logical GLSL450 requires #spirv.vce<v1.3, [Shader, StorageBuffer16BitAccess, Float16, GroupNonUniformArithmetic], [SPV_KHR_storage_buffer_storage_class, SPV_KHR_16bit_storage]> {
          spirv.GlobalVariable @__builtin__WorkgroupId__ built_in("WorkgroupId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__builtin__LocalInvocationId__ built_in("LocalInvocationId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__resource_var_0_0_ bind(0, 0) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_1_ bind(0, 1) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_2_ bind(0, 2) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
          spirv.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() "None" {
            %cst64_i32 = spirv.Constant 64 : i32
            %cst128_i32 = spirv.Constant 128 : i32
            %cst192_i32 = spirv.Constant 192 : i32
            %cst256_i32 = spirv.Constant 256 : i32
            %cst320_i32 = spirv.Constant 320 : i32
            %cst384_i32 = spirv.Constant 384 : i32
            %cst448_i32 = spirv.Constant 448 : i32
            %cst512_i32 = spirv.Constant 512 : i32
            %cst_vec_4xf16 = spirv.Constant dense<0.000000e+00> : vector<4xf16>
            %cst0_i32 = spirv.Constant 0 : i32
            %cst_f16 = spirv.Constant 0.000000e+00 : f16
            %__builtin__LocalInvocationId___addr = spirv.mlir.addressof @__builtin__LocalInvocationId__ : !spirv.ptr<vector<3xi32>, Input>
            %0 = spirv.Load "Input" %__builtin__LocalInvocationId___addr : vector<3xi32>
            %1 = spirv.CompositeExtract %0[0 : i32] : vector<3xi32>
            %__resource_var_0_0__addr = spirv.mlir.addressof @__resource_var_0_0_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_1__addr = spirv.mlir.addressof @__resource_var_0_1_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_2__addr = spirv.mlir.addressof @__resource_var_0_2_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
            %__builtin__WorkgroupId___addr = spirv.mlir.addressof @__builtin__WorkgroupId__ : !spirv.ptr<vector<3xi32>, Input>
            %2 = spirv.Load "Input" %__builtin__WorkgroupId___addr : vector<3xi32>
            %3 = spirv.CompositeExtract %2[0 : i32] : vector<3xi32>
            %4 = spirv.IMul %3, %cst512_i32 : i32
            %5 = spirv.IAdd %1, %4 : i32
            %6 = spirv.IAdd %5, %cst448_i32 : i32
            %7 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %6] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %8 = spirv.Load "StorageBuffer" %7 : vector<4xf32>
            %9 = spirv.IAdd %1, %cst448_i32 : i32
            %10 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %9] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %11 = spirv.Load "StorageBuffer" %10 : vector<4xf32>
            %12 = spirv.IAdd %5, %cst384_i32 : i32
            %13 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %12] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %14 = spirv.Load "StorageBuffer" %13 : vector<4xf32>
            %15 = spirv.IAdd %1, %cst384_i32 : i32
            %16 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %15] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %17 = spirv.Load "StorageBuffer" %16 : vector<4xf32>
            %18 = spirv.IAdd %5, %cst320_i32 : i32
            %19 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %18] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %20 = spirv.Load "StorageBuffer" %19 : vector<4xf32>
            %21 = spirv.IAdd %1, %cst320_i32 : i32
            %22 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %21] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %23 = spirv.Load "StorageBuffer" %22 : vector<4xf32>
            %24 = spirv.IAdd %5, %cst256_i32 : i32
            %25 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %24] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %26 = spirv.Load "StorageBuffer" %25 : vector<4xf32>
            %27 = spirv.IAdd %1, %cst256_i32 : i32
            %28 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %27] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %29 = spirv.Load "StorageBuffer" %28 : vector<4xf32>
            %30 = spirv.IAdd %5, %cst192_i32 : i32
            %31 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %30] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %32 = spirv.Load "StorageBuffer" %31 : vector<4xf32>
            %33 = spirv.IAdd %1, %cst192_i32 : i32
            %34 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %33] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %35 = spirv.Load "StorageBuffer" %34 : vector<4xf32>
            %36 = spirv.IAdd %5, %cst128_i32 : i32
            %37 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %36] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %38 = spirv.Load "StorageBuffer" %37 : vector<4xf32>
            %39 = spirv.IAdd %1, %cst128_i32 : i32
            %40 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %39] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %41 = spirv.Load "StorageBuffer" %40 : vector<4xf32>
            %42 = spirv.IAdd %5, %cst64_i32 : i32
            %43 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %42] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %44 = spirv.Load "StorageBuffer" %43 : vector<4xf32>
            %45 = spirv.IAdd %1, %cst64_i32 : i32
            %46 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %45] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %47 = spirv.Load "StorageBuffer" %46 : vector<4xf32>
            %48 = spirv.IAdd %4, %1 : i32
            %49 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %48] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %50 = spirv.Load "StorageBuffer" %49 : vector<4xf32>
            %51 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %1] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %52 = spirv.Load "StorageBuffer" %51 : vector<4xf32>
            %53 = spirv.VectorShuffle [0 : i32, 1 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %54 = spirv.Bitcast %53 : vector<2xf32> to vector<4xf16>
            %55 = spirv.VectorShuffle [0 : i32, 1 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %56 = spirv.Bitcast %55 : vector<2xf32> to vector<4xf16>
            %57 = spirv.FMul %54, %56 : vector<4xf16>
            %58 = spirv.VectorShuffle [2 : i32, 3 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %59 = spirv.Bitcast %58 : vector<2xf32> to vector<4xf16>
            %60 = spirv.VectorShuffle [2 : i32, 3 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %61 = spirv.Bitcast %60 : vector<2xf32> to vector<4xf16>
            %62 = spirv.FMul %59, %61 : vector<4xf16>
            %63 = spirv.FAdd %57, %cst_vec_4xf16 : vector<4xf16>
            %64 = spirv.FAdd %62, %cst_vec_4xf16 : vector<4xf16>
            %65 = spirv.VectorShuffle [0 : i32, 1 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %66 = spirv.Bitcast %65 : vector<2xf32> to vector<4xf16>
            %67 = spirv.VectorShuffle [0 : i32, 1 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %68 = spirv.Bitcast %67 : vector<2xf32> to vector<4xf16>
            %69 = spirv.FMul %66, %68 : vector<4xf16>
            %70 = spirv.VectorShuffle [2 : i32, 3 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %71 = spirv.Bitcast %70 : vector<2xf32> to vector<4xf16>
            %72 = spirv.VectorShuffle [2 : i32, 3 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %73 = spirv.Bitcast %72 : vector<2xf32> to vector<4xf16>
            %74 = spirv.FMul %71, %73 : vector<4xf16>
            %75 = spirv.FAdd %69, %63 : vector<4xf16>
            %76 = spirv.FAdd %74, %64 : vector<4xf16>
            %77 = spirv.VectorShuffle [0 : i32, 1 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %78 = spirv.Bitcast %77 : vector<2xf32> to vector<4xf16>
            %79 = spirv.VectorShuffle [0 : i32, 1 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %80 = spirv.Bitcast %79 : vector<2xf32> to vector<4xf16>
            %81 = spirv.FMul %78, %80 : vector<4xf16>
            %82 = spirv.VectorShuffle [2 : i32, 3 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %83 = spirv.Bitcast %82 : vector<2xf32> to vector<4xf16>
            %84 = spirv.VectorShuffle [2 : i32, 3 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %85 = spirv.Bitcast %84 : vector<2xf32> to vector<4xf16>
            %86 = spirv.FMul %83, %85 : vector<4xf16>
            %87 = spirv.FAdd %81, %75 : vector<4xf16>
            %88 = spirv.FAdd %86, %76 : vector<4xf16>
            %89 = spirv.VectorShuffle [0 : i32, 1 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %90 = spirv.Bitcast %89 : vector<2xf32> to vector<4xf16>
            %91 = spirv.VectorShuffle [0 : i32, 1 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %92 = spirv.Bitcast %91 : vector<2xf32> to vector<4xf16>
            %93 = spirv.FMul %90, %92 : vector<4xf16>
            %94 = spirv.VectorShuffle [2 : i32, 3 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %95 = spirv.Bitcast %94 : vector<2xf32> to vector<4xf16>
            %96 = spirv.VectorShuffle [2 : i32, 3 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %97 = spirv.Bitcast %96 : vector<2xf32> to vector<4xf16>
            %98 = spirv.FMul %95, %97 : vector<4xf16>
            %99 = spirv.FAdd %93, %87 : vector<4xf16>
            %100 = spirv.FAdd %98, %88 : vector<4xf16>
            %101 = spirv.VectorShuffle [0 : i32, 1 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %102 = spirv.Bitcast %101 : vector<2xf32> to vector<4xf16>
            %103 = spirv.VectorShuffle [0 : i32, 1 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %104 = spirv.Bitcast %103 : vector<2xf32> to vector<4xf16>
            %105 = spirv.FMul %102, %104 : vector<4xf16>
            %106 = spirv.VectorShuffle [2 : i32, 3 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %107 = spirv.Bitcast %106 : vector<2xf32> to vector<4xf16>
            %108 = spirv.VectorShuffle [2 : i32, 3 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %109 = spirv.Bitcast %108 : vector<2xf32> to vector<4xf16>
            %110 = spirv.FMul %107, %109 : vector<4xf16>
            %111 = spirv.FAdd %105, %99 : vector<4xf16>
            %112 = spirv.FAdd %110, %100 : vector<4xf16>
            %113 = spirv.VectorShuffle [0 : i32, 1 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %114 = spirv.Bitcast %113 : vector<2xf32> to vector<4xf16>
            %115 = spirv.VectorShuffle [0 : i32, 1 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %116 = spirv.Bitcast %115 : vector<2xf32> to vector<4xf16>
            %117 = spirv.FMul %114, %116 : vector<4xf16>
            %118 = spirv.VectorShuffle [2 : i32, 3 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %119 = spirv.Bitcast %118 : vector<2xf32> to vector<4xf16>
            %120 = spirv.VectorShuffle [2 : i32, 3 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %121 = spirv.Bitcast %120 : vector<2xf32> to vector<4xf16>
            %122 = spirv.FMul %119, %121 : vector<4xf16>
            %123 = spirv.FAdd %117, %111 : vector<4xf16>
            %124 = spirv.FAdd %122, %112 : vector<4xf16>
            %125 = spirv.VectorShuffle [0 : i32, 1 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %126 = spirv.Bitcast %125 : vector<2xf32> to vector<4xf16>
            %127 = spirv.VectorShuffle [0 : i32, 1 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %128 = spirv.Bitcast %127 : vector<2xf32> to vector<4xf16>
            %129 = spirv.FMul %126, %128 : vector<4xf16>
            %130 = spirv.VectorShuffle [2 : i32, 3 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %131 = spirv.Bitcast %130 : vector<2xf32> to vector<4xf16>
            %132 = spirv.VectorShuffle [2 : i32, 3 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %133 = spirv.Bitcast %132 : vector<2xf32> to vector<4xf16>
            %134 = spirv.FMul %131, %133 : vector<4xf16>
            %135 = spirv.FAdd %129, %123 : vector<4xf16>
            %136 = spirv.FAdd %134, %124 : vector<4xf16>
            %137 = spirv.VectorShuffle [0 : i32, 1 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %138 = spirv.Bitcast %137 : vector<2xf32> to vector<4xf16>
            %139 = spirv.VectorShuffle [0 : i32, 1 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %140 = spirv.Bitcast %139 : vector<2xf32> to vector<4xf16>
            %141 = spirv.FMul %138, %140 : vector<4xf16>
            %142 = spirv.VectorShuffle [2 : i32, 3 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %143 = spirv.Bitcast %142 : vector<2xf32> to vector<4xf16>
            %144 = spirv.VectorShuffle [2 : i32, 3 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %145 = spirv.Bitcast %144 : vector<2xf32> to vector<4xf16>
            %146 = spirv.FMul %143, %145 : vector<4xf16>
            %147 = spirv.FAdd %141, %135 : vector<4xf16>
            %148 = spirv.FAdd %146, %136 : vector<4xf16>
            %149 = spirv.CompositeExtract %147[0 : i32] : vector<4xf16>
            %150 = spirv.CompositeExtract %147[1 : i32] : vector<4xf16>
            %151 = spirv.CompositeExtract %147[2 : i32] : vector<4xf16>
            %152 = spirv.CompositeExtract %147[3 : i32] : vector<4xf16>
            %153 = spirv.FAdd %149, %150 : f16
            %154 = spirv.FAdd %153, %151 : f16
            %155 = spirv.FAdd %154, %152 : f16
            %156 = spirv.CompositeExtract %148[0 : i32] : vector<4xf16>
            %157 = spirv.CompositeExtract %148[1 : i32] : vector<4xf16>
            %158 = spirv.CompositeExtract %148[2 : i32] : vector<4xf16>
            %159 = spirv.CompositeExtract %148[3 : i32] : vector<4xf16>
            %160 = spirv.FAdd %156, %157 : f16
            %161 = spirv.FAdd %160, %158 : f16
            %162 = spirv.FAdd %161, %159 : f16
            %163 = spirv.FAdd %155, %162 : f16
            %164 = spirv.GroupNonUniformFAdd "Subgroup" "Reduce" %163 : f16
            %165 = spirv.FAdd %164, %cst_f16 : f16
            %166 = spirv.IEqual %1, %cst0_i32 : i32
            spirv.mlir.selection {
              spirv.BranchConditional %166, ^bb1, ^bb2
            ^bb1:  // pred: ^bb0
              %167 = spirv.AccessChain %__resource_var_0_2__addr[%cst0_i32, %3] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>, i32, i32
              spirv.Store "StorageBuffer" %167, %165 : f16
              spirv.Branch ^bb2
            ^bb2:  // 2 preds: ^bb0, ^bb1
              spirv.mlir.merge
            }
            spirv.Return
          }
          spirv.EntryPoint "GLCompute" @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16, @__builtin__LocalInvocationId__, @__builtin__WorkgroupId__
          spirv.ExecutionMode @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 "LocalSize", 64, 1, 1
        }
      }
    }
  }
  func.func private @_main() {
    %c1_i32 = arith.constant 1 : i32
    %c553648144_i32 = arith.constant 553648144 : i32
    %c32000 = arith.constant 32000 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %device = hal.ex.shared_device : !hal.device
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
    %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_2, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
    %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
    %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_9, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    %3 = arith.select %value, %c0, %c-1 : index
    scf.index_switch %3 
    case 0 {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%1 : !hal.buffer)[%c0, %len], 
        %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
        %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
      ])
      hal.command_buffer.dispatch.symbol<%cmd_14 : !hal.command_buffer> target(@_main_dispatch_0::@vulkan_spirv_fb::@_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16) workgroups([%c32000, %c1, %c1])
      scf.yield
    }
    default {
    }
    hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
    %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
    %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_17, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::ResolveExportOrdinalsPass (iree-hal-resolve-export-ordinals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<SPIRVSubgroupReduce>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.variant public @vulkan_spirv_fb target(#executable_target_vulkan_spirv_fb) {
      hal.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c32000 = arith.constant 32000 : index
        %c1 = arith.constant 1 : index
        hal.return %c32000, %c1, %c1 : index, index, index
      }
      builtin.module attributes {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>} {
        spirv.module Logical GLSL450 requires #spirv.vce<v1.3, [Shader, StorageBuffer16BitAccess, Float16, GroupNonUniformArithmetic], [SPV_KHR_storage_buffer_storage_class, SPV_KHR_16bit_storage]> {
          spirv.GlobalVariable @__builtin__WorkgroupId__ built_in("WorkgroupId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__builtin__LocalInvocationId__ built_in("LocalInvocationId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__resource_var_0_0_ bind(0, 0) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_1_ bind(0, 1) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_2_ bind(0, 2) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
          spirv.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() "None" {
            %cst64_i32 = spirv.Constant 64 : i32
            %cst128_i32 = spirv.Constant 128 : i32
            %cst192_i32 = spirv.Constant 192 : i32
            %cst256_i32 = spirv.Constant 256 : i32
            %cst320_i32 = spirv.Constant 320 : i32
            %cst384_i32 = spirv.Constant 384 : i32
            %cst448_i32 = spirv.Constant 448 : i32
            %cst512_i32 = spirv.Constant 512 : i32
            %cst_vec_4xf16 = spirv.Constant dense<0.000000e+00> : vector<4xf16>
            %cst0_i32 = spirv.Constant 0 : i32
            %cst_f16 = spirv.Constant 0.000000e+00 : f16
            %__builtin__LocalInvocationId___addr = spirv.mlir.addressof @__builtin__LocalInvocationId__ : !spirv.ptr<vector<3xi32>, Input>
            %0 = spirv.Load "Input" %__builtin__LocalInvocationId___addr : vector<3xi32>
            %1 = spirv.CompositeExtract %0[0 : i32] : vector<3xi32>
            %__resource_var_0_0__addr = spirv.mlir.addressof @__resource_var_0_0_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_1__addr = spirv.mlir.addressof @__resource_var_0_1_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_2__addr = spirv.mlir.addressof @__resource_var_0_2_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
            %__builtin__WorkgroupId___addr = spirv.mlir.addressof @__builtin__WorkgroupId__ : !spirv.ptr<vector<3xi32>, Input>
            %2 = spirv.Load "Input" %__builtin__WorkgroupId___addr : vector<3xi32>
            %3 = spirv.CompositeExtract %2[0 : i32] : vector<3xi32>
            %4 = spirv.IMul %3, %cst512_i32 : i32
            %5 = spirv.IAdd %1, %4 : i32
            %6 = spirv.IAdd %5, %cst448_i32 : i32
            %7 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %6] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %8 = spirv.Load "StorageBuffer" %7 : vector<4xf32>
            %9 = spirv.IAdd %1, %cst448_i32 : i32
            %10 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %9] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %11 = spirv.Load "StorageBuffer" %10 : vector<4xf32>
            %12 = spirv.IAdd %5, %cst384_i32 : i32
            %13 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %12] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %14 = spirv.Load "StorageBuffer" %13 : vector<4xf32>
            %15 = spirv.IAdd %1, %cst384_i32 : i32
            %16 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %15] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %17 = spirv.Load "StorageBuffer" %16 : vector<4xf32>
            %18 = spirv.IAdd %5, %cst320_i32 : i32
            %19 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %18] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %20 = spirv.Load "StorageBuffer" %19 : vector<4xf32>
            %21 = spirv.IAdd %1, %cst320_i32 : i32
            %22 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %21] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %23 = spirv.Load "StorageBuffer" %22 : vector<4xf32>
            %24 = spirv.IAdd %5, %cst256_i32 : i32
            %25 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %24] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %26 = spirv.Load "StorageBuffer" %25 : vector<4xf32>
            %27 = spirv.IAdd %1, %cst256_i32 : i32
            %28 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %27] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %29 = spirv.Load "StorageBuffer" %28 : vector<4xf32>
            %30 = spirv.IAdd %5, %cst192_i32 : i32
            %31 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %30] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %32 = spirv.Load "StorageBuffer" %31 : vector<4xf32>
            %33 = spirv.IAdd %1, %cst192_i32 : i32
            %34 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %33] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %35 = spirv.Load "StorageBuffer" %34 : vector<4xf32>
            %36 = spirv.IAdd %5, %cst128_i32 : i32
            %37 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %36] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %38 = spirv.Load "StorageBuffer" %37 : vector<4xf32>
            %39 = spirv.IAdd %1, %cst128_i32 : i32
            %40 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %39] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %41 = spirv.Load "StorageBuffer" %40 : vector<4xf32>
            %42 = spirv.IAdd %5, %cst64_i32 : i32
            %43 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %42] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %44 = spirv.Load "StorageBuffer" %43 : vector<4xf32>
            %45 = spirv.IAdd %1, %cst64_i32 : i32
            %46 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %45] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %47 = spirv.Load "StorageBuffer" %46 : vector<4xf32>
            %48 = spirv.IAdd %4, %1 : i32
            %49 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %48] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %50 = spirv.Load "StorageBuffer" %49 : vector<4xf32>
            %51 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %1] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %52 = spirv.Load "StorageBuffer" %51 : vector<4xf32>
            %53 = spirv.VectorShuffle [0 : i32, 1 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %54 = spirv.Bitcast %53 : vector<2xf32> to vector<4xf16>
            %55 = spirv.VectorShuffle [0 : i32, 1 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %56 = spirv.Bitcast %55 : vector<2xf32> to vector<4xf16>
            %57 = spirv.FMul %54, %56 : vector<4xf16>
            %58 = spirv.VectorShuffle [2 : i32, 3 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %59 = spirv.Bitcast %58 : vector<2xf32> to vector<4xf16>
            %60 = spirv.VectorShuffle [2 : i32, 3 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %61 = spirv.Bitcast %60 : vector<2xf32> to vector<4xf16>
            %62 = spirv.FMul %59, %61 : vector<4xf16>
            %63 = spirv.FAdd %57, %cst_vec_4xf16 : vector<4xf16>
            %64 = spirv.FAdd %62, %cst_vec_4xf16 : vector<4xf16>
            %65 = spirv.VectorShuffle [0 : i32, 1 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %66 = spirv.Bitcast %65 : vector<2xf32> to vector<4xf16>
            %67 = spirv.VectorShuffle [0 : i32, 1 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %68 = spirv.Bitcast %67 : vector<2xf32> to vector<4xf16>
            %69 = spirv.FMul %66, %68 : vector<4xf16>
            %70 = spirv.VectorShuffle [2 : i32, 3 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %71 = spirv.Bitcast %70 : vector<2xf32> to vector<4xf16>
            %72 = spirv.VectorShuffle [2 : i32, 3 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %73 = spirv.Bitcast %72 : vector<2xf32> to vector<4xf16>
            %74 = spirv.FMul %71, %73 : vector<4xf16>
            %75 = spirv.FAdd %69, %63 : vector<4xf16>
            %76 = spirv.FAdd %74, %64 : vector<4xf16>
            %77 = spirv.VectorShuffle [0 : i32, 1 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %78 = spirv.Bitcast %77 : vector<2xf32> to vector<4xf16>
            %79 = spirv.VectorShuffle [0 : i32, 1 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %80 = spirv.Bitcast %79 : vector<2xf32> to vector<4xf16>
            %81 = spirv.FMul %78, %80 : vector<4xf16>
            %82 = spirv.VectorShuffle [2 : i32, 3 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %83 = spirv.Bitcast %82 : vector<2xf32> to vector<4xf16>
            %84 = spirv.VectorShuffle [2 : i32, 3 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %85 = spirv.Bitcast %84 : vector<2xf32> to vector<4xf16>
            %86 = spirv.FMul %83, %85 : vector<4xf16>
            %87 = spirv.FAdd %81, %75 : vector<4xf16>
            %88 = spirv.FAdd %86, %76 : vector<4xf16>
            %89 = spirv.VectorShuffle [0 : i32, 1 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %90 = spirv.Bitcast %89 : vector<2xf32> to vector<4xf16>
            %91 = spirv.VectorShuffle [0 : i32, 1 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %92 = spirv.Bitcast %91 : vector<2xf32> to vector<4xf16>
            %93 = spirv.FMul %90, %92 : vector<4xf16>
            %94 = spirv.VectorShuffle [2 : i32, 3 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %95 = spirv.Bitcast %94 : vector<2xf32> to vector<4xf16>
            %96 = spirv.VectorShuffle [2 : i32, 3 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %97 = spirv.Bitcast %96 : vector<2xf32> to vector<4xf16>
            %98 = spirv.FMul %95, %97 : vector<4xf16>
            %99 = spirv.FAdd %93, %87 : vector<4xf16>
            %100 = spirv.FAdd %98, %88 : vector<4xf16>
            %101 = spirv.VectorShuffle [0 : i32, 1 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %102 = spirv.Bitcast %101 : vector<2xf32> to vector<4xf16>
            %103 = spirv.VectorShuffle [0 : i32, 1 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %104 = spirv.Bitcast %103 : vector<2xf32> to vector<4xf16>
            %105 = spirv.FMul %102, %104 : vector<4xf16>
            %106 = spirv.VectorShuffle [2 : i32, 3 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %107 = spirv.Bitcast %106 : vector<2xf32> to vector<4xf16>
            %108 = spirv.VectorShuffle [2 : i32, 3 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %109 = spirv.Bitcast %108 : vector<2xf32> to vector<4xf16>
            %110 = spirv.FMul %107, %109 : vector<4xf16>
            %111 = spirv.FAdd %105, %99 : vector<4xf16>
            %112 = spirv.FAdd %110, %100 : vector<4xf16>
            %113 = spirv.VectorShuffle [0 : i32, 1 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %114 = spirv.Bitcast %113 : vector<2xf32> to vector<4xf16>
            %115 = spirv.VectorShuffle [0 : i32, 1 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %116 = spirv.Bitcast %115 : vector<2xf32> to vector<4xf16>
            %117 = spirv.FMul %114, %116 : vector<4xf16>
            %118 = spirv.VectorShuffle [2 : i32, 3 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %119 = spirv.Bitcast %118 : vector<2xf32> to vector<4xf16>
            %120 = spirv.VectorShuffle [2 : i32, 3 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %121 = spirv.Bitcast %120 : vector<2xf32> to vector<4xf16>
            %122 = spirv.FMul %119, %121 : vector<4xf16>
            %123 = spirv.FAdd %117, %111 : vector<4xf16>
            %124 = spirv.FAdd %122, %112 : vector<4xf16>
            %125 = spirv.VectorShuffle [0 : i32, 1 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %126 = spirv.Bitcast %125 : vector<2xf32> to vector<4xf16>
            %127 = spirv.VectorShuffle [0 : i32, 1 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %128 = spirv.Bitcast %127 : vector<2xf32> to vector<4xf16>
            %129 = spirv.FMul %126, %128 : vector<4xf16>
            %130 = spirv.VectorShuffle [2 : i32, 3 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %131 = spirv.Bitcast %130 : vector<2xf32> to vector<4xf16>
            %132 = spirv.VectorShuffle [2 : i32, 3 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %133 = spirv.Bitcast %132 : vector<2xf32> to vector<4xf16>
            %134 = spirv.FMul %131, %133 : vector<4xf16>
            %135 = spirv.FAdd %129, %123 : vector<4xf16>
            %136 = spirv.FAdd %134, %124 : vector<4xf16>
            %137 = spirv.VectorShuffle [0 : i32, 1 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %138 = spirv.Bitcast %137 : vector<2xf32> to vector<4xf16>
            %139 = spirv.VectorShuffle [0 : i32, 1 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %140 = spirv.Bitcast %139 : vector<2xf32> to vector<4xf16>
            %141 = spirv.FMul %138, %140 : vector<4xf16>
            %142 = spirv.VectorShuffle [2 : i32, 3 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %143 = spirv.Bitcast %142 : vector<2xf32> to vector<4xf16>
            %144 = spirv.VectorShuffle [2 : i32, 3 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %145 = spirv.Bitcast %144 : vector<2xf32> to vector<4xf16>
            %146 = spirv.FMul %143, %145 : vector<4xf16>
            %147 = spirv.FAdd %141, %135 : vector<4xf16>
            %148 = spirv.FAdd %146, %136 : vector<4xf16>
            %149 = spirv.CompositeExtract %147[0 : i32] : vector<4xf16>
            %150 = spirv.CompositeExtract %147[1 : i32] : vector<4xf16>
            %151 = spirv.CompositeExtract %147[2 : i32] : vector<4xf16>
            %152 = spirv.CompositeExtract %147[3 : i32] : vector<4xf16>
            %153 = spirv.FAdd %149, %150 : f16
            %154 = spirv.FAdd %153, %151 : f16
            %155 = spirv.FAdd %154, %152 : f16
            %156 = spirv.CompositeExtract %148[0 : i32] : vector<4xf16>
            %157 = spirv.CompositeExtract %148[1 : i32] : vector<4xf16>
            %158 = spirv.CompositeExtract %148[2 : i32] : vector<4xf16>
            %159 = spirv.CompositeExtract %148[3 : i32] : vector<4xf16>
            %160 = spirv.FAdd %156, %157 : f16
            %161 = spirv.FAdd %160, %158 : f16
            %162 = spirv.FAdd %161, %159 : f16
            %163 = spirv.FAdd %155, %162 : f16
            %164 = spirv.GroupNonUniformFAdd "Subgroup" "Reduce" %163 : f16
            %165 = spirv.FAdd %164, %cst_f16 : f16
            %166 = spirv.IEqual %1, %cst0_i32 : i32
            spirv.mlir.selection {
              spirv.BranchConditional %166, ^bb1, ^bb2
            ^bb1:  // pred: ^bb0
              %167 = spirv.AccessChain %__resource_var_0_2__addr[%cst0_i32, %3] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>, i32, i32
              spirv.Store "StorageBuffer" %167, %165 : f16
              spirv.Branch ^bb2
            ^bb2:  // 2 preds: ^bb0, ^bb1
              spirv.mlir.merge
            }
            spirv.Return
          }
          spirv.EntryPoint "GLCompute" @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16, @__builtin__LocalInvocationId__, @__builtin__WorkgroupId__
          spirv.ExecutionMode @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 "LocalSize", 64, 1, 1
        }
      }
    }
  }
  func.func private @_main() {
    %c1_i32 = arith.constant 1 : i32
    %c553648144_i32 = arith.constant 553648144 : i32
    %c32000 = arith.constant 32000 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %device = hal.ex.shared_device : !hal.device
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
    %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_2, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
    %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
    %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_9, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    %3 = arith.select %value, %c0, %c-1 : index
    scf.index_switch %3 
    case 0 {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%1 : !hal.buffer)[%c0, %len], 
        %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
        %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
      ])
      %4 = hal.command_buffer.device<%cmd_14 : !hal.command_buffer> : !hal.device
      %exe = hal.executable.lookup device(%4 : !hal.device) executable(@_main_dispatch_0) : !hal.executable
      hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%exe : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
      scf.yield
    }
    default {
    }
    hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
    %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
    %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_17, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::MaterializeResourceCachesPass (iree-hal-materialize-resource-caches) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<SPIRVSubgroupReduce>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.initializer.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.initializer.return
  }
  util.global private @_executable__main_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %0 = arith.select %value, %c0, %c-1 : index
    %1 = scf.index_switch %0 -> !hal.executable 
    case 0 {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      %exe = hal.executable.create device(%device : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
      scf.yield %exe : !hal.executable
    }
    default {
      %2 = util.null : !hal.executable
      scf.yield %2 : !hal.executable
    }
    util.global.store %1, @_executable__main_dispatch_0 : !hal.executable
    util.initializer.return
  }
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.variant public @vulkan_spirv_fb target(#executable_target_vulkan_spirv_fb) {
      hal.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c32000 = arith.constant 32000 : index
        %c1 = arith.constant 1 : index
        hal.return %c32000, %c1, %c1 : index, index, index
      }
      builtin.module attributes {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>} {
        spirv.module Logical GLSL450 requires #spirv.vce<v1.3, [Shader, StorageBuffer16BitAccess, Float16, GroupNonUniformArithmetic], [SPV_KHR_storage_buffer_storage_class, SPV_KHR_16bit_storage]> {
          spirv.GlobalVariable @__builtin__WorkgroupId__ built_in("WorkgroupId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__builtin__LocalInvocationId__ built_in("LocalInvocationId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__resource_var_0_0_ bind(0, 0) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_1_ bind(0, 1) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_2_ bind(0, 2) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
          spirv.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() "None" {
            %cst64_i32 = spirv.Constant 64 : i32
            %cst128_i32 = spirv.Constant 128 : i32
            %cst192_i32 = spirv.Constant 192 : i32
            %cst256_i32 = spirv.Constant 256 : i32
            %cst320_i32 = spirv.Constant 320 : i32
            %cst384_i32 = spirv.Constant 384 : i32
            %cst448_i32 = spirv.Constant 448 : i32
            %cst512_i32 = spirv.Constant 512 : i32
            %cst_vec_4xf16 = spirv.Constant dense<0.000000e+00> : vector<4xf16>
            %cst0_i32 = spirv.Constant 0 : i32
            %cst_f16 = spirv.Constant 0.000000e+00 : f16
            %__builtin__LocalInvocationId___addr = spirv.mlir.addressof @__builtin__LocalInvocationId__ : !spirv.ptr<vector<3xi32>, Input>
            %0 = spirv.Load "Input" %__builtin__LocalInvocationId___addr : vector<3xi32>
            %1 = spirv.CompositeExtract %0[0 : i32] : vector<3xi32>
            %__resource_var_0_0__addr = spirv.mlir.addressof @__resource_var_0_0_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_1__addr = spirv.mlir.addressof @__resource_var_0_1_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_2__addr = spirv.mlir.addressof @__resource_var_0_2_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
            %__builtin__WorkgroupId___addr = spirv.mlir.addressof @__builtin__WorkgroupId__ : !spirv.ptr<vector<3xi32>, Input>
            %2 = spirv.Load "Input" %__builtin__WorkgroupId___addr : vector<3xi32>
            %3 = spirv.CompositeExtract %2[0 : i32] : vector<3xi32>
            %4 = spirv.IMul %3, %cst512_i32 : i32
            %5 = spirv.IAdd %1, %4 : i32
            %6 = spirv.IAdd %5, %cst448_i32 : i32
            %7 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %6] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %8 = spirv.Load "StorageBuffer" %7 : vector<4xf32>
            %9 = spirv.IAdd %1, %cst448_i32 : i32
            %10 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %9] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %11 = spirv.Load "StorageBuffer" %10 : vector<4xf32>
            %12 = spirv.IAdd %5, %cst384_i32 : i32
            %13 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %12] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %14 = spirv.Load "StorageBuffer" %13 : vector<4xf32>
            %15 = spirv.IAdd %1, %cst384_i32 : i32
            %16 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %15] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %17 = spirv.Load "StorageBuffer" %16 : vector<4xf32>
            %18 = spirv.IAdd %5, %cst320_i32 : i32
            %19 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %18] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %20 = spirv.Load "StorageBuffer" %19 : vector<4xf32>
            %21 = spirv.IAdd %1, %cst320_i32 : i32
            %22 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %21] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %23 = spirv.Load "StorageBuffer" %22 : vector<4xf32>
            %24 = spirv.IAdd %5, %cst256_i32 : i32
            %25 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %24] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %26 = spirv.Load "StorageBuffer" %25 : vector<4xf32>
            %27 = spirv.IAdd %1, %cst256_i32 : i32
            %28 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %27] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %29 = spirv.Load "StorageBuffer" %28 : vector<4xf32>
            %30 = spirv.IAdd %5, %cst192_i32 : i32
            %31 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %30] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %32 = spirv.Load "StorageBuffer" %31 : vector<4xf32>
            %33 = spirv.IAdd %1, %cst192_i32 : i32
            %34 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %33] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %35 = spirv.Load "StorageBuffer" %34 : vector<4xf32>
            %36 = spirv.IAdd %5, %cst128_i32 : i32
            %37 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %36] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %38 = spirv.Load "StorageBuffer" %37 : vector<4xf32>
            %39 = spirv.IAdd %1, %cst128_i32 : i32
            %40 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %39] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %41 = spirv.Load "StorageBuffer" %40 : vector<4xf32>
            %42 = spirv.IAdd %5, %cst64_i32 : i32
            %43 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %42] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %44 = spirv.Load "StorageBuffer" %43 : vector<4xf32>
            %45 = spirv.IAdd %1, %cst64_i32 : i32
            %46 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %45] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %47 = spirv.Load "StorageBuffer" %46 : vector<4xf32>
            %48 = spirv.IAdd %4, %1 : i32
            %49 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %48] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %50 = spirv.Load "StorageBuffer" %49 : vector<4xf32>
            %51 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %1] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %52 = spirv.Load "StorageBuffer" %51 : vector<4xf32>
            %53 = spirv.VectorShuffle [0 : i32, 1 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %54 = spirv.Bitcast %53 : vector<2xf32> to vector<4xf16>
            %55 = spirv.VectorShuffle [0 : i32, 1 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %56 = spirv.Bitcast %55 : vector<2xf32> to vector<4xf16>
            %57 = spirv.FMul %54, %56 : vector<4xf16>
            %58 = spirv.VectorShuffle [2 : i32, 3 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %59 = spirv.Bitcast %58 : vector<2xf32> to vector<4xf16>
            %60 = spirv.VectorShuffle [2 : i32, 3 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %61 = spirv.Bitcast %60 : vector<2xf32> to vector<4xf16>
            %62 = spirv.FMul %59, %61 : vector<4xf16>
            %63 = spirv.FAdd %57, %cst_vec_4xf16 : vector<4xf16>
            %64 = spirv.FAdd %62, %cst_vec_4xf16 : vector<4xf16>
            %65 = spirv.VectorShuffle [0 : i32, 1 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %66 = spirv.Bitcast %65 : vector<2xf32> to vector<4xf16>
            %67 = spirv.VectorShuffle [0 : i32, 1 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %68 = spirv.Bitcast %67 : vector<2xf32> to vector<4xf16>
            %69 = spirv.FMul %66, %68 : vector<4xf16>
            %70 = spirv.VectorShuffle [2 : i32, 3 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %71 = spirv.Bitcast %70 : vector<2xf32> to vector<4xf16>
            %72 = spirv.VectorShuffle [2 : i32, 3 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %73 = spirv.Bitcast %72 : vector<2xf32> to vector<4xf16>
            %74 = spirv.FMul %71, %73 : vector<4xf16>
            %75 = spirv.FAdd %69, %63 : vector<4xf16>
            %76 = spirv.FAdd %74, %64 : vector<4xf16>
            %77 = spirv.VectorShuffle [0 : i32, 1 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %78 = spirv.Bitcast %77 : vector<2xf32> to vector<4xf16>
            %79 = spirv.VectorShuffle [0 : i32, 1 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %80 = spirv.Bitcast %79 : vector<2xf32> to vector<4xf16>
            %81 = spirv.FMul %78, %80 : vector<4xf16>
            %82 = spirv.VectorShuffle [2 : i32, 3 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %83 = spirv.Bitcast %82 : vector<2xf32> to vector<4xf16>
            %84 = spirv.VectorShuffle [2 : i32, 3 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %85 = spirv.Bitcast %84 : vector<2xf32> to vector<4xf16>
            %86 = spirv.FMul %83, %85 : vector<4xf16>
            %87 = spirv.FAdd %81, %75 : vector<4xf16>
            %88 = spirv.FAdd %86, %76 : vector<4xf16>
            %89 = spirv.VectorShuffle [0 : i32, 1 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %90 = spirv.Bitcast %89 : vector<2xf32> to vector<4xf16>
            %91 = spirv.VectorShuffle [0 : i32, 1 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %92 = spirv.Bitcast %91 : vector<2xf32> to vector<4xf16>
            %93 = spirv.FMul %90, %92 : vector<4xf16>
            %94 = spirv.VectorShuffle [2 : i32, 3 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %95 = spirv.Bitcast %94 : vector<2xf32> to vector<4xf16>
            %96 = spirv.VectorShuffle [2 : i32, 3 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %97 = spirv.Bitcast %96 : vector<2xf32> to vector<4xf16>
            %98 = spirv.FMul %95, %97 : vector<4xf16>
            %99 = spirv.FAdd %93, %87 : vector<4xf16>
            %100 = spirv.FAdd %98, %88 : vector<4xf16>
            %101 = spirv.VectorShuffle [0 : i32, 1 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %102 = spirv.Bitcast %101 : vector<2xf32> to vector<4xf16>
            %103 = spirv.VectorShuffle [0 : i32, 1 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %104 = spirv.Bitcast %103 : vector<2xf32> to vector<4xf16>
            %105 = spirv.FMul %102, %104 : vector<4xf16>
            %106 = spirv.VectorShuffle [2 : i32, 3 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %107 = spirv.Bitcast %106 : vector<2xf32> to vector<4xf16>
            %108 = spirv.VectorShuffle [2 : i32, 3 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %109 = spirv.Bitcast %108 : vector<2xf32> to vector<4xf16>
            %110 = spirv.FMul %107, %109 : vector<4xf16>
            %111 = spirv.FAdd %105, %99 : vector<4xf16>
            %112 = spirv.FAdd %110, %100 : vector<4xf16>
            %113 = spirv.VectorShuffle [0 : i32, 1 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %114 = spirv.Bitcast %113 : vector<2xf32> to vector<4xf16>
            %115 = spirv.VectorShuffle [0 : i32, 1 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %116 = spirv.Bitcast %115 : vector<2xf32> to vector<4xf16>
            %117 = spirv.FMul %114, %116 : vector<4xf16>
            %118 = spirv.VectorShuffle [2 : i32, 3 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %119 = spirv.Bitcast %118 : vector<2xf32> to vector<4xf16>
            %120 = spirv.VectorShuffle [2 : i32, 3 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %121 = spirv.Bitcast %120 : vector<2xf32> to vector<4xf16>
            %122 = spirv.FMul %119, %121 : vector<4xf16>
            %123 = spirv.FAdd %117, %111 : vector<4xf16>
            %124 = spirv.FAdd %122, %112 : vector<4xf16>
            %125 = spirv.VectorShuffle [0 : i32, 1 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %126 = spirv.Bitcast %125 : vector<2xf32> to vector<4xf16>
            %127 = spirv.VectorShuffle [0 : i32, 1 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %128 = spirv.Bitcast %127 : vector<2xf32> to vector<4xf16>
            %129 = spirv.FMul %126, %128 : vector<4xf16>
            %130 = spirv.VectorShuffle [2 : i32, 3 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %131 = spirv.Bitcast %130 : vector<2xf32> to vector<4xf16>
            %132 = spirv.VectorShuffle [2 : i32, 3 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %133 = spirv.Bitcast %132 : vector<2xf32> to vector<4xf16>
            %134 = spirv.FMul %131, %133 : vector<4xf16>
            %135 = spirv.FAdd %129, %123 : vector<4xf16>
            %136 = spirv.FAdd %134, %124 : vector<4xf16>
            %137 = spirv.VectorShuffle [0 : i32, 1 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %138 = spirv.Bitcast %137 : vector<2xf32> to vector<4xf16>
            %139 = spirv.VectorShuffle [0 : i32, 1 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %140 = spirv.Bitcast %139 : vector<2xf32> to vector<4xf16>
            %141 = spirv.FMul %138, %140 : vector<4xf16>
            %142 = spirv.VectorShuffle [2 : i32, 3 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %143 = spirv.Bitcast %142 : vector<2xf32> to vector<4xf16>
            %144 = spirv.VectorShuffle [2 : i32, 3 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %145 = spirv.Bitcast %144 : vector<2xf32> to vector<4xf16>
            %146 = spirv.FMul %143, %145 : vector<4xf16>
            %147 = spirv.FAdd %141, %135 : vector<4xf16>
            %148 = spirv.FAdd %146, %136 : vector<4xf16>
            %149 = spirv.CompositeExtract %147[0 : i32] : vector<4xf16>
            %150 = spirv.CompositeExtract %147[1 : i32] : vector<4xf16>
            %151 = spirv.CompositeExtract %147[2 : i32] : vector<4xf16>
            %152 = spirv.CompositeExtract %147[3 : i32] : vector<4xf16>
            %153 = spirv.FAdd %149, %150 : f16
            %154 = spirv.FAdd %153, %151 : f16
            %155 = spirv.FAdd %154, %152 : f16
            %156 = spirv.CompositeExtract %148[0 : i32] : vector<4xf16>
            %157 = spirv.CompositeExtract %148[1 : i32] : vector<4xf16>
            %158 = spirv.CompositeExtract %148[2 : i32] : vector<4xf16>
            %159 = spirv.CompositeExtract %148[3 : i32] : vector<4xf16>
            %160 = spirv.FAdd %156, %157 : f16
            %161 = spirv.FAdd %160, %158 : f16
            %162 = spirv.FAdd %161, %159 : f16
            %163 = spirv.FAdd %155, %162 : f16
            %164 = spirv.GroupNonUniformFAdd "Subgroup" "Reduce" %163 : f16
            %165 = spirv.FAdd %164, %cst_f16 : f16
            %166 = spirv.IEqual %1, %cst0_i32 : i32
            spirv.mlir.selection {
              spirv.BranchConditional %166, ^bb1, ^bb2
            ^bb1:  // pred: ^bb0
              %167 = spirv.AccessChain %__resource_var_0_2__addr[%cst0_i32, %3] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>, i32, i32
              spirv.Store "StorageBuffer" %167, %165 : f16
              spirv.Branch ^bb2
            ^bb2:  // 2 preds: ^bb0, ^bb1
              spirv.mlir.merge
            }
            spirv.Return
          }
          spirv.EntryPoint "GLCompute" @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16, @__builtin__LocalInvocationId__, @__builtin__WorkgroupId__
          spirv.ExecutionMode @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 "LocalSize", 64, 1, 1
        }
      }
    }
  }
  func.func private @_main() {
    %c1_i32 = arith.constant 1 : i32
    %c553648144_i32 = arith.constant 553648144 : i32
    %c32000 = arith.constant 32000 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %device = hal.ex.shared_device : !hal.device
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
    %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_2, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
    %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
    %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_9, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    %3 = arith.select %value, %c0, %c-1 : index
    scf.index_switch %3 
    case 0 {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%1 : !hal.buffer)[%c0, %len], 
        %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
        %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
      ])
      %4 = hal.command_buffer.device<%cmd_14 : !hal.command_buffer> : !hal.device
      %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
      hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
      scf.yield
    }
    default {
    }
    hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
    %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
    %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_17, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::MemoizeDeviceQueriesPass (iree-hal-memoize-device-queries) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<SPIRVSubgroupReduce>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  util.global private @_device_query_0 : i1
  util.global private @_device_query_0_ok : i1
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    util.global.store %ok, @_device_query_0_ok : i1
    util.global.store %value, @_device_query_0 : i1
    util.initializer.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.initializer.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.initializer.return
  }
  util.global private @_executable__main_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %_device_query_0_ok = util.global.load @_device_query_0_ok : i1
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %0 = arith.select %_device_query_0, %c0, %c-1 : index
    %1 = scf.index_switch %0 -> !hal.executable 
    case 0 {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      %exe = hal.executable.create device(%device : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
      scf.yield %exe : !hal.executable
    }
    default {
      %2 = util.null : !hal.executable
      scf.yield %2 : !hal.executable
    }
    util.global.store %1, @_executable__main_dispatch_0 : !hal.executable
    util.initializer.return
  }
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.variant public @vulkan_spirv_fb target(#executable_target_vulkan_spirv_fb) {
      hal.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c32000 = arith.constant 32000 : index
        %c1 = arith.constant 1 : index
        hal.return %c32000, %c1, %c1 : index, index, index
      }
      builtin.module attributes {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>} {
        spirv.module Logical GLSL450 requires #spirv.vce<v1.3, [Shader, StorageBuffer16BitAccess, Float16, GroupNonUniformArithmetic], [SPV_KHR_storage_buffer_storage_class, SPV_KHR_16bit_storage]> {
          spirv.GlobalVariable @__builtin__WorkgroupId__ built_in("WorkgroupId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__builtin__LocalInvocationId__ built_in("LocalInvocationId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__resource_var_0_0_ bind(0, 0) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_1_ bind(0, 1) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_2_ bind(0, 2) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
          spirv.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() "None" {
            %cst64_i32 = spirv.Constant 64 : i32
            %cst128_i32 = spirv.Constant 128 : i32
            %cst192_i32 = spirv.Constant 192 : i32
            %cst256_i32 = spirv.Constant 256 : i32
            %cst320_i32 = spirv.Constant 320 : i32
            %cst384_i32 = spirv.Constant 384 : i32
            %cst448_i32 = spirv.Constant 448 : i32
            %cst512_i32 = spirv.Constant 512 : i32
            %cst_vec_4xf16 = spirv.Constant dense<0.000000e+00> : vector<4xf16>
            %cst0_i32 = spirv.Constant 0 : i32
            %cst_f16 = spirv.Constant 0.000000e+00 : f16
            %__builtin__LocalInvocationId___addr = spirv.mlir.addressof @__builtin__LocalInvocationId__ : !spirv.ptr<vector<3xi32>, Input>
            %0 = spirv.Load "Input" %__builtin__LocalInvocationId___addr : vector<3xi32>
            %1 = spirv.CompositeExtract %0[0 : i32] : vector<3xi32>
            %__resource_var_0_0__addr = spirv.mlir.addressof @__resource_var_0_0_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_1__addr = spirv.mlir.addressof @__resource_var_0_1_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_2__addr = spirv.mlir.addressof @__resource_var_0_2_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
            %__builtin__WorkgroupId___addr = spirv.mlir.addressof @__builtin__WorkgroupId__ : !spirv.ptr<vector<3xi32>, Input>
            %2 = spirv.Load "Input" %__builtin__WorkgroupId___addr : vector<3xi32>
            %3 = spirv.CompositeExtract %2[0 : i32] : vector<3xi32>
            %4 = spirv.IMul %3, %cst512_i32 : i32
            %5 = spirv.IAdd %1, %4 : i32
            %6 = spirv.IAdd %5, %cst448_i32 : i32
            %7 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %6] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %8 = spirv.Load "StorageBuffer" %7 : vector<4xf32>
            %9 = spirv.IAdd %1, %cst448_i32 : i32
            %10 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %9] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %11 = spirv.Load "StorageBuffer" %10 : vector<4xf32>
            %12 = spirv.IAdd %5, %cst384_i32 : i32
            %13 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %12] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %14 = spirv.Load "StorageBuffer" %13 : vector<4xf32>
            %15 = spirv.IAdd %1, %cst384_i32 : i32
            %16 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %15] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %17 = spirv.Load "StorageBuffer" %16 : vector<4xf32>
            %18 = spirv.IAdd %5, %cst320_i32 : i32
            %19 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %18] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %20 = spirv.Load "StorageBuffer" %19 : vector<4xf32>
            %21 = spirv.IAdd %1, %cst320_i32 : i32
            %22 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %21] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %23 = spirv.Load "StorageBuffer" %22 : vector<4xf32>
            %24 = spirv.IAdd %5, %cst256_i32 : i32
            %25 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %24] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %26 = spirv.Load "StorageBuffer" %25 : vector<4xf32>
            %27 = spirv.IAdd %1, %cst256_i32 : i32
            %28 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %27] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %29 = spirv.Load "StorageBuffer" %28 : vector<4xf32>
            %30 = spirv.IAdd %5, %cst192_i32 : i32
            %31 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %30] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %32 = spirv.Load "StorageBuffer" %31 : vector<4xf32>
            %33 = spirv.IAdd %1, %cst192_i32 : i32
            %34 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %33] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %35 = spirv.Load "StorageBuffer" %34 : vector<4xf32>
            %36 = spirv.IAdd %5, %cst128_i32 : i32
            %37 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %36] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %38 = spirv.Load "StorageBuffer" %37 : vector<4xf32>
            %39 = spirv.IAdd %1, %cst128_i32 : i32
            %40 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %39] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %41 = spirv.Load "StorageBuffer" %40 : vector<4xf32>
            %42 = spirv.IAdd %5, %cst64_i32 : i32
            %43 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %42] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %44 = spirv.Load "StorageBuffer" %43 : vector<4xf32>
            %45 = spirv.IAdd %1, %cst64_i32 : i32
            %46 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %45] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %47 = spirv.Load "StorageBuffer" %46 : vector<4xf32>
            %48 = spirv.IAdd %4, %1 : i32
            %49 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %48] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %50 = spirv.Load "StorageBuffer" %49 : vector<4xf32>
            %51 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %1] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %52 = spirv.Load "StorageBuffer" %51 : vector<4xf32>
            %53 = spirv.VectorShuffle [0 : i32, 1 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %54 = spirv.Bitcast %53 : vector<2xf32> to vector<4xf16>
            %55 = spirv.VectorShuffle [0 : i32, 1 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %56 = spirv.Bitcast %55 : vector<2xf32> to vector<4xf16>
            %57 = spirv.FMul %54, %56 : vector<4xf16>
            %58 = spirv.VectorShuffle [2 : i32, 3 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %59 = spirv.Bitcast %58 : vector<2xf32> to vector<4xf16>
            %60 = spirv.VectorShuffle [2 : i32, 3 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %61 = spirv.Bitcast %60 : vector<2xf32> to vector<4xf16>
            %62 = spirv.FMul %59, %61 : vector<4xf16>
            %63 = spirv.FAdd %57, %cst_vec_4xf16 : vector<4xf16>
            %64 = spirv.FAdd %62, %cst_vec_4xf16 : vector<4xf16>
            %65 = spirv.VectorShuffle [0 : i32, 1 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %66 = spirv.Bitcast %65 : vector<2xf32> to vector<4xf16>
            %67 = spirv.VectorShuffle [0 : i32, 1 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %68 = spirv.Bitcast %67 : vector<2xf32> to vector<4xf16>
            %69 = spirv.FMul %66, %68 : vector<4xf16>
            %70 = spirv.VectorShuffle [2 : i32, 3 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %71 = spirv.Bitcast %70 : vector<2xf32> to vector<4xf16>
            %72 = spirv.VectorShuffle [2 : i32, 3 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %73 = spirv.Bitcast %72 : vector<2xf32> to vector<4xf16>
            %74 = spirv.FMul %71, %73 : vector<4xf16>
            %75 = spirv.FAdd %69, %63 : vector<4xf16>
            %76 = spirv.FAdd %74, %64 : vector<4xf16>
            %77 = spirv.VectorShuffle [0 : i32, 1 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %78 = spirv.Bitcast %77 : vector<2xf32> to vector<4xf16>
            %79 = spirv.VectorShuffle [0 : i32, 1 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %80 = spirv.Bitcast %79 : vector<2xf32> to vector<4xf16>
            %81 = spirv.FMul %78, %80 : vector<4xf16>
            %82 = spirv.VectorShuffle [2 : i32, 3 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %83 = spirv.Bitcast %82 : vector<2xf32> to vector<4xf16>
            %84 = spirv.VectorShuffle [2 : i32, 3 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %85 = spirv.Bitcast %84 : vector<2xf32> to vector<4xf16>
            %86 = spirv.FMul %83, %85 : vector<4xf16>
            %87 = spirv.FAdd %81, %75 : vector<4xf16>
            %88 = spirv.FAdd %86, %76 : vector<4xf16>
            %89 = spirv.VectorShuffle [0 : i32, 1 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %90 = spirv.Bitcast %89 : vector<2xf32> to vector<4xf16>
            %91 = spirv.VectorShuffle [0 : i32, 1 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %92 = spirv.Bitcast %91 : vector<2xf32> to vector<4xf16>
            %93 = spirv.FMul %90, %92 : vector<4xf16>
            %94 = spirv.VectorShuffle [2 : i32, 3 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %95 = spirv.Bitcast %94 : vector<2xf32> to vector<4xf16>
            %96 = spirv.VectorShuffle [2 : i32, 3 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %97 = spirv.Bitcast %96 : vector<2xf32> to vector<4xf16>
            %98 = spirv.FMul %95, %97 : vector<4xf16>
            %99 = spirv.FAdd %93, %87 : vector<4xf16>
            %100 = spirv.FAdd %98, %88 : vector<4xf16>
            %101 = spirv.VectorShuffle [0 : i32, 1 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %102 = spirv.Bitcast %101 : vector<2xf32> to vector<4xf16>
            %103 = spirv.VectorShuffle [0 : i32, 1 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %104 = spirv.Bitcast %103 : vector<2xf32> to vector<4xf16>
            %105 = spirv.FMul %102, %104 : vector<4xf16>
            %106 = spirv.VectorShuffle [2 : i32, 3 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %107 = spirv.Bitcast %106 : vector<2xf32> to vector<4xf16>
            %108 = spirv.VectorShuffle [2 : i32, 3 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %109 = spirv.Bitcast %108 : vector<2xf32> to vector<4xf16>
            %110 = spirv.FMul %107, %109 : vector<4xf16>
            %111 = spirv.FAdd %105, %99 : vector<4xf16>
            %112 = spirv.FAdd %110, %100 : vector<4xf16>
            %113 = spirv.VectorShuffle [0 : i32, 1 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %114 = spirv.Bitcast %113 : vector<2xf32> to vector<4xf16>
            %115 = spirv.VectorShuffle [0 : i32, 1 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %116 = spirv.Bitcast %115 : vector<2xf32> to vector<4xf16>
            %117 = spirv.FMul %114, %116 : vector<4xf16>
            %118 = spirv.VectorShuffle [2 : i32, 3 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %119 = spirv.Bitcast %118 : vector<2xf32> to vector<4xf16>
            %120 = spirv.VectorShuffle [2 : i32, 3 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %121 = spirv.Bitcast %120 : vector<2xf32> to vector<4xf16>
            %122 = spirv.FMul %119, %121 : vector<4xf16>
            %123 = spirv.FAdd %117, %111 : vector<4xf16>
            %124 = spirv.FAdd %122, %112 : vector<4xf16>
            %125 = spirv.VectorShuffle [0 : i32, 1 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %126 = spirv.Bitcast %125 : vector<2xf32> to vector<4xf16>
            %127 = spirv.VectorShuffle [0 : i32, 1 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %128 = spirv.Bitcast %127 : vector<2xf32> to vector<4xf16>
            %129 = spirv.FMul %126, %128 : vector<4xf16>
            %130 = spirv.VectorShuffle [2 : i32, 3 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %131 = spirv.Bitcast %130 : vector<2xf32> to vector<4xf16>
            %132 = spirv.VectorShuffle [2 : i32, 3 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %133 = spirv.Bitcast %132 : vector<2xf32> to vector<4xf16>
            %134 = spirv.FMul %131, %133 : vector<4xf16>
            %135 = spirv.FAdd %129, %123 : vector<4xf16>
            %136 = spirv.FAdd %134, %124 : vector<4xf16>
            %137 = spirv.VectorShuffle [0 : i32, 1 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %138 = spirv.Bitcast %137 : vector<2xf32> to vector<4xf16>
            %139 = spirv.VectorShuffle [0 : i32, 1 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %140 = spirv.Bitcast %139 : vector<2xf32> to vector<4xf16>
            %141 = spirv.FMul %138, %140 : vector<4xf16>
            %142 = spirv.VectorShuffle [2 : i32, 3 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %143 = spirv.Bitcast %142 : vector<2xf32> to vector<4xf16>
            %144 = spirv.VectorShuffle [2 : i32, 3 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %145 = spirv.Bitcast %144 : vector<2xf32> to vector<4xf16>
            %146 = spirv.FMul %143, %145 : vector<4xf16>
            %147 = spirv.FAdd %141, %135 : vector<4xf16>
            %148 = spirv.FAdd %146, %136 : vector<4xf16>
            %149 = spirv.CompositeExtract %147[0 : i32] : vector<4xf16>
            %150 = spirv.CompositeExtract %147[1 : i32] : vector<4xf16>
            %151 = spirv.CompositeExtract %147[2 : i32] : vector<4xf16>
            %152 = spirv.CompositeExtract %147[3 : i32] : vector<4xf16>
            %153 = spirv.FAdd %149, %150 : f16
            %154 = spirv.FAdd %153, %151 : f16
            %155 = spirv.FAdd %154, %152 : f16
            %156 = spirv.CompositeExtract %148[0 : i32] : vector<4xf16>
            %157 = spirv.CompositeExtract %148[1 : i32] : vector<4xf16>
            %158 = spirv.CompositeExtract %148[2 : i32] : vector<4xf16>
            %159 = spirv.CompositeExtract %148[3 : i32] : vector<4xf16>
            %160 = spirv.FAdd %156, %157 : f16
            %161 = spirv.FAdd %160, %158 : f16
            %162 = spirv.FAdd %161, %159 : f16
            %163 = spirv.FAdd %155, %162 : f16
            %164 = spirv.GroupNonUniformFAdd "Subgroup" "Reduce" %163 : f16
            %165 = spirv.FAdd %164, %cst_f16 : f16
            %166 = spirv.IEqual %1, %cst0_i32 : i32
            spirv.mlir.selection {
              spirv.BranchConditional %166, ^bb1, ^bb2
            ^bb1:  // pred: ^bb0
              %167 = spirv.AccessChain %__resource_var_0_2__addr[%cst0_i32, %3] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>, i32, i32
              spirv.Store "StorageBuffer" %167, %165 : f16
              spirv.Branch ^bb2
            ^bb2:  // 2 preds: ^bb0, ^bb1
              spirv.mlir.merge
            }
            spirv.Return
          }
          spirv.EntryPoint "GLCompute" @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16, @__builtin__LocalInvocationId__, @__builtin__WorkgroupId__
          spirv.ExecutionMode @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 "LocalSize", 64, 1, 1
        }
      }
    }
  }
  func.func private @_main() {
    %c1_i32 = arith.constant 1 : i32
    %c553648144_i32 = arith.constant 553648144 : i32
    %c32000 = arith.constant 32000 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %device = hal.ex.shared_device : !hal.device
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
    %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_2, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
    %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
    %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_9, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %_device_query_0_ok = util.global.load @_device_query_0_ok : i1
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %3 = arith.select %_device_query_0, %c0, %c-1 : index
    scf.index_switch %3 
    case 0 {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%1 : !hal.buffer)[%c0, %len], 
        %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
        %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
      ])
      %4 = hal.command_buffer.device<%cmd_14 : !hal.command_buffer> : !hal.device
      %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
      hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
      scf.yield
    }
    default {
    }
    hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
    %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
    %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_17, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<SPIRVSubgroupReduce>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  util.global private @_device_query_0 : i1
  util.global private @_device_query_0_ok : i1
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    util.global.store %ok, @_device_query_0_ok : i1
    util.global.store %value, @_device_query_0 : i1
    util.initializer.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.initializer.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.initializer.return
  }
  util.global private @_executable__main_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %0 = arith.select %_device_query_0, %c0, %c-1 : index
    %1 = scf.index_switch %0 -> !hal.executable 
    case 0 {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      %exe = hal.executable.create device(%device : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
      scf.yield %exe : !hal.executable
    }
    default {
      %2 = util.null : !hal.executable
      scf.yield %2 : !hal.executable
    }
    util.global.store %1, @_executable__main_dispatch_0 : !hal.executable
    util.initializer.return
  }
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.variant public @vulkan_spirv_fb target(#executable_target_vulkan_spirv_fb) {
      hal.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c32000 = arith.constant 32000 : index
        %c1 = arith.constant 1 : index
        hal.return %c32000, %c1, %c1 : index, index, index
      }
      builtin.module attributes {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>} {
        spirv.module Logical GLSL450 requires #spirv.vce<v1.3, [Shader, StorageBuffer16BitAccess, Float16, GroupNonUniformArithmetic], [SPV_KHR_storage_buffer_storage_class, SPV_KHR_16bit_storage]> {
          spirv.GlobalVariable @__builtin__WorkgroupId__ built_in("WorkgroupId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__builtin__LocalInvocationId__ built_in("LocalInvocationId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__resource_var_0_0_ bind(0, 0) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_1_ bind(0, 1) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_2_ bind(0, 2) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
          spirv.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() "None" {
            %cst64_i32 = spirv.Constant 64 : i32
            %cst128_i32 = spirv.Constant 128 : i32
            %cst192_i32 = spirv.Constant 192 : i32
            %cst256_i32 = spirv.Constant 256 : i32
            %cst320_i32 = spirv.Constant 320 : i32
            %cst384_i32 = spirv.Constant 384 : i32
            %cst448_i32 = spirv.Constant 448 : i32
            %cst512_i32 = spirv.Constant 512 : i32
            %cst_vec_4xf16 = spirv.Constant dense<0.000000e+00> : vector<4xf16>
            %cst0_i32 = spirv.Constant 0 : i32
            %cst_f16 = spirv.Constant 0.000000e+00 : f16
            %__builtin__LocalInvocationId___addr = spirv.mlir.addressof @__builtin__LocalInvocationId__ : !spirv.ptr<vector<3xi32>, Input>
            %0 = spirv.Load "Input" %__builtin__LocalInvocationId___addr : vector<3xi32>
            %1 = spirv.CompositeExtract %0[0 : i32] : vector<3xi32>
            %__resource_var_0_0__addr = spirv.mlir.addressof @__resource_var_0_0_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_1__addr = spirv.mlir.addressof @__resource_var_0_1_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_2__addr = spirv.mlir.addressof @__resource_var_0_2_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
            %__builtin__WorkgroupId___addr = spirv.mlir.addressof @__builtin__WorkgroupId__ : !spirv.ptr<vector<3xi32>, Input>
            %2 = spirv.Load "Input" %__builtin__WorkgroupId___addr : vector<3xi32>
            %3 = spirv.CompositeExtract %2[0 : i32] : vector<3xi32>
            %4 = spirv.IMul %3, %cst512_i32 : i32
            %5 = spirv.IAdd %1, %4 : i32
            %6 = spirv.IAdd %5, %cst448_i32 : i32
            %7 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %6] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %8 = spirv.Load "StorageBuffer" %7 : vector<4xf32>
            %9 = spirv.IAdd %1, %cst448_i32 : i32
            %10 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %9] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %11 = spirv.Load "StorageBuffer" %10 : vector<4xf32>
            %12 = spirv.IAdd %5, %cst384_i32 : i32
            %13 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %12] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %14 = spirv.Load "StorageBuffer" %13 : vector<4xf32>
            %15 = spirv.IAdd %1, %cst384_i32 : i32
            %16 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %15] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %17 = spirv.Load "StorageBuffer" %16 : vector<4xf32>
            %18 = spirv.IAdd %5, %cst320_i32 : i32
            %19 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %18] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %20 = spirv.Load "StorageBuffer" %19 : vector<4xf32>
            %21 = spirv.IAdd %1, %cst320_i32 : i32
            %22 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %21] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %23 = spirv.Load "StorageBuffer" %22 : vector<4xf32>
            %24 = spirv.IAdd %5, %cst256_i32 : i32
            %25 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %24] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %26 = spirv.Load "StorageBuffer" %25 : vector<4xf32>
            %27 = spirv.IAdd %1, %cst256_i32 : i32
            %28 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %27] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %29 = spirv.Load "StorageBuffer" %28 : vector<4xf32>
            %30 = spirv.IAdd %5, %cst192_i32 : i32
            %31 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %30] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %32 = spirv.Load "StorageBuffer" %31 : vector<4xf32>
            %33 = spirv.IAdd %1, %cst192_i32 : i32
            %34 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %33] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %35 = spirv.Load "StorageBuffer" %34 : vector<4xf32>
            %36 = spirv.IAdd %5, %cst128_i32 : i32
            %37 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %36] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %38 = spirv.Load "StorageBuffer" %37 : vector<4xf32>
            %39 = spirv.IAdd %1, %cst128_i32 : i32
            %40 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %39] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %41 = spirv.Load "StorageBuffer" %40 : vector<4xf32>
            %42 = spirv.IAdd %5, %cst64_i32 : i32
            %43 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %42] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %44 = spirv.Load "StorageBuffer" %43 : vector<4xf32>
            %45 = spirv.IAdd %1, %cst64_i32 : i32
            %46 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %45] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %47 = spirv.Load "StorageBuffer" %46 : vector<4xf32>
            %48 = spirv.IAdd %4, %1 : i32
            %49 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %48] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %50 = spirv.Load "StorageBuffer" %49 : vector<4xf32>
            %51 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %1] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %52 = spirv.Load "StorageBuffer" %51 : vector<4xf32>
            %53 = spirv.VectorShuffle [0 : i32, 1 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %54 = spirv.Bitcast %53 : vector<2xf32> to vector<4xf16>
            %55 = spirv.VectorShuffle [0 : i32, 1 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %56 = spirv.Bitcast %55 : vector<2xf32> to vector<4xf16>
            %57 = spirv.FMul %54, %56 : vector<4xf16>
            %58 = spirv.VectorShuffle [2 : i32, 3 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %59 = spirv.Bitcast %58 : vector<2xf32> to vector<4xf16>
            %60 = spirv.VectorShuffle [2 : i32, 3 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %61 = spirv.Bitcast %60 : vector<2xf32> to vector<4xf16>
            %62 = spirv.FMul %59, %61 : vector<4xf16>
            %63 = spirv.FAdd %57, %cst_vec_4xf16 : vector<4xf16>
            %64 = spirv.FAdd %62, %cst_vec_4xf16 : vector<4xf16>
            %65 = spirv.VectorShuffle [0 : i32, 1 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %66 = spirv.Bitcast %65 : vector<2xf32> to vector<4xf16>
            %67 = spirv.VectorShuffle [0 : i32, 1 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %68 = spirv.Bitcast %67 : vector<2xf32> to vector<4xf16>
            %69 = spirv.FMul %66, %68 : vector<4xf16>
            %70 = spirv.VectorShuffle [2 : i32, 3 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %71 = spirv.Bitcast %70 : vector<2xf32> to vector<4xf16>
            %72 = spirv.VectorShuffle [2 : i32, 3 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %73 = spirv.Bitcast %72 : vector<2xf32> to vector<4xf16>
            %74 = spirv.FMul %71, %73 : vector<4xf16>
            %75 = spirv.FAdd %69, %63 : vector<4xf16>
            %76 = spirv.FAdd %74, %64 : vector<4xf16>
            %77 = spirv.VectorShuffle [0 : i32, 1 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %78 = spirv.Bitcast %77 : vector<2xf32> to vector<4xf16>
            %79 = spirv.VectorShuffle [0 : i32, 1 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %80 = spirv.Bitcast %79 : vector<2xf32> to vector<4xf16>
            %81 = spirv.FMul %78, %80 : vector<4xf16>
            %82 = spirv.VectorShuffle [2 : i32, 3 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %83 = spirv.Bitcast %82 : vector<2xf32> to vector<4xf16>
            %84 = spirv.VectorShuffle [2 : i32, 3 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %85 = spirv.Bitcast %84 : vector<2xf32> to vector<4xf16>
            %86 = spirv.FMul %83, %85 : vector<4xf16>
            %87 = spirv.FAdd %81, %75 : vector<4xf16>
            %88 = spirv.FAdd %86, %76 : vector<4xf16>
            %89 = spirv.VectorShuffle [0 : i32, 1 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %90 = spirv.Bitcast %89 : vector<2xf32> to vector<4xf16>
            %91 = spirv.VectorShuffle [0 : i32, 1 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %92 = spirv.Bitcast %91 : vector<2xf32> to vector<4xf16>
            %93 = spirv.FMul %90, %92 : vector<4xf16>
            %94 = spirv.VectorShuffle [2 : i32, 3 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %95 = spirv.Bitcast %94 : vector<2xf32> to vector<4xf16>
            %96 = spirv.VectorShuffle [2 : i32, 3 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %97 = spirv.Bitcast %96 : vector<2xf32> to vector<4xf16>
            %98 = spirv.FMul %95, %97 : vector<4xf16>
            %99 = spirv.FAdd %93, %87 : vector<4xf16>
            %100 = spirv.FAdd %98, %88 : vector<4xf16>
            %101 = spirv.VectorShuffle [0 : i32, 1 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %102 = spirv.Bitcast %101 : vector<2xf32> to vector<4xf16>
            %103 = spirv.VectorShuffle [0 : i32, 1 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %104 = spirv.Bitcast %103 : vector<2xf32> to vector<4xf16>
            %105 = spirv.FMul %102, %104 : vector<4xf16>
            %106 = spirv.VectorShuffle [2 : i32, 3 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %107 = spirv.Bitcast %106 : vector<2xf32> to vector<4xf16>
            %108 = spirv.VectorShuffle [2 : i32, 3 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %109 = spirv.Bitcast %108 : vector<2xf32> to vector<4xf16>
            %110 = spirv.FMul %107, %109 : vector<4xf16>
            %111 = spirv.FAdd %105, %99 : vector<4xf16>
            %112 = spirv.FAdd %110, %100 : vector<4xf16>
            %113 = spirv.VectorShuffle [0 : i32, 1 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %114 = spirv.Bitcast %113 : vector<2xf32> to vector<4xf16>
            %115 = spirv.VectorShuffle [0 : i32, 1 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %116 = spirv.Bitcast %115 : vector<2xf32> to vector<4xf16>
            %117 = spirv.FMul %114, %116 : vector<4xf16>
            %118 = spirv.VectorShuffle [2 : i32, 3 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %119 = spirv.Bitcast %118 : vector<2xf32> to vector<4xf16>
            %120 = spirv.VectorShuffle [2 : i32, 3 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %121 = spirv.Bitcast %120 : vector<2xf32> to vector<4xf16>
            %122 = spirv.FMul %119, %121 : vector<4xf16>
            %123 = spirv.FAdd %117, %111 : vector<4xf16>
            %124 = spirv.FAdd %122, %112 : vector<4xf16>
            %125 = spirv.VectorShuffle [0 : i32, 1 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %126 = spirv.Bitcast %125 : vector<2xf32> to vector<4xf16>
            %127 = spirv.VectorShuffle [0 : i32, 1 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %128 = spirv.Bitcast %127 : vector<2xf32> to vector<4xf16>
            %129 = spirv.FMul %126, %128 : vector<4xf16>
            %130 = spirv.VectorShuffle [2 : i32, 3 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %131 = spirv.Bitcast %130 : vector<2xf32> to vector<4xf16>
            %132 = spirv.VectorShuffle [2 : i32, 3 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %133 = spirv.Bitcast %132 : vector<2xf32> to vector<4xf16>
            %134 = spirv.FMul %131, %133 : vector<4xf16>
            %135 = spirv.FAdd %129, %123 : vector<4xf16>
            %136 = spirv.FAdd %134, %124 : vector<4xf16>
            %137 = spirv.VectorShuffle [0 : i32, 1 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %138 = spirv.Bitcast %137 : vector<2xf32> to vector<4xf16>
            %139 = spirv.VectorShuffle [0 : i32, 1 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %140 = spirv.Bitcast %139 : vector<2xf32> to vector<4xf16>
            %141 = spirv.FMul %138, %140 : vector<4xf16>
            %142 = spirv.VectorShuffle [2 : i32, 3 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %143 = spirv.Bitcast %142 : vector<2xf32> to vector<4xf16>
            %144 = spirv.VectorShuffle [2 : i32, 3 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %145 = spirv.Bitcast %144 : vector<2xf32> to vector<4xf16>
            %146 = spirv.FMul %143, %145 : vector<4xf16>
            %147 = spirv.FAdd %141, %135 : vector<4xf16>
            %148 = spirv.FAdd %146, %136 : vector<4xf16>
            %149 = spirv.CompositeExtract %147[0 : i32] : vector<4xf16>
            %150 = spirv.CompositeExtract %147[1 : i32] : vector<4xf16>
            %151 = spirv.CompositeExtract %147[2 : i32] : vector<4xf16>
            %152 = spirv.CompositeExtract %147[3 : i32] : vector<4xf16>
            %153 = spirv.FAdd %149, %150 : f16
            %154 = spirv.FAdd %153, %151 : f16
            %155 = spirv.FAdd %154, %152 : f16
            %156 = spirv.CompositeExtract %148[0 : i32] : vector<4xf16>
            %157 = spirv.CompositeExtract %148[1 : i32] : vector<4xf16>
            %158 = spirv.CompositeExtract %148[2 : i32] : vector<4xf16>
            %159 = spirv.CompositeExtract %148[3 : i32] : vector<4xf16>
            %160 = spirv.FAdd %156, %157 : f16
            %161 = spirv.FAdd %160, %158 : f16
            %162 = spirv.FAdd %161, %159 : f16
            %163 = spirv.FAdd %155, %162 : f16
            %164 = spirv.GroupNonUniformFAdd "Subgroup" "Reduce" %163 : f16
            %165 = spirv.FAdd %164, %cst_f16 : f16
            %166 = spirv.IEqual %1, %cst0_i32 : i32
            spirv.mlir.selection {
              spirv.BranchConditional %166, ^bb1, ^bb2
            ^bb1:  // pred: ^bb0
              %167 = spirv.AccessChain %__resource_var_0_2__addr[%cst0_i32, %3] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>, i32, i32
              spirv.Store "StorageBuffer" %167, %165 : f16
              spirv.Branch ^bb2
            ^bb2:  // 2 preds: ^bb0, ^bb1
              spirv.mlir.merge
            }
            spirv.Return
          }
          spirv.EntryPoint "GLCompute" @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16, @__builtin__LocalInvocationId__, @__builtin__WorkgroupId__
          spirv.ExecutionMode @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 "LocalSize", 64, 1, 1
        }
      }
    }
  }
  func.func private @_main() {
    %c1_i32 = arith.constant 1 : i32
    %c553648144_i32 = arith.constant 553648144 : i32
    %c32000 = arith.constant 32000 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %device = hal.ex.shared_device : !hal.device
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
    %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_2, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
    %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
    %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_9, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %3 = arith.select %_device_query_0, %c0, %c-1 : index
    scf.index_switch %3 
    case 0 {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%1 : !hal.buffer)[%c0, %len], 
        %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
        %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
      ])
      %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
      hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
      scf.yield
    }
    default {
    }
    hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
    %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
    %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_17, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<SPIRVSubgroupReduce>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  util.global private @_device_query_0 : i1
  util.global private @_device_query_0_ok : i1
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    util.global.store %ok, @_device_query_0_ok : i1
    util.global.store %value, @_device_query_0 : i1
    util.initializer.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.initializer.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.initializer.return
  }
  util.global private @_executable__main_dispatch_0 : !hal.executable
  util.initializer {
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %device = hal.ex.shared_device : !hal.device
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %0 = arith.select %_device_query_0, %c0, %c-1 : index
    %1 = scf.index_switch %0 -> !hal.executable 
    case 0 {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      %exe = hal.executable.create device(%device : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
      scf.yield %exe : !hal.executable
    }
    default {
      %2 = util.null : !hal.executable
      scf.yield %2 : !hal.executable
    }
    util.global.store %1, @_executable__main_dispatch_0 : !hal.executable
    util.initializer.return
  }
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.variant public @vulkan_spirv_fb target(#executable_target_vulkan_spirv_fb) {
      hal.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c32000 = arith.constant 32000 : index
        %c1 = arith.constant 1 : index
        hal.return %c32000, %c1, %c1 : index, index, index
      }
      builtin.module attributes {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>} {
        spirv.module Logical GLSL450 requires #spirv.vce<v1.3, [Shader, StorageBuffer16BitAccess, Float16, GroupNonUniformArithmetic], [SPV_KHR_storage_buffer_storage_class, SPV_KHR_16bit_storage]> {
          spirv.GlobalVariable @__builtin__WorkgroupId__ built_in("WorkgroupId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__builtin__LocalInvocationId__ built_in("LocalInvocationId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__resource_var_0_0_ bind(0, 0) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_1_ bind(0, 1) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_2_ bind(0, 2) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
          spirv.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() "None" {
            %cst64_i32 = spirv.Constant 64 : i32
            %cst128_i32 = spirv.Constant 128 : i32
            %cst192_i32 = spirv.Constant 192 : i32
            %cst256_i32 = spirv.Constant 256 : i32
            %cst320_i32 = spirv.Constant 320 : i32
            %cst384_i32 = spirv.Constant 384 : i32
            %cst448_i32 = spirv.Constant 448 : i32
            %cst512_i32 = spirv.Constant 512 : i32
            %cst_vec_4xf16 = spirv.Constant dense<0.000000e+00> : vector<4xf16>
            %cst0_i32 = spirv.Constant 0 : i32
            %cst_f16 = spirv.Constant 0.000000e+00 : f16
            %__builtin__LocalInvocationId___addr = spirv.mlir.addressof @__builtin__LocalInvocationId__ : !spirv.ptr<vector<3xi32>, Input>
            %0 = spirv.Load "Input" %__builtin__LocalInvocationId___addr : vector<3xi32>
            %1 = spirv.CompositeExtract %0[0 : i32] : vector<3xi32>
            %__resource_var_0_0__addr = spirv.mlir.addressof @__resource_var_0_0_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_1__addr = spirv.mlir.addressof @__resource_var_0_1_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_2__addr = spirv.mlir.addressof @__resource_var_0_2_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
            %__builtin__WorkgroupId___addr = spirv.mlir.addressof @__builtin__WorkgroupId__ : !spirv.ptr<vector<3xi32>, Input>
            %2 = spirv.Load "Input" %__builtin__WorkgroupId___addr : vector<3xi32>
            %3 = spirv.CompositeExtract %2[0 : i32] : vector<3xi32>
            %4 = spirv.IMul %3, %cst512_i32 : i32
            %5 = spirv.IAdd %1, %4 : i32
            %6 = spirv.IAdd %5, %cst448_i32 : i32
            %7 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %6] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %8 = spirv.Load "StorageBuffer" %7 : vector<4xf32>
            %9 = spirv.IAdd %1, %cst448_i32 : i32
            %10 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %9] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %11 = spirv.Load "StorageBuffer" %10 : vector<4xf32>
            %12 = spirv.IAdd %5, %cst384_i32 : i32
            %13 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %12] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %14 = spirv.Load "StorageBuffer" %13 : vector<4xf32>
            %15 = spirv.IAdd %1, %cst384_i32 : i32
            %16 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %15] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %17 = spirv.Load "StorageBuffer" %16 : vector<4xf32>
            %18 = spirv.IAdd %5, %cst320_i32 : i32
            %19 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %18] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %20 = spirv.Load "StorageBuffer" %19 : vector<4xf32>
            %21 = spirv.IAdd %1, %cst320_i32 : i32
            %22 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %21] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %23 = spirv.Load "StorageBuffer" %22 : vector<4xf32>
            %24 = spirv.IAdd %5, %cst256_i32 : i32
            %25 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %24] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %26 = spirv.Load "StorageBuffer" %25 : vector<4xf32>
            %27 = spirv.IAdd %1, %cst256_i32 : i32
            %28 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %27] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %29 = spirv.Load "StorageBuffer" %28 : vector<4xf32>
            %30 = spirv.IAdd %5, %cst192_i32 : i32
            %31 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %30] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %32 = spirv.Load "StorageBuffer" %31 : vector<4xf32>
            %33 = spirv.IAdd %1, %cst192_i32 : i32
            %34 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %33] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %35 = spirv.Load "StorageBuffer" %34 : vector<4xf32>
            %36 = spirv.IAdd %5, %cst128_i32 : i32
            %37 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %36] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %38 = spirv.Load "StorageBuffer" %37 : vector<4xf32>
            %39 = spirv.IAdd %1, %cst128_i32 : i32
            %40 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %39] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %41 = spirv.Load "StorageBuffer" %40 : vector<4xf32>
            %42 = spirv.IAdd %5, %cst64_i32 : i32
            %43 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %42] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %44 = spirv.Load "StorageBuffer" %43 : vector<4xf32>
            %45 = spirv.IAdd %1, %cst64_i32 : i32
            %46 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %45] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %47 = spirv.Load "StorageBuffer" %46 : vector<4xf32>
            %48 = spirv.IAdd %4, %1 : i32
            %49 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %48] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %50 = spirv.Load "StorageBuffer" %49 : vector<4xf32>
            %51 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %1] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %52 = spirv.Load "StorageBuffer" %51 : vector<4xf32>
            %53 = spirv.VectorShuffle [0 : i32, 1 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %54 = spirv.Bitcast %53 : vector<2xf32> to vector<4xf16>
            %55 = spirv.VectorShuffle [0 : i32, 1 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %56 = spirv.Bitcast %55 : vector<2xf32> to vector<4xf16>
            %57 = spirv.FMul %54, %56 : vector<4xf16>
            %58 = spirv.VectorShuffle [2 : i32, 3 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %59 = spirv.Bitcast %58 : vector<2xf32> to vector<4xf16>
            %60 = spirv.VectorShuffle [2 : i32, 3 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %61 = spirv.Bitcast %60 : vector<2xf32> to vector<4xf16>
            %62 = spirv.FMul %59, %61 : vector<4xf16>
            %63 = spirv.FAdd %57, %cst_vec_4xf16 : vector<4xf16>
            %64 = spirv.FAdd %62, %cst_vec_4xf16 : vector<4xf16>
            %65 = spirv.VectorShuffle [0 : i32, 1 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %66 = spirv.Bitcast %65 : vector<2xf32> to vector<4xf16>
            %67 = spirv.VectorShuffle [0 : i32, 1 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %68 = spirv.Bitcast %67 : vector<2xf32> to vector<4xf16>
            %69 = spirv.FMul %66, %68 : vector<4xf16>
            %70 = spirv.VectorShuffle [2 : i32, 3 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %71 = spirv.Bitcast %70 : vector<2xf32> to vector<4xf16>
            %72 = spirv.VectorShuffle [2 : i32, 3 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %73 = spirv.Bitcast %72 : vector<2xf32> to vector<4xf16>
            %74 = spirv.FMul %71, %73 : vector<4xf16>
            %75 = spirv.FAdd %69, %63 : vector<4xf16>
            %76 = spirv.FAdd %74, %64 : vector<4xf16>
            %77 = spirv.VectorShuffle [0 : i32, 1 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %78 = spirv.Bitcast %77 : vector<2xf32> to vector<4xf16>
            %79 = spirv.VectorShuffle [0 : i32, 1 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %80 = spirv.Bitcast %79 : vector<2xf32> to vector<4xf16>
            %81 = spirv.FMul %78, %80 : vector<4xf16>
            %82 = spirv.VectorShuffle [2 : i32, 3 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %83 = spirv.Bitcast %82 : vector<2xf32> to vector<4xf16>
            %84 = spirv.VectorShuffle [2 : i32, 3 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %85 = spirv.Bitcast %84 : vector<2xf32> to vector<4xf16>
            %86 = spirv.FMul %83, %85 : vector<4xf16>
            %87 = spirv.FAdd %81, %75 : vector<4xf16>
            %88 = spirv.FAdd %86, %76 : vector<4xf16>
            %89 = spirv.VectorShuffle [0 : i32, 1 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %90 = spirv.Bitcast %89 : vector<2xf32> to vector<4xf16>
            %91 = spirv.VectorShuffle [0 : i32, 1 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %92 = spirv.Bitcast %91 : vector<2xf32> to vector<4xf16>
            %93 = spirv.FMul %90, %92 : vector<4xf16>
            %94 = spirv.VectorShuffle [2 : i32, 3 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %95 = spirv.Bitcast %94 : vector<2xf32> to vector<4xf16>
            %96 = spirv.VectorShuffle [2 : i32, 3 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %97 = spirv.Bitcast %96 : vector<2xf32> to vector<4xf16>
            %98 = spirv.FMul %95, %97 : vector<4xf16>
            %99 = spirv.FAdd %93, %87 : vector<4xf16>
            %100 = spirv.FAdd %98, %88 : vector<4xf16>
            %101 = spirv.VectorShuffle [0 : i32, 1 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %102 = spirv.Bitcast %101 : vector<2xf32> to vector<4xf16>
            %103 = spirv.VectorShuffle [0 : i32, 1 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %104 = spirv.Bitcast %103 : vector<2xf32> to vector<4xf16>
            %105 = spirv.FMul %102, %104 : vector<4xf16>
            %106 = spirv.VectorShuffle [2 : i32, 3 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %107 = spirv.Bitcast %106 : vector<2xf32> to vector<4xf16>
            %108 = spirv.VectorShuffle [2 : i32, 3 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %109 = spirv.Bitcast %108 : vector<2xf32> to vector<4xf16>
            %110 = spirv.FMul %107, %109 : vector<4xf16>
            %111 = spirv.FAdd %105, %99 : vector<4xf16>
            %112 = spirv.FAdd %110, %100 : vector<4xf16>
            %113 = spirv.VectorShuffle [0 : i32, 1 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %114 = spirv.Bitcast %113 : vector<2xf32> to vector<4xf16>
            %115 = spirv.VectorShuffle [0 : i32, 1 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %116 = spirv.Bitcast %115 : vector<2xf32> to vector<4xf16>
            %117 = spirv.FMul %114, %116 : vector<4xf16>
            %118 = spirv.VectorShuffle [2 : i32, 3 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %119 = spirv.Bitcast %118 : vector<2xf32> to vector<4xf16>
            %120 = spirv.VectorShuffle [2 : i32, 3 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %121 = spirv.Bitcast %120 : vector<2xf32> to vector<4xf16>
            %122 = spirv.FMul %119, %121 : vector<4xf16>
            %123 = spirv.FAdd %117, %111 : vector<4xf16>
            %124 = spirv.FAdd %122, %112 : vector<4xf16>
            %125 = spirv.VectorShuffle [0 : i32, 1 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %126 = spirv.Bitcast %125 : vector<2xf32> to vector<4xf16>
            %127 = spirv.VectorShuffle [0 : i32, 1 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %128 = spirv.Bitcast %127 : vector<2xf32> to vector<4xf16>
            %129 = spirv.FMul %126, %128 : vector<4xf16>
            %130 = spirv.VectorShuffle [2 : i32, 3 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %131 = spirv.Bitcast %130 : vector<2xf32> to vector<4xf16>
            %132 = spirv.VectorShuffle [2 : i32, 3 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %133 = spirv.Bitcast %132 : vector<2xf32> to vector<4xf16>
            %134 = spirv.FMul %131, %133 : vector<4xf16>
            %135 = spirv.FAdd %129, %123 : vector<4xf16>
            %136 = spirv.FAdd %134, %124 : vector<4xf16>
            %137 = spirv.VectorShuffle [0 : i32, 1 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %138 = spirv.Bitcast %137 : vector<2xf32> to vector<4xf16>
            %139 = spirv.VectorShuffle [0 : i32, 1 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %140 = spirv.Bitcast %139 : vector<2xf32> to vector<4xf16>
            %141 = spirv.FMul %138, %140 : vector<4xf16>
            %142 = spirv.VectorShuffle [2 : i32, 3 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %143 = spirv.Bitcast %142 : vector<2xf32> to vector<4xf16>
            %144 = spirv.VectorShuffle [2 : i32, 3 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %145 = spirv.Bitcast %144 : vector<2xf32> to vector<4xf16>
            %146 = spirv.FMul %143, %145 : vector<4xf16>
            %147 = spirv.FAdd %141, %135 : vector<4xf16>
            %148 = spirv.FAdd %146, %136 : vector<4xf16>
            %149 = spirv.CompositeExtract %147[0 : i32] : vector<4xf16>
            %150 = spirv.CompositeExtract %147[1 : i32] : vector<4xf16>
            %151 = spirv.CompositeExtract %147[2 : i32] : vector<4xf16>
            %152 = spirv.CompositeExtract %147[3 : i32] : vector<4xf16>
            %153 = spirv.FAdd %149, %150 : f16
            %154 = spirv.FAdd %153, %151 : f16
            %155 = spirv.FAdd %154, %152 : f16
            %156 = spirv.CompositeExtract %148[0 : i32] : vector<4xf16>
            %157 = spirv.CompositeExtract %148[1 : i32] : vector<4xf16>
            %158 = spirv.CompositeExtract %148[2 : i32] : vector<4xf16>
            %159 = spirv.CompositeExtract %148[3 : i32] : vector<4xf16>
            %160 = spirv.FAdd %156, %157 : f16
            %161 = spirv.FAdd %160, %158 : f16
            %162 = spirv.FAdd %161, %159 : f16
            %163 = spirv.FAdd %155, %162 : f16
            %164 = spirv.GroupNonUniformFAdd "Subgroup" "Reduce" %163 : f16
            %165 = spirv.FAdd %164, %cst_f16 : f16
            %166 = spirv.IEqual %1, %cst0_i32 : i32
            spirv.mlir.selection {
              spirv.BranchConditional %166, ^bb1, ^bb2
            ^bb1:  // pred: ^bb0
              %167 = spirv.AccessChain %__resource_var_0_2__addr[%cst0_i32, %3] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>, i32, i32
              spirv.Store "StorageBuffer" %167, %165 : f16
              spirv.Branch ^bb2
            ^bb2:  // 2 preds: ^bb0, ^bb1
              spirv.mlir.merge
            }
            spirv.Return
          }
          spirv.EntryPoint "GLCompute" @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16, @__builtin__LocalInvocationId__, @__builtin__WorkgroupId__
          spirv.ExecutionMode @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 "LocalSize", 64, 1, 1
        }
      }
    }
  }
  func.func private @_main() {
    %c1_i32 = arith.constant 1 : i32
    %c553648144_i32 = arith.constant 553648144 : i32
    %c32000 = arith.constant 32000 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %device = hal.ex.shared_device : !hal.device
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
    %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_2, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
    %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
    %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_9, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %3 = arith.select %_device_query_0, %c0, %c-1 : index
    scf.index_switch %3 
    case 0 {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%1 : !hal.buffer)[%c0, %len], 
        %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
        %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
      ])
      %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
      hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
      scf.yield
    }
    default {
    }
    hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
    %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
    %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_17, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<SPIRVSubgroupReduce>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  util.global private @_device_query_0 : i1
  util.global private @_device_query_0_ok : i1
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    util.global.store %ok, @_device_query_0_ok : i1
    util.global.store %value, @_device_query_0 : i1
    util.initializer.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.initializer.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.initializer.return
  }
  util.global private @_executable__main_dispatch_0 : !hal.executable
  util.initializer {
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %device = hal.ex.shared_device : !hal.device
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %0 = arith.select %_device_query_0, %c0, %c-1 : index
    %1 = scf.index_switch %0 -> !hal.executable 
    case 0 {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      %exe = hal.executable.create device(%device : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
      scf.yield %exe : !hal.executable
    }
    default {
      %2 = util.null : !hal.executable
      scf.yield %2 : !hal.executable
    }
    util.global.store %1, @_executable__main_dispatch_0 : !hal.executable
    util.initializer.return
  }
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.variant public @vulkan_spirv_fb target(#executable_target_vulkan_spirv_fb) {
      hal.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c32000 = arith.constant 32000 : index
        %c1 = arith.constant 1 : index
        hal.return %c32000, %c1, %c1 : index, index, index
      }
      builtin.module attributes {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>} {
        spirv.module Logical GLSL450 requires #spirv.vce<v1.3, [Shader, StorageBuffer16BitAccess, Float16, GroupNonUniformArithmetic], [SPV_KHR_storage_buffer_storage_class, SPV_KHR_16bit_storage]> {
          spirv.GlobalVariable @__builtin__WorkgroupId__ built_in("WorkgroupId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__builtin__LocalInvocationId__ built_in("LocalInvocationId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__resource_var_0_0_ bind(0, 0) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_1_ bind(0, 1) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_2_ bind(0, 2) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
          spirv.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() "None" {
            %cst64_i32 = spirv.Constant 64 : i32
            %cst128_i32 = spirv.Constant 128 : i32
            %cst192_i32 = spirv.Constant 192 : i32
            %cst256_i32 = spirv.Constant 256 : i32
            %cst320_i32 = spirv.Constant 320 : i32
            %cst384_i32 = spirv.Constant 384 : i32
            %cst448_i32 = spirv.Constant 448 : i32
            %cst512_i32 = spirv.Constant 512 : i32
            %cst_vec_4xf16 = spirv.Constant dense<0.000000e+00> : vector<4xf16>
            %cst0_i32 = spirv.Constant 0 : i32
            %cst_f16 = spirv.Constant 0.000000e+00 : f16
            %__builtin__LocalInvocationId___addr = spirv.mlir.addressof @__builtin__LocalInvocationId__ : !spirv.ptr<vector<3xi32>, Input>
            %0 = spirv.Load "Input" %__builtin__LocalInvocationId___addr : vector<3xi32>
            %1 = spirv.CompositeExtract %0[0 : i32] : vector<3xi32>
            %__resource_var_0_0__addr = spirv.mlir.addressof @__resource_var_0_0_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_1__addr = spirv.mlir.addressof @__resource_var_0_1_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_2__addr = spirv.mlir.addressof @__resource_var_0_2_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
            %__builtin__WorkgroupId___addr = spirv.mlir.addressof @__builtin__WorkgroupId__ : !spirv.ptr<vector<3xi32>, Input>
            %2 = spirv.Load "Input" %__builtin__WorkgroupId___addr : vector<3xi32>
            %3 = spirv.CompositeExtract %2[0 : i32] : vector<3xi32>
            %4 = spirv.IMul %3, %cst512_i32 : i32
            %5 = spirv.IAdd %1, %4 : i32
            %6 = spirv.IAdd %5, %cst448_i32 : i32
            %7 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %6] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %8 = spirv.Load "StorageBuffer" %7 : vector<4xf32>
            %9 = spirv.IAdd %1, %cst448_i32 : i32
            %10 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %9] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %11 = spirv.Load "StorageBuffer" %10 : vector<4xf32>
            %12 = spirv.IAdd %5, %cst384_i32 : i32
            %13 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %12] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %14 = spirv.Load "StorageBuffer" %13 : vector<4xf32>
            %15 = spirv.IAdd %1, %cst384_i32 : i32
            %16 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %15] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %17 = spirv.Load "StorageBuffer" %16 : vector<4xf32>
            %18 = spirv.IAdd %5, %cst320_i32 : i32
            %19 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %18] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %20 = spirv.Load "StorageBuffer" %19 : vector<4xf32>
            %21 = spirv.IAdd %1, %cst320_i32 : i32
            %22 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %21] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %23 = spirv.Load "StorageBuffer" %22 : vector<4xf32>
            %24 = spirv.IAdd %5, %cst256_i32 : i32
            %25 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %24] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %26 = spirv.Load "StorageBuffer" %25 : vector<4xf32>
            %27 = spirv.IAdd %1, %cst256_i32 : i32
            %28 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %27] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %29 = spirv.Load "StorageBuffer" %28 : vector<4xf32>
            %30 = spirv.IAdd %5, %cst192_i32 : i32
            %31 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %30] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %32 = spirv.Load "StorageBuffer" %31 : vector<4xf32>
            %33 = spirv.IAdd %1, %cst192_i32 : i32
            %34 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %33] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %35 = spirv.Load "StorageBuffer" %34 : vector<4xf32>
            %36 = spirv.IAdd %5, %cst128_i32 : i32
            %37 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %36] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %38 = spirv.Load "StorageBuffer" %37 : vector<4xf32>
            %39 = spirv.IAdd %1, %cst128_i32 : i32
            %40 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %39] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %41 = spirv.Load "StorageBuffer" %40 : vector<4xf32>
            %42 = spirv.IAdd %5, %cst64_i32 : i32
            %43 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %42] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %44 = spirv.Load "StorageBuffer" %43 : vector<4xf32>
            %45 = spirv.IAdd %1, %cst64_i32 : i32
            %46 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %45] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %47 = spirv.Load "StorageBuffer" %46 : vector<4xf32>
            %48 = spirv.IAdd %4, %1 : i32
            %49 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %48] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %50 = spirv.Load "StorageBuffer" %49 : vector<4xf32>
            %51 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %1] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %52 = spirv.Load "StorageBuffer" %51 : vector<4xf32>
            %53 = spirv.VectorShuffle [0 : i32, 1 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %54 = spirv.Bitcast %53 : vector<2xf32> to vector<4xf16>
            %55 = spirv.VectorShuffle [0 : i32, 1 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %56 = spirv.Bitcast %55 : vector<2xf32> to vector<4xf16>
            %57 = spirv.FMul %54, %56 : vector<4xf16>
            %58 = spirv.VectorShuffle [2 : i32, 3 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %59 = spirv.Bitcast %58 : vector<2xf32> to vector<4xf16>
            %60 = spirv.VectorShuffle [2 : i32, 3 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %61 = spirv.Bitcast %60 : vector<2xf32> to vector<4xf16>
            %62 = spirv.FMul %59, %61 : vector<4xf16>
            %63 = spirv.FAdd %57, %cst_vec_4xf16 : vector<4xf16>
            %64 = spirv.FAdd %62, %cst_vec_4xf16 : vector<4xf16>
            %65 = spirv.VectorShuffle [0 : i32, 1 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %66 = spirv.Bitcast %65 : vector<2xf32> to vector<4xf16>
            %67 = spirv.VectorShuffle [0 : i32, 1 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %68 = spirv.Bitcast %67 : vector<2xf32> to vector<4xf16>
            %69 = spirv.FMul %66, %68 : vector<4xf16>
            %70 = spirv.VectorShuffle [2 : i32, 3 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %71 = spirv.Bitcast %70 : vector<2xf32> to vector<4xf16>
            %72 = spirv.VectorShuffle [2 : i32, 3 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %73 = spirv.Bitcast %72 : vector<2xf32> to vector<4xf16>
            %74 = spirv.FMul %71, %73 : vector<4xf16>
            %75 = spirv.FAdd %69, %63 : vector<4xf16>
            %76 = spirv.FAdd %74, %64 : vector<4xf16>
            %77 = spirv.VectorShuffle [0 : i32, 1 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %78 = spirv.Bitcast %77 : vector<2xf32> to vector<4xf16>
            %79 = spirv.VectorShuffle [0 : i32, 1 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %80 = spirv.Bitcast %79 : vector<2xf32> to vector<4xf16>
            %81 = spirv.FMul %78, %80 : vector<4xf16>
            %82 = spirv.VectorShuffle [2 : i32, 3 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %83 = spirv.Bitcast %82 : vector<2xf32> to vector<4xf16>
            %84 = spirv.VectorShuffle [2 : i32, 3 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %85 = spirv.Bitcast %84 : vector<2xf32> to vector<4xf16>
            %86 = spirv.FMul %83, %85 : vector<4xf16>
            %87 = spirv.FAdd %81, %75 : vector<4xf16>
            %88 = spirv.FAdd %86, %76 : vector<4xf16>
            %89 = spirv.VectorShuffle [0 : i32, 1 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %90 = spirv.Bitcast %89 : vector<2xf32> to vector<4xf16>
            %91 = spirv.VectorShuffle [0 : i32, 1 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %92 = spirv.Bitcast %91 : vector<2xf32> to vector<4xf16>
            %93 = spirv.FMul %90, %92 : vector<4xf16>
            %94 = spirv.VectorShuffle [2 : i32, 3 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %95 = spirv.Bitcast %94 : vector<2xf32> to vector<4xf16>
            %96 = spirv.VectorShuffle [2 : i32, 3 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %97 = spirv.Bitcast %96 : vector<2xf32> to vector<4xf16>
            %98 = spirv.FMul %95, %97 : vector<4xf16>
            %99 = spirv.FAdd %93, %87 : vector<4xf16>
            %100 = spirv.FAdd %98, %88 : vector<4xf16>
            %101 = spirv.VectorShuffle [0 : i32, 1 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %102 = spirv.Bitcast %101 : vector<2xf32> to vector<4xf16>
            %103 = spirv.VectorShuffle [0 : i32, 1 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %104 = spirv.Bitcast %103 : vector<2xf32> to vector<4xf16>
            %105 = spirv.FMul %102, %104 : vector<4xf16>
            %106 = spirv.VectorShuffle [2 : i32, 3 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %107 = spirv.Bitcast %106 : vector<2xf32> to vector<4xf16>
            %108 = spirv.VectorShuffle [2 : i32, 3 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %109 = spirv.Bitcast %108 : vector<2xf32> to vector<4xf16>
            %110 = spirv.FMul %107, %109 : vector<4xf16>
            %111 = spirv.FAdd %105, %99 : vector<4xf16>
            %112 = spirv.FAdd %110, %100 : vector<4xf16>
            %113 = spirv.VectorShuffle [0 : i32, 1 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %114 = spirv.Bitcast %113 : vector<2xf32> to vector<4xf16>
            %115 = spirv.VectorShuffle [0 : i32, 1 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %116 = spirv.Bitcast %115 : vector<2xf32> to vector<4xf16>
            %117 = spirv.FMul %114, %116 : vector<4xf16>
            %118 = spirv.VectorShuffle [2 : i32, 3 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %119 = spirv.Bitcast %118 : vector<2xf32> to vector<4xf16>
            %120 = spirv.VectorShuffle [2 : i32, 3 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %121 = spirv.Bitcast %120 : vector<2xf32> to vector<4xf16>
            %122 = spirv.FMul %119, %121 : vector<4xf16>
            %123 = spirv.FAdd %117, %111 : vector<4xf16>
            %124 = spirv.FAdd %122, %112 : vector<4xf16>
            %125 = spirv.VectorShuffle [0 : i32, 1 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %126 = spirv.Bitcast %125 : vector<2xf32> to vector<4xf16>
            %127 = spirv.VectorShuffle [0 : i32, 1 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %128 = spirv.Bitcast %127 : vector<2xf32> to vector<4xf16>
            %129 = spirv.FMul %126, %128 : vector<4xf16>
            %130 = spirv.VectorShuffle [2 : i32, 3 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %131 = spirv.Bitcast %130 : vector<2xf32> to vector<4xf16>
            %132 = spirv.VectorShuffle [2 : i32, 3 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %133 = spirv.Bitcast %132 : vector<2xf32> to vector<4xf16>
            %134 = spirv.FMul %131, %133 : vector<4xf16>
            %135 = spirv.FAdd %129, %123 : vector<4xf16>
            %136 = spirv.FAdd %134, %124 : vector<4xf16>
            %137 = spirv.VectorShuffle [0 : i32, 1 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %138 = spirv.Bitcast %137 : vector<2xf32> to vector<4xf16>
            %139 = spirv.VectorShuffle [0 : i32, 1 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %140 = spirv.Bitcast %139 : vector<2xf32> to vector<4xf16>
            %141 = spirv.FMul %138, %140 : vector<4xf16>
            %142 = spirv.VectorShuffle [2 : i32, 3 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %143 = spirv.Bitcast %142 : vector<2xf32> to vector<4xf16>
            %144 = spirv.VectorShuffle [2 : i32, 3 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %145 = spirv.Bitcast %144 : vector<2xf32> to vector<4xf16>
            %146 = spirv.FMul %143, %145 : vector<4xf16>
            %147 = spirv.FAdd %141, %135 : vector<4xf16>
            %148 = spirv.FAdd %146, %136 : vector<4xf16>
            %149 = spirv.CompositeExtract %147[0 : i32] : vector<4xf16>
            %150 = spirv.CompositeExtract %147[1 : i32] : vector<4xf16>
            %151 = spirv.CompositeExtract %147[2 : i32] : vector<4xf16>
            %152 = spirv.CompositeExtract %147[3 : i32] : vector<4xf16>
            %153 = spirv.FAdd %149, %150 : f16
            %154 = spirv.FAdd %153, %151 : f16
            %155 = spirv.FAdd %154, %152 : f16
            %156 = spirv.CompositeExtract %148[0 : i32] : vector<4xf16>
            %157 = spirv.CompositeExtract %148[1 : i32] : vector<4xf16>
            %158 = spirv.CompositeExtract %148[2 : i32] : vector<4xf16>
            %159 = spirv.CompositeExtract %148[3 : i32] : vector<4xf16>
            %160 = spirv.FAdd %156, %157 : f16
            %161 = spirv.FAdd %160, %158 : f16
            %162 = spirv.FAdd %161, %159 : f16
            %163 = spirv.FAdd %155, %162 : f16
            %164 = spirv.GroupNonUniformFAdd "Subgroup" "Reduce" %163 : f16
            %165 = spirv.FAdd %164, %cst_f16 : f16
            %166 = spirv.IEqual %1, %cst0_i32 : i32
            spirv.mlir.selection {
              spirv.BranchConditional %166, ^bb1, ^bb2
            ^bb1:  // pred: ^bb0
              %167 = spirv.AccessChain %__resource_var_0_2__addr[%cst0_i32, %3] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>, i32, i32
              spirv.Store "StorageBuffer" %167, %165 : f16
              spirv.Branch ^bb2
            ^bb2:  // 2 preds: ^bb0, ^bb1
              spirv.mlir.merge
            }
            spirv.Return
          }
          spirv.EntryPoint "GLCompute" @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16, @__builtin__LocalInvocationId__, @__builtin__WorkgroupId__
          spirv.ExecutionMode @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 "LocalSize", 64, 1, 1
        }
      }
    }
  }
  func.func private @_main() {
    %c1_i32 = arith.constant 1 : i32
    %c553648144_i32 = arith.constant 553648144 : i32
    %c32000 = arith.constant 32000 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %device = hal.ex.shared_device : !hal.device
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
    %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_2, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
    %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
    %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_9, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %3 = arith.select %_device_query_0, %c0, %c-1 : index
    scf.index_switch %3 
    case 0 {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%1 : !hal.buffer)[%c0, %len], 
        %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
        %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
      ])
      %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
      hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
      scf.yield
    }
    default {
    }
    hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
    %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
    %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_17, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
  util.global.store %value, @_device_query_0 : i1
  util.global.store %ok, @_device_query_0_ok : i1
  util.initializer.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
  util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  %device = hal.ex.shared_device : !hal.device
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %c0 = arith.constant 0 : index
  %c-1 = arith.constant -1 : index
  %device = hal.ex.shared_device : !hal.device
  %0 = arith.select %_device_query_0, %c0, %c-1 : index
  %1 = scf.index_switch %0 -> !hal.executable 
  case 0 {
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    scf.yield %exe : !hal.executable
  }
  default {
    %2 = util.null : !hal.executable
    scf.yield %2 : !hal.executable
  }
  util.global.store %1, @_executable__main_dispatch_0 : !hal.executable
  util.initializer.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func private @_main() {
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %c1_i32 = arith.constant 1 : i32
  %c553648144_i32 = arith.constant 553648144 : i32
  %c32000 = arith.constant 32000 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c-1 = arith.constant -1 : index
  %c-1_i32 = arith.constant -1 : i32
  %c0_i64 = arith.constant 0 : i64
  %c-1_i64 = arith.constant -1 : i64
  %c128000 = arith.constant 128000 : index
  %c0 = arith.constant 0 : index
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %device = hal.ex.shared_device : !hal.device
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
  hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
  %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_2, "failed to wait on timepoint"
  %1 = util.optimization_barrier %transient_buffer : !hal.buffer
  %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
  %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
  %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
  hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
  %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
  %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_9, "failed to wait on timepoint"
  %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
  %len = hal.buffer.length<%1 : !hal.buffer> : index
  %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
  %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
  %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
  %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  %3 = arith.select %_device_query_0, %c0, %c-1 : index
  scf.index_switch %3 
  case 0 {
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%1 : !hal.buffer)[%c0, %len], 
      %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
      %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
    ])
    %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
    hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
    scf.yield
  }
  default {
  }
  hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
  %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
  %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_17, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
  %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
  check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
  return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<SPIRVSubgroupReduce>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  util.global private @_device_query_0 : i1
  util.global private @_device_query_0_ok : i1
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    util.global.store %ok, @_device_query_0_ok : i1
    util.initializer.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.initializer.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.initializer.return
  }
  util.global private @_executable__main_dispatch_0 : !hal.executable
  util.initializer {
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device = hal.ex.shared_device : !hal.device
    %0 = arith.select %_device_query_0, %c0, %c-1 : index
    %1 = scf.index_switch %0 -> !hal.executable 
    case 0 {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      %exe = hal.executable.create device(%device : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
      scf.yield %exe : !hal.executable
    }
    default {
      %2 = util.null : !hal.executable
      scf.yield %2 : !hal.executable
    }
    util.global.store %1, @_executable__main_dispatch_0 : !hal.executable
    util.initializer.return
  }
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.variant public @vulkan_spirv_fb target(#executable_target_vulkan_spirv_fb) {
      hal.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c32000 = arith.constant 32000 : index
        %c1 = arith.constant 1 : index
        hal.return %c32000, %c1, %c1 : index, index, index
      }
      builtin.module attributes {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>} {
        spirv.module Logical GLSL450 requires #spirv.vce<v1.3, [Shader, StorageBuffer16BitAccess, Float16, GroupNonUniformArithmetic], [SPV_KHR_storage_buffer_storage_class, SPV_KHR_16bit_storage]> {
          spirv.GlobalVariable @__builtin__WorkgroupId__ built_in("WorkgroupId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__builtin__LocalInvocationId__ built_in("LocalInvocationId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__resource_var_0_0_ bind(0, 0) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_1_ bind(0, 1) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_2_ bind(0, 2) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
          spirv.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() "None" {
            %cst64_i32 = spirv.Constant 64 : i32
            %cst128_i32 = spirv.Constant 128 : i32
            %cst192_i32 = spirv.Constant 192 : i32
            %cst256_i32 = spirv.Constant 256 : i32
            %cst320_i32 = spirv.Constant 320 : i32
            %cst384_i32 = spirv.Constant 384 : i32
            %cst448_i32 = spirv.Constant 448 : i32
            %cst512_i32 = spirv.Constant 512 : i32
            %cst_vec_4xf16 = spirv.Constant dense<0.000000e+00> : vector<4xf16>
            %cst0_i32 = spirv.Constant 0 : i32
            %cst_f16 = spirv.Constant 0.000000e+00 : f16
            %__builtin__LocalInvocationId___addr = spirv.mlir.addressof @__builtin__LocalInvocationId__ : !spirv.ptr<vector<3xi32>, Input>
            %0 = spirv.Load "Input" %__builtin__LocalInvocationId___addr : vector<3xi32>
            %1 = spirv.CompositeExtract %0[0 : i32] : vector<3xi32>
            %__resource_var_0_0__addr = spirv.mlir.addressof @__resource_var_0_0_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_1__addr = spirv.mlir.addressof @__resource_var_0_1_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_2__addr = spirv.mlir.addressof @__resource_var_0_2_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
            %__builtin__WorkgroupId___addr = spirv.mlir.addressof @__builtin__WorkgroupId__ : !spirv.ptr<vector<3xi32>, Input>
            %2 = spirv.Load "Input" %__builtin__WorkgroupId___addr : vector<3xi32>
            %3 = spirv.CompositeExtract %2[0 : i32] : vector<3xi32>
            %4 = spirv.IMul %3, %cst512_i32 : i32
            %5 = spirv.IAdd %1, %4 : i32
            %6 = spirv.IAdd %5, %cst448_i32 : i32
            %7 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %6] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %8 = spirv.Load "StorageBuffer" %7 : vector<4xf32>
            %9 = spirv.IAdd %1, %cst448_i32 : i32
            %10 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %9] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %11 = spirv.Load "StorageBuffer" %10 : vector<4xf32>
            %12 = spirv.IAdd %5, %cst384_i32 : i32
            %13 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %12] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %14 = spirv.Load "StorageBuffer" %13 : vector<4xf32>
            %15 = spirv.IAdd %1, %cst384_i32 : i32
            %16 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %15] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %17 = spirv.Load "StorageBuffer" %16 : vector<4xf32>
            %18 = spirv.IAdd %5, %cst320_i32 : i32
            %19 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %18] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %20 = spirv.Load "StorageBuffer" %19 : vector<4xf32>
            %21 = spirv.IAdd %1, %cst320_i32 : i32
            %22 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %21] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %23 = spirv.Load "StorageBuffer" %22 : vector<4xf32>
            %24 = spirv.IAdd %5, %cst256_i32 : i32
            %25 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %24] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %26 = spirv.Load "StorageBuffer" %25 : vector<4xf32>
            %27 = spirv.IAdd %1, %cst256_i32 : i32
            %28 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %27] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %29 = spirv.Load "StorageBuffer" %28 : vector<4xf32>
            %30 = spirv.IAdd %5, %cst192_i32 : i32
            %31 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %30] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %32 = spirv.Load "StorageBuffer" %31 : vector<4xf32>
            %33 = spirv.IAdd %1, %cst192_i32 : i32
            %34 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %33] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %35 = spirv.Load "StorageBuffer" %34 : vector<4xf32>
            %36 = spirv.IAdd %5, %cst128_i32 : i32
            %37 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %36] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %38 = spirv.Load "StorageBuffer" %37 : vector<4xf32>
            %39 = spirv.IAdd %1, %cst128_i32 : i32
            %40 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %39] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %41 = spirv.Load "StorageBuffer" %40 : vector<4xf32>
            %42 = spirv.IAdd %5, %cst64_i32 : i32
            %43 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %42] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %44 = spirv.Load "StorageBuffer" %43 : vector<4xf32>
            %45 = spirv.IAdd %1, %cst64_i32 : i32
            %46 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %45] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %47 = spirv.Load "StorageBuffer" %46 : vector<4xf32>
            %48 = spirv.IAdd %4, %1 : i32
            %49 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %48] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %50 = spirv.Load "StorageBuffer" %49 : vector<4xf32>
            %51 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %1] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %52 = spirv.Load "StorageBuffer" %51 : vector<4xf32>
            %53 = spirv.VectorShuffle [0 : i32, 1 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %54 = spirv.Bitcast %53 : vector<2xf32> to vector<4xf16>
            %55 = spirv.VectorShuffle [0 : i32, 1 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %56 = spirv.Bitcast %55 : vector<2xf32> to vector<4xf16>
            %57 = spirv.FMul %54, %56 : vector<4xf16>
            %58 = spirv.VectorShuffle [2 : i32, 3 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %59 = spirv.Bitcast %58 : vector<2xf32> to vector<4xf16>
            %60 = spirv.VectorShuffle [2 : i32, 3 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %61 = spirv.Bitcast %60 : vector<2xf32> to vector<4xf16>
            %62 = spirv.FMul %59, %61 : vector<4xf16>
            %63 = spirv.FAdd %57, %cst_vec_4xf16 : vector<4xf16>
            %64 = spirv.FAdd %62, %cst_vec_4xf16 : vector<4xf16>
            %65 = spirv.VectorShuffle [0 : i32, 1 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %66 = spirv.Bitcast %65 : vector<2xf32> to vector<4xf16>
            %67 = spirv.VectorShuffle [0 : i32, 1 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %68 = spirv.Bitcast %67 : vector<2xf32> to vector<4xf16>
            %69 = spirv.FMul %66, %68 : vector<4xf16>
            %70 = spirv.VectorShuffle [2 : i32, 3 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %71 = spirv.Bitcast %70 : vector<2xf32> to vector<4xf16>
            %72 = spirv.VectorShuffle [2 : i32, 3 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %73 = spirv.Bitcast %72 : vector<2xf32> to vector<4xf16>
            %74 = spirv.FMul %71, %73 : vector<4xf16>
            %75 = spirv.FAdd %69, %63 : vector<4xf16>
            %76 = spirv.FAdd %74, %64 : vector<4xf16>
            %77 = spirv.VectorShuffle [0 : i32, 1 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %78 = spirv.Bitcast %77 : vector<2xf32> to vector<4xf16>
            %79 = spirv.VectorShuffle [0 : i32, 1 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %80 = spirv.Bitcast %79 : vector<2xf32> to vector<4xf16>
            %81 = spirv.FMul %78, %80 : vector<4xf16>
            %82 = spirv.VectorShuffle [2 : i32, 3 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %83 = spirv.Bitcast %82 : vector<2xf32> to vector<4xf16>
            %84 = spirv.VectorShuffle [2 : i32, 3 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %85 = spirv.Bitcast %84 : vector<2xf32> to vector<4xf16>
            %86 = spirv.FMul %83, %85 : vector<4xf16>
            %87 = spirv.FAdd %81, %75 : vector<4xf16>
            %88 = spirv.FAdd %86, %76 : vector<4xf16>
            %89 = spirv.VectorShuffle [0 : i32, 1 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %90 = spirv.Bitcast %89 : vector<2xf32> to vector<4xf16>
            %91 = spirv.VectorShuffle [0 : i32, 1 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %92 = spirv.Bitcast %91 : vector<2xf32> to vector<4xf16>
            %93 = spirv.FMul %90, %92 : vector<4xf16>
            %94 = spirv.VectorShuffle [2 : i32, 3 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %95 = spirv.Bitcast %94 : vector<2xf32> to vector<4xf16>
            %96 = spirv.VectorShuffle [2 : i32, 3 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %97 = spirv.Bitcast %96 : vector<2xf32> to vector<4xf16>
            %98 = spirv.FMul %95, %97 : vector<4xf16>
            %99 = spirv.FAdd %93, %87 : vector<4xf16>
            %100 = spirv.FAdd %98, %88 : vector<4xf16>
            %101 = spirv.VectorShuffle [0 : i32, 1 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %102 = spirv.Bitcast %101 : vector<2xf32> to vector<4xf16>
            %103 = spirv.VectorShuffle [0 : i32, 1 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %104 = spirv.Bitcast %103 : vector<2xf32> to vector<4xf16>
            %105 = spirv.FMul %102, %104 : vector<4xf16>
            %106 = spirv.VectorShuffle [2 : i32, 3 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %107 = spirv.Bitcast %106 : vector<2xf32> to vector<4xf16>
            %108 = spirv.VectorShuffle [2 : i32, 3 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %109 = spirv.Bitcast %108 : vector<2xf32> to vector<4xf16>
            %110 = spirv.FMul %107, %109 : vector<4xf16>
            %111 = spirv.FAdd %105, %99 : vector<4xf16>
            %112 = spirv.FAdd %110, %100 : vector<4xf16>
            %113 = spirv.VectorShuffle [0 : i32, 1 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %114 = spirv.Bitcast %113 : vector<2xf32> to vector<4xf16>
            %115 = spirv.VectorShuffle [0 : i32, 1 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %116 = spirv.Bitcast %115 : vector<2xf32> to vector<4xf16>
            %117 = spirv.FMul %114, %116 : vector<4xf16>
            %118 = spirv.VectorShuffle [2 : i32, 3 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %119 = spirv.Bitcast %118 : vector<2xf32> to vector<4xf16>
            %120 = spirv.VectorShuffle [2 : i32, 3 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %121 = spirv.Bitcast %120 : vector<2xf32> to vector<4xf16>
            %122 = spirv.FMul %119, %121 : vector<4xf16>
            %123 = spirv.FAdd %117, %111 : vector<4xf16>
            %124 = spirv.FAdd %122, %112 : vector<4xf16>
            %125 = spirv.VectorShuffle [0 : i32, 1 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %126 = spirv.Bitcast %125 : vector<2xf32> to vector<4xf16>
            %127 = spirv.VectorShuffle [0 : i32, 1 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %128 = spirv.Bitcast %127 : vector<2xf32> to vector<4xf16>
            %129 = spirv.FMul %126, %128 : vector<4xf16>
            %130 = spirv.VectorShuffle [2 : i32, 3 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %131 = spirv.Bitcast %130 : vector<2xf32> to vector<4xf16>
            %132 = spirv.VectorShuffle [2 : i32, 3 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %133 = spirv.Bitcast %132 : vector<2xf32> to vector<4xf16>
            %134 = spirv.FMul %131, %133 : vector<4xf16>
            %135 = spirv.FAdd %129, %123 : vector<4xf16>
            %136 = spirv.FAdd %134, %124 : vector<4xf16>
            %137 = spirv.VectorShuffle [0 : i32, 1 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %138 = spirv.Bitcast %137 : vector<2xf32> to vector<4xf16>
            %139 = spirv.VectorShuffle [0 : i32, 1 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %140 = spirv.Bitcast %139 : vector<2xf32> to vector<4xf16>
            %141 = spirv.FMul %138, %140 : vector<4xf16>
            %142 = spirv.VectorShuffle [2 : i32, 3 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %143 = spirv.Bitcast %142 : vector<2xf32> to vector<4xf16>
            %144 = spirv.VectorShuffle [2 : i32, 3 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %145 = spirv.Bitcast %144 : vector<2xf32> to vector<4xf16>
            %146 = spirv.FMul %143, %145 : vector<4xf16>
            %147 = spirv.FAdd %141, %135 : vector<4xf16>
            %148 = spirv.FAdd %146, %136 : vector<4xf16>
            %149 = spirv.CompositeExtract %147[0 : i32] : vector<4xf16>
            %150 = spirv.CompositeExtract %147[1 : i32] : vector<4xf16>
            %151 = spirv.CompositeExtract %147[2 : i32] : vector<4xf16>
            %152 = spirv.CompositeExtract %147[3 : i32] : vector<4xf16>
            %153 = spirv.FAdd %149, %150 : f16
            %154 = spirv.FAdd %153, %151 : f16
            %155 = spirv.FAdd %154, %152 : f16
            %156 = spirv.CompositeExtract %148[0 : i32] : vector<4xf16>
            %157 = spirv.CompositeExtract %148[1 : i32] : vector<4xf16>
            %158 = spirv.CompositeExtract %148[2 : i32] : vector<4xf16>
            %159 = spirv.CompositeExtract %148[3 : i32] : vector<4xf16>
            %160 = spirv.FAdd %156, %157 : f16
            %161 = spirv.FAdd %160, %158 : f16
            %162 = spirv.FAdd %161, %159 : f16
            %163 = spirv.FAdd %155, %162 : f16
            %164 = spirv.GroupNonUniformFAdd "Subgroup" "Reduce" %163 : f16
            %165 = spirv.FAdd %164, %cst_f16 : f16
            %166 = spirv.IEqual %1, %cst0_i32 : i32
            spirv.mlir.selection {
              spirv.BranchConditional %166, ^bb1, ^bb2
            ^bb1:  // pred: ^bb0
              %167 = spirv.AccessChain %__resource_var_0_2__addr[%cst0_i32, %3] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>, i32, i32
              spirv.Store "StorageBuffer" %167, %165 : f16
              spirv.Branch ^bb2
            ^bb2:  // 2 preds: ^bb0, ^bb1
              spirv.mlir.merge
            }
            spirv.Return
          }
          spirv.EntryPoint "GLCompute" @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16, @__builtin__LocalInvocationId__, @__builtin__WorkgroupId__
          spirv.ExecutionMode @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 "LocalSize", 64, 1, 1
        }
      }
    }
  }
  func.func private @_main() {
    %c8192 = arith.constant 8192 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c262144000 = arith.constant 262144000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c64000 = arith.constant 64000 : index
    %c0 = arith.constant 0 : index
    %c128000 = arith.constant 128000 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c32000 = arith.constant 32000 : index
    %c553648144_i32 = arith.constant 553648144 : i32
    %c1_i32 = arith.constant 1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device = hal.ex.shared_device : !hal.device
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
    %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_2, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
    %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
    %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_9, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %3 = arith.select %_device_query_0, %c0, %c-1 : index
    scf.index_switch %3 
    case 0 {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%1 : !hal.buffer)[%c0, %len], 
        %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
        %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
      ])
      %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
      hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
      scf.yield
    }
    default {
    }
    hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
    %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
    %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_17, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<SPIRVSubgroupReduce>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  util.global private @_device_query_0 : i1
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    util.initializer.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.initializer.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.initializer.return
  }
  util.global private @_executable__main_dispatch_0 : !hal.executable
  util.initializer {
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device = hal.ex.shared_device : !hal.device
    %0 = arith.select %_device_query_0, %c0, %c-1 : index
    %1 = scf.index_switch %0 -> !hal.executable 
    case 0 {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      %exe = hal.executable.create device(%device : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
      scf.yield %exe : !hal.executable
    }
    default {
      %2 = util.null : !hal.executable
      scf.yield %2 : !hal.executable
    }
    util.global.store %1, @_executable__main_dispatch_0 : !hal.executable
    util.initializer.return
  }
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.variant public @vulkan_spirv_fb target(#executable_target_vulkan_spirv_fb) {
      hal.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c32000 = arith.constant 32000 : index
        %c1 = arith.constant 1 : index
        hal.return %c32000, %c1, %c1 : index, index, index
      }
      builtin.module attributes {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>} {
        spirv.module Logical GLSL450 requires #spirv.vce<v1.3, [Shader, StorageBuffer16BitAccess, Float16, GroupNonUniformArithmetic], [SPV_KHR_storage_buffer_storage_class, SPV_KHR_16bit_storage]> {
          spirv.GlobalVariable @__builtin__WorkgroupId__ built_in("WorkgroupId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__builtin__LocalInvocationId__ built_in("LocalInvocationId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__resource_var_0_0_ bind(0, 0) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_1_ bind(0, 1) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_2_ bind(0, 2) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
          spirv.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() "None" {
            %cst64_i32 = spirv.Constant 64 : i32
            %cst128_i32 = spirv.Constant 128 : i32
            %cst192_i32 = spirv.Constant 192 : i32
            %cst256_i32 = spirv.Constant 256 : i32
            %cst320_i32 = spirv.Constant 320 : i32
            %cst384_i32 = spirv.Constant 384 : i32
            %cst448_i32 = spirv.Constant 448 : i32
            %cst512_i32 = spirv.Constant 512 : i32
            %cst_vec_4xf16 = spirv.Constant dense<0.000000e+00> : vector<4xf16>
            %cst0_i32 = spirv.Constant 0 : i32
            %cst_f16 = spirv.Constant 0.000000e+00 : f16
            %__builtin__LocalInvocationId___addr = spirv.mlir.addressof @__builtin__LocalInvocationId__ : !spirv.ptr<vector<3xi32>, Input>
            %0 = spirv.Load "Input" %__builtin__LocalInvocationId___addr : vector<3xi32>
            %1 = spirv.CompositeExtract %0[0 : i32] : vector<3xi32>
            %__resource_var_0_0__addr = spirv.mlir.addressof @__resource_var_0_0_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_1__addr = spirv.mlir.addressof @__resource_var_0_1_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_2__addr = spirv.mlir.addressof @__resource_var_0_2_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
            %__builtin__WorkgroupId___addr = spirv.mlir.addressof @__builtin__WorkgroupId__ : !spirv.ptr<vector<3xi32>, Input>
            %2 = spirv.Load "Input" %__builtin__WorkgroupId___addr : vector<3xi32>
            %3 = spirv.CompositeExtract %2[0 : i32] : vector<3xi32>
            %4 = spirv.IMul %3, %cst512_i32 : i32
            %5 = spirv.IAdd %1, %4 : i32
            %6 = spirv.IAdd %5, %cst448_i32 : i32
            %7 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %6] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %8 = spirv.Load "StorageBuffer" %7 : vector<4xf32>
            %9 = spirv.IAdd %1, %cst448_i32 : i32
            %10 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %9] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %11 = spirv.Load "StorageBuffer" %10 : vector<4xf32>
            %12 = spirv.IAdd %5, %cst384_i32 : i32
            %13 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %12] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %14 = spirv.Load "StorageBuffer" %13 : vector<4xf32>
            %15 = spirv.IAdd %1, %cst384_i32 : i32
            %16 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %15] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %17 = spirv.Load "StorageBuffer" %16 : vector<4xf32>
            %18 = spirv.IAdd %5, %cst320_i32 : i32
            %19 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %18] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %20 = spirv.Load "StorageBuffer" %19 : vector<4xf32>
            %21 = spirv.IAdd %1, %cst320_i32 : i32
            %22 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %21] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %23 = spirv.Load "StorageBuffer" %22 : vector<4xf32>
            %24 = spirv.IAdd %5, %cst256_i32 : i32
            %25 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %24] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %26 = spirv.Load "StorageBuffer" %25 : vector<4xf32>
            %27 = spirv.IAdd %1, %cst256_i32 : i32
            %28 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %27] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %29 = spirv.Load "StorageBuffer" %28 : vector<4xf32>
            %30 = spirv.IAdd %5, %cst192_i32 : i32
            %31 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %30] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %32 = spirv.Load "StorageBuffer" %31 : vector<4xf32>
            %33 = spirv.IAdd %1, %cst192_i32 : i32
            %34 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %33] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %35 = spirv.Load "StorageBuffer" %34 : vector<4xf32>
            %36 = spirv.IAdd %5, %cst128_i32 : i32
            %37 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %36] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %38 = spirv.Load "StorageBuffer" %37 : vector<4xf32>
            %39 = spirv.IAdd %1, %cst128_i32 : i32
            %40 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %39] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %41 = spirv.Load "StorageBuffer" %40 : vector<4xf32>
            %42 = spirv.IAdd %5, %cst64_i32 : i32
            %43 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %42] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %44 = spirv.Load "StorageBuffer" %43 : vector<4xf32>
            %45 = spirv.IAdd %1, %cst64_i32 : i32
            %46 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %45] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %47 = spirv.Load "StorageBuffer" %46 : vector<4xf32>
            %48 = spirv.IAdd %4, %1 : i32
            %49 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %48] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %50 = spirv.Load "StorageBuffer" %49 : vector<4xf32>
            %51 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %1] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %52 = spirv.Load "StorageBuffer" %51 : vector<4xf32>
            %53 = spirv.VectorShuffle [0 : i32, 1 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %54 = spirv.Bitcast %53 : vector<2xf32> to vector<4xf16>
            %55 = spirv.VectorShuffle [0 : i32, 1 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %56 = spirv.Bitcast %55 : vector<2xf32> to vector<4xf16>
            %57 = spirv.FMul %54, %56 : vector<4xf16>
            %58 = spirv.VectorShuffle [2 : i32, 3 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %59 = spirv.Bitcast %58 : vector<2xf32> to vector<4xf16>
            %60 = spirv.VectorShuffle [2 : i32, 3 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %61 = spirv.Bitcast %60 : vector<2xf32> to vector<4xf16>
            %62 = spirv.FMul %59, %61 : vector<4xf16>
            %63 = spirv.FAdd %57, %cst_vec_4xf16 : vector<4xf16>
            %64 = spirv.FAdd %62, %cst_vec_4xf16 : vector<4xf16>
            %65 = spirv.VectorShuffle [0 : i32, 1 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %66 = spirv.Bitcast %65 : vector<2xf32> to vector<4xf16>
            %67 = spirv.VectorShuffle [0 : i32, 1 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %68 = spirv.Bitcast %67 : vector<2xf32> to vector<4xf16>
            %69 = spirv.FMul %66, %68 : vector<4xf16>
            %70 = spirv.VectorShuffle [2 : i32, 3 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %71 = spirv.Bitcast %70 : vector<2xf32> to vector<4xf16>
            %72 = spirv.VectorShuffle [2 : i32, 3 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %73 = spirv.Bitcast %72 : vector<2xf32> to vector<4xf16>
            %74 = spirv.FMul %71, %73 : vector<4xf16>
            %75 = spirv.FAdd %69, %63 : vector<4xf16>
            %76 = spirv.FAdd %74, %64 : vector<4xf16>
            %77 = spirv.VectorShuffle [0 : i32, 1 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %78 = spirv.Bitcast %77 : vector<2xf32> to vector<4xf16>
            %79 = spirv.VectorShuffle [0 : i32, 1 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %80 = spirv.Bitcast %79 : vector<2xf32> to vector<4xf16>
            %81 = spirv.FMul %78, %80 : vector<4xf16>
            %82 = spirv.VectorShuffle [2 : i32, 3 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %83 = spirv.Bitcast %82 : vector<2xf32> to vector<4xf16>
            %84 = spirv.VectorShuffle [2 : i32, 3 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %85 = spirv.Bitcast %84 : vector<2xf32> to vector<4xf16>
            %86 = spirv.FMul %83, %85 : vector<4xf16>
            %87 = spirv.FAdd %81, %75 : vector<4xf16>
            %88 = spirv.FAdd %86, %76 : vector<4xf16>
            %89 = spirv.VectorShuffle [0 : i32, 1 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %90 = spirv.Bitcast %89 : vector<2xf32> to vector<4xf16>
            %91 = spirv.VectorShuffle [0 : i32, 1 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %92 = spirv.Bitcast %91 : vector<2xf32> to vector<4xf16>
            %93 = spirv.FMul %90, %92 : vector<4xf16>
            %94 = spirv.VectorShuffle [2 : i32, 3 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %95 = spirv.Bitcast %94 : vector<2xf32> to vector<4xf16>
            %96 = spirv.VectorShuffle [2 : i32, 3 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %97 = spirv.Bitcast %96 : vector<2xf32> to vector<4xf16>
            %98 = spirv.FMul %95, %97 : vector<4xf16>
            %99 = spirv.FAdd %93, %87 : vector<4xf16>
            %100 = spirv.FAdd %98, %88 : vector<4xf16>
            %101 = spirv.VectorShuffle [0 : i32, 1 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %102 = spirv.Bitcast %101 : vector<2xf32> to vector<4xf16>
            %103 = spirv.VectorShuffle [0 : i32, 1 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %104 = spirv.Bitcast %103 : vector<2xf32> to vector<4xf16>
            %105 = spirv.FMul %102, %104 : vector<4xf16>
            %106 = spirv.VectorShuffle [2 : i32, 3 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %107 = spirv.Bitcast %106 : vector<2xf32> to vector<4xf16>
            %108 = spirv.VectorShuffle [2 : i32, 3 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %109 = spirv.Bitcast %108 : vector<2xf32> to vector<4xf16>
            %110 = spirv.FMul %107, %109 : vector<4xf16>
            %111 = spirv.FAdd %105, %99 : vector<4xf16>
            %112 = spirv.FAdd %110, %100 : vector<4xf16>
            %113 = spirv.VectorShuffle [0 : i32, 1 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %114 = spirv.Bitcast %113 : vector<2xf32> to vector<4xf16>
            %115 = spirv.VectorShuffle [0 : i32, 1 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %116 = spirv.Bitcast %115 : vector<2xf32> to vector<4xf16>
            %117 = spirv.FMul %114, %116 : vector<4xf16>
            %118 = spirv.VectorShuffle [2 : i32, 3 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %119 = spirv.Bitcast %118 : vector<2xf32> to vector<4xf16>
            %120 = spirv.VectorShuffle [2 : i32, 3 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %121 = spirv.Bitcast %120 : vector<2xf32> to vector<4xf16>
            %122 = spirv.FMul %119, %121 : vector<4xf16>
            %123 = spirv.FAdd %117, %111 : vector<4xf16>
            %124 = spirv.FAdd %122, %112 : vector<4xf16>
            %125 = spirv.VectorShuffle [0 : i32, 1 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %126 = spirv.Bitcast %125 : vector<2xf32> to vector<4xf16>
            %127 = spirv.VectorShuffle [0 : i32, 1 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %128 = spirv.Bitcast %127 : vector<2xf32> to vector<4xf16>
            %129 = spirv.FMul %126, %128 : vector<4xf16>
            %130 = spirv.VectorShuffle [2 : i32, 3 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %131 = spirv.Bitcast %130 : vector<2xf32> to vector<4xf16>
            %132 = spirv.VectorShuffle [2 : i32, 3 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %133 = spirv.Bitcast %132 : vector<2xf32> to vector<4xf16>
            %134 = spirv.FMul %131, %133 : vector<4xf16>
            %135 = spirv.FAdd %129, %123 : vector<4xf16>
            %136 = spirv.FAdd %134, %124 : vector<4xf16>
            %137 = spirv.VectorShuffle [0 : i32, 1 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %138 = spirv.Bitcast %137 : vector<2xf32> to vector<4xf16>
            %139 = spirv.VectorShuffle [0 : i32, 1 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %140 = spirv.Bitcast %139 : vector<2xf32> to vector<4xf16>
            %141 = spirv.FMul %138, %140 : vector<4xf16>
            %142 = spirv.VectorShuffle [2 : i32, 3 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %143 = spirv.Bitcast %142 : vector<2xf32> to vector<4xf16>
            %144 = spirv.VectorShuffle [2 : i32, 3 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %145 = spirv.Bitcast %144 : vector<2xf32> to vector<4xf16>
            %146 = spirv.FMul %143, %145 : vector<4xf16>
            %147 = spirv.FAdd %141, %135 : vector<4xf16>
            %148 = spirv.FAdd %146, %136 : vector<4xf16>
            %149 = spirv.CompositeExtract %147[0 : i32] : vector<4xf16>
            %150 = spirv.CompositeExtract %147[1 : i32] : vector<4xf16>
            %151 = spirv.CompositeExtract %147[2 : i32] : vector<4xf16>
            %152 = spirv.CompositeExtract %147[3 : i32] : vector<4xf16>
            %153 = spirv.FAdd %149, %150 : f16
            %154 = spirv.FAdd %153, %151 : f16
            %155 = spirv.FAdd %154, %152 : f16
            %156 = spirv.CompositeExtract %148[0 : i32] : vector<4xf16>
            %157 = spirv.CompositeExtract %148[1 : i32] : vector<4xf16>
            %158 = spirv.CompositeExtract %148[2 : i32] : vector<4xf16>
            %159 = spirv.CompositeExtract %148[3 : i32] : vector<4xf16>
            %160 = spirv.FAdd %156, %157 : f16
            %161 = spirv.FAdd %160, %158 : f16
            %162 = spirv.FAdd %161, %159 : f16
            %163 = spirv.FAdd %155, %162 : f16
            %164 = spirv.GroupNonUniformFAdd "Subgroup" "Reduce" %163 : f16
            %165 = spirv.FAdd %164, %cst_f16 : f16
            %166 = spirv.IEqual %1, %cst0_i32 : i32
            spirv.mlir.selection {
              spirv.BranchConditional %166, ^bb1, ^bb2
            ^bb1:  // pred: ^bb0
              %167 = spirv.AccessChain %__resource_var_0_2__addr[%cst0_i32, %3] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>, i32, i32
              spirv.Store "StorageBuffer" %167, %165 : f16
              spirv.Branch ^bb2
            ^bb2:  // 2 preds: ^bb0, ^bb1
              spirv.mlir.merge
            }
            spirv.Return
          }
          spirv.EntryPoint "GLCompute" @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16, @__builtin__LocalInvocationId__, @__builtin__WorkgroupId__
          spirv.ExecutionMode @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 "LocalSize", 64, 1, 1
        }
      }
    }
  }
  func.func private @_main() {
    %c8192 = arith.constant 8192 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c262144000 = arith.constant 262144000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c64000 = arith.constant 64000 : index
    %c0 = arith.constant 0 : index
    %c128000 = arith.constant 128000 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c32000 = arith.constant 32000 : index
    %c553648144_i32 = arith.constant 553648144 : i32
    %c1_i32 = arith.constant 1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device = hal.ex.shared_device : !hal.device
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
    %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_2, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
    %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
    %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_9, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %3 = arith.select %_device_query_0, %c0, %c-1 : index
    scf.index_switch %3 
    case 0 {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%1 : !hal.buffer)[%c0, %len], 
        %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
        %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
      ])
      %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
      hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
      scf.yield
    }
    default {
    }
    hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
    %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
    %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_17, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<SPIRVSubgroupReduce>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  util.global private @_device_query_0 : i1
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    util.initializer.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.initializer.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.initializer.return
  }
  util.global private @_executable__main_dispatch_0 : !hal.executable
  util.initializer {
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device = hal.ex.shared_device : !hal.device
    %0 = arith.select %_device_query_0, %c0, %c-1 : index
    %1 = scf.index_switch %0 -> !hal.executable 
    case 0 {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      %exe = hal.executable.create device(%device : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
      scf.yield %exe : !hal.executable
    }
    default {
      %2 = util.null : !hal.executable
      scf.yield %2 : !hal.executable
    }
    util.global.store %1, @_executable__main_dispatch_0 : !hal.executable
    util.initializer.return
  }
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.variant public @vulkan_spirv_fb target(#executable_target_vulkan_spirv_fb) {
      hal.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c32000 = arith.constant 32000 : index
        %c1 = arith.constant 1 : index
        hal.return %c32000, %c1, %c1 : index, index, index
      }
      builtin.module attributes {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>} {
        spirv.module Logical GLSL450 requires #spirv.vce<v1.3, [Shader, StorageBuffer16BitAccess, Float16, GroupNonUniformArithmetic], [SPV_KHR_storage_buffer_storage_class, SPV_KHR_16bit_storage]> {
          spirv.GlobalVariable @__builtin__WorkgroupId__ built_in("WorkgroupId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__builtin__LocalInvocationId__ built_in("LocalInvocationId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__resource_var_0_0_ bind(0, 0) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_1_ bind(0, 1) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_2_ bind(0, 2) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
          spirv.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() "None" {
            %cst64_i32 = spirv.Constant 64 : i32
            %cst128_i32 = spirv.Constant 128 : i32
            %cst192_i32 = spirv.Constant 192 : i32
            %cst256_i32 = spirv.Constant 256 : i32
            %cst320_i32 = spirv.Constant 320 : i32
            %cst384_i32 = spirv.Constant 384 : i32
            %cst448_i32 = spirv.Constant 448 : i32
            %cst512_i32 = spirv.Constant 512 : i32
            %cst_vec_4xf16 = spirv.Constant dense<0.000000e+00> : vector<4xf16>
            %cst0_i32 = spirv.Constant 0 : i32
            %cst_f16 = spirv.Constant 0.000000e+00 : f16
            %__builtin__LocalInvocationId___addr = spirv.mlir.addressof @__builtin__LocalInvocationId__ : !spirv.ptr<vector<3xi32>, Input>
            %0 = spirv.Load "Input" %__builtin__LocalInvocationId___addr : vector<3xi32>
            %1 = spirv.CompositeExtract %0[0 : i32] : vector<3xi32>
            %__resource_var_0_0__addr = spirv.mlir.addressof @__resource_var_0_0_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_1__addr = spirv.mlir.addressof @__resource_var_0_1_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_2__addr = spirv.mlir.addressof @__resource_var_0_2_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
            %__builtin__WorkgroupId___addr = spirv.mlir.addressof @__builtin__WorkgroupId__ : !spirv.ptr<vector<3xi32>, Input>
            %2 = spirv.Load "Input" %__builtin__WorkgroupId___addr : vector<3xi32>
            %3 = spirv.CompositeExtract %2[0 : i32] : vector<3xi32>
            %4 = spirv.IMul %3, %cst512_i32 : i32
            %5 = spirv.IAdd %1, %4 : i32
            %6 = spirv.IAdd %5, %cst448_i32 : i32
            %7 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %6] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %8 = spirv.Load "StorageBuffer" %7 : vector<4xf32>
            %9 = spirv.IAdd %1, %cst448_i32 : i32
            %10 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %9] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %11 = spirv.Load "StorageBuffer" %10 : vector<4xf32>
            %12 = spirv.IAdd %5, %cst384_i32 : i32
            %13 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %12] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %14 = spirv.Load "StorageBuffer" %13 : vector<4xf32>
            %15 = spirv.IAdd %1, %cst384_i32 : i32
            %16 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %15] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %17 = spirv.Load "StorageBuffer" %16 : vector<4xf32>
            %18 = spirv.IAdd %5, %cst320_i32 : i32
            %19 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %18] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %20 = spirv.Load "StorageBuffer" %19 : vector<4xf32>
            %21 = spirv.IAdd %1, %cst320_i32 : i32
            %22 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %21] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %23 = spirv.Load "StorageBuffer" %22 : vector<4xf32>
            %24 = spirv.IAdd %5, %cst256_i32 : i32
            %25 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %24] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %26 = spirv.Load "StorageBuffer" %25 : vector<4xf32>
            %27 = spirv.IAdd %1, %cst256_i32 : i32
            %28 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %27] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %29 = spirv.Load "StorageBuffer" %28 : vector<4xf32>
            %30 = spirv.IAdd %5, %cst192_i32 : i32
            %31 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %30] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %32 = spirv.Load "StorageBuffer" %31 : vector<4xf32>
            %33 = spirv.IAdd %1, %cst192_i32 : i32
            %34 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %33] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %35 = spirv.Load "StorageBuffer" %34 : vector<4xf32>
            %36 = spirv.IAdd %5, %cst128_i32 : i32
            %37 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %36] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %38 = spirv.Load "StorageBuffer" %37 : vector<4xf32>
            %39 = spirv.IAdd %1, %cst128_i32 : i32
            %40 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %39] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %41 = spirv.Load "StorageBuffer" %40 : vector<4xf32>
            %42 = spirv.IAdd %5, %cst64_i32 : i32
            %43 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %42] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %44 = spirv.Load "StorageBuffer" %43 : vector<4xf32>
            %45 = spirv.IAdd %1, %cst64_i32 : i32
            %46 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %45] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %47 = spirv.Load "StorageBuffer" %46 : vector<4xf32>
            %48 = spirv.IAdd %4, %1 : i32
            %49 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %48] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %50 = spirv.Load "StorageBuffer" %49 : vector<4xf32>
            %51 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %1] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %52 = spirv.Load "StorageBuffer" %51 : vector<4xf32>
            %53 = spirv.VectorShuffle [0 : i32, 1 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %54 = spirv.Bitcast %53 : vector<2xf32> to vector<4xf16>
            %55 = spirv.VectorShuffle [0 : i32, 1 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %56 = spirv.Bitcast %55 : vector<2xf32> to vector<4xf16>
            %57 = spirv.FMul %54, %56 : vector<4xf16>
            %58 = spirv.VectorShuffle [2 : i32, 3 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %59 = spirv.Bitcast %58 : vector<2xf32> to vector<4xf16>
            %60 = spirv.VectorShuffle [2 : i32, 3 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %61 = spirv.Bitcast %60 : vector<2xf32> to vector<4xf16>
            %62 = spirv.FMul %59, %61 : vector<4xf16>
            %63 = spirv.FAdd %57, %cst_vec_4xf16 : vector<4xf16>
            %64 = spirv.FAdd %62, %cst_vec_4xf16 : vector<4xf16>
            %65 = spirv.VectorShuffle [0 : i32, 1 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %66 = spirv.Bitcast %65 : vector<2xf32> to vector<4xf16>
            %67 = spirv.VectorShuffle [0 : i32, 1 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %68 = spirv.Bitcast %67 : vector<2xf32> to vector<4xf16>
            %69 = spirv.FMul %66, %68 : vector<4xf16>
            %70 = spirv.VectorShuffle [2 : i32, 3 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %71 = spirv.Bitcast %70 : vector<2xf32> to vector<4xf16>
            %72 = spirv.VectorShuffle [2 : i32, 3 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %73 = spirv.Bitcast %72 : vector<2xf32> to vector<4xf16>
            %74 = spirv.FMul %71, %73 : vector<4xf16>
            %75 = spirv.FAdd %69, %63 : vector<4xf16>
            %76 = spirv.FAdd %74, %64 : vector<4xf16>
            %77 = spirv.VectorShuffle [0 : i32, 1 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %78 = spirv.Bitcast %77 : vector<2xf32> to vector<4xf16>
            %79 = spirv.VectorShuffle [0 : i32, 1 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %80 = spirv.Bitcast %79 : vector<2xf32> to vector<4xf16>
            %81 = spirv.FMul %78, %80 : vector<4xf16>
            %82 = spirv.VectorShuffle [2 : i32, 3 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %83 = spirv.Bitcast %82 : vector<2xf32> to vector<4xf16>
            %84 = spirv.VectorShuffle [2 : i32, 3 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %85 = spirv.Bitcast %84 : vector<2xf32> to vector<4xf16>
            %86 = spirv.FMul %83, %85 : vector<4xf16>
            %87 = spirv.FAdd %81, %75 : vector<4xf16>
            %88 = spirv.FAdd %86, %76 : vector<4xf16>
            %89 = spirv.VectorShuffle [0 : i32, 1 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %90 = spirv.Bitcast %89 : vector<2xf32> to vector<4xf16>
            %91 = spirv.VectorShuffle [0 : i32, 1 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %92 = spirv.Bitcast %91 : vector<2xf32> to vector<4xf16>
            %93 = spirv.FMul %90, %92 : vector<4xf16>
            %94 = spirv.VectorShuffle [2 : i32, 3 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %95 = spirv.Bitcast %94 : vector<2xf32> to vector<4xf16>
            %96 = spirv.VectorShuffle [2 : i32, 3 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %97 = spirv.Bitcast %96 : vector<2xf32> to vector<4xf16>
            %98 = spirv.FMul %95, %97 : vector<4xf16>
            %99 = spirv.FAdd %93, %87 : vector<4xf16>
            %100 = spirv.FAdd %98, %88 : vector<4xf16>
            %101 = spirv.VectorShuffle [0 : i32, 1 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %102 = spirv.Bitcast %101 : vector<2xf32> to vector<4xf16>
            %103 = spirv.VectorShuffle [0 : i32, 1 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %104 = spirv.Bitcast %103 : vector<2xf32> to vector<4xf16>
            %105 = spirv.FMul %102, %104 : vector<4xf16>
            %106 = spirv.VectorShuffle [2 : i32, 3 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %107 = spirv.Bitcast %106 : vector<2xf32> to vector<4xf16>
            %108 = spirv.VectorShuffle [2 : i32, 3 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %109 = spirv.Bitcast %108 : vector<2xf32> to vector<4xf16>
            %110 = spirv.FMul %107, %109 : vector<4xf16>
            %111 = spirv.FAdd %105, %99 : vector<4xf16>
            %112 = spirv.FAdd %110, %100 : vector<4xf16>
            %113 = spirv.VectorShuffle [0 : i32, 1 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %114 = spirv.Bitcast %113 : vector<2xf32> to vector<4xf16>
            %115 = spirv.VectorShuffle [0 : i32, 1 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %116 = spirv.Bitcast %115 : vector<2xf32> to vector<4xf16>
            %117 = spirv.FMul %114, %116 : vector<4xf16>
            %118 = spirv.VectorShuffle [2 : i32, 3 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %119 = spirv.Bitcast %118 : vector<2xf32> to vector<4xf16>
            %120 = spirv.VectorShuffle [2 : i32, 3 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %121 = spirv.Bitcast %120 : vector<2xf32> to vector<4xf16>
            %122 = spirv.FMul %119, %121 : vector<4xf16>
            %123 = spirv.FAdd %117, %111 : vector<4xf16>
            %124 = spirv.FAdd %122, %112 : vector<4xf16>
            %125 = spirv.VectorShuffle [0 : i32, 1 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %126 = spirv.Bitcast %125 : vector<2xf32> to vector<4xf16>
            %127 = spirv.VectorShuffle [0 : i32, 1 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %128 = spirv.Bitcast %127 : vector<2xf32> to vector<4xf16>
            %129 = spirv.FMul %126, %128 : vector<4xf16>
            %130 = spirv.VectorShuffle [2 : i32, 3 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %131 = spirv.Bitcast %130 : vector<2xf32> to vector<4xf16>
            %132 = spirv.VectorShuffle [2 : i32, 3 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %133 = spirv.Bitcast %132 : vector<2xf32> to vector<4xf16>
            %134 = spirv.FMul %131, %133 : vector<4xf16>
            %135 = spirv.FAdd %129, %123 : vector<4xf16>
            %136 = spirv.FAdd %134, %124 : vector<4xf16>
            %137 = spirv.VectorShuffle [0 : i32, 1 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %138 = spirv.Bitcast %137 : vector<2xf32> to vector<4xf16>
            %139 = spirv.VectorShuffle [0 : i32, 1 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %140 = spirv.Bitcast %139 : vector<2xf32> to vector<4xf16>
            %141 = spirv.FMul %138, %140 : vector<4xf16>
            %142 = spirv.VectorShuffle [2 : i32, 3 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %143 = spirv.Bitcast %142 : vector<2xf32> to vector<4xf16>
            %144 = spirv.VectorShuffle [2 : i32, 3 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %145 = spirv.Bitcast %144 : vector<2xf32> to vector<4xf16>
            %146 = spirv.FMul %143, %145 : vector<4xf16>
            %147 = spirv.FAdd %141, %135 : vector<4xf16>
            %148 = spirv.FAdd %146, %136 : vector<4xf16>
            %149 = spirv.CompositeExtract %147[0 : i32] : vector<4xf16>
            %150 = spirv.CompositeExtract %147[1 : i32] : vector<4xf16>
            %151 = spirv.CompositeExtract %147[2 : i32] : vector<4xf16>
            %152 = spirv.CompositeExtract %147[3 : i32] : vector<4xf16>
            %153 = spirv.FAdd %149, %150 : f16
            %154 = spirv.FAdd %153, %151 : f16
            %155 = spirv.FAdd %154, %152 : f16
            %156 = spirv.CompositeExtract %148[0 : i32] : vector<4xf16>
            %157 = spirv.CompositeExtract %148[1 : i32] : vector<4xf16>
            %158 = spirv.CompositeExtract %148[2 : i32] : vector<4xf16>
            %159 = spirv.CompositeExtract %148[3 : i32] : vector<4xf16>
            %160 = spirv.FAdd %156, %157 : f16
            %161 = spirv.FAdd %160, %158 : f16
            %162 = spirv.FAdd %161, %159 : f16
            %163 = spirv.FAdd %155, %162 : f16
            %164 = spirv.GroupNonUniformFAdd "Subgroup" "Reduce" %163 : f16
            %165 = spirv.FAdd %164, %cst_f16 : f16
            %166 = spirv.IEqual %1, %cst0_i32 : i32
            spirv.mlir.selection {
              spirv.BranchConditional %166, ^bb1, ^bb2
            ^bb1:  // pred: ^bb0
              %167 = spirv.AccessChain %__resource_var_0_2__addr[%cst0_i32, %3] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>, i32, i32
              spirv.Store "StorageBuffer" %167, %165 : f16
              spirv.Branch ^bb2
            ^bb2:  // 2 preds: ^bb0, ^bb1
              spirv.mlir.merge
            }
            spirv.Return
          }
          spirv.EntryPoint "GLCompute" @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16, @__builtin__LocalInvocationId__, @__builtin__WorkgroupId__
          spirv.ExecutionMode @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 "LocalSize", 64, 1, 1
        }
      }
    }
  }
  func.func private @_main() {
    %c8192 = arith.constant 8192 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c262144000 = arith.constant 262144000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c64000 = arith.constant 64000 : index
    %c0 = arith.constant 0 : index
    %c128000 = arith.constant 128000 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c32000 = arith.constant 32000 : index
    %c553648144_i32 = arith.constant 553648144 : i32
    %c1_i32 = arith.constant 1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device = hal.ex.shared_device : !hal.device
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
    %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_2, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
    %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
    %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_9, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %3 = arith.select %_device_query_0, %c0, %c-1 : index
    scf.index_switch %3 
    case 0 {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%1 : !hal.buffer)[%c0, %len], 
        %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
        %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
      ])
      %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
      hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
      scf.yield
    }
    default {
    }
    hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
    %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
    %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_17, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::ElideRedundantCommandsPass (iree-hal-elide-redundant-commands) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
  util.global.store %value, @_device_query_0 : i1
  util.initializer.return
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::ElideRedundantCommandsPass (iree-hal-elide-redundant-commands) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
  util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer.return
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::ElideRedundantCommandsPass (iree-hal-elide-redundant-commands) //----- //
util.initializer {
  %c-1 = arith.constant -1 : index
  %c0 = arith.constant 0 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %device = hal.ex.shared_device : !hal.device
  %0 = arith.select %_device_query_0, %c0, %c-1 : index
  %1 = scf.index_switch %0 -> !hal.executable 
  case 0 {
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    scf.yield %exe : !hal.executable
  }
  default {
    %2 = util.null : !hal.executable
    scf.yield %2 : !hal.executable
  }
  util.global.store %1, @_executable__main_dispatch_0 : !hal.executable
  util.initializer.return
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::ElideRedundantCommandsPass (iree-hal-elide-redundant-commands) //----- //
util.initializer {
  %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  %device = hal.ex.shared_device : !hal.device
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer.return
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::ElideRedundantCommandsPass (iree-hal-elide-redundant-commands) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::ElideRedundantCommandsPass (iree-hal-elide-redundant-commands) //----- //
func.func private @_main() {
  %c8192 = arith.constant 8192 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c262144000 = arith.constant 262144000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c64000 = arith.constant 64000 : index
  %c0 = arith.constant 0 : index
  %c128000 = arith.constant 128000 : index
  %c-1_i64 = arith.constant -1 : i64
  %c0_i64 = arith.constant 0 : i64
  %c-1_i32 = arith.constant -1 : i32
  %c-1 = arith.constant -1 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c32000 = arith.constant 32000 : index
  %c553648144_i32 = arith.constant 553648144 : i32
  %c1_i32 = arith.constant 1 : i32
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %device = hal.ex.shared_device : !hal.device
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
  hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
  %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_2, "failed to wait on timepoint"
  %1 = util.optimization_barrier %transient_buffer : !hal.buffer
  %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
  %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
  %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
  hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
  %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
  %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_9, "failed to wait on timepoint"
  %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
  %len = hal.buffer.length<%1 : !hal.buffer> : index
  %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
  %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
  %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
  %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  %3 = arith.select %_device_query_0, %c0, %c-1 : index
  scf.index_switch %3 
  case 0 {
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%1 : !hal.buffer)[%c0, %len], 
      %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
      %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
    ])
    %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
    hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
    scf.yield
  }
  default {
  }
  hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
  %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
  %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_17, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
  %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
  check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
  return
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<SPIRVSubgroupReduce>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  util.global private @_device_query_0 : i1
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    util.initializer.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.initializer.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.initializer.return
  }
  util.global private @_executable__main_dispatch_0 : !hal.executable
  util.initializer {
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device = hal.ex.shared_device : !hal.device
    %0 = arith.select %_device_query_0, %c0, %c-1 : index
    %1 = scf.index_switch %0 -> !hal.executable 
    case 0 {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      %exe = hal.executable.create device(%device : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
      scf.yield %exe : !hal.executable
    }
    default {
      %2 = util.null : !hal.executable
      scf.yield %2 : !hal.executable
    }
    util.global.store %1, @_executable__main_dispatch_0 : !hal.executable
    util.initializer.return
  }
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.variant public @vulkan_spirv_fb target(#executable_target_vulkan_spirv_fb) {
      hal.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c32000 = arith.constant 32000 : index
        %c1 = arith.constant 1 : index
        hal.return %c32000, %c1, %c1 : index, index, index
      }
      builtin.module attributes {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>} {
        spirv.module Logical GLSL450 requires #spirv.vce<v1.3, [Shader, StorageBuffer16BitAccess, Float16, GroupNonUniformArithmetic], [SPV_KHR_storage_buffer_storage_class, SPV_KHR_16bit_storage]> {
          spirv.GlobalVariable @__builtin__WorkgroupId__ built_in("WorkgroupId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__builtin__LocalInvocationId__ built_in("LocalInvocationId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__resource_var_0_0_ bind(0, 0) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_1_ bind(0, 1) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_2_ bind(0, 2) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
          spirv.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() "None" {
            %cst64_i32 = spirv.Constant 64 : i32
            %cst128_i32 = spirv.Constant 128 : i32
            %cst192_i32 = spirv.Constant 192 : i32
            %cst256_i32 = spirv.Constant 256 : i32
            %cst320_i32 = spirv.Constant 320 : i32
            %cst384_i32 = spirv.Constant 384 : i32
            %cst448_i32 = spirv.Constant 448 : i32
            %cst512_i32 = spirv.Constant 512 : i32
            %cst_vec_4xf16 = spirv.Constant dense<0.000000e+00> : vector<4xf16>
            %cst0_i32 = spirv.Constant 0 : i32
            %cst_f16 = spirv.Constant 0.000000e+00 : f16
            %__builtin__LocalInvocationId___addr = spirv.mlir.addressof @__builtin__LocalInvocationId__ : !spirv.ptr<vector<3xi32>, Input>
            %0 = spirv.Load "Input" %__builtin__LocalInvocationId___addr : vector<3xi32>
            %1 = spirv.CompositeExtract %0[0 : i32] : vector<3xi32>
            %__resource_var_0_0__addr = spirv.mlir.addressof @__resource_var_0_0_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_1__addr = spirv.mlir.addressof @__resource_var_0_1_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_2__addr = spirv.mlir.addressof @__resource_var_0_2_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
            %__builtin__WorkgroupId___addr = spirv.mlir.addressof @__builtin__WorkgroupId__ : !spirv.ptr<vector<3xi32>, Input>
            %2 = spirv.Load "Input" %__builtin__WorkgroupId___addr : vector<3xi32>
            %3 = spirv.CompositeExtract %2[0 : i32] : vector<3xi32>
            %4 = spirv.IMul %3, %cst512_i32 : i32
            %5 = spirv.IAdd %1, %4 : i32
            %6 = spirv.IAdd %5, %cst448_i32 : i32
            %7 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %6] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %8 = spirv.Load "StorageBuffer" %7 : vector<4xf32>
            %9 = spirv.IAdd %1, %cst448_i32 : i32
            %10 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %9] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %11 = spirv.Load "StorageBuffer" %10 : vector<4xf32>
            %12 = spirv.IAdd %5, %cst384_i32 : i32
            %13 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %12] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %14 = spirv.Load "StorageBuffer" %13 : vector<4xf32>
            %15 = spirv.IAdd %1, %cst384_i32 : i32
            %16 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %15] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %17 = spirv.Load "StorageBuffer" %16 : vector<4xf32>
            %18 = spirv.IAdd %5, %cst320_i32 : i32
            %19 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %18] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %20 = spirv.Load "StorageBuffer" %19 : vector<4xf32>
            %21 = spirv.IAdd %1, %cst320_i32 : i32
            %22 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %21] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %23 = spirv.Load "StorageBuffer" %22 : vector<4xf32>
            %24 = spirv.IAdd %5, %cst256_i32 : i32
            %25 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %24] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %26 = spirv.Load "StorageBuffer" %25 : vector<4xf32>
            %27 = spirv.IAdd %1, %cst256_i32 : i32
            %28 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %27] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %29 = spirv.Load "StorageBuffer" %28 : vector<4xf32>
            %30 = spirv.IAdd %5, %cst192_i32 : i32
            %31 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %30] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %32 = spirv.Load "StorageBuffer" %31 : vector<4xf32>
            %33 = spirv.IAdd %1, %cst192_i32 : i32
            %34 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %33] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %35 = spirv.Load "StorageBuffer" %34 : vector<4xf32>
            %36 = spirv.IAdd %5, %cst128_i32 : i32
            %37 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %36] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %38 = spirv.Load "StorageBuffer" %37 : vector<4xf32>
            %39 = spirv.IAdd %1, %cst128_i32 : i32
            %40 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %39] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %41 = spirv.Load "StorageBuffer" %40 : vector<4xf32>
            %42 = spirv.IAdd %5, %cst64_i32 : i32
            %43 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %42] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %44 = spirv.Load "StorageBuffer" %43 : vector<4xf32>
            %45 = spirv.IAdd %1, %cst64_i32 : i32
            %46 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %45] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %47 = spirv.Load "StorageBuffer" %46 : vector<4xf32>
            %48 = spirv.IAdd %4, %1 : i32
            %49 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %48] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %50 = spirv.Load "StorageBuffer" %49 : vector<4xf32>
            %51 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %1] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %52 = spirv.Load "StorageBuffer" %51 : vector<4xf32>
            %53 = spirv.VectorShuffle [0 : i32, 1 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %54 = spirv.Bitcast %53 : vector<2xf32> to vector<4xf16>
            %55 = spirv.VectorShuffle [0 : i32, 1 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %56 = spirv.Bitcast %55 : vector<2xf32> to vector<4xf16>
            %57 = spirv.FMul %54, %56 : vector<4xf16>
            %58 = spirv.VectorShuffle [2 : i32, 3 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %59 = spirv.Bitcast %58 : vector<2xf32> to vector<4xf16>
            %60 = spirv.VectorShuffle [2 : i32, 3 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %61 = spirv.Bitcast %60 : vector<2xf32> to vector<4xf16>
            %62 = spirv.FMul %59, %61 : vector<4xf16>
            %63 = spirv.FAdd %57, %cst_vec_4xf16 : vector<4xf16>
            %64 = spirv.FAdd %62, %cst_vec_4xf16 : vector<4xf16>
            %65 = spirv.VectorShuffle [0 : i32, 1 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %66 = spirv.Bitcast %65 : vector<2xf32> to vector<4xf16>
            %67 = spirv.VectorShuffle [0 : i32, 1 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %68 = spirv.Bitcast %67 : vector<2xf32> to vector<4xf16>
            %69 = spirv.FMul %66, %68 : vector<4xf16>
            %70 = spirv.VectorShuffle [2 : i32, 3 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %71 = spirv.Bitcast %70 : vector<2xf32> to vector<4xf16>
            %72 = spirv.VectorShuffle [2 : i32, 3 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %73 = spirv.Bitcast %72 : vector<2xf32> to vector<4xf16>
            %74 = spirv.FMul %71, %73 : vector<4xf16>
            %75 = spirv.FAdd %69, %63 : vector<4xf16>
            %76 = spirv.FAdd %74, %64 : vector<4xf16>
            %77 = spirv.VectorShuffle [0 : i32, 1 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %78 = spirv.Bitcast %77 : vector<2xf32> to vector<4xf16>
            %79 = spirv.VectorShuffle [0 : i32, 1 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %80 = spirv.Bitcast %79 : vector<2xf32> to vector<4xf16>
            %81 = spirv.FMul %78, %80 : vector<4xf16>
            %82 = spirv.VectorShuffle [2 : i32, 3 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %83 = spirv.Bitcast %82 : vector<2xf32> to vector<4xf16>
            %84 = spirv.VectorShuffle [2 : i32, 3 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %85 = spirv.Bitcast %84 : vector<2xf32> to vector<4xf16>
            %86 = spirv.FMul %83, %85 : vector<4xf16>
            %87 = spirv.FAdd %81, %75 : vector<4xf16>
            %88 = spirv.FAdd %86, %76 : vector<4xf16>
            %89 = spirv.VectorShuffle [0 : i32, 1 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %90 = spirv.Bitcast %89 : vector<2xf32> to vector<4xf16>
            %91 = spirv.VectorShuffle [0 : i32, 1 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %92 = spirv.Bitcast %91 : vector<2xf32> to vector<4xf16>
            %93 = spirv.FMul %90, %92 : vector<4xf16>
            %94 = spirv.VectorShuffle [2 : i32, 3 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %95 = spirv.Bitcast %94 : vector<2xf32> to vector<4xf16>
            %96 = spirv.VectorShuffle [2 : i32, 3 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %97 = spirv.Bitcast %96 : vector<2xf32> to vector<4xf16>
            %98 = spirv.FMul %95, %97 : vector<4xf16>
            %99 = spirv.FAdd %93, %87 : vector<4xf16>
            %100 = spirv.FAdd %98, %88 : vector<4xf16>
            %101 = spirv.VectorShuffle [0 : i32, 1 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %102 = spirv.Bitcast %101 : vector<2xf32> to vector<4xf16>
            %103 = spirv.VectorShuffle [0 : i32, 1 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %104 = spirv.Bitcast %103 : vector<2xf32> to vector<4xf16>
            %105 = spirv.FMul %102, %104 : vector<4xf16>
            %106 = spirv.VectorShuffle [2 : i32, 3 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %107 = spirv.Bitcast %106 : vector<2xf32> to vector<4xf16>
            %108 = spirv.VectorShuffle [2 : i32, 3 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %109 = spirv.Bitcast %108 : vector<2xf32> to vector<4xf16>
            %110 = spirv.FMul %107, %109 : vector<4xf16>
            %111 = spirv.FAdd %105, %99 : vector<4xf16>
            %112 = spirv.FAdd %110, %100 : vector<4xf16>
            %113 = spirv.VectorShuffle [0 : i32, 1 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %114 = spirv.Bitcast %113 : vector<2xf32> to vector<4xf16>
            %115 = spirv.VectorShuffle [0 : i32, 1 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %116 = spirv.Bitcast %115 : vector<2xf32> to vector<4xf16>
            %117 = spirv.FMul %114, %116 : vector<4xf16>
            %118 = spirv.VectorShuffle [2 : i32, 3 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %119 = spirv.Bitcast %118 : vector<2xf32> to vector<4xf16>
            %120 = spirv.VectorShuffle [2 : i32, 3 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %121 = spirv.Bitcast %120 : vector<2xf32> to vector<4xf16>
            %122 = spirv.FMul %119, %121 : vector<4xf16>
            %123 = spirv.FAdd %117, %111 : vector<4xf16>
            %124 = spirv.FAdd %122, %112 : vector<4xf16>
            %125 = spirv.VectorShuffle [0 : i32, 1 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %126 = spirv.Bitcast %125 : vector<2xf32> to vector<4xf16>
            %127 = spirv.VectorShuffle [0 : i32, 1 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %128 = spirv.Bitcast %127 : vector<2xf32> to vector<4xf16>
            %129 = spirv.FMul %126, %128 : vector<4xf16>
            %130 = spirv.VectorShuffle [2 : i32, 3 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %131 = spirv.Bitcast %130 : vector<2xf32> to vector<4xf16>
            %132 = spirv.VectorShuffle [2 : i32, 3 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %133 = spirv.Bitcast %132 : vector<2xf32> to vector<4xf16>
            %134 = spirv.FMul %131, %133 : vector<4xf16>
            %135 = spirv.FAdd %129, %123 : vector<4xf16>
            %136 = spirv.FAdd %134, %124 : vector<4xf16>
            %137 = spirv.VectorShuffle [0 : i32, 1 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %138 = spirv.Bitcast %137 : vector<2xf32> to vector<4xf16>
            %139 = spirv.VectorShuffle [0 : i32, 1 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %140 = spirv.Bitcast %139 : vector<2xf32> to vector<4xf16>
            %141 = spirv.FMul %138, %140 : vector<4xf16>
            %142 = spirv.VectorShuffle [2 : i32, 3 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %143 = spirv.Bitcast %142 : vector<2xf32> to vector<4xf16>
            %144 = spirv.VectorShuffle [2 : i32, 3 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %145 = spirv.Bitcast %144 : vector<2xf32> to vector<4xf16>
            %146 = spirv.FMul %143, %145 : vector<4xf16>
            %147 = spirv.FAdd %141, %135 : vector<4xf16>
            %148 = spirv.FAdd %146, %136 : vector<4xf16>
            %149 = spirv.CompositeExtract %147[0 : i32] : vector<4xf16>
            %150 = spirv.CompositeExtract %147[1 : i32] : vector<4xf16>
            %151 = spirv.CompositeExtract %147[2 : i32] : vector<4xf16>
            %152 = spirv.CompositeExtract %147[3 : i32] : vector<4xf16>
            %153 = spirv.FAdd %149, %150 : f16
            %154 = spirv.FAdd %153, %151 : f16
            %155 = spirv.FAdd %154, %152 : f16
            %156 = spirv.CompositeExtract %148[0 : i32] : vector<4xf16>
            %157 = spirv.CompositeExtract %148[1 : i32] : vector<4xf16>
            %158 = spirv.CompositeExtract %148[2 : i32] : vector<4xf16>
            %159 = spirv.CompositeExtract %148[3 : i32] : vector<4xf16>
            %160 = spirv.FAdd %156, %157 : f16
            %161 = spirv.FAdd %160, %158 : f16
            %162 = spirv.FAdd %161, %159 : f16
            %163 = spirv.FAdd %155, %162 : f16
            %164 = spirv.GroupNonUniformFAdd "Subgroup" "Reduce" %163 : f16
            %165 = spirv.FAdd %164, %cst_f16 : f16
            %166 = spirv.IEqual %1, %cst0_i32 : i32
            spirv.mlir.selection {
              spirv.BranchConditional %166, ^bb1, ^bb2
            ^bb1:  // pred: ^bb0
              %167 = spirv.AccessChain %__resource_var_0_2__addr[%cst0_i32, %3] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>, i32, i32
              spirv.Store "StorageBuffer" %167, %165 : f16
              spirv.Branch ^bb2
            ^bb2:  // 2 preds: ^bb0, ^bb1
              spirv.mlir.merge
            }
            spirv.Return
          }
          spirv.EntryPoint "GLCompute" @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16, @__builtin__LocalInvocationId__, @__builtin__WorkgroupId__
          spirv.ExecutionMode @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 "LocalSize", 64, 1, 1
        }
      }
    }
  }
  func.func private @_main() {
    %c8192 = arith.constant 8192 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c262144000 = arith.constant 262144000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c64000 = arith.constant 64000 : index
    %c0 = arith.constant 0 : index
    %c128000 = arith.constant 128000 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c32000 = arith.constant 32000 : index
    %c553648144_i32 = arith.constant 553648144 : i32
    %c1_i32 = arith.constant 1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device = hal.ex.shared_device : !hal.device
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
    %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_2, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
    %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
    %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_9, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %3 = arith.select %_device_query_0, %c0, %c-1 : index
    scf.index_switch %3 
    case 0 {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%1 : !hal.buffer)[%c0, %len], 
        %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
        %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
      ])
      %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
      hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
      scf.yield
    }
    default {
    }
    hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
    %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
    %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_17, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
  util.global.store %value, @_device_query_0 : i1
  util.initializer.return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.initializer {
  %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  %device = hal.ex.shared_device : !hal.device
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer.return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
  util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer.return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.initializer {
  %c-1 = arith.constant -1 : index
  %c0 = arith.constant 0 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %device = hal.ex.shared_device : !hal.device
  %0 = arith.select %_device_query_0, %c0, %c-1 : index
  %1 = arith.index_cast %0 : index to i32
  cf.switch %1 : i32, [
    default: ^bb2,
    0: ^bb1
  ]
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %2 = util.null : !hal.executable
  cf.br ^bb3(%2 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %3, @_executable__main_dispatch_0 : !hal.executable
  util.initializer.return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
func.func private @_main() {
  %c8192 = arith.constant 8192 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c262144000 = arith.constant 262144000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c64000 = arith.constant 64000 : index
  %c0 = arith.constant 0 : index
  %c128000 = arith.constant 128000 : index
  %c-1_i64 = arith.constant -1 : i64
  %c0_i64 = arith.constant 0 : i64
  %c-1_i32 = arith.constant -1 : i32
  %c-1 = arith.constant -1 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c32000 = arith.constant 32000 : index
  %c553648144_i32 = arith.constant 553648144 : i32
  %c1_i32 = arith.constant 1 : i32
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %device = hal.ex.shared_device : !hal.device
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
  hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
  %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_2, "failed to wait on timepoint"
  %1 = util.optimization_barrier %transient_buffer : !hal.buffer
  %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
  %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
  %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
  hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
  %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
  %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_9, "failed to wait on timepoint"
  %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
  %len = hal.buffer.length<%1 : !hal.buffer> : index
  %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
  %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
  %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
  %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  %3 = arith.select %_device_query_0, %c0, %c-1 : index
  %4 = arith.index_cast %3 : index to i32
  cf.switch %4 : i32, [
    default: ^bb2,
    0: ^bb1
  ]
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%1 : !hal.buffer)[%c0, %len], 
    %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
    %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
  ])
  %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
  hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
  cf.br ^bb3
^bb2:  // pred: ^bb0
  cf.br ^bb3
^bb3:  // 2 preds: ^bb1, ^bb2
  hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
  %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
  %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_17, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
  %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
  check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
  return
}

// -----// IR Dump After CombineInitializers (iree-util-combine-initializers) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<SPIRVSubgroupReduce>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  util.global private @_device_query_0 : i1
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable__main_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    %device_0 = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device_1 = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device_1 : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device_2 = hal.ex.shared_device : !hal.device
    %0 = arith.select %_device_query_0, %c0, %c-1 : index
    %1 = arith.index_cast %0 : index to i32
    cf.switch %1 : i32, [
      default: ^bb2,
      0: ^bb1
    ]
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device_2 : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @_executable__main_dispatch_0 : !hal.executable
    cf.br ^bb4
  ^bb4:  // pred: ^bb3
    util.initializer.return
  }
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.variant public @vulkan_spirv_fb target(#executable_target_vulkan_spirv_fb) {
      hal.executable.export public @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device):
        %c32000 = arith.constant 32000 : index
        %c1 = arith.constant 1 : index
        hal.return %c32000, %c1, %c1 : index, index, index
      }
      builtin.module attributes {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>} {
        spirv.module Logical GLSL450 requires #spirv.vce<v1.3, [Shader, StorageBuffer16BitAccess, Float16, GroupNonUniformArithmetic], [SPV_KHR_storage_buffer_storage_class, SPV_KHR_16bit_storage]> {
          spirv.GlobalVariable @__builtin__WorkgroupId__ built_in("WorkgroupId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__builtin__LocalInvocationId__ built_in("LocalInvocationId") : !spirv.ptr<vector<3xi32>, Input>
          spirv.GlobalVariable @__resource_var_0_0_ bind(0, 0) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_1_ bind(0, 1) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
          spirv.GlobalVariable @__resource_var_0_2_ bind(0, 2) : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
          spirv.func @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16() "None" {
            %cst64_i32 = spirv.Constant 64 : i32
            %cst128_i32 = spirv.Constant 128 : i32
            %cst192_i32 = spirv.Constant 192 : i32
            %cst256_i32 = spirv.Constant 256 : i32
            %cst320_i32 = spirv.Constant 320 : i32
            %cst384_i32 = spirv.Constant 384 : i32
            %cst448_i32 = spirv.Constant 448 : i32
            %cst512_i32 = spirv.Constant 512 : i32
            %cst_vec_4xf16 = spirv.Constant dense<0.000000e+00> : vector<4xf16>
            %cst0_i32 = spirv.Constant 0 : i32
            %cst_f16 = spirv.Constant 0.000000e+00 : f16
            %__builtin__LocalInvocationId___addr = spirv.mlir.addressof @__builtin__LocalInvocationId__ : !spirv.ptr<vector<3xi32>, Input>
            %0 = spirv.Load "Input" %__builtin__LocalInvocationId___addr : vector<3xi32>
            %1 = spirv.CompositeExtract %0[0 : i32] : vector<3xi32>
            %__resource_var_0_0__addr = spirv.mlir.addressof @__resource_var_0_0_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_1__addr = spirv.mlir.addressof @__resource_var_0_1_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>
            %__resource_var_0_2__addr = spirv.mlir.addressof @__resource_var_0_2_ : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>
            %__builtin__WorkgroupId___addr = spirv.mlir.addressof @__builtin__WorkgroupId__ : !spirv.ptr<vector<3xi32>, Input>
            %2 = spirv.Load "Input" %__builtin__WorkgroupId___addr : vector<3xi32>
            %3 = spirv.CompositeExtract %2[0 : i32] : vector<3xi32>
            %4 = spirv.IMul %3, %cst512_i32 : i32
            %5 = spirv.IAdd %1, %4 : i32
            %6 = spirv.IAdd %5, %cst448_i32 : i32
            %7 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %6] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %8 = spirv.Load "StorageBuffer" %7 : vector<4xf32>
            %9 = spirv.IAdd %1, %cst448_i32 : i32
            %10 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %9] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %11 = spirv.Load "StorageBuffer" %10 : vector<4xf32>
            %12 = spirv.IAdd %5, %cst384_i32 : i32
            %13 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %12] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %14 = spirv.Load "StorageBuffer" %13 : vector<4xf32>
            %15 = spirv.IAdd %1, %cst384_i32 : i32
            %16 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %15] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %17 = spirv.Load "StorageBuffer" %16 : vector<4xf32>
            %18 = spirv.IAdd %5, %cst320_i32 : i32
            %19 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %18] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %20 = spirv.Load "StorageBuffer" %19 : vector<4xf32>
            %21 = spirv.IAdd %1, %cst320_i32 : i32
            %22 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %21] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %23 = spirv.Load "StorageBuffer" %22 : vector<4xf32>
            %24 = spirv.IAdd %5, %cst256_i32 : i32
            %25 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %24] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %26 = spirv.Load "StorageBuffer" %25 : vector<4xf32>
            %27 = spirv.IAdd %1, %cst256_i32 : i32
            %28 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %27] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %29 = spirv.Load "StorageBuffer" %28 : vector<4xf32>
            %30 = spirv.IAdd %5, %cst192_i32 : i32
            %31 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %30] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %32 = spirv.Load "StorageBuffer" %31 : vector<4xf32>
            %33 = spirv.IAdd %1, %cst192_i32 : i32
            %34 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %33] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %35 = spirv.Load "StorageBuffer" %34 : vector<4xf32>
            %36 = spirv.IAdd %5, %cst128_i32 : i32
            %37 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %36] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %38 = spirv.Load "StorageBuffer" %37 : vector<4xf32>
            %39 = spirv.IAdd %1, %cst128_i32 : i32
            %40 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %39] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %41 = spirv.Load "StorageBuffer" %40 : vector<4xf32>
            %42 = spirv.IAdd %5, %cst64_i32 : i32
            %43 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %42] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %44 = spirv.Load "StorageBuffer" %43 : vector<4xf32>
            %45 = spirv.IAdd %1, %cst64_i32 : i32
            %46 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %45] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %47 = spirv.Load "StorageBuffer" %46 : vector<4xf32>
            %48 = spirv.IAdd %4, %1 : i32
            %49 = spirv.AccessChain %__resource_var_0_1__addr[%cst0_i32, %48] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %50 = spirv.Load "StorageBuffer" %49 : vector<4xf32>
            %51 = spirv.AccessChain %__resource_var_0_0__addr[%cst0_i32, %1] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<vector<4xf32>, stride=16> [0])>, StorageBuffer>, i32, i32
            %52 = spirv.Load "StorageBuffer" %51 : vector<4xf32>
            %53 = spirv.VectorShuffle [0 : i32, 1 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %54 = spirv.Bitcast %53 : vector<2xf32> to vector<4xf16>
            %55 = spirv.VectorShuffle [0 : i32, 1 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %56 = spirv.Bitcast %55 : vector<2xf32> to vector<4xf16>
            %57 = spirv.FMul %54, %56 : vector<4xf16>
            %58 = spirv.VectorShuffle [2 : i32, 3 : i32] %11 : vector<4xf32>, %11 : vector<4xf32> -> vector<2xf32>
            %59 = spirv.Bitcast %58 : vector<2xf32> to vector<4xf16>
            %60 = spirv.VectorShuffle [2 : i32, 3 : i32] %8 : vector<4xf32>, %8 : vector<4xf32> -> vector<2xf32>
            %61 = spirv.Bitcast %60 : vector<2xf32> to vector<4xf16>
            %62 = spirv.FMul %59, %61 : vector<4xf16>
            %63 = spirv.FAdd %57, %cst_vec_4xf16 : vector<4xf16>
            %64 = spirv.FAdd %62, %cst_vec_4xf16 : vector<4xf16>
            %65 = spirv.VectorShuffle [0 : i32, 1 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %66 = spirv.Bitcast %65 : vector<2xf32> to vector<4xf16>
            %67 = spirv.VectorShuffle [0 : i32, 1 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %68 = spirv.Bitcast %67 : vector<2xf32> to vector<4xf16>
            %69 = spirv.FMul %66, %68 : vector<4xf16>
            %70 = spirv.VectorShuffle [2 : i32, 3 : i32] %17 : vector<4xf32>, %17 : vector<4xf32> -> vector<2xf32>
            %71 = spirv.Bitcast %70 : vector<2xf32> to vector<4xf16>
            %72 = spirv.VectorShuffle [2 : i32, 3 : i32] %14 : vector<4xf32>, %14 : vector<4xf32> -> vector<2xf32>
            %73 = spirv.Bitcast %72 : vector<2xf32> to vector<4xf16>
            %74 = spirv.FMul %71, %73 : vector<4xf16>
            %75 = spirv.FAdd %69, %63 : vector<4xf16>
            %76 = spirv.FAdd %74, %64 : vector<4xf16>
            %77 = spirv.VectorShuffle [0 : i32, 1 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %78 = spirv.Bitcast %77 : vector<2xf32> to vector<4xf16>
            %79 = spirv.VectorShuffle [0 : i32, 1 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %80 = spirv.Bitcast %79 : vector<2xf32> to vector<4xf16>
            %81 = spirv.FMul %78, %80 : vector<4xf16>
            %82 = spirv.VectorShuffle [2 : i32, 3 : i32] %23 : vector<4xf32>, %23 : vector<4xf32> -> vector<2xf32>
            %83 = spirv.Bitcast %82 : vector<2xf32> to vector<4xf16>
            %84 = spirv.VectorShuffle [2 : i32, 3 : i32] %20 : vector<4xf32>, %20 : vector<4xf32> -> vector<2xf32>
            %85 = spirv.Bitcast %84 : vector<2xf32> to vector<4xf16>
            %86 = spirv.FMul %83, %85 : vector<4xf16>
            %87 = spirv.FAdd %81, %75 : vector<4xf16>
            %88 = spirv.FAdd %86, %76 : vector<4xf16>
            %89 = spirv.VectorShuffle [0 : i32, 1 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %90 = spirv.Bitcast %89 : vector<2xf32> to vector<4xf16>
            %91 = spirv.VectorShuffle [0 : i32, 1 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %92 = spirv.Bitcast %91 : vector<2xf32> to vector<4xf16>
            %93 = spirv.FMul %90, %92 : vector<4xf16>
            %94 = spirv.VectorShuffle [2 : i32, 3 : i32] %29 : vector<4xf32>, %29 : vector<4xf32> -> vector<2xf32>
            %95 = spirv.Bitcast %94 : vector<2xf32> to vector<4xf16>
            %96 = spirv.VectorShuffle [2 : i32, 3 : i32] %26 : vector<4xf32>, %26 : vector<4xf32> -> vector<2xf32>
            %97 = spirv.Bitcast %96 : vector<2xf32> to vector<4xf16>
            %98 = spirv.FMul %95, %97 : vector<4xf16>
            %99 = spirv.FAdd %93, %87 : vector<4xf16>
            %100 = spirv.FAdd %98, %88 : vector<4xf16>
            %101 = spirv.VectorShuffle [0 : i32, 1 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %102 = spirv.Bitcast %101 : vector<2xf32> to vector<4xf16>
            %103 = spirv.VectorShuffle [0 : i32, 1 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %104 = spirv.Bitcast %103 : vector<2xf32> to vector<4xf16>
            %105 = spirv.FMul %102, %104 : vector<4xf16>
            %106 = spirv.VectorShuffle [2 : i32, 3 : i32] %35 : vector<4xf32>, %35 : vector<4xf32> -> vector<2xf32>
            %107 = spirv.Bitcast %106 : vector<2xf32> to vector<4xf16>
            %108 = spirv.VectorShuffle [2 : i32, 3 : i32] %32 : vector<4xf32>, %32 : vector<4xf32> -> vector<2xf32>
            %109 = spirv.Bitcast %108 : vector<2xf32> to vector<4xf16>
            %110 = spirv.FMul %107, %109 : vector<4xf16>
            %111 = spirv.FAdd %105, %99 : vector<4xf16>
            %112 = spirv.FAdd %110, %100 : vector<4xf16>
            %113 = spirv.VectorShuffle [0 : i32, 1 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %114 = spirv.Bitcast %113 : vector<2xf32> to vector<4xf16>
            %115 = spirv.VectorShuffle [0 : i32, 1 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %116 = spirv.Bitcast %115 : vector<2xf32> to vector<4xf16>
            %117 = spirv.FMul %114, %116 : vector<4xf16>
            %118 = spirv.VectorShuffle [2 : i32, 3 : i32] %41 : vector<4xf32>, %41 : vector<4xf32> -> vector<2xf32>
            %119 = spirv.Bitcast %118 : vector<2xf32> to vector<4xf16>
            %120 = spirv.VectorShuffle [2 : i32, 3 : i32] %38 : vector<4xf32>, %38 : vector<4xf32> -> vector<2xf32>
            %121 = spirv.Bitcast %120 : vector<2xf32> to vector<4xf16>
            %122 = spirv.FMul %119, %121 : vector<4xf16>
            %123 = spirv.FAdd %117, %111 : vector<4xf16>
            %124 = spirv.FAdd %122, %112 : vector<4xf16>
            %125 = spirv.VectorShuffle [0 : i32, 1 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %126 = spirv.Bitcast %125 : vector<2xf32> to vector<4xf16>
            %127 = spirv.VectorShuffle [0 : i32, 1 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %128 = spirv.Bitcast %127 : vector<2xf32> to vector<4xf16>
            %129 = spirv.FMul %126, %128 : vector<4xf16>
            %130 = spirv.VectorShuffle [2 : i32, 3 : i32] %47 : vector<4xf32>, %47 : vector<4xf32> -> vector<2xf32>
            %131 = spirv.Bitcast %130 : vector<2xf32> to vector<4xf16>
            %132 = spirv.VectorShuffle [2 : i32, 3 : i32] %44 : vector<4xf32>, %44 : vector<4xf32> -> vector<2xf32>
            %133 = spirv.Bitcast %132 : vector<2xf32> to vector<4xf16>
            %134 = spirv.FMul %131, %133 : vector<4xf16>
            %135 = spirv.FAdd %129, %123 : vector<4xf16>
            %136 = spirv.FAdd %134, %124 : vector<4xf16>
            %137 = spirv.VectorShuffle [0 : i32, 1 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %138 = spirv.Bitcast %137 : vector<2xf32> to vector<4xf16>
            %139 = spirv.VectorShuffle [0 : i32, 1 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %140 = spirv.Bitcast %139 : vector<2xf32> to vector<4xf16>
            %141 = spirv.FMul %138, %140 : vector<4xf16>
            %142 = spirv.VectorShuffle [2 : i32, 3 : i32] %52 : vector<4xf32>, %52 : vector<4xf32> -> vector<2xf32>
            %143 = spirv.Bitcast %142 : vector<2xf32> to vector<4xf16>
            %144 = spirv.VectorShuffle [2 : i32, 3 : i32] %50 : vector<4xf32>, %50 : vector<4xf32> -> vector<2xf32>
            %145 = spirv.Bitcast %144 : vector<2xf32> to vector<4xf16>
            %146 = spirv.FMul %143, %145 : vector<4xf16>
            %147 = spirv.FAdd %141, %135 : vector<4xf16>
            %148 = spirv.FAdd %146, %136 : vector<4xf16>
            %149 = spirv.CompositeExtract %147[0 : i32] : vector<4xf16>
            %150 = spirv.CompositeExtract %147[1 : i32] : vector<4xf16>
            %151 = spirv.CompositeExtract %147[2 : i32] : vector<4xf16>
            %152 = spirv.CompositeExtract %147[3 : i32] : vector<4xf16>
            %153 = spirv.FAdd %149, %150 : f16
            %154 = spirv.FAdd %153, %151 : f16
            %155 = spirv.FAdd %154, %152 : f16
            %156 = spirv.CompositeExtract %148[0 : i32] : vector<4xf16>
            %157 = spirv.CompositeExtract %148[1 : i32] : vector<4xf16>
            %158 = spirv.CompositeExtract %148[2 : i32] : vector<4xf16>
            %159 = spirv.CompositeExtract %148[3 : i32] : vector<4xf16>
            %160 = spirv.FAdd %156, %157 : f16
            %161 = spirv.FAdd %160, %158 : f16
            %162 = spirv.FAdd %161, %159 : f16
            %163 = spirv.FAdd %155, %162 : f16
            %164 = spirv.GroupNonUniformFAdd "Subgroup" "Reduce" %163 : f16
            %165 = spirv.FAdd %164, %cst_f16 : f16
            %166 = spirv.IEqual %1, %cst0_i32 : i32
            spirv.mlir.selection {
              spirv.BranchConditional %166, ^bb1, ^bb2
            ^bb1:  // pred: ^bb0
              %167 = spirv.AccessChain %__resource_var_0_2__addr[%cst0_i32, %3] : !spirv.ptr<!spirv.struct<(!spirv.rtarray<f16, stride=2> [0])>, StorageBuffer>, i32, i32
              spirv.Store "StorageBuffer" %167, %165 : f16
              spirv.Branch ^bb2
            ^bb2:  // 2 preds: ^bb0, ^bb1
              spirv.mlir.merge
            }
            spirv.Return
          }
          spirv.EntryPoint "GLCompute" @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16, @__builtin__LocalInvocationId__, @__builtin__WorkgroupId__
          spirv.ExecutionMode @_main_dispatch_0_matmul_transpose_b_1x32000x4096_f16 "LocalSize", 64, 1, 1
        }
      }
    }
  }
  func.func private @_main() {
    %c8192 = arith.constant 8192 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c262144000 = arith.constant 262144000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c64000 = arith.constant 64000 : index
    %c0 = arith.constant 0 : index
    %c128000 = arith.constant 128000 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c32000 = arith.constant 32000 : index
    %c553648144_i32 = arith.constant 553648144 : i32
    %c1_i32 = arith.constant 1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device = hal.ex.shared_device : !hal.device
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
    %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_2, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
    %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
    %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_9, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %3 = arith.select %_device_query_0, %c0, %c-1 : index
    %4 = arith.index_cast %3 : index to i32
    cf.switch %4 : i32, [
      default: ^bb2,
      0: ^bb1
    ]
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%1 : !hal.buffer)[%c0, %len], 
      %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
      %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
    ])
    %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
    hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
    cf.br ^bb3
  ^bb2:  // pred: ^bb0
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
    %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
    %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_17, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::SerializeTargetExecutablesPass (iree-hal-serialize-target-executables) //----- //
hal.executable private @_main_dispatch_0 {
  hal.executable.binary public @vulkan_spirv_fb attributes {data = dense<"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vector<5020xi8>, format = "vulkan-spirv-fb", mime_type = "application/x-flatbuffers"}
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::SerializeExecutablesPass (iree-hal-serialize-executables) //----- //
hal.executable private @_main_dispatch_0 {
  hal.executable.binary public @vulkan_spirv_fb attributes {data = dense<"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vector<5020xi8>, format = "vulkan-spirv-fb", mime_type = "application/x-flatbuffers"}
}

// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  util.global private @_device_query_0 : i1
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable__main_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    %device_0 = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device_1 = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device_1 : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device_2 = hal.ex.shared_device : !hal.device
    %0 = arith.select %_device_query_0, %c0, %c-1 : index
    %1 = arith.index_cast %0 : index to i32
    cf.switch %1 : i32, [
      default: ^bb2,
      0: ^bb1
    ]
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device_2 : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @_executable__main_dispatch_0 : !hal.executable
    cf.br ^bb4
  ^bb4:  // pred: ^bb3
    util.initializer.return
  }
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.binary public @vulkan_spirv_fb attributes {data = dense<"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vector<5020xi8>, format = "vulkan-spirv-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func private @_main() {
    %c8192 = arith.constant 8192 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c262144000 = arith.constant 262144000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c64000 = arith.constant 64000 : index
    %c0 = arith.constant 0 : index
    %c128000 = arith.constant 128000 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c32000 = arith.constant 32000 : index
    %c553648144_i32 = arith.constant 553648144 : i32
    %c1_i32 = arith.constant 1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device = hal.ex.shared_device : !hal.device
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
    %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_2, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
    %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
    %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_9, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %3 = arith.select %_device_query_0, %c0, %c-1 : index
    %4 = arith.index_cast %3 : index to i32
    cf.switch %4 : i32, [
      default: ^bb2,
      0: ^bb1
    ]
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%1 : !hal.buffer)[%c0, %len], 
      %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
      %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
    ])
    %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
    hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
    cf.br ^bb3
  ^bb2:  // pred: ^bb0
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
    %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
    %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_17, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_device_query_0 : i1
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable__main_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %0 = arith.select %_device_query_0, %c0, %c-1 : index
    %1 = arith.index_cast %0 : index to i32
    cf.switch %1 : i32, [
      default: ^bb2,
      0: ^bb1
    ]
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @_executable__main_dispatch_0 : !hal.executable
    cf.br ^bb4
  ^bb4:  // pred: ^bb3
    util.initializer.return
  }
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.binary public @vulkan_spirv_fb attributes {data = dense<"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vector<5020xi8>, format = "vulkan-spirv-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func private @_main() {
    %c8192 = arith.constant 8192 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c262144000 = arith.constant 262144000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c64000 = arith.constant 64000 : index
    %c0 = arith.constant 0 : index
    %c128000 = arith.constant 128000 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c32000 = arith.constant 32000 : index
    %c553648144_i32 = arith.constant 553648144 : i32
    %c1_i32 = arith.constant 1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device = hal.ex.shared_device : !hal.device
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
    %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_2, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
    %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
    %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_9, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %3 = arith.select %_device_query_0, %c0, %c-1 : index
    %4 = arith.index_cast %3 : index to i32
    cf.switch %4 : i32, [
      default: ^bb2,
      0: ^bb1
    ]
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%1 : !hal.buffer)[%c0, %len], 
      %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
      %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
    ])
    %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
    hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
    cf.br ^bb3
  ^bb2:  // pred: ^bb0
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
    %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
    %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_17, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_device_query_0 : i1
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable__main_dispatch_0 : !hal.executable
  util.initializer {
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %0 = arith.select %_device_query_0, %c0, %c-1 : index
    %1 = arith.index_cast %0 : index to i32
    cf.switch %1 : i32, [
      default: ^bb2,
      0: ^bb1
    ]
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @_executable__main_dispatch_0 : !hal.executable
    util.initializer.return
  }
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.binary public @vulkan_spirv_fb attributes {data = dense<"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vector<5020xi8>, format = "vulkan-spirv-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func private @_main() {
    %c8192 = arith.constant 8192 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c262144000 = arith.constant 262144000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c64000 = arith.constant 64000 : index
    %c0 = arith.constant 0 : index
    %c128000 = arith.constant 128000 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c32000 = arith.constant 32000 : index
    %c553648144_i32 = arith.constant 553648144 : i32
    %c1_i32 = arith.constant 1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device = hal.ex.shared_device : !hal.device
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
    %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_2, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
    %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
    %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_9, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %3 = arith.select %_device_query_0, %c0, %c-1 : index
    %4 = arith.index_cast %3 : index to i32
    cf.switch %4 : i32, [
      default: ^bb2,
      0: ^bb1
    ]
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%1 : !hal.buffer)[%c0, %len], 
      %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
      %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
    ])
    %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
    hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
    cf.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
    %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
    %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_17, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_device_query_0 : i1
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable__main_dispatch_0 : !hal.executable
  util.initializer {
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %0 = arith.select %_device_query_0, %c0, %c-1 : index
    %1 = arith.index_cast %0 : index to i32
    cf.switch %1 : i32, [
      default: ^bb2,
      0: ^bb1
    ]
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @_executable__main_dispatch_0 : !hal.executable
    util.initializer.return
  }
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.binary public @vulkan_spirv_fb attributes {data = dense<"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vector<5020xi8>, format = "vulkan-spirv-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func private @_main() {
    %c8192 = arith.constant 8192 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c262144000 = arith.constant 262144000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c64000 = arith.constant 64000 : index
    %c0 = arith.constant 0 : index
    %c128000 = arith.constant 128000 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c32000 = arith.constant 32000 : index
    %c553648144_i32 = arith.constant 553648144 : i32
    %c1_i32 = arith.constant 1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device = hal.ex.shared_device : !hal.device
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
    %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_2, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
    %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
    %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_9, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %3 = arith.select %_device_query_0, %c0, %c-1 : index
    %4 = arith.index_cast %3 : index to i32
    cf.switch %4 : i32, [
      default: ^bb2,
      0: ^bb1
    ]
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%1 : !hal.buffer)[%c0, %len], 
      %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
      %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
    ])
    %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
    hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
    cf.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
    %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
    %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_17, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %c0 = arith.constant 0 : index
  %c-1 = arith.constant -1 : index
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  %0 = arith.select %value, %c0, %c-1 : index
  %1 = arith.index_cast %0 : index to i32
  util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.switch %1 : i32, [
    default: ^bb2,
    0: ^bb1
  ]
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %2 = util.null : !hal.executable
  cf.br ^bb3(%2 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %3, @_executable__main_dispatch_0 : !hal.executable
  util.initializer.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func private @_main() {
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
  %c8192 = arith.constant 8192 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c262144000 = arith.constant 262144000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c64000 = arith.constant 64000 : index
  %c0 = arith.constant 0 : index
  %c128000 = arith.constant 128000 : index
  %c-1_i64 = arith.constant -1 : i64
  %c0_i64 = arith.constant 0 : i64
  %c-1_i32 = arith.constant -1 : i32
  %c-1 = arith.constant -1 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c32000 = arith.constant 32000 : index
  %c553648144_i32 = arith.constant 553648144 : i32
  %c1_i32 = arith.constant 1 : i32
  %device = hal.ex.shared_device : !hal.device
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
  hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
  %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_2, "failed to wait on timepoint"
  %1 = util.optimization_barrier %transient_buffer : !hal.buffer
  %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
  %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
  %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
  hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
  %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
  %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_9, "failed to wait on timepoint"
  %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
  %len = hal.buffer.length<%1 : !hal.buffer> : index
  %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
  %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
  %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
  %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  %3 = arith.select %_device_query_0, %c0, %c-1 : index
  %4 = arith.index_cast %3 : index to i32
  cf.switch %4 : i32, [
    default: ^bb2,
    0: ^bb1
  ]
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%1 : !hal.buffer)[%c0, %len], 
    %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
    %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
  ])
  hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
  cf.br ^bb2
^bb2:  // 2 preds: ^bb0, ^bb1
  hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
  %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
  %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_17, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
  %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
  check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
  return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_device_query_0 : i1
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable__main_dispatch_0 : !hal.executable
  util.initializer {
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    %0 = arith.select %value, %c0, %c-1 : index
    %1 = arith.index_cast %0 : index to i32
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.switch %1 : i32, [
      default: ^bb2,
      0: ^bb1
    ]
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @_executable__main_dispatch_0 : !hal.executable
    util.initializer.return
  }
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.binary public @vulkan_spirv_fb attributes {data = dense<"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vector<5020xi8>, format = "vulkan-spirv-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func private @_main() {
    %c1_i32 = arith.constant 1 : i32
    %c553648144_i32 = arith.constant 553648144 : i32
    %c32000 = arith.constant 32000 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
    %device = hal.ex.shared_device : !hal.device
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
    %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_2, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
    %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
    %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_9, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %3 = arith.select %_device_query_0, %c0, %c-1 : index
    %4 = arith.index_cast %3 : index to i32
    cf.switch %4 : i32, [
      default: ^bb2,
      0: ^bb1
    ]
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%1 : !hal.buffer)[%c0, %len], 
      %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
      %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
    ])
    hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
    cf.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
    %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
    %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_17, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable__main_dispatch_0 : !hal.executable
  util.initializer {
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    %0 = arith.select %value, %c0, %c-1 : index
    %1 = arith.index_cast %0 : index to i32
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.switch %1 : i32, [
      default: ^bb2,
      0: ^bb1
    ]
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @_executable__main_dispatch_0 : !hal.executable
    util.initializer.return
  }
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.binary public @vulkan_spirv_fb attributes {data = dense<"0x080000005350564578ECFFFF14000000940000000400000001000000480000000100000004000000340000005F6D61696E5F64697370617463685F305F6D61746D756C5F7472616E73706F73655F625F3178333230303078343039365F66313600000000DCECFFFF08000000090000002D00000064756D70732F636F6E666967757265645F6D6F64756C655F5F6D61696E5F64697370617463685F302E6D6C6972000000B8040000030223070003010016000000D200000000000000110002000100000011000200511100001100020009000000110002003F0000000A000B005350565F4B48525F73746F726167655F6275666665725F73746F726167655F636C617373000000000A0007005350565F4B48525F31366269745F73746F726167650000000E00030000000000010000000F00130005000000140000005F6D61696E5F64697370617463685F305F6D61746D756C5F7472616E73706F73655F625F3178333230303078343039365F66313600000000050000000400000010000600140000001100000040000000010000000100000005000900040000005F5F6275696C74696E5F5F576F726B67726F757049645F5F0000000005000A00050000005F5F6275696C74696E5F5F4C6F63616C496E766F636174696F6E49645F5F0000050007000B0000005F5F7265736F757263655F7661725F305F305F00050007000C0000005F5F7265736F757263655F7661725F305F315F0005000700110000005F5F7265736F757263655F7661725F305F325F0005001000140000005F6D61696E5F64697370617463685F305F6D61746D756C5F7472616E73706F73655F625F3178333230303078343039365F6631360000000047000400040000000B0000001A00000047000400050000000B0000001B000000470004000800000006000000100000004800050007000000000000002300000000000000470003000700000002000000470004000B0000002100000000000000470004000B0000002200000000000000470004000C0000002100000001000000470004000C0000002200000000000000470004000F0000000600000002000000480005000E000000000000002300000000000000470003000E0000000200000047000400110000002100000002000000470004001100000022000000000000001500040003000000200000000000000017000400020000000300000003000000200004000100000001000000020000003B0004000100000004000000010000003B000400010000000500000001000000160003000A0000002000000017000400090000000A000000040000001D00030008000000090000001E000300070000000800000020000400060000000C000000070000003B000400060000000B0000000C0000003B000400060000000C0000000C0000001600030010000000100000001D0003000F000000100000001E0003000E0000000F000000200004000D0000000C0000000E0000003B0004000D000000110000000C00000013000200130000002100030012000000130000002B0004000300000016000000400000002B0004000300000017000000800000002B0004000300000018000000C00000002B0004000300000019000000000100002B000400030000001A000000400100002B000400030000001B000000800100002B000400030000001C000000C00100002B000400030000001D00000000020000170004001E00000010000000040000002B0004001000000020000000000000002C0007001E0000001F000000200000002000000020000000200000002B00040003000000210000000000000020000400290000000C0000000900000017000400580000000A000000020000002B00040003000000C90000000300000014000200CB00000020000400D00000000C000000100000003600050013000000140000000000000012000000F8000200150000003D00040002000000220000000500000051000500030000002300000022000000000000003D0004000200000024000000040000005100050003000000250000002400000000000000840005000300000026000000250000001D0000008000050003000000270000002300000026000000800005000300000028000000270000001C00000041000600290000002A0000000C00000021000000280000003D000400090000002B0000002A00000080000500030000002C000000230000001C00000041000600290000002D0000000B000000210000002C0000003D000400090000002E0000002D00000080000500030000002F000000270000001B0000004100060029000000300000000C000000210000002F0000003D000400090000003100000030000000800005000300000032000000230000001B0000004100060029000000330000000B00000021000000320000003D000400090000003400000033000000800005000300000035000000270000001A0000004100060029000000360000000C00000021000000350000003D000400090000003700000036000000800005000300000038000000230000001A0000004100060029000000390000000B00000021000000380000003D000400090000003A0000003900000080000500030000003B000000270000001900000041000600290000003C0000000C000000210000003B0000003D000400090000003D0000003C00000080000500030000003E000000230000001900000041000600290000003F0000000B000000210000003E0000003D00040009000000400000003F00000080000500030000004100000027000000180000004100060029000000420000000C00000021000000410000003D00040009000000430000004200000080000500030000004400000023000000180000004100060029000000450000000B00000021000000440000003D00040009000000460000004500000080000500030000004700000027000000170000004100060029000000480000000C00000021000000470000003D00040009000000490000004800000080000500030000004A000000230000001700000041000600290000004B0000000B000000210000004A0000003D000400090000004C0000004B00000080000500030000004D000000270000001600000041000600290000004E0000000C000000210000004D0000003D000400090000004F0000004E00000080000500030000005000000023000000160000004100060029000000510000000B00000021000000500000003D00040009000000520000005100000080000500030000005300000026000000230000004100060029000000540000000C00000021000000530000003D0004000900000055000000540000004100060029000000560000000B00000021000000230000003D0004000900000057000000560000004F00070058000000590000002E0000002E00000000000000010000007C0004001E0000005A000000590000004F000700580000005B0000002B0000002B00000000000000010000007C0004001E0000005C0000005B000000850005001E0000005D0000005A0000005C0000004F000700580000005E0000002E0000002E00000002000000030000007C0004001E0000005F0000005E0000004F00070058000000600000002B0000002B00000002000000030000007C0004001E0000006100000060000000850005001E000000620000005F00000061000000810005001E000000630000005D0000001F000000810005001E00000064000000620000001F0000004F0007005800000065000000340000003400000000000000010000007C0004001E00000066000000650000004F0007005800000067000000310000003100000000000000010000007C0004001E0000006800000067000000850005001E0000006900000066000000680000004F000700580000006A000000340000003400000002000000030000007C0004001E0000006B0000006A0000004F000700580000006C000000310000003100000002000000030000007C0004001E0000006D0000006C000000850005001E0000006E0000006B0000006D000000810005001E0000006F0000006900000063000000810005001E000000700000006E000000640000004F00070058000000710000003A0000003A00000000000000010000007C0004001E00000072000000710000004F0007005800000073000000370000003700000000000000010000007C0004001E0000007400000073000000850005001E0000007500000072000000740000004F00070058000000760000003A0000003A00000002000000030000007C0004001E00000077000000760000004F0007005800000078000000370000003700000002000000030000007C0004001E0000007900000078000000850005001E0000007A0000007700000079000000810005001E0000007B000000750000006F000000810005001E0000007C0000007A000000700000004F000700580000007D000000400000004000000000000000010000007C0004001E0000007E0000007D0000004F000700580000007F0000003D0000003D00000000000000010000007C0004001E000000800000007F000000850005001E000000810000007E000000800000004F0007005800000082000000400000004000000002000000030000007C0004001E00000083000000820000004F00070058000000840000003D0000003D00000002000000030000007C0004001E0000008500000084000000850005001E000000860000008300000085000000810005001E00000087000000810000007B000000810005001E00000088000000860000007C0000004F0007005800000089000000460000004600000000000000010000007C0004001E0000008A000000890000004F000700580000008B000000430000004300000000000000010000007C0004001E0000008C0000008B000000850005001E0000008D0000008A0000008C0000004F000700580000008E000000460000004600000002000000030000007C0004001E0000008F0000008E0000004F0007005800000090000000430000004300000002000000030000007C0004001E0000009100000090000000850005001E000000920000008F00000091000000810005001E000000930000008D00000087000000810005001E0000009400000092000000880000004F00070058000000950000004C0000004C00000000000000010000007C0004001E00000096000000950000004F0007005800000097000000490000004900000000000000010000007C0004001E0000009800000097000000850005001E0000009900000096000000980000004F000700580000009A0000004C0000004C00000002000000030000007C0004001E0000009B0000009A0000004F000700580000009C000000490000004900000002000000030000007C0004001E0000009D0000009C000000850005001E0000009E0000009B0000009D000000810005001E0000009F0000009900000093000000810005001E000000A00000009E000000940000004F00070058000000A1000000520000005200000000000000010000007C0004001E000000A2000000A10000004F00070058000000A30000004F0000004F00000000000000010000007C0004001E000000A4000000A3000000850005001E000000A5000000A2000000A40000004F00070058000000A6000000520000005200000002000000030000007C0004001E000000A7000000A60000004F00070058000000A80000004F0000004F00000002000000030000007C0004001E000000A9000000A8000000850005001E000000AA000000A7000000A9000000810005001E000000AB000000A50000009F000000810005001E000000AC000000AA000000A00000004F00070058000000AD000000570000005700000000000000010000007C0004001E000000AE000000AD0000004F00070058000000AF000000550000005500000000000000010000007C0004001E000000B0000000AF000000850005001E000000B1000000AE000000B00000004F00070058000000B2000000570000005700000002000000030000007C0004001E000000B3000000B20000004F00070058000000B4000000550000005500000002000000030000007C0004001E000000B5000000B4000000850005001E000000B6000000B3000000B5000000810005001E000000B7000000B1000000AB000000810005001E000000B8000000B6000000AC0000005100050010000000B9000000B7000000000000005100050010000000BA000000B7000000010000005100050010000000BB000000B7000000020000005100050010000000BC000000B7000000030000008100050010000000BD000000B9000000BA0000008100050010000000BE000000BD000000BB0000008100050010000000BF000000BE000000BC0000005100050010000000C0000000B8000000000000005100050010000000C1000000B8000000010000005100050010000000C2000000B8000000020000005100050010000000C3000000B8000000030000008100050010000000C4000000C0000000C10000008100050010000000C5000000C4000000C20000008100050010000000C6000000C5000000C30000008100050010000000C7000000BF000000C60000005E01060010000000C8000000C900000000000000C70000008100050010000000CA000000C800000020000000AA000500CB000000CC0000002300000021000000F9000200CD000000F8000200CD000000F7000300CF00000000000000FA000400CC000000CE000000CF000000F8000200CE00000041000600D0000000D10000001100000021000000250000003E000300D1000000CA000000F9000200CF000000F8000200CF000000FD0001003800010008000C00040008000C0010000400000008000C00"> : vector<5020xi8>, format = "vulkan-spirv-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func private @_main() {
    %c1_i32 = arith.constant 1 : i32
    %c553648144_i32 = arith.constant 553648144 : i32
    %c32000 = arith.constant 32000 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
    %device = hal.ex.shared_device : !hal.device
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
    %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_2, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
    %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
    %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_9, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %3 = arith.select %_device_query_0, %c0, %c-1 : index
    %4 = arith.index_cast %3 : index to i32
    cf.switch %4 : i32, [
      default: ^bb2,
      0: ^bb1
    ]
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%1 : !hal.buffer)[%c0, %len], 
      %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
      %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
    ])
    hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
    cf.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
    %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
    %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_17, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable__main_dispatch_0 : !hal.executable
  util.initializer {
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    %0 = arith.select %value, %c0, %c-1 : index
    %1 = arith.index_cast %0 : index to i32
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.switch %1 : i32, [
      default: ^bb2,
      0: ^bb1
    ]
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @_executable__main_dispatch_0 : !hal.executable
    util.initializer.return
  }
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.binary public @vulkan_spirv_fb attributes {data = dense<"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vector<5020xi8>, format = "vulkan-spirv-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func private @_main() {
    %c1_i32 = arith.constant 1 : i32
    %c553648144_i32 = arith.constant 553648144 : i32
    %c32000 = arith.constant 32000 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
    %device = hal.ex.shared_device : !hal.device
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
    %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_2, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
    %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
    %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_9, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %3 = arith.select %_device_query_0, %c0, %c-1 : index
    %4 = arith.index_cast %3 : index to i32
    cf.switch %4 : i32, [
      default: ^bb2,
      0: ^bb1
    ]
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%1 : !hal.buffer)[%c0, %len], 
      %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
      %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
    ])
    hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
    cf.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
    %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
    %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_17, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable__main_dispatch_0 : !hal.executable
  util.initializer {
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    %0 = arith.select %value, %c0, %c-1 : index
    %1 = arith.index_cast %0 : index to i32
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.switch %1 : i32, [
      default: ^bb2,
      0: ^bb1
    ]
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @_executable__main_dispatch_0 : !hal.executable
    util.initializer.return
  }
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.binary public @vulkan_spirv_fb attributes {data = dense<"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vector<5020xi8>, format = "vulkan-spirv-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func private @_main() {
    %c1_i32 = arith.constant 1 : i32
    %c553648144_i32 = arith.constant 553648144 : i32
    %c32000 = arith.constant 32000 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
    %device = hal.ex.shared_device : !hal.device
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
    %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_2, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
    %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
    %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_9, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %3 = arith.select %_device_query_0, %c0, %c-1 : index
    %4 = arith.index_cast %3 : index to i32
    cf.switch %4 : i32, [
      default: ^bb2,
      0: ^bb1
    ]
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%1 : !hal.buffer)[%c0, %len], 
      %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
      %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
    ])
    hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
    cf.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
    %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
    %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_17, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After FixedPointIterator (iree-util-fixed-point-iterator) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable__main_dispatch_0 : !hal.executable
  util.initializer {
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    %0 = arith.select %value, %c0, %c-1 : index
    %1 = arith.index_cast %0 : index to i32
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.switch %1 : i32, [
      default: ^bb2,
      0: ^bb1
    ]
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @_executable__main_dispatch_0 : !hal.executable
    util.initializer.return
  }
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.binary public @vulkan_spirv_fb attributes {data = dense<"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vector<5020xi8>, format = "vulkan-spirv-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func private @_main() {
    %c1_i32 = arith.constant 1 : i32
    %c553648144_i32 = arith.constant 553648144 : i32
    %c32000 = arith.constant 32000 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
    %device = hal.ex.shared_device : !hal.device
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
    %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_2, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
    %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
    %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_9, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %3 = arith.select %_device_query_0, %c0, %c-1 : index
    %4 = arith.index_cast %3 : index to i32
    cf.switch %4 : i32, [
      default: ^bb2,
      0: ^bb1
    ]
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%1 : !hal.buffer)[%c0, %len], 
      %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
      %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
    ])
    hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
    cf.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
    %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
    %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_17, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.initializer {
  %c0 = arith.constant 0 : index
  %c-1 = arith.constant -1 : index
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  %0 = arith.select %value, %c0, %c-1 : index
  %1 = arith.index_cast %0 : index to i32
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.switch %1 : i32, [
    default: ^bb2,
    0: ^bb1
  ]
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %2 = util.null : !hal.executable
  cf.br ^bb3(%2 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %3, @_executable__main_dispatch_0 : !hal.executable
  util.initializer.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func private @_main() {
  %c1_i32 = arith.constant 1 : i32
  %c553648144_i32 = arith.constant 553648144 : i32
  %c32000 = arith.constant 32000 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c-1 = arith.constant -1 : index
  %c-1_i32 = arith.constant -1 : i32
  %c0_i64 = arith.constant 0 : i64
  %c-1_i64 = arith.constant -1 : i64
  %c128000 = arith.constant 128000 : index
  %c0 = arith.constant 0 : index
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
  %device = hal.ex.shared_device : !hal.device
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
  hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
  %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_2, "failed to wait on timepoint"
  %1 = util.optimization_barrier %transient_buffer : !hal.buffer
  %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
  %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
  %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
  hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
  %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
  %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_9, "failed to wait on timepoint"
  %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
  %len = hal.buffer.length<%1 : !hal.buffer> : index
  %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
  %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
  %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
  %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  %3 = arith.select %_device_query_0, %c0, %c-1 : index
  %4 = arith.index_cast %3 : index to i32
  cf.switch %4 : i32, [
    default: ^bb2,
    0: ^bb1
  ]
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%1 : !hal.buffer)[%c0, %len], 
    %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
    %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
  ])
  hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
  cf.br ^bb2
^bb2:  // 2 preds: ^bb0, ^bb1
  hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
  %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
  %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_17, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
  %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
  check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After Inliner (inline) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable__main_dispatch_0 : !hal.executable
  util.initializer {
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    %0 = arith.select %value, %c0, %c-1 : index
    %1 = arith.index_cast %0 : index to i32
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.switch %1 : i32, [
      default: ^bb2,
      0: ^bb1
    ]
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @_executable__main_dispatch_0 : !hal.executable
    util.initializer.return
  }
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.binary public @vulkan_spirv_fb attributes {data = dense<"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vector<5020xi8>, format = "vulkan-spirv-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func private @_main() {
    %c1_i32 = arith.constant 1 : i32
    %c553648144_i32 = arith.constant 553648144 : i32
    %c32000 = arith.constant 32000 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
    %device = hal.ex.shared_device : !hal.device
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
    %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_2, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
    %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
    %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_9, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %3 = arith.select %_device_query_0, %c0, %c-1 : index
    %4 = arith.index_cast %3 : index to i32
    cf.switch %4 : i32, [
      default: ^bb2,
      0: ^bb1
    ]
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%1 : !hal.buffer)[%c0, %len], 
      %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
      %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
    ])
    hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
    cf.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
    %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
    %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_17, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable__main_dispatch_0 : !hal.executable
  util.initializer {
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    %0 = arith.select %value, %c0, %c-1 : index
    %1 = arith.index_cast %0 : index to i32
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.switch %1 : i32, [
      default: ^bb2,
      0: ^bb1
    ]
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @_executable__main_dispatch_0 : !hal.executable
    util.initializer.return
  }
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.binary public @vulkan_spirv_fb attributes {data = dense<"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vector<5020xi8>, format = "vulkan-spirv-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func private @_main() {
    %c1_i32 = arith.constant 1 : i32
    %c553648144_i32 = arith.constant 553648144 : i32
    %c32000 = arith.constant 32000 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
    %device = hal.ex.shared_device : !hal.device
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
    %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_2, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
    %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
    %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_9, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %3 = arith.select %_device_query_0, %c0, %c-1 : index
    %4 = arith.index_cast %3 : index to i32
    cf.switch %4 : i32, [
      default: ^bb2,
      0: ^bb1
    ]
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%1 : !hal.buffer)[%c0, %len], 
      %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
      %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
    ])
    hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
    cf.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
    %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
    %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_17, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After SCFForLoopCanonicalization (scf-for-loop-canonicalization) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After LoopCoalescing (affine-loop-coalescing) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After LoopInvariantCodeMotion (loop-invariant-code-motion) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After SCFForLoopCanonicalization (scf-for-loop-canonicalization) //----- //
util.initializer {
  %c0 = arith.constant 0 : index
  %c-1 = arith.constant -1 : index
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  %0 = arith.select %value, %c0, %c-1 : index
  %1 = arith.index_cast %0 : index to i32
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.switch %1 : i32, [
    default: ^bb2,
    0: ^bb1
  ]
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %2 = util.null : !hal.executable
  cf.br ^bb3(%2 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %3, @_executable__main_dispatch_0 : !hal.executable
  util.initializer.return
}

// -----// IR Dump After LoopInvariantCodeMotion (loop-invariant-code-motion) //----- //
util.initializer {
  %c0 = arith.constant 0 : index
  %c-1 = arith.constant -1 : index
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  %0 = arith.select %value, %c0, %c-1 : index
  %1 = arith.index_cast %0 : index to i32
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.switch %1 : i32, [
    default: ^bb2,
    0: ^bb1
  ]
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %2 = util.null : !hal.executable
  cf.br ^bb3(%2 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %3, @_executable__main_dispatch_0 : !hal.executable
  util.initializer.return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After SCFForLoopCanonicalization (scf-for-loop-canonicalization) //----- //
func.func private @_main() {
  %c1_i32 = arith.constant 1 : i32
  %c553648144_i32 = arith.constant 553648144 : i32
  %c32000 = arith.constant 32000 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c-1 = arith.constant -1 : index
  %c-1_i32 = arith.constant -1 : i32
  %c0_i64 = arith.constant 0 : i64
  %c-1_i64 = arith.constant -1 : i64
  %c128000 = arith.constant 128000 : index
  %c0 = arith.constant 0 : index
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
  %device = hal.ex.shared_device : !hal.device
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
  hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
  %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_2, "failed to wait on timepoint"
  %1 = util.optimization_barrier %transient_buffer : !hal.buffer
  %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
  %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
  %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
  hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
  %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
  %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_9, "failed to wait on timepoint"
  %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
  %len = hal.buffer.length<%1 : !hal.buffer> : index
  %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
  %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
  %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
  %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  %3 = arith.select %_device_query_0, %c0, %c-1 : index
  %4 = arith.index_cast %3 : index to i32
  cf.switch %4 : i32, [
    default: ^bb2,
    0: ^bb1
  ]
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%1 : !hal.buffer)[%c0, %len], 
    %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
    %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
  ])
  hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
  cf.br ^bb2
^bb2:  // 2 preds: ^bb0, ^bb1
  hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
  %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
  %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_17, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
  %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
  check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
  return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.initializer {
  %c0 = arith.constant 0 : index
  %c-1 = arith.constant -1 : index
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  %0 = arith.select %value, %c0, %c-1 : index
  %1 = arith.index_cast %0 : index to i32
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.switch %1 : i32, [
    default: ^bb2,
    0: ^bb1
  ]
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %2 = util.null : !hal.executable
  cf.br ^bb3(%2 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %3, @_executable__main_dispatch_0 : !hal.executable
  util.initializer.return
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After ArithUnsignedWhenEquivalent (arith-unsigned-when-equivalent) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
util.initializer {
  %c0 = arith.constant 0 : index
  %c-1 = arith.constant -1 : index
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  %0 = arith.select %value, %c0, %c-1 : index
  %1 = arith.index_cast %0 : index to i32
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.switch %1 : i32, [
    default: ^bb2,
    0: ^bb1
  ]
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %2 = util.null : !hal.executable
  cf.br ^bb3(%2 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %3, @_executable__main_dispatch_0 : !hal.executable
  util.initializer.return
}

// -----// IR Dump After LoopCoalescing (affine-loop-coalescing) //----- //
func.func private @_main() {
  %c1_i32 = arith.constant 1 : i32
  %c553648144_i32 = arith.constant 553648144 : i32
  %c32000 = arith.constant 32000 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c-1 = arith.constant -1 : index
  %c-1_i32 = arith.constant -1 : i32
  %c0_i64 = arith.constant 0 : i64
  %c-1_i64 = arith.constant -1 : i64
  %c128000 = arith.constant 128000 : index
  %c0 = arith.constant 0 : index
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
  %device = hal.ex.shared_device : !hal.device
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
  hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
  %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_2, "failed to wait on timepoint"
  %1 = util.optimization_barrier %transient_buffer : !hal.buffer
  %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
  %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
  %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
  hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
  %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
  %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_9, "failed to wait on timepoint"
  %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
  %len = hal.buffer.length<%1 : !hal.buffer> : index
  %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
  %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
  %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
  %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  %3 = arith.select %_device_query_0, %c0, %c-1 : index
  %4 = arith.index_cast %3 : index to i32
  cf.switch %4 : i32, [
    default: ^bb2,
    0: ^bb1
  ]
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%1 : !hal.buffer)[%c0, %len], 
    %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
    %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
  ])
  hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
  cf.br ^bb2
^bb2:  // 2 preds: ^bb0, ^bb1
  hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
  %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
  %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_17, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
  %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
  check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
  return
}

// -----// IR Dump After ArithUnsignedWhenEquivalent (arith-unsigned-when-equivalent) //----- //
util.initializer {
  %c0 = arith.constant 0 : index
  %c-1 = arith.constant -1 : index
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  %0 = arith.select %value, %c0, %c-1 : index
  %1 = arith.index_cast %0 : index to i32
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.switch %1 : i32, [
    default: ^bb2,
    0: ^bb1
  ]
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %2 = util.null : !hal.executable
  cf.br ^bb3(%2 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %3, @_executable__main_dispatch_0 : !hal.executable
  util.initializer.return
}

// -----// IR Dump After LoopInvariantCodeMotion (loop-invariant-code-motion) //----- //
func.func private @_main() {
  %c1_i32 = arith.constant 1 : i32
  %c553648144_i32 = arith.constant 553648144 : i32
  %c32000 = arith.constant 32000 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c-1 = arith.constant -1 : index
  %c-1_i32 = arith.constant -1 : i32
  %c0_i64 = arith.constant 0 : i64
  %c-1_i64 = arith.constant -1 : i64
  %c128000 = arith.constant 128000 : index
  %c0 = arith.constant 0 : index
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
  %device = hal.ex.shared_device : !hal.device
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
  hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
  %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_2, "failed to wait on timepoint"
  %1 = util.optimization_barrier %transient_buffer : !hal.buffer
  %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
  %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
  %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
  hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
  %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
  %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_9, "failed to wait on timepoint"
  %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
  %len = hal.buffer.length<%1 : !hal.buffer> : index
  %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
  %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
  %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
  %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  %3 = arith.select %_device_query_0, %c0, %c-1 : index
  %4 = arith.index_cast %3 : index to i32
  cf.switch %4 : i32, [
    default: ^bb2,
    0: ^bb1
  ]
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%1 : !hal.buffer)[%c0, %len], 
    %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
    %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
  ])
  hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
  cf.br ^bb2
^bb2:  // 2 preds: ^bb0, ^bb1
  hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
  %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
  %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_17, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
  %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
  check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
  return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
func.func private @_main() {
  %c1_i32 = arith.constant 1 : i32
  %c553648144_i32 = arith.constant 553648144 : i32
  %c32000 = arith.constant 32000 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c-1 = arith.constant -1 : index
  %c-1_i32 = arith.constant -1 : i32
  %c0_i64 = arith.constant 0 : i64
  %c-1_i64 = arith.constant -1 : i64
  %c128000 = arith.constant 128000 : index
  %c0 = arith.constant 0 : index
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
  %device = hal.ex.shared_device : !hal.device
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
  hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
  %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_2, "failed to wait on timepoint"
  %1 = util.optimization_barrier %transient_buffer : !hal.buffer
  %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
  %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
  %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
  hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
  %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
  %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_9, "failed to wait on timepoint"
  %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
  %len = hal.buffer.length<%1 : !hal.buffer> : index
  %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
  %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
  %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
  %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  %3 = arith.select %_device_query_0, %c0, %c-1 : index
  %4 = arith.index_cast %3 : index to i32
  cf.switch %4 : i32, [
    default: ^bb2,
    0: ^bb1
  ]
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%1 : !hal.buffer)[%c0, %len], 
    %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
    %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
  ])
  hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
  cf.br ^bb2
^bb2:  // 2 preds: ^bb0, ^bb1
  hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
  %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
  %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_17, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
  %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
  check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
  return
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
func.func private @_main() {
  %c1_i32 = arith.constant 1 : i32
  %c553648144_i32 = arith.constant 553648144 : i32
  %c32000 = arith.constant 32000 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c-1 = arith.constant -1 : index
  %c-1_i32 = arith.constant -1 : i32
  %c0_i64 = arith.constant 0 : i64
  %c-1_i64 = arith.constant -1 : i64
  %c128000 = arith.constant 128000 : index
  %c0 = arith.constant 0 : index
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
  %device = hal.ex.shared_device : !hal.device
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
  hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
  %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_2, "failed to wait on timepoint"
  %1 = util.optimization_barrier %transient_buffer : !hal.buffer
  %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
  %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
  %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
  hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
  %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
  %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_9, "failed to wait on timepoint"
  %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
  %len = hal.buffer.length<%1 : !hal.buffer> : index
  %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
  %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
  %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
  %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  %3 = arith.select %_device_query_0, %c0, %c-1 : index
  %4 = arith.index_cast %3 : index to i32
  cf.switch %4 : i32, [
    default: ^bb2,
    0: ^bb1
  ]
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%1 : !hal.buffer)[%c0, %len], 
    %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
    %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
  ])
  hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
  cf.br ^bb2
^bb2:  // 2 preds: ^bb0, ^bb1
  hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
  %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
  %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_17, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
  %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
  check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
  return
}

// -----// IR Dump After ArithUnsignedWhenEquivalent (arith-unsigned-when-equivalent) //----- //
func.func private @_main() {
  %c1_i32 = arith.constant 1 : i32
  %c553648144_i32 = arith.constant 553648144 : i32
  %c32000 = arith.constant 32000 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c-1 = arith.constant -1 : index
  %c-1_i32 = arith.constant -1 : i32
  %c0_i64 = arith.constant 0 : i64
  %c-1_i64 = arith.constant -1 : i64
  %c128000 = arith.constant 128000 : index
  %c0 = arith.constant 0 : index
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
  %device = hal.ex.shared_device : !hal.device
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
  hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
  %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_2, "failed to wait on timepoint"
  %1 = util.optimization_barrier %transient_buffer : !hal.buffer
  %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
  %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
  %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
  hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
  %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
  %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_9, "failed to wait on timepoint"
  %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
  %len = hal.buffer.length<%1 : !hal.buffer> : index
  %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
  %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
  %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
  %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  %3 = arith.select %_device_query_0, %c0, %c-1 : index
  %4 = arith.index_cast %3 : index to i32
  cf.switch %4 : i32, [
    default: ^bb2,
    0: ^bb1
  ]
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%1 : !hal.buffer)[%c0, %len], 
    %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
    %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
  ])
  hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
  cf.br ^bb2
^bb2:  // 2 preds: ^bb0, ^bb1
  hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
  %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
  %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_17, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
  %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
  check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
  return
}

// -----// IR Dump After PropagateSubranges (iree-util-propagate-subranges) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable__main_dispatch_0 : !hal.executable
  util.initializer {
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    %0 = arith.select %value, %c0, %c-1 : index
    %1 = arith.index_cast %0 : index to i32
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.switch %1 : i32, [
      default: ^bb2,
      0: ^bb1
    ]
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @_executable__main_dispatch_0 : !hal.executable
    util.initializer.return
  }
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.binary public @vulkan_spirv_fb attributes {data = dense<"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vector<5020xi8>, format = "vulkan-spirv-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func private @_main() {
    %c1_i32 = arith.constant 1 : i32
    %c553648144_i32 = arith.constant 553648144 : i32
    %c32000 = arith.constant 32000 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
    %device = hal.ex.shared_device : !hal.device
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
    %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_2, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
    %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
    %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_9, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %3 = arith.select %_device_query_0, %c0, %c-1 : index
    %4 = arith.index_cast %3 : index to i32
    cf.switch %4 : i32, [
      default: ^bb2,
      0: ^bb1
    ]
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%1 : !hal.buffer)[%c0, %len], 
      %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
      %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
    ])
    hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
    cf.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
    %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
    %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_17, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable__main_dispatch_0 : !hal.executable
  util.initializer {
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    %0 = arith.select %value, %c0, %c-1 : index
    %1 = arith.index_cast %0 : index to i32
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.switch %1 : i32, [
      default: ^bb2,
      0: ^bb1
    ]
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @_executable__main_dispatch_0 : !hal.executable
    util.initializer.return
  }
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.binary public @vulkan_spirv_fb attributes {data = dense<"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vector<5020xi8>, format = "vulkan-spirv-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func private @_main() {
    %c1_i32 = arith.constant 1 : i32
    %c553648144_i32 = arith.constant 553648144 : i32
    %c32000 = arith.constant 32000 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
    %device = hal.ex.shared_device : !hal.device
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
    %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_2, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
    %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
    %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_9, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %3 = arith.select %_device_query_0, %c0, %c-1 : index
    %4 = arith.index_cast %3 : index to i32
    cf.switch %4 : i32, [
      default: ^bb2,
      0: ^bb1
    ]
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%1 : !hal.buffer)[%c0, %len], 
      %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
      %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
    ])
    hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
    cf.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
    %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
    %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_17, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable__main_dispatch_0 : !hal.executable
  util.initializer {
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    %0 = arith.select %value, %c0, %c-1 : index
    %1 = arith.index_cast %0 : index to i32
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.switch %1 : i32, [
      default: ^bb2,
      0: ^bb1
    ]
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @_executable__main_dispatch_0 : !hal.executable
    util.initializer.return
  }
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.binary public @vulkan_spirv_fb attributes {data = dense<"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vector<5020xi8>, format = "vulkan-spirv-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func private @_main() {
    %c1_i32 = arith.constant 1 : i32
    %c553648144_i32 = arith.constant 553648144 : i32
    %c32000 = arith.constant 32000 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128000 = arith.constant 128000 : index
    %c0 = arith.constant 0 : index
    %c64000 = arith.constant 64000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c262144000 = arith.constant 262144000 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c8192 = arith.constant 8192 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
    %device = hal.ex.shared_device : !hal.device
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
    %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_2, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
    %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
    %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_9, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %3 = arith.select %_device_query_0, %c0, %c-1 : index
    %4 = arith.index_cast %3 : index to i32
    cf.switch %4 : i32, [
      default: ^bb2,
      0: ^bb1
    ]
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%1 : !hal.buffer)[%c0, %len], 
      %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
      %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
    ])
    hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
    cf.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
    %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
    %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_17, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %c0 = arith.constant 0 : index
  %c-1 = arith.constant -1 : index
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  %0 = arith.select %value, %c0, %c-1 : index
  %1 = arith.index_cast %0 : index to i32
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.switch %1 : i32, [
    default: ^bb2,
    0: ^bb1
  ]
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %2 = util.null : !hal.executable
  cf.br ^bb3(%2 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %3, @_executable__main_dispatch_0 : !hal.executable
  util.initializer.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @main() attributes {iree.abi.stub} {
  call @_main() : () -> ()
  return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func private @_main() {
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
  %c1_i32 = arith.constant 1 : i32
  %c553648144_i32 = arith.constant 553648144 : i32
  %c32000 = arith.constant 32000 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c-1 = arith.constant -1 : index
  %c-1_i32 = arith.constant -1 : i32
  %c0_i64 = arith.constant 0 : i64
  %c-1_i64 = arith.constant -1 : i64
  %c128000 = arith.constant 128000 : index
  %c0 = arith.constant 0 : index
  %c64000 = arith.constant 64000 : index
  %c27648_i16 = arith.constant 27648 : i16
  %c262144000 = arith.constant 262144000 : index
  %c15360_i16 = arith.constant 15360 : i16
  %c8192 = arith.constant 8192 : index
  %device = hal.ex.shared_device : !hal.device
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
  hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
  %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_2, "failed to wait on timepoint"
  %1 = util.optimization_barrier %transient_buffer : !hal.buffer
  %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
  %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
  %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
  hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
  %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
  %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_9, "failed to wait on timepoint"
  %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
  %len = hal.buffer.length<%1 : !hal.buffer> : index
  %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
  %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
  %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
  %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  %3 = arith.select %_device_query_0, %c0, %c-1 : index
  %4 = arith.index_cast %3 : index to i32
  cf.switch %4 : i32, [
    default: ^bb2,
    0: ^bb1
  ]
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%1 : !hal.buffer)[%c0, %len], 
    %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
    %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
  ])
  hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
  cf.br ^bb2
^bb2:  // 2 preds: ^bb0, ^bb1
  hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
  hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
  %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
  %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status_17, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
  %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
  check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
  return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable__main_dispatch_0 : !hal.executable
  util.initializer {
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    %0 = arith.select %value, %c0, %c-1 : index
    %1 = arith.index_cast %0 : index to i32
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.switch %1 : i32, [
      default: ^bb2,
      0: ^bb1
    ]
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @_executable__main_dispatch_0 : !hal.executable
    util.initializer.return
  }
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.binary public @vulkan_spirv_fb attributes {data = dense<"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vector<5020xi8>, format = "vulkan-spirv-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func private @_main() {
    %c8192 = arith.constant 8192 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c262144000 = arith.constant 262144000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c64000 = arith.constant 64000 : index
    %c0 = arith.constant 0 : index
    %c128000 = arith.constant 128000 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c32000 = arith.constant 32000 : index
    %c553648144_i32 = arith.constant 553648144 : i32
    %c1_i32 = arith.constant 1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
    %device = hal.ex.shared_device : !hal.device
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
    %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_2, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
    %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
    %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_9, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %3 = arith.select %_device_query_0, %c0, %c-1 : index
    %4 = arith.index_cast %3 : index to i32
    cf.switch %4 : i32, [
      default: ^bb2,
      0: ^bb1
    ]
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%1 : !hal.buffer)[%c0, %len], 
      %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
      %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
    ])
    hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
    cf.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
    %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
    %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_17, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable__main_dispatch_0 : !hal.executable
  util.initializer {
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    %0 = arith.select %value, %c0, %c-1 : index
    %1 = arith.index_cast %0 : index to i32
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.switch %1 : i32, [
      default: ^bb2,
      0: ^bb1
    ]
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @_executable__main_dispatch_0 : !hal.executable
    util.initializer.return
  }
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.binary public @vulkan_spirv_fb attributes {data = dense<"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vector<5020xi8>, format = "vulkan-spirv-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func private @_main() {
    %c8192 = arith.constant 8192 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c262144000 = arith.constant 262144000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c64000 = arith.constant 64000 : index
    %c0 = arith.constant 0 : index
    %c128000 = arith.constant 128000 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c32000 = arith.constant 32000 : index
    %c553648144_i32 = arith.constant 553648144 : i32
    %c1_i32 = arith.constant 1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
    %device = hal.ex.shared_device : !hal.device
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
    %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_2, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
    %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
    %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_9, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %3 = arith.select %_device_query_0, %c0, %c-1 : index
    %4 = arith.index_cast %3 : index to i32
    cf.switch %4 : i32, [
      default: ^bb2,
      0: ^bb1
    ]
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%1 : !hal.buffer)[%c0, %len], 
      %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
      %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
    ])
    hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
    cf.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
    %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
    %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_17, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable__main_dispatch_0 : !hal.executable
  util.initializer {
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "vulkan-spirv-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    %0 = arith.select %value, %c0, %c-1 : index
    %1 = arith.index_cast %0 : index to i32
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.switch %1 : i32, [
      default: ^bb2,
      0: ^bb1
    ]
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@_main_dispatch_0::@vulkan_spirv_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @_executable__main_dispatch_0 : !hal.executable
    util.initializer.return
  }
  func.func @main() attributes {iree.abi.stub} {
    call @_main() : () -> ()
    return
  }
  hal.executable private @_main_dispatch_0 {
    hal.executable.binary public @vulkan_spirv_fb attributes {data = dense<"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vector<5020xi8>, format = "vulkan-spirv-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func private @_main() {
    %c8192 = arith.constant 8192 : index
    %c15360_i16 = arith.constant 15360 : i16
    %c262144000 = arith.constant 262144000 : index
    %c27648_i16 = arith.constant 27648 : i16
    %c64000 = arith.constant 64000 : index
    %c0 = arith.constant 0 : index
    %c128000 = arith.constant 128000 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c32000 = arith.constant 32000 : index
    %c553648144_i32 = arith.constant 553648144 : i32
    %c1_i32 = arith.constant 1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable__main_dispatch_0 = util.global.load @_executable__main_dispatch_0 : !hal.executable
    %device = hal.ex.shared_device : !hal.device
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c8192}
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd : !hal.command_buffer> target(%transient_buffer : !hal.buffer)[%c0, %c8192] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_0 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_1 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_0) commands([%cmd])
    %status_2 = hal.fence.await until([%fence_0]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_2, "failed to wait on timepoint"
    %1 = util.optimization_barrier %transient_buffer : !hal.buffer
    %fence_3 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_4 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_3) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c262144000}
    %status_5 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    %cmd_6 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories(Transfer) : !hal.command_buffer
    hal.command_buffer.fill_buffer<%cmd_6 : !hal.command_buffer> target(%transient_buffer_4 : !hal.buffer)[%c0, %c262144000] pattern(%c15360_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_6 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_6 : !hal.command_buffer>
    %fence_7 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_8 = hal.fence.await until([%fence_3]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_7) commands([%cmd_6])
    %status_9 = hal.fence.await until([%fence_7]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_9, "failed to wait on timepoint"
    %2 = util.optimization_barrier %transient_buffer_4 : !hal.buffer
    %len = hal.buffer.length<%1 : !hal.buffer> : index
    %len_10 = hal.buffer.length<%2 : !hal.buffer> : index
    %fence_11 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer_12 = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_11) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128000}
    %status_13 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    %cmd_14 = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %3 = arith.select %_device_query_0, %c0, %c-1 : index
    %4 = arith.index_cast %3 : index to i32
    cf.switch %4 : i32, [
      default: ^bb2,
      0: ^bb1
    ]
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd_14 : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%1 : !hal.buffer)[%c0, %len], 
      %c1 = (%2 : !hal.buffer)[%c0, %len_10], 
      %c2 = (%transient_buffer_12 : !hal.buffer)[%c0, %c128000]
    ])
    hal.command_buffer.dispatch<%cmd_14 : !hal.command_buffer> target(%_executable__main_dispatch_0 : !hal.executable)[0] workgroups([%c32000, %c1, %c1])
    cf.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    hal.command_buffer.fill_buffer<%cmd_14 : !hal.command_buffer> target(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] pattern(%c27648_i16 : i16)
    hal.command_buffer.execution_barrier<%cmd_14 : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd_14 : !hal.command_buffer>
    %fence_15 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %status_16 = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence_15) commands([%cmd_14])
    %status_17 = hal.fence.await until([%fence_15]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status_17, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c64000, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    %view_18 = hal.buffer_view.create buffer(%transient_buffer_12 : !hal.buffer)[%c0, %c64000] shape([%c1, %c32000]) type(%c553648144_i32) encoding(%c1_i32) : !hal.buffer_view
    check.expect_eq<%device>(%view_18, %view) : !hal.buffer_view
    return
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::ConversionPass (iree-vm-conversion) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @_main_dispatch_0_vulkan_spirv_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<5020xi8>
    vm.initializer {
      %zero = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %buffer = vm.rodata.inline "_utf8_hal_executable_format_EAB228F999C2D3A1" {alignment = 1 : i64} : !vm.buffer = "hal.executable.format"
      %buffer_0 = vm.rodata.inline "_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0" {alignment = 1 : i64} : !vm.buffer = "vulkan-spirv-fb"
      %0:2 = vm.call @hal.device.query.i64(%ref, %buffer, %buffer_0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %c1 = vm.const.i32 1
      %2 = vm.and.i32 %1, %c1 : i32
      %zero_1 = vm.const.i32.zero
      %3 = vm.select.i32 %0#0, %2, %zero_1 : i32
      %c1_2 = vm.const.i32 1
      %zero_3 = vm.const.i32.zero
      %zero_4 = vm.const.i32.zero
      %c7 = vm.const.i32 7
      %c1_5 = vm.const.i32 1
      %c1_6 = vm.const.i32 1
      %c7_7 = vm.const.i32 7
      %c1_8 = vm.const.i32 1
      %c2 = vm.const.i32 2
      %c7_9 = vm.const.i32 7
      %zero_10 = vm.const.i32.zero
      %ref_11 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_3, [(%zero_4, %c7, %c1_5), (%c1_6, %c7_7, %c1_8), (%c2, %c7_9, %zero_10)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %zero_12 = vm.const.i32.zero
      %ref_13 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_12, [%ref_11]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      %4 = vm.select.i64 %3, %zero, %c-1 : i64
      %5 = vm.trunc.i64.i32 %4 : i64 -> i32
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_13, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.br_table %5 {
        default: ^bb2,
        0: ^bb1
      }
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_main_dispatch_0_vulkan_spirv_fb = vm.const.ref.rodata @_main_dispatch_0_vulkan_spirv_fb : !vm.buffer
      %buffer_14 = vm.rodata.inline "_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0" {alignment = 1 : i64} : !vm.buffer = "vulkan-spirv-fb"
      %null = vm.const.ref.zero : !vm.buffer
      %ref_15 = vm.call.variadic @hal.executable.create(%ref, %buffer_14, %_main_dispatch_0_vulkan_spirv_fb, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_15 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      %null_16 = vm.const.ref.zero : !vm.ref<!hal.executable>
      vm.br ^bb3(%null_16 : !vm.ref<!hal.executable>)
    ^bb3(%6: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %6, @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.func private @main() {
      vm.call @_main() : () -> ()
      vm.return
    }
    vm.export @main attributes {iree.abi.stub}
    vm.import private optional @check.expect_true(%operand : i32)
    vm.import private optional @check.expect_false(%operand : i32)
    vm.import private optional @check.expect_all_true(%device : !vm.ref<!hal.device>, %operand : !vm.ref<!hal.buffer_view>)
    vm.import private optional @check.expect_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
    vm.import private optional @check.expect_almost_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
    vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.ex.file.from_memory(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %access : i32, %buffer : !vm.buffer, %offset : i64, %length : i64, %flags : i32) -> !vm.ref<!hal.file>
    vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
    vm.import private @hal.allocator.import(%allocator : !vm.ref<!hal.allocator>, %try : i32, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32
    vm.import private @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...)
    vm.import private @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %flags : i32, %id : !vm.buffer, %group : !vm.buffer, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.split(%channel : !vm.ref<!hal.channel>, %color : i32, %key : i32, %flags : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer)
    vm.import private @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
    vm.import private @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64)
    vm.import private @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64)
    vm.import private @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64)
    vm.import private @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
    vm.import private @hal.device.queue.read(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_file : !vm.ref<!hal.file>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %flags : i32)
    vm.import private @hal.device.queue.write(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_file : !vm.ref<!hal.file>, %target_offset : i64, %length : i64, %flags : i32)
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64)
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects}
    vm.import private @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32
    vm.import private @hal.fence.signal(%fence : !vm.ref<!hal.fence>)
    vm.import private @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32)
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.func private @_main() {
      %c8192 = vm.const.i64 8192
      %c15360 = vm.const.i32 15360
      %c262144000 = vm.const.i64 262144000
      %c27648 = vm.const.i32 27648
      %c64000 = vm.const.i64 64000
      %zero = vm.const.i64.zero
      %c128000 = vm.const.i64 128000
      %c-1 = vm.const.i64 -1
      %zero_0 = vm.const.i64.zero
      %c-1_1 = vm.const.i32 -1
      %c-1_2 = vm.const.i64 -1
      %c1 = vm.const.i64 1
      %c2 = vm.const.i64 2
      %c32000 = vm.const.i64 32000
      %c553648144 = vm.const.i32 553648144
      %c1_3 = vm.const.i32 1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable__main_dispatch_0 = vm.global.load.ref @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %zero_4 = vm.const.i32.zero
      %ref_5 = vm.call @hal.fence.create(%ref, %zero_4) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %zero_6 = vm.const.i32.zero
      %c48 = vm.const.i32 48
      %c3075 = vm.const.i32 3075
      %ref_7 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_5, %zero_6, %c48, %c3075, %c8192) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_5]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %c17 = vm.const.i32 17
      %c1_8 = vm.const.i32 1
      %zero_9 = vm.const.i32.zero
      %ref_10 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_8, %zero_9) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      %c2_11 = vm.const.i32 2
      %c15360_12 = vm.const.i32 15360
      vm.call @hal.command_buffer.fill_buffer(%ref_10, %ref_7, %zero, %c8192, %c15360_12, %c2_11) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      %c28 = vm.const.i32 28
      %c13 = vm.const.i32 13
      %zero_13 = vm.const.i32.zero
      vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero_13) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
      %zero_14 = vm.const.i32.zero
      %ref_15 = vm.call @hal.fence.create(%ref, %zero_14) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %1 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_5]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_15, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %2 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_15]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_fail %2, "failed to wait on timepoint"
      %3 = util.optimization_barrier %ref_7 : !vm.ref<!hal.buffer>
      %zero_16 = vm.const.i32.zero
      %ref_17 = vm.call @hal.fence.create(%ref, %zero_16) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %zero_18 = vm.const.i32.zero
      %c48_19 = vm.const.i32 48
      %c3075_20 = vm.const.i32 3075
      %ref_21 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_17, %zero_18, %c48_19, %c3075_20, %c262144000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %4 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_17]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %c17_22 = vm.const.i32 17
      %c1_23 = vm.const.i32 1
      %zero_24 = vm.const.i32.zero
      %ref_25 = vm.call @hal.command_buffer.create(%ref, %c17_22, %c1_23, %zero_24) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      %c2_26 = vm.const.i32 2
      %c15360_27 = vm.const.i32 15360
      vm.call @hal.command_buffer.fill_buffer(%ref_25, %ref_21, %zero, %c262144000, %c15360_27, %c2_26) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      %c28_28 = vm.const.i32 28
      %c13_29 = vm.const.i32 13
      %zero_30 = vm.const.i32.zero
      vm.call @hal.command_buffer.execution_barrier(%ref_25, %c28_28, %c13_29, %zero_30) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_25) : (!vm.ref<!hal.command_buffer>) -> ()
      %zero_31 = vm.const.i32.zero
      %ref_32 = vm.call @hal.fence.create(%ref, %zero_31) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %5 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_17]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_32, [%ref_25]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %6 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_32]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_fail %6, "failed to wait on timepoint"
      %7 = util.optimization_barrier %ref_21 : !vm.ref<!hal.buffer>
      %8 = vm.call @hal.buffer.length(%3) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
      %9 = vm.call @hal.buffer.length(%7) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
      %zero_33 = vm.const.i32.zero
      %ref_34 = vm.call @hal.fence.create(%ref, %zero_33) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %zero_35 = vm.const.i32.zero
      %c48_36 = vm.const.i32 48
      %c3075_37 = vm.const.i32 3075
      %ref_38 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_34, %zero_35, %c48_36, %c3075_37, %c128000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %10 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_34]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %c17_39 = vm.const.i32 17
      %c3 = vm.const.i32 3
      %zero_40 = vm.const.i32.zero
      %ref_41 = vm.call @hal.command_buffer.create(%ref, %c17_39, %c3, %zero_40) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      %11 = vm.select.i64 %_device_query_0, %zero, %c-1_2 : i64
      %12 = vm.trunc.i64.i32 %11 : i64 -> i32
      vm.br_table %12 {
        default: ^bb2,
        0: ^bb1
      }
    ^bb1:  // pred: ^bb0
      %zero_42 = vm.const.i32.zero
      %zero_43 = vm.const.i32.zero
      %zero_44 = vm.const.i32.zero
      %c1_45 = vm.const.i32 1
      %c2_46 = vm.const.i32 2
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_41, %_pipeline_layout_0, %zero_42, [(%zero_43, %zero_44, %3, %zero, %8), (%c1_45, %zero_44, %7, %zero, %9), (%c2_46, %zero_44, %ref_38, %zero, %c128000)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      %zero_47 = vm.const.i32.zero
      %c32000_48 = vm.const.i32 32000
      %c1_49 = vm.const.i32 1
      %c1_50 = vm.const.i32 1
      vm.call @hal.command_buffer.dispatch(%ref_41, %_executable__main_dispatch_0, %zero_47, %c32000_48, %c1_49, %c1_50) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.br ^bb2
    ^bb2:  // 2 preds: ^bb0, ^bb1
      %c2_51 = vm.const.i32 2
      %c27648_52 = vm.const.i32 27648
      vm.call @hal.command_buffer.fill_buffer(%ref_41, %ref_38, %c64000, %c64000, %c27648_52, %c2_51) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      %c28_53 = vm.const.i32 28
      %c13_54 = vm.const.i32 13
      %zero_55 = vm.const.i32.zero
      vm.call @hal.command_buffer.execution_barrier(%ref_41, %c28_53, %c13_54, %zero_55) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_41) : (!vm.ref<!hal.command_buffer>) -> ()
      %zero_56 = vm.const.i32.zero
      %ref_57 = vm.call @hal.fence.create(%ref, %zero_56) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %13 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_34]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_57, [%ref_41]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %14 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_57]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_fail %14, "failed to wait on timepoint"
      %ref_58 = vm.call.variadic @hal.buffer_view.create(%ref_38, %c64000, %c64000, %c553648144, %c1_3, [%c1, %c32000]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      %ref_59 = vm.call.variadic @hal.buffer_view.create(%ref_38, %zero, %c64000, %c553648144, %c1_3, [%c1, %c32000]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      %has_check_expect_eq = vm.import.resolved @check.expect_eq : i32
      vm.cond_br %has_check_expect_eq, ^bb3, ^bb4
    ^bb3:  // pred: ^bb2
      vm.call @check.expect_eq(%ref, %ref_59, %ref_58) : (!vm.ref<!hal.device>, !vm.ref<!hal.buffer_view>, !vm.ref<!hal.buffer_view>) -> ()
      vm.br ^bb4
    ^bb4:  // 2 preds: ^bb2, ^bb3
      vm.return
    }
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::HoistInlinedRodataPass (iree-vm-hoist-inlined-rodata) //----- //
vm.module public @module {
  vm.global.i32 private @_device_query_0 : i32
  vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @_main_dispatch_0_vulkan_spirv_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<5020xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 {alignment = 1 : i64} "vulkan-spirv-fb"
  vm.rodata private @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0_0 {alignment = 1 : i64} "vulkan-spirv-fb"
  vm.initializer {
    %zero = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 = vm.const.ref.rodata @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %c1 = vm.const.i32 1
    %2 = vm.and.i32 %1, %c1 : i32
    %zero_0 = vm.const.i32.zero
    %3 = vm.select.i32 %0#0, %2, %zero_0 : i32
    %c1_1 = vm.const.i32 1
    %zero_2 = vm.const.i32.zero
    %zero_3 = vm.const.i32.zero
    %c7 = vm.const.i32 7
    %c1_4 = vm.const.i32 1
    %c1_5 = vm.const.i32 1
    %c7_6 = vm.const.i32 7
    %c1_7 = vm.const.i32 1
    %c2 = vm.const.i32 2
    %c7_8 = vm.const.i32 7
    %zero_9 = vm.const.i32.zero
    %ref_10 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_2, [(%zero_3, %c7, %c1_4), (%c1_5, %c7_6, %c1_7), (%c2, %c7_8, %zero_9)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %zero_11 = vm.const.i32.zero
    %ref_12 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_11, [%ref_10]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    %4 = vm.select.i64 %3, %zero, %c-1 : i64
    %5 = vm.trunc.i64.i32 %4 : i64 -> i32
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_12, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.br_table %5 {
      default: ^bb2,
      0: ^bb1
    }
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_main_dispatch_0_vulkan_spirv_fb = vm.const.ref.rodata @_main_dispatch_0_vulkan_spirv_fb : !vm.buffer
    %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0_0 = vm.const.ref.rodata @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0_0 : !vm.buffer
    %null = vm.const.ref.zero : !vm.buffer
    %ref_13 = vm.call.variadic @hal.executable.create(%ref, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0_0, %_main_dispatch_0_vulkan_spirv_fb, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_13 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    %null_14 = vm.const.ref.zero : !vm.ref<!hal.executable>
    vm.br ^bb3(%null_14 : !vm.ref<!hal.executable>)
  ^bb3(%6: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %6, @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  }
  vm.func private @main() {
    vm.call @_main() : () -> ()
    vm.return
  }
  vm.export @main attributes {iree.abi.stub}
  vm.import private optional @check.expect_true(%operand : i32)
  vm.import private optional @check.expect_false(%operand : i32)
  vm.import private optional @check.expect_all_true(%device : !vm.ref<!hal.device>, %operand : !vm.ref<!hal.buffer_view>)
  vm.import private optional @check.expect_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
  vm.import private optional @check.expect_almost_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
  vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
  vm.import private @hal.ex.file.from_memory(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %access : i32, %buffer : !vm.buffer, %offset : i64, %length : i64, %flags : i32) -> !vm.ref<!hal.file>
  vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
  vm.import private @hal.allocator.import(%allocator : !vm.ref<!hal.allocator>, %try : i32, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
  vm.import private @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32
  vm.import private @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects}
  vm.import private @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...)
  vm.import private @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %flags : i32, %id : !vm.buffer, %group : !vm.buffer, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
  vm.import private @hal.channel.split(%channel : !vm.ref<!hal.channel>, %color : i32, %key : i32, %flags : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
  vm.import private @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer)
  vm.import private @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
  vm.import private @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64)
  vm.import private @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64)
  vm.import private @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...)
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
  vm.import private @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64)
  vm.import private @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
  vm.import private @hal.device.queue.read(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_file : !vm.ref<!hal.file>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %flags : i32)
  vm.import private @hal.device.queue.write(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_file : !vm.ref<!hal.file>, %target_offset : i64, %length : i64, %flags : i32)
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64)
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects}
  vm.import private @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32
  vm.import private @hal.fence.signal(%fence : !vm.ref<!hal.fence>)
  vm.import private @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32)
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  vm.func private @_main() {
    %c8192 = vm.const.i64 8192
    %c15360 = vm.const.i32 15360
    %c262144000 = vm.const.i64 262144000
    %c27648 = vm.const.i32 27648
    %c64000 = vm.const.i64 64000
    %zero = vm.const.i64.zero
    %c128000 = vm.const.i64 128000
    %c-1 = vm.const.i64 -1
    %zero_0 = vm.const.i64.zero
    %c-1_1 = vm.const.i32 -1
    %c-1_2 = vm.const.i64 -1
    %c1 = vm.const.i64 1
    %c2 = vm.const.i64 2
    %c32000 = vm.const.i64 32000
    %c553648144 = vm.const.i32 553648144
    %c1_3 = vm.const.i32 1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable__main_dispatch_0 = vm.global.load.ref @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %zero_4 = vm.const.i32.zero
    %ref_5 = vm.call @hal.fence.create(%ref, %zero_4) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %zero_6 = vm.const.i32.zero
    %c48 = vm.const.i32 48
    %c3075 = vm.const.i32 3075
    %ref_7 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_5, %zero_6, %c48, %c3075, %c8192) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_5]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    %c17 = vm.const.i32 17
    %c1_8 = vm.const.i32 1
    %zero_9 = vm.const.i32.zero
    %ref_10 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_8, %zero_9) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    %c2_11 = vm.const.i32 2
    %c15360_12 = vm.const.i32 15360
    vm.call @hal.command_buffer.fill_buffer(%ref_10, %ref_7, %zero, %c8192, %c15360_12, %c2_11) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
    %c28 = vm.const.i32 28
    %c13 = vm.const.i32 13
    %zero_13 = vm.const.i32.zero
    vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero_13) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
    %zero_14 = vm.const.i32.zero
    %ref_15 = vm.call @hal.fence.create(%ref, %zero_14) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %1 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_5]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_15, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %2 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_15]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_fail %2, "failed to wait on timepoint"
    %3 = util.optimization_barrier %ref_7 : !vm.ref<!hal.buffer>
    %zero_16 = vm.const.i32.zero
    %ref_17 = vm.call @hal.fence.create(%ref, %zero_16) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %zero_18 = vm.const.i32.zero
    %c48_19 = vm.const.i32 48
    %c3075_20 = vm.const.i32 3075
    %ref_21 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_17, %zero_18, %c48_19, %c3075_20, %c262144000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %4 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_17]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    %c17_22 = vm.const.i32 17
    %c1_23 = vm.const.i32 1
    %zero_24 = vm.const.i32.zero
    %ref_25 = vm.call @hal.command_buffer.create(%ref, %c17_22, %c1_23, %zero_24) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    %c2_26 = vm.const.i32 2
    %c15360_27 = vm.const.i32 15360
    vm.call @hal.command_buffer.fill_buffer(%ref_25, %ref_21, %zero, %c262144000, %c15360_27, %c2_26) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
    %c28_28 = vm.const.i32 28
    %c13_29 = vm.const.i32 13
    %zero_30 = vm.const.i32.zero
    vm.call @hal.command_buffer.execution_barrier(%ref_25, %c28_28, %c13_29, %zero_30) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_25) : (!vm.ref<!hal.command_buffer>) -> ()
    %zero_31 = vm.const.i32.zero
    %ref_32 = vm.call @hal.fence.create(%ref, %zero_31) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %5 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_17]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_32, [%ref_25]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %6 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_32]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_fail %6, "failed to wait on timepoint"
    %7 = util.optimization_barrier %ref_21 : !vm.ref<!hal.buffer>
    %8 = vm.call @hal.buffer.length(%3) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
    %9 = vm.call @hal.buffer.length(%7) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
    %zero_33 = vm.const.i32.zero
    %ref_34 = vm.call @hal.fence.create(%ref, %zero_33) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %zero_35 = vm.const.i32.zero
    %c48_36 = vm.const.i32 48
    %c3075_37 = vm.const.i32 3075
    %ref_38 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_34, %zero_35, %c48_36, %c3075_37, %c128000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %10 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_34]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    %c17_39 = vm.const.i32 17
    %c3 = vm.const.i32 3
    %zero_40 = vm.const.i32.zero
    %ref_41 = vm.call @hal.command_buffer.create(%ref, %c17_39, %c3, %zero_40) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    %11 = vm.select.i64 %_device_query_0, %zero, %c-1_2 : i64
    %12 = vm.trunc.i64.i32 %11 : i64 -> i32
    vm.br_table %12 {
      default: ^bb2,
      0: ^bb1
    }
  ^bb1:  // pred: ^bb0
    %zero_42 = vm.const.i32.zero
    %zero_43 = vm.const.i32.zero
    %zero_44 = vm.const.i32.zero
    %c1_45 = vm.const.i32 1
    %c2_46 = vm.const.i32 2
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_41, %_pipeline_layout_0, %zero_42, [(%zero_43, %zero_44, %3, %zero, %8), (%c1_45, %zero_44, %7, %zero, %9), (%c2_46, %zero_44, %ref_38, %zero, %c128000)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    %zero_47 = vm.const.i32.zero
    %c32000_48 = vm.const.i32 32000
    %c1_49 = vm.const.i32 1
    %c1_50 = vm.const.i32 1
    vm.call @hal.command_buffer.dispatch(%ref_41, %_executable__main_dispatch_0, %zero_47, %c32000_48, %c1_49, %c1_50) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    %c2_51 = vm.const.i32 2
    %c27648_52 = vm.const.i32 27648
    vm.call @hal.command_buffer.fill_buffer(%ref_41, %ref_38, %c64000, %c64000, %c27648_52, %c2_51) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
    %c28_53 = vm.const.i32 28
    %c13_54 = vm.const.i32 13
    %zero_55 = vm.const.i32.zero
    vm.call @hal.command_buffer.execution_barrier(%ref_41, %c28_53, %c13_54, %zero_55) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_41) : (!vm.ref<!hal.command_buffer>) -> ()
    %zero_56 = vm.const.i32.zero
    %ref_57 = vm.call @hal.fence.create(%ref, %zero_56) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %13 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_34]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_57, [%ref_41]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %14 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_57]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_fail %14, "failed to wait on timepoint"
    %ref_58 = vm.call.variadic @hal.buffer_view.create(%ref_38, %c64000, %c64000, %c553648144, %c1_3, [%c1, %c32000]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    %ref_59 = vm.call.variadic @hal.buffer_view.create(%ref_38, %zero, %c64000, %c553648144, %c1_3, [%c1, %c32000]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    %has_check_expect_eq = vm.import.resolved @check.expect_eq : i32
    vm.cond_br %has_check_expect_eq, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    vm.call @check.expect_eq(%ref, %ref_59, %ref_58) : (!vm.ref<!hal.device>, !vm.ref<!hal.buffer_view>, !vm.ref<!hal.buffer_view>) -> ()
    vm.br ^bb4
  ^bb4:  // 2 preds: ^bb2, ^bb3
    vm.return
  }
}

// -----// IR Dump After mlir::iree_compiler::IREE::VM::DeduplicateRodataPass (iree-vm-deduplicate-rodata) //----- //
vm.module public @module {
  vm.global.i32 private @_device_query_0 : i32
  vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @_main_dispatch_0_vulkan_spirv_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<5020xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 {alignment = 1 : i64} "vulkan-spirv-fb"
  vm.initializer {
    %zero = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 = vm.const.ref.rodata @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %c1 = vm.const.i32 1
    %2 = vm.and.i32 %1, %c1 : i32
    %zero_0 = vm.const.i32.zero
    %3 = vm.select.i32 %0#0, %2, %zero_0 : i32
    %c1_1 = vm.const.i32 1
    %zero_2 = vm.const.i32.zero
    %zero_3 = vm.const.i32.zero
    %c7 = vm.const.i32 7
    %c1_4 = vm.const.i32 1
    %c1_5 = vm.const.i32 1
    %c7_6 = vm.const.i32 7
    %c1_7 = vm.const.i32 1
    %c2 = vm.const.i32 2
    %c7_8 = vm.const.i32 7
    %zero_9 = vm.const.i32.zero
    %ref_10 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_2, [(%zero_3, %c7, %c1_4), (%c1_5, %c7_6, %c1_7), (%c2, %c7_8, %zero_9)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %zero_11 = vm.const.i32.zero
    %ref_12 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_11, [%ref_10]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    %4 = vm.select.i64 %3, %zero, %c-1 : i64
    %5 = vm.trunc.i64.i32 %4 : i64 -> i32
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_12, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.br_table %5 {
      default: ^bb2,
      0: ^bb1
    }
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_main_dispatch_0_vulkan_spirv_fb = vm.const.ref.rodata @_main_dispatch_0_vulkan_spirv_fb : !vm.buffer
    %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0_13 = vm.const.ref.rodata @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 : !vm.buffer
    %null = vm.const.ref.zero : !vm.buffer
    %ref_14 = vm.call.variadic @hal.executable.create(%ref, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0_13, %_main_dispatch_0_vulkan_spirv_fb, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_14 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    %null_15 = vm.const.ref.zero : !vm.ref<!hal.executable>
    vm.br ^bb3(%null_15 : !vm.ref<!hal.executable>)
  ^bb3(%6: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %6, @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  }
  vm.func private @main() {
    vm.call @_main() : () -> ()
    vm.return
  }
  vm.export @main attributes {iree.abi.stub}
  vm.import private optional @check.expect_true(%operand : i32)
  vm.import private optional @check.expect_false(%operand : i32)
  vm.import private optional @check.expect_all_true(%device : !vm.ref<!hal.device>, %operand : !vm.ref<!hal.buffer_view>)
  vm.import private optional @check.expect_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
  vm.import private optional @check.expect_almost_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
  vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
  vm.import private @hal.ex.file.from_memory(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %access : i32, %buffer : !vm.buffer, %offset : i64, %length : i64, %flags : i32) -> !vm.ref<!hal.file>
  vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
  vm.import private @hal.allocator.import(%allocator : !vm.ref<!hal.allocator>, %try : i32, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
  vm.import private @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32
  vm.import private @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects}
  vm.import private @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...)
  vm.import private @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %flags : i32, %id : !vm.buffer, %group : !vm.buffer, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
  vm.import private @hal.channel.split(%channel : !vm.ref<!hal.channel>, %color : i32, %key : i32, %flags : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
  vm.import private @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer)
  vm.import private @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
  vm.import private @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64)
  vm.import private @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64)
  vm.import private @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...)
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
  vm.import private @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64)
  vm.import private @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
  vm.import private @hal.device.queue.read(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_file : !vm.ref<!hal.file>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %flags : i32)
  vm.import private @hal.device.queue.write(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_file : !vm.ref<!hal.file>, %target_offset : i64, %length : i64, %flags : i32)
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64)
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects}
  vm.import private @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32
  vm.import private @hal.fence.signal(%fence : !vm.ref<!hal.fence>)
  vm.import private @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32)
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  vm.func private @_main() {
    %c8192 = vm.const.i64 8192
    %c15360 = vm.const.i32 15360
    %c262144000 = vm.const.i64 262144000
    %c27648 = vm.const.i32 27648
    %c64000 = vm.const.i64 64000
    %zero = vm.const.i64.zero
    %c128000 = vm.const.i64 128000
    %c-1 = vm.const.i64 -1
    %zero_0 = vm.const.i64.zero
    %c-1_1 = vm.const.i32 -1
    %c-1_2 = vm.const.i64 -1
    %c1 = vm.const.i64 1
    %c2 = vm.const.i64 2
    %c32000 = vm.const.i64 32000
    %c553648144 = vm.const.i32 553648144
    %c1_3 = vm.const.i32 1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable__main_dispatch_0 = vm.global.load.ref @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %zero_4 = vm.const.i32.zero
    %ref_5 = vm.call @hal.fence.create(%ref, %zero_4) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %zero_6 = vm.const.i32.zero
    %c48 = vm.const.i32 48
    %c3075 = vm.const.i32 3075
    %ref_7 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_5, %zero_6, %c48, %c3075, %c8192) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_5]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    %c17 = vm.const.i32 17
    %c1_8 = vm.const.i32 1
    %zero_9 = vm.const.i32.zero
    %ref_10 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_8, %zero_9) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    %c2_11 = vm.const.i32 2
    %c15360_12 = vm.const.i32 15360
    vm.call @hal.command_buffer.fill_buffer(%ref_10, %ref_7, %zero, %c8192, %c15360_12, %c2_11) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
    %c28 = vm.const.i32 28
    %c13 = vm.const.i32 13
    %zero_13 = vm.const.i32.zero
    vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero_13) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
    %zero_14 = vm.const.i32.zero
    %ref_15 = vm.call @hal.fence.create(%ref, %zero_14) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %1 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_5]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_15, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %2 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_15]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_fail %2, "failed to wait on timepoint"
    %3 = util.optimization_barrier %ref_7 : !vm.ref<!hal.buffer>
    %zero_16 = vm.const.i32.zero
    %ref_17 = vm.call @hal.fence.create(%ref, %zero_16) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %zero_18 = vm.const.i32.zero
    %c48_19 = vm.const.i32 48
    %c3075_20 = vm.const.i32 3075
    %ref_21 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_17, %zero_18, %c48_19, %c3075_20, %c262144000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %4 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_17]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    %c17_22 = vm.const.i32 17
    %c1_23 = vm.const.i32 1
    %zero_24 = vm.const.i32.zero
    %ref_25 = vm.call @hal.command_buffer.create(%ref, %c17_22, %c1_23, %zero_24) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    %c2_26 = vm.const.i32 2
    %c15360_27 = vm.const.i32 15360
    vm.call @hal.command_buffer.fill_buffer(%ref_25, %ref_21, %zero, %c262144000, %c15360_27, %c2_26) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
    %c28_28 = vm.const.i32 28
    %c13_29 = vm.const.i32 13
    %zero_30 = vm.const.i32.zero
    vm.call @hal.command_buffer.execution_barrier(%ref_25, %c28_28, %c13_29, %zero_30) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_25) : (!vm.ref<!hal.command_buffer>) -> ()
    %zero_31 = vm.const.i32.zero
    %ref_32 = vm.call @hal.fence.create(%ref, %zero_31) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %5 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_17]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_32, [%ref_25]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %6 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_32]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_fail %6, "failed to wait on timepoint"
    %7 = util.optimization_barrier %ref_21 : !vm.ref<!hal.buffer>
    %8 = vm.call @hal.buffer.length(%3) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
    %9 = vm.call @hal.buffer.length(%7) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
    %zero_33 = vm.const.i32.zero
    %ref_34 = vm.call @hal.fence.create(%ref, %zero_33) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %zero_35 = vm.const.i32.zero
    %c48_36 = vm.const.i32 48
    %c3075_37 = vm.const.i32 3075
    %ref_38 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_34, %zero_35, %c48_36, %c3075_37, %c128000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %10 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_34]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    %c17_39 = vm.const.i32 17
    %c3 = vm.const.i32 3
    %zero_40 = vm.const.i32.zero
    %ref_41 = vm.call @hal.command_buffer.create(%ref, %c17_39, %c3, %zero_40) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    %11 = vm.select.i64 %_device_query_0, %zero, %c-1_2 : i64
    %12 = vm.trunc.i64.i32 %11 : i64 -> i32
    vm.br_table %12 {
      default: ^bb2,
      0: ^bb1
    }
  ^bb1:  // pred: ^bb0
    %zero_42 = vm.const.i32.zero
    %zero_43 = vm.const.i32.zero
    %zero_44 = vm.const.i32.zero
    %c1_45 = vm.const.i32 1
    %c2_46 = vm.const.i32 2
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_41, %_pipeline_layout_0, %zero_42, [(%zero_43, %zero_44, %3, %zero, %8), (%c1_45, %zero_44, %7, %zero, %9), (%c2_46, %zero_44, %ref_38, %zero, %c128000)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    %zero_47 = vm.const.i32.zero
    %c32000_48 = vm.const.i32 32000
    %c1_49 = vm.const.i32 1
    %c1_50 = vm.const.i32 1
    vm.call @hal.command_buffer.dispatch(%ref_41, %_executable__main_dispatch_0, %zero_47, %c32000_48, %c1_49, %c1_50) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    %c2_51 = vm.const.i32 2
    %c27648_52 = vm.const.i32 27648
    vm.call @hal.command_buffer.fill_buffer(%ref_41, %ref_38, %c64000, %c64000, %c27648_52, %c2_51) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
    %c28_53 = vm.const.i32 28
    %c13_54 = vm.const.i32 13
    %zero_55 = vm.const.i32.zero
    vm.call @hal.command_buffer.execution_barrier(%ref_41, %c28_53, %c13_54, %zero_55) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_41) : (!vm.ref<!hal.command_buffer>) -> ()
    %zero_56 = vm.const.i32.zero
    %ref_57 = vm.call @hal.fence.create(%ref, %zero_56) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %13 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_34]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_57, [%ref_41]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %14 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_57]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_fail %14, "failed to wait on timepoint"
    %ref_58 = vm.call.variadic @hal.buffer_view.create(%ref_38, %c64000, %c64000, %c553648144, %c1_3, [%c1, %c32000]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    %ref_59 = vm.call.variadic @hal.buffer_view.create(%ref_38, %zero, %c64000, %c553648144, %c1_3, [%c1, %c32000]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    %has_check_expect_eq = vm.import.resolved @check.expect_eq : i32
    vm.cond_br %has_check_expect_eq, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    vm.call @check.expect_eq(%ref, %ref_59, %ref_58) : (!vm.ref<!hal.device>, !vm.ref<!hal.buffer_view>, !vm.ref<!hal.buffer_view>) -> ()
    vm.br ^bb4
  ^bb4:  // 2 preds: ^bb2, ^bb3
    vm.return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @_main_dispatch_0_vulkan_spirv_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<5020xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 {alignment = 1 : i64} "vulkan-spirv-fb"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 = vm.const.ref.rodata @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_2 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_3 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_2]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      %4 = vm.select.i64 %3, %zero_1, %c-1 : i64
      %5 = vm.trunc.i64.i32 %4 : i64 -> i32
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_3, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.br_table %5 {
        default: ^bb2,
        0: ^bb1
      }
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_main_dispatch_0_vulkan_spirv_fb = vm.const.ref.rodata @_main_dispatch_0_vulkan_spirv_fb : !vm.buffer
      %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0_4 = vm.const.ref.rodata @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 : !vm.buffer
      %ref_5 = vm.call.variadic @hal.executable.create(%ref, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0_4, %_main_dispatch_0_vulkan_spirv_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_5 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%6: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %6, @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.func private @main() {
      vm.call @_main() : () -> ()
      vm.return
    }
    vm.export @main attributes {iree.abi.stub}
    vm.import private optional @check.expect_true(%operand : i32)
    vm.import private optional @check.expect_false(%operand : i32)
    vm.import private optional @check.expect_all_true(%device : !vm.ref<!hal.device>, %operand : !vm.ref<!hal.buffer_view>)
    vm.import private optional @check.expect_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
    vm.import private optional @check.expect_almost_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
    vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.ex.file.from_memory(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %access : i32, %buffer : !vm.buffer, %offset : i64, %length : i64, %flags : i32) -> !vm.ref<!hal.file>
    vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
    vm.import private @hal.allocator.import(%allocator : !vm.ref<!hal.allocator>, %try : i32, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32
    vm.import private @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...)
    vm.import private @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %flags : i32, %id : !vm.buffer, %group : !vm.buffer, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.split(%channel : !vm.ref<!hal.channel>, %color : i32, %key : i32, %flags : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer)
    vm.import private @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
    vm.import private @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64)
    vm.import private @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64)
    vm.import private @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64)
    vm.import private @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
    vm.import private @hal.device.queue.read(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_file : !vm.ref<!hal.file>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %flags : i32)
    vm.import private @hal.device.queue.write(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_file : !vm.ref<!hal.file>, %target_offset : i64, %length : i64, %flags : i32)
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64)
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects}
    vm.import private @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32
    vm.import private @hal.fence.signal(%fence : !vm.ref<!hal.fence>)
    vm.import private @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32)
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.func private @_main() {
      %c32000 = vm.const.i32 32000
      %c3 = vm.const.i32 3
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %c17 = vm.const.i32 17
      %c3075 = vm.const.i32 3075
      %c48 = vm.const.i32 48
      %zero = vm.const.i32.zero
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c8192 = vm.const.i64 8192
      %c15360 = vm.const.i32 15360
      %c262144000 = vm.const.i64 262144000
      %c27648 = vm.const.i32 27648
      %c64000 = vm.const.i64 64000
      %zero_0 = vm.const.i64.zero
      %c128000 = vm.const.i64 128000
      %c-1 = vm.const.i64 -1
      %c-1_1 = vm.const.i32 -1
      %c1 = vm.const.i64 1
      %c32000_2 = vm.const.i64 32000
      %c553648144 = vm.const.i32 553648144
      %c1_3 = vm.const.i32 1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable__main_dispatch_0 = vm.global.load.ref @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_4 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_5 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_4, %zero, %c48, %c3075, %c8192) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_6 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.call @hal.command_buffer.fill_buffer(%ref_6, %ref_5, %zero_0, %c8192, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_6, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_6) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_7 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %1 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_7, [%ref_6]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %2 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %2, ^bb3(%2 : i32), ^bb1
    ^bb1:  // pred: ^bb0
      %3 = util.optimization_barrier %ref_5 : !vm.ref<!hal.buffer>
      %ref_8 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_9 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c262144000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %4 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_10 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.call @hal.command_buffer.fill_buffer(%ref_10, %ref_9, %zero_0, %c262144000, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_11 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %5 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %6 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %6, ^bb3(%6 : i32), ^bb2
    ^bb2:  // pred: ^bb1
      %7 = util.optimization_barrier %ref_9 : !vm.ref<!hal.buffer>
      %8 = vm.call @hal.buffer.length(%3) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
      %9 = vm.call @hal.buffer.length(%7) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
      %ref_12 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_13 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_12, %zero, %c48, %c3075, %c128000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %10 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_14 = vm.call @hal.command_buffer.create(%ref, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      %11 = vm.select.i64 %_device_query_0, %zero_0, %c-1 : i64
      %12 = vm.trunc.i64.i32 %11 : i64 -> i32
      vm.br_table %12 {
        default: ^bb5,
        0: ^bb4
      }
    ^bb3(%13: i32):  // 3 preds: ^bb0, ^bb1, ^bb5
      vm.fail %13, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb2
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_14, %_pipeline_layout_0, %zero, [(%zero, %zero, %3, %zero_0, %8), (%c1_3, %zero, %7, %zero_0, %9), (%c2, %zero, %ref_13, %zero_0, %c128000)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_14, %_executable__main_dispatch_0, %zero, %c32000, %c1_3, %c1_3) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.br ^bb5
    ^bb5:  // 2 preds: ^bb2, ^bb4
      vm.call @hal.command_buffer.fill_buffer(%ref_14, %ref_13, %c64000, %c64000, %c27648, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_14, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_14) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_15 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %14 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_15, [%ref_14]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %15 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_15]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %15, ^bb3(%15 : i32), ^bb6
    ^bb6:  // pred: ^bb5
      %ref_16 = vm.call.variadic @hal.buffer_view.create(%ref_13, %c64000, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      %ref_17 = vm.call.variadic @hal.buffer_view.create(%ref_13, %zero_0, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      %has_check_expect_eq = vm.import.resolved @check.expect_eq : i32
      vm.cond_br %has_check_expect_eq, ^bb7, ^bb8
    ^bb7:  // pred: ^bb6
      vm.call @check.expect_eq(%ref, %ref_17, %ref_16) : (!vm.ref<!hal.device>, !vm.ref<!hal.buffer_view>, !vm.ref<!hal.buffer_view>) -> ()
      vm.br ^bb8
    ^bb8:  // 2 preds: ^bb6, ^bb7
      vm.return
    }
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @_main_dispatch_0_vulkan_spirv_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<5020xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 {alignment = 1 : i64} "vulkan-spirv-fb"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 = vm.const.ref.rodata @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_2 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_3 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_2]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      %4 = vm.select.i64 %3, %zero_1, %c-1 : i64
      %5 = vm.trunc.i64.i32 %4 : i64 -> i32
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_3, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.br_table %5 {
        default: ^bb2,
        0: ^bb1
      }
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_main_dispatch_0_vulkan_spirv_fb = vm.const.ref.rodata @_main_dispatch_0_vulkan_spirv_fb : !vm.buffer
      %ref_4 = vm.call.variadic @hal.executable.create(%ref, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0, %_main_dispatch_0_vulkan_spirv_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_4 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%6: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %6, @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.func private @main() {
      vm.call @_main() : () -> ()
      vm.return
    }
    vm.export @main attributes {iree.abi.stub}
    vm.import private optional @check.expect_true(%operand : i32)
    vm.import private optional @check.expect_false(%operand : i32)
    vm.import private optional @check.expect_all_true(%device : !vm.ref<!hal.device>, %operand : !vm.ref<!hal.buffer_view>)
    vm.import private optional @check.expect_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
    vm.import private optional @check.expect_almost_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
    vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.ex.file.from_memory(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %access : i32, %buffer : !vm.buffer, %offset : i64, %length : i64, %flags : i32) -> !vm.ref<!hal.file>
    vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
    vm.import private @hal.allocator.import(%allocator : !vm.ref<!hal.allocator>, %try : i32, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32
    vm.import private @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...)
    vm.import private @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %flags : i32, %id : !vm.buffer, %group : !vm.buffer, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.split(%channel : !vm.ref<!hal.channel>, %color : i32, %key : i32, %flags : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer)
    vm.import private @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
    vm.import private @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64)
    vm.import private @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64)
    vm.import private @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64)
    vm.import private @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
    vm.import private @hal.device.queue.read(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_file : !vm.ref<!hal.file>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %flags : i32)
    vm.import private @hal.device.queue.write(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_file : !vm.ref<!hal.file>, %target_offset : i64, %length : i64, %flags : i32)
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64)
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects}
    vm.import private @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32
    vm.import private @hal.fence.signal(%fence : !vm.ref<!hal.fence>)
    vm.import private @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32)
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.func private @_main() {
      %c32000 = vm.const.i32 32000
      %c3 = vm.const.i32 3
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %c17 = vm.const.i32 17
      %c3075 = vm.const.i32 3075
      %c48 = vm.const.i32 48
      %zero = vm.const.i32.zero
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c8192 = vm.const.i64 8192
      %c15360 = vm.const.i32 15360
      %c262144000 = vm.const.i64 262144000
      %c27648 = vm.const.i32 27648
      %c64000 = vm.const.i64 64000
      %zero_0 = vm.const.i64.zero
      %c128000 = vm.const.i64 128000
      %c-1 = vm.const.i64 -1
      %c-1_1 = vm.const.i32 -1
      %c1 = vm.const.i64 1
      %c32000_2 = vm.const.i64 32000
      %c553648144 = vm.const.i32 553648144
      %c1_3 = vm.const.i32 1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable__main_dispatch_0 = vm.global.load.ref @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_4 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_5 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_4, %zero, %c48, %c3075, %c8192) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_6 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.call @hal.command_buffer.fill_buffer(%ref_6, %ref_5, %zero_0, %c8192, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_6, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_6) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_7 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %1 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_7, [%ref_6]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %2 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %2, ^bb3(%2 : i32), ^bb1
    ^bb1:  // pred: ^bb0
      %3 = util.optimization_barrier %ref_5 : !vm.ref<!hal.buffer>
      %ref_8 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_9 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c262144000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %4 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_10 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.call @hal.command_buffer.fill_buffer(%ref_10, %ref_9, %zero_0, %c262144000, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_11 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %5 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %6 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %6, ^bb3(%6 : i32), ^bb2
    ^bb2:  // pred: ^bb1
      %7 = util.optimization_barrier %ref_9 : !vm.ref<!hal.buffer>
      %8 = vm.call @hal.buffer.length(%3) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
      %9 = vm.call @hal.buffer.length(%7) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
      %ref_12 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_13 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_12, %zero, %c48, %c3075, %c128000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %10 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_14 = vm.call @hal.command_buffer.create(%ref, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      %11 = vm.select.i64 %_device_query_0, %zero_0, %c-1 : i64
      %12 = vm.trunc.i64.i32 %11 : i64 -> i32
      vm.br_table %12 {
        default: ^bb5,
        0: ^bb4
      }
    ^bb3(%13: i32):  // 3 preds: ^bb0, ^bb1, ^bb5
      vm.fail %13, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb2
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_14, %_pipeline_layout_0, %zero, [(%zero, %zero, %3, %zero_0, %8), (%c1_3, %zero, %7, %zero_0, %9), (%c2, %zero, %ref_13, %zero_0, %c128000)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_14, %_executable__main_dispatch_0, %zero, %c32000, %c1_3, %c1_3) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.br ^bb5
    ^bb5:  // 2 preds: ^bb2, ^bb4
      vm.call @hal.command_buffer.fill_buffer(%ref_14, %ref_13, %c64000, %c64000, %c27648, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_14, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_14) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_15 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %14 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_15, [%ref_14]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %15 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_15]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %15, ^bb3(%15 : i32), ^bb6
    ^bb6:  // pred: ^bb5
      %ref_16 = vm.call.variadic @hal.buffer_view.create(%ref_13, %c64000, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      %ref_17 = vm.call.variadic @hal.buffer_view.create(%ref_13, %zero_0, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      %has_check_expect_eq = vm.import.resolved @check.expect_eq : i32
      vm.cond_br %has_check_expect_eq, ^bb7, ^bb8
    ^bb7:  // pred: ^bb6
      vm.call @check.expect_eq(%ref, %ref_17, %ref_16) : (!vm.ref<!hal.device>, !vm.ref<!hal.buffer_view>, !vm.ref<!hal.buffer_view>) -> ()
      vm.br ^bb8
    ^bb8:  // 2 preds: ^bb6, ^bb7
      vm.return
    }
  }
}


// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @_main_dispatch_0_vulkan_spirv_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<5020xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 {alignment = 1 : i64} "vulkan-spirv-fb"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 = vm.const.ref.rodata @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_2 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_3 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_2]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      %4 = vm.select.i64 %3, %zero_1, %c-1 : i64
      %5 = vm.trunc.i64.i32 %4 : i64 -> i32
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_3, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.br_table %5 {
        default: ^bb2,
        0: ^bb1
      }
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_main_dispatch_0_vulkan_spirv_fb = vm.const.ref.rodata @_main_dispatch_0_vulkan_spirv_fb : !vm.buffer
      %ref_4 = vm.call.variadic @hal.executable.create(%ref, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0, %_main_dispatch_0_vulkan_spirv_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_4 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%6: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %6, @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.func private @main() {
      vm.call @_main() : () -> ()
      vm.return
    }
    vm.export @main attributes {iree.abi.stub}
    vm.import private optional @check.expect_true(%operand : i32)
    vm.import private optional @check.expect_false(%operand : i32)
    vm.import private optional @check.expect_all_true(%device : !vm.ref<!hal.device>, %operand : !vm.ref<!hal.buffer_view>)
    vm.import private optional @check.expect_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
    vm.import private optional @check.expect_almost_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
    vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.ex.file.from_memory(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %access : i32, %buffer : !vm.buffer, %offset : i64, %length : i64, %flags : i32) -> !vm.ref<!hal.file>
    vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
    vm.import private @hal.allocator.import(%allocator : !vm.ref<!hal.allocator>, %try : i32, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32
    vm.import private @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...)
    vm.import private @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %flags : i32, %id : !vm.buffer, %group : !vm.buffer, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.split(%channel : !vm.ref<!hal.channel>, %color : i32, %key : i32, %flags : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer)
    vm.import private @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
    vm.import private @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64)
    vm.import private @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64)
    vm.import private @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64)
    vm.import private @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
    vm.import private @hal.device.queue.read(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_file : !vm.ref<!hal.file>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %flags : i32)
    vm.import private @hal.device.queue.write(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_file : !vm.ref<!hal.file>, %target_offset : i64, %length : i64, %flags : i32)
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64)
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects}
    vm.import private @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32
    vm.import private @hal.fence.signal(%fence : !vm.ref<!hal.fence>)
    vm.import private @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32)
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.func private @_main() {
      %c32000 = vm.const.i32 32000
      %c3 = vm.const.i32 3
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %c17 = vm.const.i32 17
      %c3075 = vm.const.i32 3075
      %c48 = vm.const.i32 48
      %zero = vm.const.i32.zero
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c8192 = vm.const.i64 8192
      %c15360 = vm.const.i32 15360
      %c262144000 = vm.const.i64 262144000
      %c27648 = vm.const.i32 27648
      %c64000 = vm.const.i64 64000
      %zero_0 = vm.const.i64.zero
      %c128000 = vm.const.i64 128000
      %c-1 = vm.const.i64 -1
      %c-1_1 = vm.const.i32 -1
      %c1 = vm.const.i64 1
      %c32000_2 = vm.const.i64 32000
      %c553648144 = vm.const.i32 553648144
      %c1_3 = vm.const.i32 1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable__main_dispatch_0 = vm.global.load.ref @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_4 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_5 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_4, %zero, %c48, %c3075, %c8192) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_6 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.call @hal.command_buffer.fill_buffer(%ref_6, %ref_5, %zero_0, %c8192, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_6, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_6) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_7 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %1 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_7, [%ref_6]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %2 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %2, ^bb3(%2 : i32), ^bb1
    ^bb1:  // pred: ^bb0
      %3 = util.optimization_barrier %ref_5 : !vm.ref<!hal.buffer>
      %ref_8 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_9 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c262144000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %4 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_10 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.call @hal.command_buffer.fill_buffer(%ref_10, %ref_9, %zero_0, %c262144000, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_11 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %5 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %6 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %6, ^bb3(%6 : i32), ^bb2
    ^bb2:  // pred: ^bb1
      %7 = util.optimization_barrier %ref_9 : !vm.ref<!hal.buffer>
      %8 = vm.call @hal.buffer.length(%3) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
      %9 = vm.call @hal.buffer.length(%7) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
      %ref_12 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_13 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_12, %zero, %c48, %c3075, %c128000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %10 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_14 = vm.call @hal.command_buffer.create(%ref, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      %11 = vm.select.i64 %_device_query_0, %zero_0, %c-1 : i64
      %12 = vm.trunc.i64.i32 %11 : i64 -> i32
      vm.br_table %12 {
        default: ^bb5,
        0: ^bb4
      }
    ^bb3(%13: i32):  // 3 preds: ^bb0, ^bb1, ^bb5
      vm.fail %13, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb2
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_14, %_pipeline_layout_0, %zero, [(%zero, %zero, %3, %zero_0, %8), (%c1_3, %zero, %7, %zero_0, %9), (%c2, %zero, %ref_13, %zero_0, %c128000)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_14, %_executable__main_dispatch_0, %zero, %c32000, %c1_3, %c1_3) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.br ^bb5
    ^bb5:  // 2 preds: ^bb2, ^bb4
      vm.call @hal.command_buffer.fill_buffer(%ref_14, %ref_13, %c64000, %c64000, %c27648, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_14, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_14) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_15 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %14 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_15, [%ref_14]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %15 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_15]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %15, ^bb3(%15 : i32), ^bb6
    ^bb6:  // pred: ^bb5
      %ref_16 = vm.call.variadic @hal.buffer_view.create(%ref_13, %c64000, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      %ref_17 = vm.call.variadic @hal.buffer_view.create(%ref_13, %zero_0, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      %has_check_expect_eq = vm.import.resolved @check.expect_eq : i32
      vm.cond_br %has_check_expect_eq, ^bb7, ^bb8
    ^bb7:  // pred: ^bb6
      vm.call @check.expect_eq(%ref, %ref_17, %ref_16) : (!vm.ref<!hal.device>, !vm.ref<!hal.buffer_view>, !vm.ref<!hal.buffer_view>) -> ()
      vm.br ^bb8
    ^bb8:  // 2 preds: ^bb6, ^bb7
      vm.return
    }
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @_main_dispatch_0_vulkan_spirv_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<5020xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 {alignment = 1 : i64} "vulkan-spirv-fb"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 = vm.const.ref.rodata @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_2 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_3 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_2]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      %4 = vm.select.i64 %3, %zero_1, %c-1 : i64
      %5 = vm.trunc.i64.i32 %4 : i64 -> i32
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_3, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.br_table %5 {
        default: ^bb2,
        0: ^bb1
      }
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_main_dispatch_0_vulkan_spirv_fb = vm.const.ref.rodata @_main_dispatch_0_vulkan_spirv_fb : !vm.buffer
      %ref_4 = vm.call.variadic @hal.executable.create(%ref, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0, %_main_dispatch_0_vulkan_spirv_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_4 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%6: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %6, @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.func private @main() {
      vm.call @_main() : () -> ()
      vm.return
    }
    vm.export @main attributes {iree.abi.stub}
    vm.import private optional @check.expect_true(%operand : i32)
    vm.import private optional @check.expect_false(%operand : i32)
    vm.import private optional @check.expect_all_true(%device : !vm.ref<!hal.device>, %operand : !vm.ref<!hal.buffer_view>)
    vm.import private optional @check.expect_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
    vm.import private optional @check.expect_almost_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
    vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.ex.file.from_memory(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %access : i32, %buffer : !vm.buffer, %offset : i64, %length : i64, %flags : i32) -> !vm.ref<!hal.file>
    vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
    vm.import private @hal.allocator.import(%allocator : !vm.ref<!hal.allocator>, %try : i32, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32
    vm.import private @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...)
    vm.import private @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %flags : i32, %id : !vm.buffer, %group : !vm.buffer, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.split(%channel : !vm.ref<!hal.channel>, %color : i32, %key : i32, %flags : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer)
    vm.import private @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
    vm.import private @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64)
    vm.import private @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64)
    vm.import private @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64)
    vm.import private @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
    vm.import private @hal.device.queue.read(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_file : !vm.ref<!hal.file>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %flags : i32)
    vm.import private @hal.device.queue.write(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_file : !vm.ref<!hal.file>, %target_offset : i64, %length : i64, %flags : i32)
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64)
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects}
    vm.import private @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32
    vm.import private @hal.fence.signal(%fence : !vm.ref<!hal.fence>)
    vm.import private @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32)
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.func private @_main() {
      %c32000 = vm.const.i32 32000
      %c3 = vm.const.i32 3
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %c17 = vm.const.i32 17
      %c3075 = vm.const.i32 3075
      %c48 = vm.const.i32 48
      %zero = vm.const.i32.zero
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c8192 = vm.const.i64 8192
      %c15360 = vm.const.i32 15360
      %c262144000 = vm.const.i64 262144000
      %c27648 = vm.const.i32 27648
      %c64000 = vm.const.i64 64000
      %zero_0 = vm.const.i64.zero
      %c128000 = vm.const.i64 128000
      %c-1 = vm.const.i64 -1
      %c-1_1 = vm.const.i32 -1
      %c1 = vm.const.i64 1
      %c32000_2 = vm.const.i64 32000
      %c553648144 = vm.const.i32 553648144
      %c1_3 = vm.const.i32 1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable__main_dispatch_0 = vm.global.load.ref @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_4 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_5 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_4, %zero, %c48, %c3075, %c8192) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_6 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.call @hal.command_buffer.fill_buffer(%ref_6, %ref_5, %zero_0, %c8192, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_6, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_6) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_7 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %1 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_7, [%ref_6]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %2 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %2, ^bb3(%2 : i32), ^bb1
    ^bb1:  // pred: ^bb0
      %3 = util.optimization_barrier %ref_5 : !vm.ref<!hal.buffer>
      %ref_8 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_9 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c262144000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %4 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_10 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.call @hal.command_buffer.fill_buffer(%ref_10, %ref_9, %zero_0, %c262144000, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_11 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %5 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %6 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %6, ^bb3(%6 : i32), ^bb2
    ^bb2:  // pred: ^bb1
      %7 = util.optimization_barrier %ref_9 : !vm.ref<!hal.buffer>
      %8 = vm.call @hal.buffer.length(%3) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
      %9 = vm.call @hal.buffer.length(%7) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
      %ref_12 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_13 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_12, %zero, %c48, %c3075, %c128000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %10 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_14 = vm.call @hal.command_buffer.create(%ref, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      %11 = vm.select.i64 %_device_query_0, %zero_0, %c-1 : i64
      %12 = vm.trunc.i64.i32 %11 : i64 -> i32
      vm.br_table %12 {
        default: ^bb5,
        0: ^bb4
      }
    ^bb3(%13: i32):  // 3 preds: ^bb0, ^bb1, ^bb5
      vm.fail %13, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb2
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_14, %_pipeline_layout_0, %zero, [(%zero, %zero, %3, %zero_0, %8), (%c1_3, %zero, %7, %zero_0, %9), (%c2, %zero, %ref_13, %zero_0, %c128000)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_14, %_executable__main_dispatch_0, %zero, %c32000, %c1_3, %c1_3) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.br ^bb5
    ^bb5:  // 2 preds: ^bb2, ^bb4
      vm.call @hal.command_buffer.fill_buffer(%ref_14, %ref_13, %c64000, %c64000, %c27648, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_14, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_14) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_15 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %14 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_15, [%ref_14]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %15 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_15]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %15, ^bb3(%15 : i32), ^bb6
    ^bb6:  // pred: ^bb5
      %ref_16 = vm.call.variadic @hal.buffer_view.create(%ref_13, %c64000, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      %ref_17 = vm.call.variadic @hal.buffer_view.create(%ref_13, %zero_0, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      %has_check_expect_eq = vm.import.resolved @check.expect_eq : i32
      vm.cond_br %has_check_expect_eq, ^bb7, ^bb8
    ^bb7:  // pred: ^bb6
      vm.call @check.expect_eq(%ref, %ref_17, %ref_16) : (!vm.ref<!hal.device>, !vm.ref<!hal.buffer_view>, !vm.ref<!hal.buffer_view>) -> ()
      vm.br ^bb8
    ^bb8:  // 2 preds: ^bb6, ^bb7
      vm.return
    }
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @_main_dispatch_0_vulkan_spirv_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<5020xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 {alignment = 1 : i64} "vulkan-spirv-fb"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 = vm.const.ref.rodata @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_2 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_3 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_2]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      %4 = vm.select.i64 %3, %zero_1, %c-1 : i64
      %5 = vm.trunc.i64.i32 %4 : i64 -> i32
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_3, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.br_table %5 {
        default: ^bb2,
        0: ^bb1
      }
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_main_dispatch_0_vulkan_spirv_fb = vm.const.ref.rodata @_main_dispatch_0_vulkan_spirv_fb : !vm.buffer
      %ref_4 = vm.call.variadic @hal.executable.create(%ref, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0, %_main_dispatch_0_vulkan_spirv_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_4 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%6: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %6, @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.func private @main() {
      vm.call @_main() : () -> ()
      vm.return
    }
    vm.export @main attributes {iree.abi.stub}
    vm.import private optional @check.expect_true(%operand : i32)
    vm.import private optional @check.expect_false(%operand : i32)
    vm.import private optional @check.expect_all_true(%device : !vm.ref<!hal.device>, %operand : !vm.ref<!hal.buffer_view>)
    vm.import private optional @check.expect_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
    vm.import private optional @check.expect_almost_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
    vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.ex.file.from_memory(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %access : i32, %buffer : !vm.buffer, %offset : i64, %length : i64, %flags : i32) -> !vm.ref<!hal.file>
    vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
    vm.import private @hal.allocator.import(%allocator : !vm.ref<!hal.allocator>, %try : i32, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32
    vm.import private @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...)
    vm.import private @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %flags : i32, %id : !vm.buffer, %group : !vm.buffer, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.split(%channel : !vm.ref<!hal.channel>, %color : i32, %key : i32, %flags : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer)
    vm.import private @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
    vm.import private @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64)
    vm.import private @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64)
    vm.import private @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64)
    vm.import private @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
    vm.import private @hal.device.queue.read(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_file : !vm.ref<!hal.file>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %flags : i32)
    vm.import private @hal.device.queue.write(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_file : !vm.ref<!hal.file>, %target_offset : i64, %length : i64, %flags : i32)
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64)
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects}
    vm.import private @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32
    vm.import private @hal.fence.signal(%fence : !vm.ref<!hal.fence>)
    vm.import private @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32)
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.func private @_main() {
      %c32000 = vm.const.i32 32000
      %c3 = vm.const.i32 3
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %c17 = vm.const.i32 17
      %c3075 = vm.const.i32 3075
      %c48 = vm.const.i32 48
      %zero = vm.const.i32.zero
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c8192 = vm.const.i64 8192
      %c15360 = vm.const.i32 15360
      %c262144000 = vm.const.i64 262144000
      %c27648 = vm.const.i32 27648
      %c64000 = vm.const.i64 64000
      %zero_0 = vm.const.i64.zero
      %c128000 = vm.const.i64 128000
      %c-1 = vm.const.i64 -1
      %c-1_1 = vm.const.i32 -1
      %c1 = vm.const.i64 1
      %c32000_2 = vm.const.i64 32000
      %c553648144 = vm.const.i32 553648144
      %c1_3 = vm.const.i32 1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable__main_dispatch_0 = vm.global.load.ref @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_4 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_5 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_4, %zero, %c48, %c3075, %c8192) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_6 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.call @hal.command_buffer.fill_buffer(%ref_6, %ref_5, %zero_0, %c8192, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_6, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_6) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_7 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %1 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_7, [%ref_6]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %2 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %2, ^bb3(%2 : i32), ^bb1
    ^bb1:  // pred: ^bb0
      %3 = util.optimization_barrier %ref_5 : !vm.ref<!hal.buffer>
      %ref_8 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_9 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c262144000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %4 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_10 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.call @hal.command_buffer.fill_buffer(%ref_10, %ref_9, %zero_0, %c262144000, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_11 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %5 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %6 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %6, ^bb3(%6 : i32), ^bb2
    ^bb2:  // pred: ^bb1
      %7 = util.optimization_barrier %ref_9 : !vm.ref<!hal.buffer>
      %8 = vm.call @hal.buffer.length(%3) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
      %9 = vm.call @hal.buffer.length(%7) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
      %ref_12 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_13 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_12, %zero, %c48, %c3075, %c128000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %10 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_14 = vm.call @hal.command_buffer.create(%ref, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      %11 = vm.select.i64 %_device_query_0, %zero_0, %c-1 : i64
      %12 = vm.trunc.i64.i32 %11 : i64 -> i32
      vm.br_table %12 {
        default: ^bb5,
        0: ^bb4
      }
    ^bb3(%13: i32):  // 3 preds: ^bb0, ^bb1, ^bb5
      vm.fail %13, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb2
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_14, %_pipeline_layout_0, %zero, [(%zero, %zero, %3, %zero_0, %8), (%c1_3, %zero, %7, %zero_0, %9), (%c2, %zero, %ref_13, %zero_0, %c128000)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_14, %_executable__main_dispatch_0, %zero, %c32000, %c1_3, %c1_3) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.br ^bb5
    ^bb5:  // 2 preds: ^bb2, ^bb4
      vm.call @hal.command_buffer.fill_buffer(%ref_14, %ref_13, %c64000, %c64000, %c27648, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_14, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_14) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_15 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %14 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_15, [%ref_14]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %15 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_15]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %15, ^bb3(%15 : i32), ^bb6
    ^bb6:  // pred: ^bb5
      %ref_16 = vm.call.variadic @hal.buffer_view.create(%ref_13, %c64000, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      %ref_17 = vm.call.variadic @hal.buffer_view.create(%ref_13, %zero_0, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      %has_check_expect_eq = vm.import.resolved @check.expect_eq : i32
      vm.cond_br %has_check_expect_eq, ^bb7, ^bb8
    ^bb7:  // pred: ^bb6
      vm.call @check.expect_eq(%ref, %ref_17, %ref_16) : (!vm.ref<!hal.device>, !vm.ref<!hal.buffer_view>, !vm.ref<!hal.buffer_view>) -> ()
      vm.br ^bb8
    ^bb8:  // 2 preds: ^bb6, ^bb7
      vm.return
    }
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::ResolveRodataLoadsPass (iree-vm-resolve-rodata-loads) //----- //
vm.module public @module {
  vm.global.i32 private @_device_query_0 : i32
  vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @_main_dispatch_0_vulkan_spirv_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<5020xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 {alignment = 1 : i64} "vulkan-spirv-fb"
  vm.initializer {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %zero_1 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 = vm.const.ref.rodata @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_2 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_3 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_2]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    %4 = vm.select.i64 %3, %zero_1, %c-1 : i64
    %5 = vm.trunc.i64.i32 %4 : i64 -> i32
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_3, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.br_table %5 {
      default: ^bb2,
      0: ^bb1
    }
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_main_dispatch_0_vulkan_spirv_fb = vm.const.ref.rodata @_main_dispatch_0_vulkan_spirv_fb : !vm.buffer
    %ref_4 = vm.call.variadic @hal.executable.create(%ref, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0, %_main_dispatch_0_vulkan_spirv_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_4 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%6: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %6, @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  }
  vm.func private @main() {
    vm.call @_main() : () -> ()
    vm.return
  }
  vm.export @main attributes {iree.abi.stub}
  vm.import private optional @check.expect_true(%operand : i32)
  vm.import private optional @check.expect_false(%operand : i32)
  vm.import private optional @check.expect_all_true(%device : !vm.ref<!hal.device>, %operand : !vm.ref<!hal.buffer_view>)
  vm.import private optional @check.expect_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
  vm.import private optional @check.expect_almost_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
  vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
  vm.import private @hal.ex.file.from_memory(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %access : i32, %buffer : !vm.buffer, %offset : i64, %length : i64, %flags : i32) -> !vm.ref<!hal.file>
  vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
  vm.import private @hal.allocator.import(%allocator : !vm.ref<!hal.allocator>, %try : i32, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
  vm.import private @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32
  vm.import private @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects}
  vm.import private @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...)
  vm.import private @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %flags : i32, %id : !vm.buffer, %group : !vm.buffer, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
  vm.import private @hal.channel.split(%channel : !vm.ref<!hal.channel>, %color : i32, %key : i32, %flags : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
  vm.import private @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer)
  vm.import private @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
  vm.import private @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64)
  vm.import private @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64)
  vm.import private @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...)
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
  vm.import private @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64)
  vm.import private @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
  vm.import private @hal.device.queue.read(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_file : !vm.ref<!hal.file>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %flags : i32)
  vm.import private @hal.device.queue.write(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_file : !vm.ref<!hal.file>, %target_offset : i64, %length : i64, %flags : i32)
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64)
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects}
  vm.import private @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32
  vm.import private @hal.fence.signal(%fence : !vm.ref<!hal.fence>)
  vm.import private @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32)
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  vm.func private @_main() {
    %c32000 = vm.const.i32 32000
    %c3 = vm.const.i32 3
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c2 = vm.const.i32 2
    %c17 = vm.const.i32 17
    %c3075 = vm.const.i32 3075
    %c48 = vm.const.i32 48
    %zero = vm.const.i32.zero
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c8192 = vm.const.i64 8192
    %c15360 = vm.const.i32 15360
    %c262144000 = vm.const.i64 262144000
    %c27648 = vm.const.i32 27648
    %c64000 = vm.const.i64 64000
    %zero_0 = vm.const.i64.zero
    %c128000 = vm.const.i64 128000
    %c-1 = vm.const.i64 -1
    %c-1_1 = vm.const.i32 -1
    %c1 = vm.const.i64 1
    %c32000_2 = vm.const.i64 32000
    %c553648144 = vm.const.i32 553648144
    %c1_3 = vm.const.i32 1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable__main_dispatch_0 = vm.global.load.ref @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_4 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_5 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_4, %zero, %c48, %c3075, %c8192) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    %ref_6 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.call @hal.command_buffer.fill_buffer(%ref_6, %ref_5, %zero_0, %c8192, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_6, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_6) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_7 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %1 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_7, [%ref_6]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %2 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %2, ^bb3(%2 : i32), ^bb1
  ^bb1:  // pred: ^bb0
    %3 = util.optimization_barrier %ref_5 : !vm.ref<!hal.buffer>
    %ref_8 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_9 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c262144000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %4 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    %ref_10 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.call @hal.command_buffer.fill_buffer(%ref_10, %ref_9, %zero_0, %c262144000, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_11 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %5 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %6 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %6, ^bb3(%6 : i32), ^bb2
  ^bb2:  // pred: ^bb1
    %7 = util.optimization_barrier %ref_9 : !vm.ref<!hal.buffer>
    %8 = vm.call @hal.buffer.length(%3) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
    %9 = vm.call @hal.buffer.length(%7) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
    %ref_12 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_13 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_12, %zero, %c48, %c3075, %c128000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %10 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    %ref_14 = vm.call @hal.command_buffer.create(%ref, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    %11 = vm.select.i64 %_device_query_0, %zero_0, %c-1 : i64
    %12 = vm.trunc.i64.i32 %11 : i64 -> i32
    vm.br_table %12 {
      default: ^bb5,
      0: ^bb4
    }
  ^bb3(%13: i32):  // 3 preds: ^bb0, ^bb1, ^bb5
    vm.fail %13, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb2
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_14, %_pipeline_layout_0, %zero, [(%zero, %zero, %3, %zero_0, %8), (%c1_3, %zero, %7, %zero_0, %9), (%c2, %zero, %ref_13, %zero_0, %c128000)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_14, %_executable__main_dispatch_0, %zero, %c32000, %c1_3, %c1_3) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.br ^bb5
  ^bb5:  // 2 preds: ^bb2, ^bb4
    vm.call @hal.command_buffer.fill_buffer(%ref_14, %ref_13, %c64000, %c64000, %c27648, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_14, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_14) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_15 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %14 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_15, [%ref_14]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %15 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_15]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %15, ^bb3(%15 : i32), ^bb6
  ^bb6:  // pred: ^bb5
    %ref_16 = vm.call.variadic @hal.buffer_view.create(%ref_13, %c64000, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    %ref_17 = vm.call.variadic @hal.buffer_view.create(%ref_13, %zero_0, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    %has_check_expect_eq = vm.import.resolved @check.expect_eq : i32
    vm.cond_br %has_check_expect_eq, ^bb7, ^bb8
  ^bb7:  // pred: ^bb6
    vm.call @check.expect_eq(%ref, %ref_17, %ref_16) : (!vm.ref<!hal.device>, !vm.ref<!hal.buffer_view>, !vm.ref<!hal.buffer_view>) -> ()
    vm.br ^bb8
  ^bb8:  // 2 preds: ^bb6, ^bb7
    vm.return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
vm.initializer {
  %null = vm.const.ref.zero : !vm.ref<!hal.executable>
  %null_0 = vm.const.ref.zero : !vm.buffer
  %c2 = vm.const.i32 2
  %c7 = vm.const.i32 7
  %zero = vm.const.i32.zero
  %c1 = vm.const.i32 1
  %zero_1 = vm.const.i64.zero
  %c-1 = vm.const.i64 -1
  %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
  %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
  %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 = vm.const.ref.rodata @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 : !vm.buffer
  %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
  %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
  %2 = vm.and.i32 %1, %c1 : i32
  %3 = vm.select.i32 %0#0, %2, %zero : i32
  %ref_2 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
  %ref_3 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_2]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
  %4 = vm.select.i64 %3, %zero_1, %c-1 : i64
  %5 = vm.trunc.i64.i32 %4 : i64 -> i32
  vm.global.store.i32 %3, @_device_query_0 : i32
  vm.global.store.ref %ref_3, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.br_table %5 {
    default: ^bb2,
    0: ^bb1
  }
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  %_main_dispatch_0_vulkan_spirv_fb = vm.const.ref.rodata @_main_dispatch_0_vulkan_spirv_fb : !vm.buffer
  %ref_4 = vm.call.variadic @hal.executable.create(%ref, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0, %_main_dispatch_0_vulkan_spirv_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
  vm.br ^bb3(%ref_4 : !vm.ref<!hal.executable>)
^bb2:  // pred: ^bb0
  vm.br ^bb3(%null : !vm.ref<!hal.executable>)
^bb3(%6: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
  vm.global.store.ref %6, @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
  vm.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
vm.func private @_main() {
  %c32000 = vm.const.i32 32000
  %c3 = vm.const.i32 3
  %c13 = vm.const.i32 13
  %c28 = vm.const.i32 28
  %c2 = vm.const.i32 2
  %c17 = vm.const.i32 17
  %c3075 = vm.const.i32 3075
  %c48 = vm.const.i32 48
  %zero = vm.const.i32.zero
  %null = vm.const.ref.zero : !vm.ref<!hal.fence>
  %c8192 = vm.const.i64 8192
  %c15360 = vm.const.i32 15360
  %c262144000 = vm.const.i64 262144000
  %c27648 = vm.const.i32 27648
  %c64000 = vm.const.i64 64000
  %zero_0 = vm.const.i64.zero
  %c128000 = vm.const.i64 128000
  %c-1 = vm.const.i64 -1
  %c-1_1 = vm.const.i32 -1
  %c1 = vm.const.i64 1
  %c32000_2 = vm.const.i64 32000
  %c553648144 = vm.const.i32 553648144
  %c1_3 = vm.const.i32 1
  %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
  %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  %_executable__main_dispatch_0 = vm.global.load.ref @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
  %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
  %ref_4 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
  %ref_5 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_4, %zero, %c48, %c3075, %c8192) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
  %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  %ref_6 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
  vm.call @hal.command_buffer.fill_buffer(%ref_6, %ref_5, %zero_0, %c8192, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
  vm.call @hal.command_buffer.execution_barrier(%ref_6, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
  vm.call @hal.command_buffer.finalize(%ref_6) : (!vm.ref<!hal.command_buffer>) -> ()
  %ref_7 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
  %1 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_7, [%ref_6]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
  %2 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  vm.cond_br %2, ^bb3(%2 : i32), ^bb1
^bb1:  // pred: ^bb0
  %3 = util.optimization_barrier %ref_5 : !vm.ref<!hal.buffer>
  %ref_8 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
  %ref_9 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c262144000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
  %4 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  %ref_10 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
  vm.call @hal.command_buffer.fill_buffer(%ref_10, %ref_9, %zero_0, %c262144000, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
  vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
  vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
  %ref_11 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
  %5 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
  %6 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  vm.cond_br %6, ^bb3(%6 : i32), ^bb2
^bb2:  // pred: ^bb1
  %7 = util.optimization_barrier %ref_9 : !vm.ref<!hal.buffer>
  %8 = vm.call @hal.buffer.length(%3) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
  %9 = vm.call @hal.buffer.length(%7) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
  %ref_12 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
  %ref_13 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_12, %zero, %c48, %c3075, %c128000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
  %10 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  %ref_14 = vm.call @hal.command_buffer.create(%ref, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
  %11 = vm.select.i64 %_device_query_0, %zero_0, %c-1 : i64
  %12 = vm.trunc.i64.i32 %11 : i64 -> i32
  vm.br_table %12 {
    default: ^bb5,
    0: ^bb4
  }
^bb3(%13: i32):  // 3 preds: ^bb0, ^bb1, ^bb5
  vm.fail %13, "failed to wait on timepoint"
^bb4:  // pred: ^bb2
  vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_14, %_pipeline_layout_0, %zero, [(%zero, %zero, %3, %zero_0, %8), (%c1_3, %zero, %7, %zero_0, %9), (%c2, %zero, %ref_13, %zero_0, %c128000)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.call @hal.command_buffer.dispatch(%ref_14, %_executable__main_dispatch_0, %zero, %c32000, %c1_3, %c1_3) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
  vm.br ^bb5
^bb5:  // 2 preds: ^bb2, ^bb4
  vm.call @hal.command_buffer.fill_buffer(%ref_14, %ref_13, %c64000, %c64000, %c27648, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
  vm.call @hal.command_buffer.execution_barrier(%ref_14, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
  vm.call @hal.command_buffer.finalize(%ref_14) : (!vm.ref<!hal.command_buffer>) -> ()
  %ref_15 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
  %14 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_15, [%ref_14]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
  %15 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_15]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  vm.cond_br %15, ^bb3(%15 : i32), ^bb6
^bb6:  // pred: ^bb5
  %ref_16 = vm.call.variadic @hal.buffer_view.create(%ref_13, %c64000, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
  %ref_17 = vm.call.variadic @hal.buffer_view.create(%ref_13, %zero_0, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
  %has_check_expect_eq = vm.import.resolved @check.expect_eq : i32
  vm.cond_br %has_check_expect_eq, ^bb7, ^bb8
^bb7:  // pred: ^bb6
  vm.call @check.expect_eq(%ref, %ref_17, %ref_16) : (!vm.ref<!hal.device>, !vm.ref<!hal.buffer_view>, !vm.ref<!hal.buffer_view>) -> ()
  vm.br ^bb8
^bb8:  // 2 preds: ^bb6, ^bb7
  vm.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
vm.func private @main() {
  vm.call @_main() : () -> ()
  vm.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
vm.func private @main() {
  %c32000 = vm.const.i32 32000
  %c3 = vm.const.i32 3
  %c13 = vm.const.i32 13
  %c28 = vm.const.i32 28
  %c2 = vm.const.i32 2
  %c17 = vm.const.i32 17
  %c3075 = vm.const.i32 3075
  %c48 = vm.const.i32 48
  %zero = vm.const.i32.zero
  %null = vm.const.ref.zero : !vm.ref<!hal.fence>
  %c8192 = vm.const.i64 8192
  %c15360 = vm.const.i32 15360
  %c262144000 = vm.const.i64 262144000
  %c27648 = vm.const.i32 27648
  %c64000 = vm.const.i64 64000
  %zero_0 = vm.const.i64.zero
  %c128000 = vm.const.i64 128000
  %c-1 = vm.const.i64 -1
  %c-1_1 = vm.const.i32 -1
  %c1 = vm.const.i64 1
  %c32000_2 = vm.const.i64 32000
  %c553648144 = vm.const.i32 553648144
  %c1_3 = vm.const.i32 1
  %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
  %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  %_executable__main_dispatch_0 = vm.global.load.ref @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
  %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
  %ref_4 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
  %ref_5 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_4, %zero, %c48, %c3075, %c8192) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
  %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  %ref_6 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
  vm.call @hal.command_buffer.fill_buffer(%ref_6, %ref_5, %zero_0, %c8192, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
  vm.call @hal.command_buffer.execution_barrier(%ref_6, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
  vm.call @hal.command_buffer.finalize(%ref_6) : (!vm.ref<!hal.command_buffer>) -> ()
  %ref_7 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
  %1 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_7, [%ref_6]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
  %2 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  vm.cond_br %2, ^bb3(%2 : i32), ^bb1
^bb1:  // pred: ^bb0
  %3 = util.optimization_barrier %ref_5 : !vm.ref<!hal.buffer>
  %ref_8 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
  %ref_9 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c262144000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
  %4 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  %ref_10 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
  vm.call @hal.command_buffer.fill_buffer(%ref_10, %ref_9, %zero_0, %c262144000, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
  vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
  vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
  %ref_11 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
  %5 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
  %6 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  vm.cond_br %6, ^bb3(%6 : i32), ^bb2
^bb2:  // pred: ^bb1
  %7 = util.optimization_barrier %ref_9 : !vm.ref<!hal.buffer>
  %8 = vm.call @hal.buffer.length(%3) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
  %9 = vm.call @hal.buffer.length(%7) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
  %ref_12 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
  %ref_13 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_12, %zero, %c48, %c3075, %c128000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
  %10 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  %ref_14 = vm.call @hal.command_buffer.create(%ref, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
  %11 = vm.select.i64 %_device_query_0, %zero_0, %c-1 : i64
  %12 = vm.trunc.i64.i32 %11 : i64 -> i32
  vm.br_table %12 {
    default: ^bb5,
    0: ^bb4
  }
^bb3(%13: i32):  // 3 preds: ^bb0, ^bb1, ^bb5
  vm.fail %13, "failed to wait on timepoint"
^bb4:  // pred: ^bb2
  vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_14, %_pipeline_layout_0, %zero, [(%zero, %zero, %3, %zero_0, %8), (%c1_3, %zero, %7, %zero_0, %9), (%c2, %zero, %ref_13, %zero_0, %c128000)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.call @hal.command_buffer.dispatch(%ref_14, %_executable__main_dispatch_0, %zero, %c32000, %c1_3, %c1_3) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
  vm.br ^bb5
^bb5:  // 2 preds: ^bb2, ^bb4
  vm.call @hal.command_buffer.fill_buffer(%ref_14, %ref_13, %c64000, %c64000, %c27648, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
  vm.call @hal.command_buffer.execution_barrier(%ref_14, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
  vm.call @hal.command_buffer.finalize(%ref_14) : (!vm.ref<!hal.command_buffer>) -> ()
  %ref_15 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
  %14 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_15, [%ref_14]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
  %15 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_15]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  vm.cond_br %15, ^bb3(%15 : i32), ^bb6
^bb6:  // pred: ^bb5
  %ref_16 = vm.call.variadic @hal.buffer_view.create(%ref_13, %c64000, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
  %ref_17 = vm.call.variadic @hal.buffer_view.create(%ref_13, %zero_0, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
  %has_check_expect_eq = vm.import.resolved @check.expect_eq : i32
  vm.cond_br %has_check_expect_eq, ^bb7, ^bb8
^bb7:  // pred: ^bb6
  vm.call @check.expect_eq(%ref, %ref_17, %ref_16) : (!vm.ref<!hal.device>, !vm.ref<!hal.buffer_view>, !vm.ref<!hal.buffer_view>) -> ()
  vm.br ^bb8
^bb8:  // 2 preds: ^bb6, ^bb7
  vm.return
}

// -----// IR Dump After Inliner (inline) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @_main_dispatch_0_vulkan_spirv_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<5020xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 {alignment = 1 : i64} "vulkan-spirv-fb"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 = vm.const.ref.rodata @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_2 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_3 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_2]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      %4 = vm.select.i64 %3, %zero_1, %c-1 : i64
      %5 = vm.trunc.i64.i32 %4 : i64 -> i32
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_3, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.br_table %5 {
        default: ^bb2,
        0: ^bb1
      }
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_main_dispatch_0_vulkan_spirv_fb = vm.const.ref.rodata @_main_dispatch_0_vulkan_spirv_fb : !vm.buffer
      %ref_4 = vm.call.variadic @hal.executable.create(%ref, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0, %_main_dispatch_0_vulkan_spirv_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_4 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%6: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %6, @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.func private @main() {
      %c32000 = vm.const.i32 32000
      %c3 = vm.const.i32 3
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %c17 = vm.const.i32 17
      %c3075 = vm.const.i32 3075
      %c48 = vm.const.i32 48
      %zero = vm.const.i32.zero
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c8192 = vm.const.i64 8192
      %c15360 = vm.const.i32 15360
      %c262144000 = vm.const.i64 262144000
      %c27648 = vm.const.i32 27648
      %c64000 = vm.const.i64 64000
      %zero_0 = vm.const.i64.zero
      %c128000 = vm.const.i64 128000
      %c-1 = vm.const.i64 -1
      %c-1_1 = vm.const.i32 -1
      %c1 = vm.const.i64 1
      %c32000_2 = vm.const.i64 32000
      %c553648144 = vm.const.i32 553648144
      %c1_3 = vm.const.i32 1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable__main_dispatch_0 = vm.global.load.ref @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_4 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_5 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_4, %zero, %c48, %c3075, %c8192) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_6 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.call @hal.command_buffer.fill_buffer(%ref_6, %ref_5, %zero_0, %c8192, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_6, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_6) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_7 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %1 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_7, [%ref_6]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %2 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %2, ^bb3(%2 : i32), ^bb1
    ^bb1:  // pred: ^bb0
      %3 = util.optimization_barrier %ref_5 : !vm.ref<!hal.buffer>
      %ref_8 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_9 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c262144000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %4 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_10 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.call @hal.command_buffer.fill_buffer(%ref_10, %ref_9, %zero_0, %c262144000, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_11 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %5 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %6 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %6, ^bb3(%6 : i32), ^bb2
    ^bb2:  // pred: ^bb1
      %7 = util.optimization_barrier %ref_9 : !vm.ref<!hal.buffer>
      %8 = vm.call @hal.buffer.length(%3) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
      %9 = vm.call @hal.buffer.length(%7) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
      %ref_12 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_13 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_12, %zero, %c48, %c3075, %c128000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %10 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_14 = vm.call @hal.command_buffer.create(%ref, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      %11 = vm.select.i64 %_device_query_0, %zero_0, %c-1 : i64
      %12 = vm.trunc.i64.i32 %11 : i64 -> i32
      vm.br_table %12 {
        default: ^bb5,
        0: ^bb4
      }
    ^bb3(%13: i32):  // 3 preds: ^bb0, ^bb1, ^bb5
      vm.fail %13, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb2
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_14, %_pipeline_layout_0, %zero, [(%zero, %zero, %3, %zero_0, %8), (%c1_3, %zero, %7, %zero_0, %9), (%c2, %zero, %ref_13, %zero_0, %c128000)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_14, %_executable__main_dispatch_0, %zero, %c32000, %c1_3, %c1_3) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.br ^bb5
    ^bb5:  // 2 preds: ^bb2, ^bb4
      vm.call @hal.command_buffer.fill_buffer(%ref_14, %ref_13, %c64000, %c64000, %c27648, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_14, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_14) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_15 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %14 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_15, [%ref_14]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %15 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_15]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %15, ^bb3(%15 : i32), ^bb6
    ^bb6:  // pred: ^bb5
      %ref_16 = vm.call.variadic @hal.buffer_view.create(%ref_13, %c64000, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      %ref_17 = vm.call.variadic @hal.buffer_view.create(%ref_13, %zero_0, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      %has_check_expect_eq = vm.import.resolved @check.expect_eq : i32
      vm.cond_br %has_check_expect_eq, ^bb7, ^bb8
    ^bb7:  // pred: ^bb6
      vm.call @check.expect_eq(%ref, %ref_17, %ref_16) : (!vm.ref<!hal.device>, !vm.ref<!hal.buffer_view>, !vm.ref<!hal.buffer_view>) -> ()
      vm.br ^bb8
    ^bb8:  // 2 preds: ^bb6, ^bb7
      vm.return
    }
    vm.export @main attributes {iree.abi.stub}
    vm.import private optional @check.expect_true(%operand : i32)
    vm.import private optional @check.expect_false(%operand : i32)
    vm.import private optional @check.expect_all_true(%device : !vm.ref<!hal.device>, %operand : !vm.ref<!hal.buffer_view>)
    vm.import private optional @check.expect_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
    vm.import private optional @check.expect_almost_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
    vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.ex.file.from_memory(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %access : i32, %buffer : !vm.buffer, %offset : i64, %length : i64, %flags : i32) -> !vm.ref<!hal.file>
    vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
    vm.import private @hal.allocator.import(%allocator : !vm.ref<!hal.allocator>, %try : i32, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32
    vm.import private @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...)
    vm.import private @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %flags : i32, %id : !vm.buffer, %group : !vm.buffer, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.split(%channel : !vm.ref<!hal.channel>, %color : i32, %key : i32, %flags : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer)
    vm.import private @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
    vm.import private @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64)
    vm.import private @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64)
    vm.import private @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64)
    vm.import private @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
    vm.import private @hal.device.queue.read(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_file : !vm.ref<!hal.file>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %flags : i32)
    vm.import private @hal.device.queue.write(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_file : !vm.ref<!hal.file>, %target_offset : i64, %length : i64, %flags : i32)
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64)
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects}
    vm.import private @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32
    vm.import private @hal.fence.signal(%fence : !vm.ref<!hal.fence>)
    vm.import private @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32)
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  }
}


// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @_main_dispatch_0_vulkan_spirv_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<5020xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 {alignment = 1 : i64} "vulkan-spirv-fb"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 = vm.const.ref.rodata @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_2 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_3 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_2]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      %4 = vm.select.i64 %3, %zero_1, %c-1 : i64
      %5 = vm.trunc.i64.i32 %4 : i64 -> i32
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_3, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.br_table %5 {
        default: ^bb2,
        0: ^bb1
      }
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_main_dispatch_0_vulkan_spirv_fb = vm.const.ref.rodata @_main_dispatch_0_vulkan_spirv_fb : !vm.buffer
      %ref_4 = vm.call.variadic @hal.executable.create(%ref, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0, %_main_dispatch_0_vulkan_spirv_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_4 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%6: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %6, @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.func private @main() {
      %c32000 = vm.const.i32 32000
      %c3 = vm.const.i32 3
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %c17 = vm.const.i32 17
      %c3075 = vm.const.i32 3075
      %c48 = vm.const.i32 48
      %zero = vm.const.i32.zero
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c8192 = vm.const.i64 8192
      %c15360 = vm.const.i32 15360
      %c262144000 = vm.const.i64 262144000
      %c27648 = vm.const.i32 27648
      %c64000 = vm.const.i64 64000
      %zero_0 = vm.const.i64.zero
      %c128000 = vm.const.i64 128000
      %c-1 = vm.const.i64 -1
      %c-1_1 = vm.const.i32 -1
      %c1 = vm.const.i64 1
      %c32000_2 = vm.const.i64 32000
      %c553648144 = vm.const.i32 553648144
      %c1_3 = vm.const.i32 1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable__main_dispatch_0 = vm.global.load.ref @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_4 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_5 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_4, %zero, %c48, %c3075, %c8192) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_6 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.call @hal.command_buffer.fill_buffer(%ref_6, %ref_5, %zero_0, %c8192, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_6, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_6) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_7 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %1 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_7, [%ref_6]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %2 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %2, ^bb3(%2 : i32), ^bb1
    ^bb1:  // pred: ^bb0
      %3 = util.optimization_barrier %ref_5 : !vm.ref<!hal.buffer>
      %ref_8 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_9 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c262144000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %4 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_10 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.call @hal.command_buffer.fill_buffer(%ref_10, %ref_9, %zero_0, %c262144000, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_11 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %5 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %6 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %6, ^bb3(%6 : i32), ^bb2
    ^bb2:  // pred: ^bb1
      %7 = util.optimization_barrier %ref_9 : !vm.ref<!hal.buffer>
      %8 = vm.call @hal.buffer.length(%3) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
      %9 = vm.call @hal.buffer.length(%7) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
      %ref_12 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_13 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_12, %zero, %c48, %c3075, %c128000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %10 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_14 = vm.call @hal.command_buffer.create(%ref, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      %11 = vm.select.i64 %_device_query_0, %zero_0, %c-1 : i64
      %12 = vm.trunc.i64.i32 %11 : i64 -> i32
      vm.br_table %12 {
        default: ^bb5,
        0: ^bb4
      }
    ^bb3(%13: i32):  // 3 preds: ^bb0, ^bb1, ^bb5
      vm.fail %13, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb2
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_14, %_pipeline_layout_0, %zero, [(%zero, %zero, %3, %zero_0, %8), (%c1_3, %zero, %7, %zero_0, %9), (%c2, %zero, %ref_13, %zero_0, %c128000)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_14, %_executable__main_dispatch_0, %zero, %c32000, %c1_3, %c1_3) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.br ^bb5
    ^bb5:  // 2 preds: ^bb2, ^bb4
      vm.call @hal.command_buffer.fill_buffer(%ref_14, %ref_13, %c64000, %c64000, %c27648, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_14, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_14) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_15 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %14 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_15, [%ref_14]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %15 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_15]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %15, ^bb3(%15 : i32), ^bb6
    ^bb6:  // pred: ^bb5
      %ref_16 = vm.call.variadic @hal.buffer_view.create(%ref_13, %c64000, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      %ref_17 = vm.call.variadic @hal.buffer_view.create(%ref_13, %zero_0, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      %has_check_expect_eq = vm.import.resolved @check.expect_eq : i32
      vm.cond_br %has_check_expect_eq, ^bb7, ^bb8
    ^bb7:  // pred: ^bb6
      vm.call @check.expect_eq(%ref, %ref_17, %ref_16) : (!vm.ref<!hal.device>, !vm.ref<!hal.buffer_view>, !vm.ref<!hal.buffer_view>) -> ()
      vm.br ^bb8
    ^bb8:  // 2 preds: ^bb6, ^bb7
      vm.return
    }
    vm.export @main attributes {iree.abi.stub}
    vm.import private optional @check.expect_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
    vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @_main_dispatch_0_vulkan_spirv_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<5020xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 {alignment = 1 : i64} "vulkan-spirv-fb"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 = vm.const.ref.rodata @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_2 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_3 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_2]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      %4 = vm.select.i64 %3, %zero_1, %c-1 : i64
      %5 = vm.trunc.i64.i32 %4 : i64 -> i32
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_3, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.br_table %5 {
        default: ^bb2,
        0: ^bb1
      }
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_main_dispatch_0_vulkan_spirv_fb = vm.const.ref.rodata @_main_dispatch_0_vulkan_spirv_fb : !vm.buffer
      %ref_4 = vm.call.variadic @hal.executable.create(%ref, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0, %_main_dispatch_0_vulkan_spirv_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_4 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%6: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %6, @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.func private @main() {
      %c32000 = vm.const.i32 32000
      %c3 = vm.const.i32 3
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %c17 = vm.const.i32 17
      %c3075 = vm.const.i32 3075
      %c48 = vm.const.i32 48
      %zero = vm.const.i32.zero
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c8192 = vm.const.i64 8192
      %c15360 = vm.const.i32 15360
      %c262144000 = vm.const.i64 262144000
      %c27648 = vm.const.i32 27648
      %c64000 = vm.const.i64 64000
      %zero_0 = vm.const.i64.zero
      %c128000 = vm.const.i64 128000
      %c-1 = vm.const.i64 -1
      %c-1_1 = vm.const.i32 -1
      %c1 = vm.const.i64 1
      %c32000_2 = vm.const.i64 32000
      %c553648144 = vm.const.i32 553648144
      %c1_3 = vm.const.i32 1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable__main_dispatch_0 = vm.global.load.ref @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_4 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_5 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_4, %zero, %c48, %c3075, %c8192) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_6 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.call @hal.command_buffer.fill_buffer(%ref_6, %ref_5, %zero_0, %c8192, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_6, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_6) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_7 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %1 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_7, [%ref_6]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %2 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %2, ^bb3(%2 : i32), ^bb1
    ^bb1:  // pred: ^bb0
      %3 = util.optimization_barrier %ref_5 : !vm.ref<!hal.buffer>
      %ref_8 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_9 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c262144000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %4 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_10 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.call @hal.command_buffer.fill_buffer(%ref_10, %ref_9, %zero_0, %c262144000, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_11 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %5 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %6 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %6, ^bb3(%6 : i32), ^bb2
    ^bb2:  // pred: ^bb1
      %7 = util.optimization_barrier %ref_9 : !vm.ref<!hal.buffer>
      %8 = vm.call @hal.buffer.length(%3) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
      %9 = vm.call @hal.buffer.length(%7) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
      %ref_12 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_13 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_12, %zero, %c48, %c3075, %c128000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %10 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_14 = vm.call @hal.command_buffer.create(%ref, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      %11 = vm.select.i64 %_device_query_0, %zero_0, %c-1 : i64
      %12 = vm.trunc.i64.i32 %11 : i64 -> i32
      vm.br_table %12 {
        default: ^bb5,
        0: ^bb4
      }
    ^bb3(%13: i32):  // 3 preds: ^bb0, ^bb1, ^bb5
      vm.fail %13, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb2
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_14, %_pipeline_layout_0, %zero, [(%zero, %zero, %3, %zero_0, %8), (%c1_3, %zero, %7, %zero_0, %9), (%c2, %zero, %ref_13, %zero_0, %c128000)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_14, %_executable__main_dispatch_0, %zero, %c32000, %c1_3, %c1_3) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.br ^bb5
    ^bb5:  // 2 preds: ^bb2, ^bb4
      vm.call @hal.command_buffer.fill_buffer(%ref_14, %ref_13, %c64000, %c64000, %c27648, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_14, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_14) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_15 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %14 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_15, [%ref_14]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %15 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_15]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %15, ^bb3(%15 : i32), ^bb6
    ^bb6:  // pred: ^bb5
      %ref_16 = vm.call.variadic @hal.buffer_view.create(%ref_13, %c64000, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      %ref_17 = vm.call.variadic @hal.buffer_view.create(%ref_13, %zero_0, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      %has_check_expect_eq = vm.import.resolved @check.expect_eq : i32
      vm.cond_br %has_check_expect_eq, ^bb7, ^bb8
    ^bb7:  // pred: ^bb6
      vm.call @check.expect_eq(%ref, %ref_17, %ref_16) : (!vm.ref<!hal.device>, !vm.ref<!hal.buffer_view>, !vm.ref<!hal.buffer_view>) -> ()
      vm.br ^bb8
    ^bb8:  // 2 preds: ^bb6, ^bb7
      vm.return
    }
    vm.export @main attributes {iree.abi.stub}
    vm.import private optional @check.expect_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
    vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @_main_dispatch_0_vulkan_spirv_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<5020xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 {alignment = 1 : i64} "vulkan-spirv-fb"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 = vm.const.ref.rodata @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_2 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_3 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_2]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      %4 = vm.select.i64 %3, %zero_1, %c-1 : i64
      %5 = vm.trunc.i64.i32 %4 : i64 -> i32
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_3, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.br_table %5 {
        default: ^bb2,
        0: ^bb1
      }
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_main_dispatch_0_vulkan_spirv_fb = vm.const.ref.rodata @_main_dispatch_0_vulkan_spirv_fb : !vm.buffer
      %ref_4 = vm.call.variadic @hal.executable.create(%ref, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0, %_main_dispatch_0_vulkan_spirv_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_4 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%6: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %6, @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.func private @main() {
      %c32000 = vm.const.i32 32000
      %c3 = vm.const.i32 3
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %c17 = vm.const.i32 17
      %c3075 = vm.const.i32 3075
      %c48 = vm.const.i32 48
      %zero = vm.const.i32.zero
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c8192 = vm.const.i64 8192
      %c15360 = vm.const.i32 15360
      %c262144000 = vm.const.i64 262144000
      %c27648 = vm.const.i32 27648
      %c64000 = vm.const.i64 64000
      %zero_0 = vm.const.i64.zero
      %c128000 = vm.const.i64 128000
      %c-1 = vm.const.i64 -1
      %c-1_1 = vm.const.i32 -1
      %c1 = vm.const.i64 1
      %c32000_2 = vm.const.i64 32000
      %c553648144 = vm.const.i32 553648144
      %c1_3 = vm.const.i32 1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable__main_dispatch_0 = vm.global.load.ref @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_4 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_5 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_4, %zero, %c48, %c3075, %c8192) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_6 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.call @hal.command_buffer.fill_buffer(%ref_6, %ref_5, %zero_0, %c8192, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_6, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_6) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_7 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %1 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_7, [%ref_6]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %2 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %2, ^bb3(%2 : i32), ^bb1
    ^bb1:  // pred: ^bb0
      %3 = util.optimization_barrier %ref_5 : !vm.ref<!hal.buffer>
      %ref_8 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_9 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c262144000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %4 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_10 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.call @hal.command_buffer.fill_buffer(%ref_10, %ref_9, %zero_0, %c262144000, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_11 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %5 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %6 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %6, ^bb3(%6 : i32), ^bb2
    ^bb2:  // pred: ^bb1
      %7 = util.optimization_barrier %ref_9 : !vm.ref<!hal.buffer>
      %8 = vm.call @hal.buffer.length(%3) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
      %9 = vm.call @hal.buffer.length(%7) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
      %ref_12 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_13 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_12, %zero, %c48, %c3075, %c128000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %10 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_14 = vm.call @hal.command_buffer.create(%ref, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      %11 = vm.select.i64 %_device_query_0, %zero_0, %c-1 : i64
      %12 = vm.trunc.i64.i32 %11 : i64 -> i32
      vm.br_table %12 {
        default: ^bb5,
        0: ^bb4
      }
    ^bb3(%13: i32):  // 3 preds: ^bb0, ^bb1, ^bb5
      vm.fail %13, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb2
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_14, %_pipeline_layout_0, %zero, [(%zero, %zero, %3, %zero_0, %8), (%c1_3, %zero, %7, %zero_0, %9), (%c2, %zero, %ref_13, %zero_0, %c128000)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_14, %_executable__main_dispatch_0, %zero, %c32000, %c1_3, %c1_3) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.br ^bb5
    ^bb5:  // 2 preds: ^bb2, ^bb4
      vm.call @hal.command_buffer.fill_buffer(%ref_14, %ref_13, %c64000, %c64000, %c27648, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_14, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_14) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_15 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %14 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_15, [%ref_14]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %15 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_15]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %15, ^bb3(%15 : i32), ^bb6
    ^bb6:  // pred: ^bb5
      %ref_16 = vm.call.variadic @hal.buffer_view.create(%ref_13, %c64000, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      %ref_17 = vm.call.variadic @hal.buffer_view.create(%ref_13, %zero_0, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      %has_check_expect_eq = vm.import.resolved @check.expect_eq : i32
      vm.cond_br %has_check_expect_eq, ^bb7, ^bb8
    ^bb7:  // pred: ^bb6
      vm.call @check.expect_eq(%ref, %ref_17, %ref_16) : (!vm.ref<!hal.device>, !vm.ref<!hal.buffer_view>, !vm.ref<!hal.buffer_view>) -> ()
      vm.br ^bb8
    ^bb8:  // 2 preds: ^bb6, ^bb7
      vm.return
    }
    vm.export @main attributes {iree.abi.stub}
    vm.import private optional @check.expect_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
    vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  }
}


// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @_main_dispatch_0_vulkan_spirv_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"0x080000005350564578ECFFFF14000000940000000400000001000000480000000100000004000000340000005F6D61696E5F64697370617463685F305F6D61746D756C5F7472616E73706F73655F625F3178333230303078343039365F66313600000000DCECFFFF08000000090000002D00000064756D70732F636F6E666967757265645F6D6F64756C655F5F6D61696E5F64697370617463685F302E6D6C6972000000B8040000030223070003010016000000D200000000000000110002000100000011000200511100001100020009000000110002003F0000000A000B005350565F4B48525F73746F726167655F6275666665725F73746F726167655F636C617373000000000A0007005350565F4B48525F31366269745F73746F726167650000000E00030000000000010000000F00130005000000140000005F6D61696E5F64697370617463685F305F6D61746D756C5F7472616E73706F73655F625F3178333230303078343039365F66313600000000050000000400000010000600140000001100000040000000010000000100000005000900040000005F5F6275696C74696E5F5F576F726B67726F757049645F5F0000000005000A00050000005F5F6275696C74696E5F5F4C6F63616C496E766F636174696F6E49645F5F0000050007000B0000005F5F7265736F757263655F7661725F305F305F00050007000C0000005F5F7265736F757263655F7661725F305F315F0005000700110000005F5F7265736F757263655F7661725F305F325F0005001000140000005F6D61696E5F64697370617463685F305F6D61746D756C5F7472616E73706F73655F625F3178333230303078343039365F6631360000000047000400040000000B0000001A00000047000400050000000B0000001B000000470004000800000006000000100000004800050007000000000000002300000000000000470003000700000002000000470004000B0000002100000000000000470004000B0000002200000000000000470004000C0000002100000001000000470004000C0000002200000000000000470004000F0000000600000002000000480005000E000000000000002300000000000000470003000E0000000200000047000400110000002100000002000000470004001100000022000000000000001500040003000000200000000000000017000400020000000300000003000000200004000100000001000000020000003B0004000100000004000000010000003B000400010000000500000001000000160003000A0000002000000017000400090000000A000000040000001D00030008000000090000001E000300070000000800000020000400060000000C000000070000003B000400060000000B0000000C0000003B000400060000000C0000000C0000001600030010000000100000001D0003000F000000100000001E0003000E0000000F000000200004000D0000000C0000000E0000003B0004000D000000110000000C00000013000200130000002100030012000000130000002B0004000300000016000000400000002B0004000300000017000000800000002B0004000300000018000000C00000002B0004000300000019000000000100002B000400030000001A000000400100002B000400030000001B000000800100002B000400030000001C000000C00100002B000400030000001D00000000020000170004001E00000010000000040000002B0004001000000020000000000000002C0007001E0000001F000000200000002000000020000000200000002B00040003000000210000000000000020000400290000000C0000000900000017000400580000000A000000020000002B00040003000000C90000000300000014000200CB00000020000400D00000000C000000100000003600050013000000140000000000000012000000F8000200150000003D00040002000000220000000500000051000500030000002300000022000000000000003D0004000200000024000000040000005100050003000000250000002400000000000000840005000300000026000000250000001D0000008000050003000000270000002300000026000000800005000300000028000000270000001C00000041000600290000002A0000000C00000021000000280000003D000400090000002B0000002A00000080000500030000002C000000230000001C00000041000600290000002D0000000B000000210000002C0000003D000400090000002E0000002D00000080000500030000002F000000270000001B0000004100060029000000300000000C000000210000002F0000003D000400090000003100000030000000800005000300000032000000230000001B0000004100060029000000330000000B00000021000000320000003D000400090000003400000033000000800005000300000035000000270000001A0000004100060029000000360000000C00000021000000350000003D000400090000003700000036000000800005000300000038000000230000001A0000004100060029000000390000000B00000021000000380000003D000400090000003A0000003900000080000500030000003B000000270000001900000041000600290000003C0000000C000000210000003B0000003D000400090000003D0000003C00000080000500030000003E000000230000001900000041000600290000003F0000000B000000210000003E0000003D00040009000000400000003F00000080000500030000004100000027000000180000004100060029000000420000000C00000021000000410000003D00040009000000430000004200000080000500030000004400000023000000180000004100060029000000450000000B00000021000000440000003D00040009000000460000004500000080000500030000004700000027000000170000004100060029000000480000000C00000021000000470000003D00040009000000490000004800000080000500030000004A000000230000001700000041000600290000004B0000000B000000210000004A0000003D000400090000004C0000004B00000080000500030000004D000000270000001600000041000600290000004E0000000C000000210000004D0000003D000400090000004F0000004E00000080000500030000005000000023000000160000004100060029000000510000000B00000021000000500000003D00040009000000520000005100000080000500030000005300000026000000230000004100060029000000540000000C00000021000000530000003D0004000900000055000000540000004100060029000000560000000B00000021000000230000003D0004000900000057000000560000004F00070058000000590000002E0000002E00000000000000010000007C0004001E0000005A000000590000004F000700580000005B0000002B0000002B00000000000000010000007C0004001E0000005C0000005B000000850005001E0000005D0000005A0000005C0000004F000700580000005E0000002E0000002E00000002000000030000007C0004001E0000005F0000005E0000004F00070058000000600000002B0000002B00000002000000030000007C0004001E0000006100000060000000850005001E000000620000005F00000061000000810005001E000000630000005D0000001F000000810005001E00000064000000620000001F0000004F0007005800000065000000340000003400000000000000010000007C0004001E00000066000000650000004F0007005800000067000000310000003100000000000000010000007C0004001E0000006800000067000000850005001E0000006900000066000000680000004F000700580000006A000000340000003400000002000000030000007C0004001E0000006B0000006A0000004F000700580000006C000000310000003100000002000000030000007C0004001E0000006D0000006C000000850005001E0000006E0000006B0000006D000000810005001E0000006F0000006900000063000000810005001E000000700000006E000000640000004F00070058000000710000003A0000003A00000000000000010000007C0004001E00000072000000710000004F0007005800000073000000370000003700000000000000010000007C0004001E0000007400000073000000850005001E0000007500000072000000740000004F00070058000000760000003A0000003A00000002000000030000007C0004001E00000077000000760000004F0007005800000078000000370000003700000002000000030000007C0004001E0000007900000078000000850005001E0000007A0000007700000079000000810005001E0000007B000000750000006F000000810005001E0000007C0000007A000000700000004F000700580000007D000000400000004000000000000000010000007C0004001E0000007E0000007D0000004F000700580000007F0000003D0000003D00000000000000010000007C0004001E000000800000007F000000850005001E000000810000007E000000800000004F0007005800000082000000400000004000000002000000030000007C0004001E00000083000000820000004F00070058000000840000003D0000003D00000002000000030000007C0004001E0000008500000084000000850005001E000000860000008300000085000000810005001E00000087000000810000007B000000810005001E00000088000000860000007C0000004F0007005800000089000000460000004600000000000000010000007C0004001E0000008A000000890000004F000700580000008B000000430000004300000000000000010000007C0004001E0000008C0000008B000000850005001E0000008D0000008A0000008C0000004F000700580000008E000000460000004600000002000000030000007C0004001E0000008F0000008E0000004F0007005800000090000000430000004300000002000000030000007C0004001E0000009100000090000000850005001E000000920000008F00000091000000810005001E000000930000008D00000087000000810005001E0000009400000092000000880000004F00070058000000950000004C0000004C00000000000000010000007C0004001E00000096000000950000004F0007005800000097000000490000004900000000000000010000007C0004001E0000009800000097000000850005001E0000009900000096000000980000004F000700580000009A0000004C0000004C00000002000000030000007C0004001E0000009B0000009A0000004F000700580000009C000000490000004900000002000000030000007C0004001E0000009D0000009C000000850005001E0000009E0000009B0000009D000000810005001E0000009F0000009900000093000000810005001E000000A00000009E000000940000004F00070058000000A1000000520000005200000000000000010000007C0004001E000000A2000000A10000004F00070058000000A30000004F0000004F00000000000000010000007C0004001E000000A4000000A3000000850005001E000000A5000000A2000000A40000004F00070058000000A6000000520000005200000002000000030000007C0004001E000000A7000000A60000004F00070058000000A80000004F0000004F00000002000000030000007C0004001E000000A9000000A8000000850005001E000000AA000000A7000000A9000000810005001E000000AB000000A50000009F000000810005001E000000AC000000AA000000A00000004F00070058000000AD000000570000005700000000000000010000007C0004001E000000AE000000AD0000004F00070058000000AF000000550000005500000000000000010000007C0004001E000000B0000000AF000000850005001E000000B1000000AE000000B00000004F00070058000000B2000000570000005700000002000000030000007C0004001E000000B3000000B20000004F00070058000000B4000000550000005500000002000000030000007C0004001E000000B5000000B4000000850005001E000000B6000000B3000000B5000000810005001E000000B7000000B1000000AB000000810005001E000000B8000000B6000000AC0000005100050010000000B9000000B7000000000000005100050010000000BA000000B7000000010000005100050010000000BB000000B7000000020000005100050010000000BC000000B7000000030000008100050010000000BD000000B9000000BA0000008100050010000000BE000000BD000000BB0000008100050010000000BF000000BE000000BC0000005100050010000000C0000000B8000000000000005100050010000000C1000000B8000000010000005100050010000000C2000000B8000000020000005100050010000000C3000000B8000000030000008100050010000000C4000000C0000000C10000008100050010000000C5000000C4000000C20000008100050010000000C6000000C5000000C30000008100050010000000C7000000BF000000C60000005E01060010000000C8000000C900000000000000C70000008100050010000000CA000000C800000020000000AA000500CB000000CC0000002300000021000000F9000200CD000000F8000200CD000000F7000300CF00000000000000FA000400CC000000CE000000CF000000F8000200CE00000041000600D0000000D10000001100000021000000250000003E000300D1000000CA000000F9000200CF000000F8000200CF000000FD0001003800010008000C00040008000C0010000400000008000C00"> : vector<5020xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 {alignment = 1 : i64} "vulkan-spirv-fb"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 = vm.const.ref.rodata @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_2 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_3 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_2]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      %4 = vm.select.i64 %3, %zero_1, %c-1 : i64
      %5 = vm.trunc.i64.i32 %4 : i64 -> i32
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_3, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.br_table %5 {
        default: ^bb2,
        0: ^bb1
      }
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_main_dispatch_0_vulkan_spirv_fb = vm.const.ref.rodata @_main_dispatch_0_vulkan_spirv_fb : !vm.buffer
      %ref_4 = vm.call.variadic @hal.executable.create(%ref, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0, %_main_dispatch_0_vulkan_spirv_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_4 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%6: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %6, @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.func private @main() {
      %c32000 = vm.const.i32 32000
      %c3 = vm.const.i32 3
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %c17 = vm.const.i32 17
      %c3075 = vm.const.i32 3075
      %c48 = vm.const.i32 48
      %zero = vm.const.i32.zero
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c8192 = vm.const.i64 8192
      %c15360 = vm.const.i32 15360
      %c262144000 = vm.const.i64 262144000
      %c27648 = vm.const.i32 27648
      %c64000 = vm.const.i64 64000
      %zero_0 = vm.const.i64.zero
      %c128000 = vm.const.i64 128000
      %c-1 = vm.const.i64 -1
      %c-1_1 = vm.const.i32 -1
      %c1 = vm.const.i64 1
      %c32000_2 = vm.const.i64 32000
      %c553648144 = vm.const.i32 553648144
      %c1_3 = vm.const.i32 1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable__main_dispatch_0 = vm.global.load.ref @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_4 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_5 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_4, %zero, %c48, %c3075, %c8192) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_6 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.call @hal.command_buffer.fill_buffer(%ref_6, %ref_5, %zero_0, %c8192, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_6, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_6) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_7 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %1 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_7, [%ref_6]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %2 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %2, ^bb3(%2 : i32), ^bb1
    ^bb1:  // pred: ^bb0
      %3 = util.optimization_barrier %ref_5 : !vm.ref<!hal.buffer>
      %ref_8 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_9 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c262144000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %4 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_10 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.call @hal.command_buffer.fill_buffer(%ref_10, %ref_9, %zero_0, %c262144000, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_11 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %5 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %6 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %6, ^bb3(%6 : i32), ^bb2
    ^bb2:  // pred: ^bb1
      %7 = util.optimization_barrier %ref_9 : !vm.ref<!hal.buffer>
      %8 = vm.call @hal.buffer.length(%3) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
      %9 = vm.call @hal.buffer.length(%7) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
      %ref_12 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_13 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_12, %zero, %c48, %c3075, %c128000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %10 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_14 = vm.call @hal.command_buffer.create(%ref, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      %11 = vm.select.i64 %_device_query_0, %zero_0, %c-1 : i64
      %12 = vm.trunc.i64.i32 %11 : i64 -> i32
      vm.br_table %12 {
        default: ^bb5,
        0: ^bb4
      }
    ^bb3(%13: i32):  // 3 preds: ^bb0, ^bb1, ^bb5
      vm.fail %13, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb2
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_14, %_pipeline_layout_0, %zero, [(%zero, %zero, %3, %zero_0, %8), (%c1_3, %zero, %7, %zero_0, %9), (%c2, %zero, %ref_13, %zero_0, %c128000)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_14, %_executable__main_dispatch_0, %zero, %c32000, %c1_3, %c1_3) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.br ^bb5
    ^bb5:  // 2 preds: ^bb2, ^bb4
      vm.call @hal.command_buffer.fill_buffer(%ref_14, %ref_13, %c64000, %c64000, %c27648, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_14, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_14) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_15 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %14 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_15, [%ref_14]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %15 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_15]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %15, ^bb3(%15 : i32), ^bb6
    ^bb6:  // pred: ^bb5
      %ref_16 = vm.call.variadic @hal.buffer_view.create(%ref_13, %c64000, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      %ref_17 = vm.call.variadic @hal.buffer_view.create(%ref_13, %zero_0, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      %has_check_expect_eq = vm.import.resolved @check.expect_eq : i32
      vm.cond_br %has_check_expect_eq, ^bb7, ^bb8
    ^bb7:  // pred: ^bb6
      vm.call @check.expect_eq(%ref, %ref_17, %ref_16) : (!vm.ref<!hal.device>, !vm.ref<!hal.buffer_view>, !vm.ref<!hal.buffer_view>) -> ()
      vm.br ^bb8
    ^bb8:  // 2 preds: ^bb6, ^bb7
      vm.return
    }
    vm.export @main attributes {iree.abi.stub}
    vm.import private optional @check.expect_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
    vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @_main_dispatch_0_vulkan_spirv_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<5020xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 {alignment = 1 : i64} "vulkan-spirv-fb"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 = vm.const.ref.rodata @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_2 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_3 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_2]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      %4 = vm.select.i64 %3, %zero_1, %c-1 : i64
      %5 = vm.trunc.i64.i32 %4 : i64 -> i32
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_3, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.br_table %5 {
        default: ^bb2,
        0: ^bb1
      }
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_main_dispatch_0_vulkan_spirv_fb = vm.const.ref.rodata @_main_dispatch_0_vulkan_spirv_fb : !vm.buffer
      %ref_4 = vm.call.variadic @hal.executable.create(%ref, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0, %_main_dispatch_0_vulkan_spirv_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_4 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%6: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %6, @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.func private @main() {
      %c32000 = vm.const.i32 32000
      %c3 = vm.const.i32 3
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %c17 = vm.const.i32 17
      %c3075 = vm.const.i32 3075
      %c48 = vm.const.i32 48
      %zero = vm.const.i32.zero
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c8192 = vm.const.i64 8192
      %c15360 = vm.const.i32 15360
      %c262144000 = vm.const.i64 262144000
      %c27648 = vm.const.i32 27648
      %c64000 = vm.const.i64 64000
      %zero_0 = vm.const.i64.zero
      %c128000 = vm.const.i64 128000
      %c-1 = vm.const.i64 -1
      %c-1_1 = vm.const.i32 -1
      %c1 = vm.const.i64 1
      %c32000_2 = vm.const.i64 32000
      %c553648144 = vm.const.i32 553648144
      %c1_3 = vm.const.i32 1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable__main_dispatch_0 = vm.global.load.ref @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_4 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_5 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_4, %zero, %c48, %c3075, %c8192) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_6 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.call @hal.command_buffer.fill_buffer(%ref_6, %ref_5, %zero_0, %c8192, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_6, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_6) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_7 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %1 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_7, [%ref_6]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %2 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %2, ^bb3(%2 : i32), ^bb1
    ^bb1:  // pred: ^bb0
      %3 = util.optimization_barrier %ref_5 : !vm.ref<!hal.buffer>
      %ref_8 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_9 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c262144000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %4 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_10 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.call @hal.command_buffer.fill_buffer(%ref_10, %ref_9, %zero_0, %c262144000, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_11 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %5 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %6 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %6, ^bb3(%6 : i32), ^bb2
    ^bb2:  // pred: ^bb1
      %7 = util.optimization_barrier %ref_9 : !vm.ref<!hal.buffer>
      %8 = vm.call @hal.buffer.length(%3) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
      %9 = vm.call @hal.buffer.length(%7) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
      %ref_12 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_13 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_12, %zero, %c48, %c3075, %c128000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %10 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_14 = vm.call @hal.command_buffer.create(%ref, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      %11 = vm.select.i64 %_device_query_0, %zero_0, %c-1 : i64
      %12 = vm.trunc.i64.i32 %11 : i64 -> i32
      vm.br_table %12 {
        default: ^bb5,
        0: ^bb4
      }
    ^bb3(%13: i32):  // 3 preds: ^bb0, ^bb1, ^bb5
      vm.fail %13, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb2
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_14, %_pipeline_layout_0, %zero, [(%zero, %zero, %3, %zero_0, %8), (%c1_3, %zero, %7, %zero_0, %9), (%c2, %zero, %ref_13, %zero_0, %c128000)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_14, %_executable__main_dispatch_0, %zero, %c32000, %c1_3, %c1_3) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.br ^bb5
    ^bb5:  // 2 preds: ^bb2, ^bb4
      vm.call @hal.command_buffer.fill_buffer(%ref_14, %ref_13, %c64000, %c64000, %c27648, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_14, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_14) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_15 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %14 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_15, [%ref_14]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %15 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_15]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %15, ^bb3(%15 : i32), ^bb6
    ^bb6:  // pred: ^bb5
      %ref_16 = vm.call.variadic @hal.buffer_view.create(%ref_13, %c64000, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      %ref_17 = vm.call.variadic @hal.buffer_view.create(%ref_13, %zero_0, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      %has_check_expect_eq = vm.import.resolved @check.expect_eq : i32
      vm.cond_br %has_check_expect_eq, ^bb7, ^bb8
    ^bb7:  // pred: ^bb6
      vm.call @check.expect_eq(%ref, %ref_17, %ref_16) : (!vm.ref<!hal.device>, !vm.ref<!hal.buffer_view>, !vm.ref<!hal.buffer_view>) -> ()
      vm.br ^bb8
    ^bb8:  // 2 preds: ^bb6, ^bb7
      vm.return
    }
    vm.export @main attributes {iree.abi.stub}
    vm.import private optional @check.expect_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
    vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @_main_dispatch_0_vulkan_spirv_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"0x080000005350564578ECFFFF14000000940000000400000001000000480000000100000004000000340000005F6D61696E5F64697370617463685F305F6D61746D756C5F7472616E73706F73655F625F3178333230303078343039365F66313600000000DCECFFFF08000000090000002D00000064756D70732F636F6E666967757265645F6D6F64756C655F5F6D61696E5F64697370617463685F302E6D6C6972000000B8040000030223070003010016000000D200000000000000110002000100000011000200511100001100020009000000110002003F0000000A000B005350565F4B48525F73746F726167655F6275666665725F73746F726167655F636C617373000000000A0007005350565F4B48525F31366269745F73746F726167650000000E00030000000000010000000F00130005000000140000005F6D61696E5F64697370617463685F305F6D61746D756C5F7472616E73706F73655F625F3178333230303078343039365F66313600000000050000000400000010000600140000001100000040000000010000000100000005000900040000005F5F6275696C74696E5F5F576F726B67726F757049645F5F0000000005000A00050000005F5F6275696C74696E5F5F4C6F63616C496E766F636174696F6E49645F5F0000050007000B0000005F5F7265736F757263655F7661725F305F305F00050007000C0000005F5F7265736F757263655F7661725F305F315F0005000700110000005F5F7265736F757263655F7661725F305F325F0005001000140000005F6D61696E5F64697370617463685F305F6D61746D756C5F7472616E73706F73655F625F3178333230303078343039365F6631360000000047000400040000000B0000001A00000047000400050000000B0000001B000000470004000800000006000000100000004800050007000000000000002300000000000000470003000700000002000000470004000B0000002100000000000000470004000B0000002200000000000000470004000C0000002100000001000000470004000C0000002200000000000000470004000F0000000600000002000000480005000E000000000000002300000000000000470003000E0000000200000047000400110000002100000002000000470004001100000022000000000000001500040003000000200000000000000017000400020000000300000003000000200004000100000001000000020000003B0004000100000004000000010000003B000400010000000500000001000000160003000A0000002000000017000400090000000A000000040000001D00030008000000090000001E000300070000000800000020000400060000000C000000070000003B000400060000000B0000000C0000003B000400060000000C0000000C0000001600030010000000100000001D0003000F000000100000001E0003000E0000000F000000200004000D0000000C0000000E0000003B0004000D000000110000000C00000013000200130000002100030012000000130000002B0004000300000016000000400000002B0004000300000017000000800000002B0004000300000018000000C00000002B0004000300000019000000000100002B000400030000001A000000400100002B000400030000001B000000800100002B000400030000001C000000C00100002B000400030000001D00000000020000170004001E00000010000000040000002B0004001000000020000000000000002C0007001E0000001F000000200000002000000020000000200000002B00040003000000210000000000000020000400290000000C0000000900000017000400580000000A000000020000002B00040003000000C90000000300000014000200CB00000020000400D00000000C000000100000003600050013000000140000000000000012000000F8000200150000003D00040002000000220000000500000051000500030000002300000022000000000000003D0004000200000024000000040000005100050003000000250000002400000000000000840005000300000026000000250000001D0000008000050003000000270000002300000026000000800005000300000028000000270000001C00000041000600290000002A0000000C00000021000000280000003D000400090000002B0000002A00000080000500030000002C000000230000001C00000041000600290000002D0000000B000000210000002C0000003D000400090000002E0000002D00000080000500030000002F000000270000001B0000004100060029000000300000000C000000210000002F0000003D000400090000003100000030000000800005000300000032000000230000001B0000004100060029000000330000000B00000021000000320000003D000400090000003400000033000000800005000300000035000000270000001A0000004100060029000000360000000C00000021000000350000003D000400090000003700000036000000800005000300000038000000230000001A0000004100060029000000390000000B00000021000000380000003D000400090000003A0000003900000080000500030000003B000000270000001900000041000600290000003C0000000C000000210000003B0000003D000400090000003D0000003C00000080000500030000003E000000230000001900000041000600290000003F0000000B000000210000003E0000003D00040009000000400000003F00000080000500030000004100000027000000180000004100060029000000420000000C00000021000000410000003D00040009000000430000004200000080000500030000004400000023000000180000004100060029000000450000000B00000021000000440000003D00040009000000460000004500000080000500030000004700000027000000170000004100060029000000480000000C00000021000000470000003D00040009000000490000004800000080000500030000004A000000230000001700000041000600290000004B0000000B000000210000004A0000003D000400090000004C0000004B00000080000500030000004D000000270000001600000041000600290000004E0000000C000000210000004D0000003D000400090000004F0000004E00000080000500030000005000000023000000160000004100060029000000510000000B00000021000000500000003D00040009000000520000005100000080000500030000005300000026000000230000004100060029000000540000000C00000021000000530000003D0004000900000055000000540000004100060029000000560000000B00000021000000230000003D0004000900000057000000560000004F00070058000000590000002E0000002E00000000000000010000007C0004001E0000005A000000590000004F000700580000005B0000002B0000002B00000000000000010000007C0004001E0000005C0000005B000000850005001E0000005D0000005A0000005C0000004F000700580000005E0000002E0000002E00000002000000030000007C0004001E0000005F0000005E0000004F00070058000000600000002B0000002B00000002000000030000007C0004001E0000006100000060000000850005001E000000620000005F00000061000000810005001E000000630000005D0000001F000000810005001E00000064000000620000001F0000004F0007005800000065000000340000003400000000000000010000007C0004001E00000066000000650000004F0007005800000067000000310000003100000000000000010000007C0004001E0000006800000067000000850005001E0000006900000066000000680000004F000700580000006A000000340000003400000002000000030000007C0004001E0000006B0000006A0000004F000700580000006C000000310000003100000002000000030000007C0004001E0000006D0000006C000000850005001E0000006E0000006B0000006D000000810005001E0000006F0000006900000063000000810005001E000000700000006E000000640000004F00070058000000710000003A0000003A00000000000000010000007C0004001E00000072000000710000004F0007005800000073000000370000003700000000000000010000007C0004001E0000007400000073000000850005001E0000007500000072000000740000004F00070058000000760000003A0000003A00000002000000030000007C0004001E00000077000000760000004F0007005800000078000000370000003700000002000000030000007C0004001E0000007900000078000000850005001E0000007A0000007700000079000000810005001E0000007B000000750000006F000000810005001E0000007C0000007A000000700000004F000700580000007D000000400000004000000000000000010000007C0004001E0000007E0000007D0000004F000700580000007F0000003D0000003D00000000000000010000007C0004001E000000800000007F000000850005001E000000810000007E000000800000004F0007005800000082000000400000004000000002000000030000007C0004001E00000083000000820000004F00070058000000840000003D0000003D00000002000000030000007C0004001E0000008500000084000000850005001E000000860000008300000085000000810005001E00000087000000810000007B000000810005001E00000088000000860000007C0000004F0007005800000089000000460000004600000000000000010000007C0004001E0000008A000000890000004F000700580000008B000000430000004300000000000000010000007C0004001E0000008C0000008B000000850005001E0000008D0000008A0000008C0000004F000700580000008E000000460000004600000002000000030000007C0004001E0000008F0000008E0000004F0007005800000090000000430000004300000002000000030000007C0004001E0000009100000090000000850005001E000000920000008F00000091000000810005001E000000930000008D00000087000000810005001E0000009400000092000000880000004F00070058000000950000004C0000004C00000000000000010000007C0004001E00000096000000950000004F0007005800000097000000490000004900000000000000010000007C0004001E0000009800000097000000850005001E0000009900000096000000980000004F000700580000009A0000004C0000004C00000002000000030000007C0004001E0000009B0000009A0000004F000700580000009C000000490000004900000002000000030000007C0004001E0000009D0000009C000000850005001E0000009E0000009B0000009D000000810005001E0000009F0000009900000093000000810005001E000000A00000009E000000940000004F00070058000000A1000000520000005200000000000000010000007C0004001E000000A2000000A10000004F00070058000000A30000004F0000004F00000000000000010000007C0004001E000000A4000000A3000000850005001E000000A5000000A2000000A40000004F00070058000000A6000000520000005200000002000000030000007C0004001E000000A7000000A60000004F00070058000000A80000004F0000004F00000002000000030000007C0004001E000000A9000000A8000000850005001E000000AA000000A7000000A9000000810005001E000000AB000000A50000009F000000810005001E000000AC000000AA000000A00000004F00070058000000AD000000570000005700000000000000010000007C0004001E000000AE000000AD0000004F00070058000000AF000000550000005500000000000000010000007C0004001E000000B0000000AF000000850005001E000000B1000000AE000000B00000004F00070058000000B2000000570000005700000002000000030000007C0004001E000000B3000000B20000004F00070058000000B4000000550000005500000002000000030000007C0004001E000000B5000000B4000000850005001E000000B6000000B3000000B5000000810005001E000000B7000000B1000000AB000000810005001E000000B8000000B6000000AC0000005100050010000000B9000000B7000000000000005100050010000000BA000000B7000000010000005100050010000000BB000000B7000000020000005100050010000000BC000000B7000000030000008100050010000000BD000000B9000000BA0000008100050010000000BE000000BD000000BB0000008100050010000000BF000000BE000000BC0000005100050010000000C0000000B8000000000000005100050010000000C1000000B8000000010000005100050010000000C2000000B8000000020000005100050010000000C3000000B8000000030000008100050010000000C4000000C0000000C10000008100050010000000C5000000C4000000C20000008100050010000000C6000000C5000000C30000008100050010000000C7000000BF000000C60000005E01060010000000C8000000C900000000000000C70000008100050010000000CA000000C800000020000000AA000500CB000000CC0000002300000021000000F9000200CD000000F8000200CD000000F7000300CF00000000000000FA000400CC000000CE000000CF000000F8000200CE00000041000600D0000000D10000001100000021000000250000003E000300D1000000CA000000F9000200CF000000F8000200CF000000FD0001003800010008000C00040008000C0010000400000008000C00"> : vector<5020xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 {alignment = 1 : i64} "vulkan-spirv-fb"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 = vm.const.ref.rodata @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_2 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_3 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_2]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      %4 = vm.select.i64 %3, %zero_1, %c-1 : i64
      %5 = vm.trunc.i64.i32 %4 : i64 -> i32
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_3, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.br_table %5 {
        default: ^bb2,
        0: ^bb1
      }
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_main_dispatch_0_vulkan_spirv_fb = vm.const.ref.rodata @_main_dispatch_0_vulkan_spirv_fb : !vm.buffer
      %ref_4 = vm.call.variadic @hal.executable.create(%ref, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0, %_main_dispatch_0_vulkan_spirv_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_4 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%6: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %6, @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.func private @main() {
      %c32000 = vm.const.i32 32000
      %c3 = vm.const.i32 3
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %c17 = vm.const.i32 17
      %c3075 = vm.const.i32 3075
      %c48 = vm.const.i32 48
      %zero = vm.const.i32.zero
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c8192 = vm.const.i64 8192
      %c15360 = vm.const.i32 15360
      %c262144000 = vm.const.i64 262144000
      %c27648 = vm.const.i32 27648
      %c64000 = vm.const.i64 64000
      %zero_0 = vm.const.i64.zero
      %c128000 = vm.const.i64 128000
      %c-1 = vm.const.i64 -1
      %c-1_1 = vm.const.i32 -1
      %c1 = vm.const.i64 1
      %c32000_2 = vm.const.i64 32000
      %c553648144 = vm.const.i32 553648144
      %c1_3 = vm.const.i32 1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable__main_dispatch_0 = vm.global.load.ref @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_4 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_5 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_4, %zero, %c48, %c3075, %c8192) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_6 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.call @hal.command_buffer.fill_buffer(%ref_6, %ref_5, %zero_0, %c8192, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_6, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_6) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_7 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %1 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_7, [%ref_6]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %2 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %2, ^bb3(%2 : i32), ^bb1
    ^bb1:  // pred: ^bb0
      %3 = util.optimization_barrier %ref_5 : !vm.ref<!hal.buffer>
      %ref_8 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_9 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c262144000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %4 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_10 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.call @hal.command_buffer.fill_buffer(%ref_10, %ref_9, %zero_0, %c262144000, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_11 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %5 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %6 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %6, ^bb3(%6 : i32), ^bb2
    ^bb2:  // pred: ^bb1
      %7 = util.optimization_barrier %ref_9 : !vm.ref<!hal.buffer>
      %8 = vm.call @hal.buffer.length(%3) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
      %9 = vm.call @hal.buffer.length(%7) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
      %ref_12 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_13 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_12, %zero, %c48, %c3075, %c128000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %10 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_14 = vm.call @hal.command_buffer.create(%ref, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      %11 = vm.select.i64 %_device_query_0, %zero_0, %c-1 : i64
      %12 = vm.trunc.i64.i32 %11 : i64 -> i32
      vm.br_table %12 {
        default: ^bb5,
        0: ^bb4
      }
    ^bb3(%13: i32):  // 3 preds: ^bb0, ^bb1, ^bb5
      vm.fail %13, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb2
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_14, %_pipeline_layout_0, %zero, [(%zero, %zero, %3, %zero_0, %8), (%c1_3, %zero, %7, %zero_0, %9), (%c2, %zero, %ref_13, %zero_0, %c128000)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_14, %_executable__main_dispatch_0, %zero, %c32000, %c1_3, %c1_3) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.br ^bb5
    ^bb5:  // 2 preds: ^bb2, ^bb4
      vm.call @hal.command_buffer.fill_buffer(%ref_14, %ref_13, %c64000, %c64000, %c27648, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_14, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_14) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_15 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %14 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_15, [%ref_14]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %15 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_15]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %15, ^bb3(%15 : i32), ^bb6
    ^bb6:  // pred: ^bb5
      %ref_16 = vm.call.variadic @hal.buffer_view.create(%ref_13, %c64000, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      %ref_17 = vm.call.variadic @hal.buffer_view.create(%ref_13, %zero_0, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      %has_check_expect_eq = vm.import.resolved @check.expect_eq : i32
      vm.cond_br %has_check_expect_eq, ^bb7, ^bb8
    ^bb7:  // pred: ^bb6
      vm.call @check.expect_eq(%ref, %ref_17, %ref_16) : (!vm.ref<!hal.device>, !vm.ref<!hal.buffer_view>, !vm.ref<!hal.buffer_view>) -> ()
      vm.br ^bb8
    ^bb8:  // 2 preds: ^bb6, ^bb7
      vm.return
    }
    vm.export @main attributes {iree.abi.stub}
    vm.import private optional @check.expect_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
    vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::GlobalInitializationPass (iree-vm-global-initialization) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @_main_dispatch_0_vulkan_spirv_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<5020xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 {alignment = 1 : i64} "vulkan-spirv-fb"
  vm.func private @main() {
    %c32000 = vm.const.i32 32000
    %c3 = vm.const.i32 3
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c2 = vm.const.i32 2
    %c17 = vm.const.i32 17
    %c3075 = vm.const.i32 3075
    %c48 = vm.const.i32 48
    %zero = vm.const.i32.zero
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c8192 = vm.const.i64 8192
    %c15360 = vm.const.i32 15360
    %c262144000 = vm.const.i64 262144000
    %c27648 = vm.const.i32 27648
    %c64000 = vm.const.i64 64000
    %zero_0 = vm.const.i64.zero
    %c128000 = vm.const.i64 128000
    %c-1 = vm.const.i64 -1
    %c-1_1 = vm.const.i32 -1
    %c1 = vm.const.i64 1
    %c32000_2 = vm.const.i64 32000
    %c553648144 = vm.const.i32 553648144
    %c1_3 = vm.const.i32 1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable__main_dispatch_0 = vm.global.load.ref @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_4 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_5 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_4, %zero, %c48, %c3075, %c8192) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    %ref_6 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.call @hal.command_buffer.fill_buffer(%ref_6, %ref_5, %zero_0, %c8192, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_6, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_6) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_7 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %1 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_7, [%ref_6]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %2 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %2, ^bb3(%2 : i32), ^bb1
  ^bb1:  // pred: ^bb0
    %3 = util.optimization_barrier %ref_5 : !vm.ref<!hal.buffer>
    %ref_8 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_9 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c262144000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %4 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    %ref_10 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.call @hal.command_buffer.fill_buffer(%ref_10, %ref_9, %zero_0, %c262144000, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_11 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %5 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %6 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %6, ^bb3(%6 : i32), ^bb2
  ^bb2:  // pred: ^bb1
    %7 = util.optimization_barrier %ref_9 : !vm.ref<!hal.buffer>
    %8 = vm.call @hal.buffer.length(%3) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
    %9 = vm.call @hal.buffer.length(%7) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
    %ref_12 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_13 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_12, %zero, %c48, %c3075, %c128000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %10 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    %ref_14 = vm.call @hal.command_buffer.create(%ref, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    %11 = vm.select.i64 %_device_query_0, %zero_0, %c-1 : i64
    %12 = vm.trunc.i64.i32 %11 : i64 -> i32
    vm.br_table %12 {
      default: ^bb5,
      0: ^bb4
    }
  ^bb3(%13: i32):  // 3 preds: ^bb0, ^bb1, ^bb5
    vm.fail %13, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb2
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_14, %_pipeline_layout_0, %zero, [(%zero, %zero, %3, %zero_0, %8), (%c1_3, %zero, %7, %zero_0, %9), (%c2, %zero, %ref_13, %zero_0, %c128000)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_14, %_executable__main_dispatch_0, %zero, %c32000, %c1_3, %c1_3) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.br ^bb5
  ^bb5:  // 2 preds: ^bb2, ^bb4
    vm.call @hal.command_buffer.fill_buffer(%ref_14, %ref_13, %c64000, %c64000, %c27648, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_14, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_14) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_15 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %14 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_15, [%ref_14]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %15 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_15]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %15, ^bb3(%15 : i32), ^bb6
  ^bb6:  // pred: ^bb5
    %ref_16 = vm.call.variadic @hal.buffer_view.create(%ref_13, %c64000, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    %ref_17 = vm.call.variadic @hal.buffer_view.create(%ref_13, %zero_0, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    %has_check_expect_eq = vm.import.resolved @check.expect_eq : i32
    vm.cond_br %has_check_expect_eq, ^bb7, ^bb8
  ^bb7:  // pred: ^bb6
    vm.call @check.expect_eq(%ref, %ref_17, %ref_16) : (!vm.ref<!hal.device>, !vm.ref<!hal.buffer_view>, !vm.ref<!hal.buffer_view>) -> ()
    vm.br ^bb8
  ^bb8:  // 2 preds: ^bb6, ^bb7
    vm.return
  }
  vm.export @main attributes {iree.abi.stub}
  vm.import private optional @check.expect_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
  vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
  vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %zero_1 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 = vm.const.ref.rodata @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_2 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_3 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_2]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    %4 = vm.select.i64 %3, %zero_1, %c-1 : i64
    %5 = vm.trunc.i64.i32 %4 : i64 -> i32
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_3, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.br_table %5 {
      default: ^bb2,
      0: ^bb1
    }
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_main_dispatch_0_vulkan_spirv_fb = vm.const.ref.rodata @_main_dispatch_0_vulkan_spirv_fb : !vm.buffer
    %ref_4 = vm.call.variadic @hal.executable.create(%ref, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0, %_main_dispatch_0_vulkan_spirv_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_4 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%6: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %6, @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    vm.br ^bb4
  ^bb4:  // pred: ^bb3
    vm.return
  }
  vm.export @__deinit
  vm.func private @__deinit() {
    vm.return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private mutable @_device_query_0 : i32
    vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private mutable @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @_main_dispatch_0_vulkan_spirv_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<5020xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 {alignment = 1 : i64} "vulkan-spirv-fb"
    vm.func private @main() {
      %c32000 = vm.const.i32 32000
      %c3 = vm.const.i32 3
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %c17 = vm.const.i32 17
      %c3075 = vm.const.i32 3075
      %c48 = vm.const.i32 48
      %zero = vm.const.i32.zero
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c8192 = vm.const.i64 8192
      %c15360 = vm.const.i32 15360
      %c262144000 = vm.const.i64 262144000
      %c27648 = vm.const.i32 27648
      %c64000 = vm.const.i64 64000
      %zero_0 = vm.const.i64.zero
      %c128000 = vm.const.i64 128000
      %c-1 = vm.const.i64 -1
      %c-1_1 = vm.const.i32 -1
      %c1 = vm.const.i64 1
      %c32000_2 = vm.const.i64 32000
      %c553648144 = vm.const.i32 553648144
      %c1_3 = vm.const.i32 1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable__main_dispatch_0 = vm.global.load.ref @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_4 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_5 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_4, %zero, %c48, %c3075, %c8192) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_6 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.call @hal.command_buffer.fill_buffer(%ref_6, %ref_5, %zero_0, %c8192, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_6, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_6) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_7 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %1 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_7, [%ref_6]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %2 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %2, ^bb3(%2 : i32), ^bb1
    ^bb1:  // pred: ^bb0
      %3 = util.optimization_barrier %ref_5 : !vm.ref<!hal.buffer>
      %ref_8 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_9 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c262144000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %4 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_10 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.call @hal.command_buffer.fill_buffer(%ref_10, %ref_9, %zero_0, %c262144000, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_11 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %5 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %6 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %6, ^bb3(%6 : i32), ^bb2
    ^bb2:  // pred: ^bb1
      %7 = util.optimization_barrier %ref_9 : !vm.ref<!hal.buffer>
      %8 = vm.call @hal.buffer.length(%3) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
      %9 = vm.call @hal.buffer.length(%7) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
      %ref_12 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_13 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_12, %zero, %c48, %c3075, %c128000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %10 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_14 = vm.call @hal.command_buffer.create(%ref, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      %11 = vm.select.i64 %_device_query_0, %zero_0, %c-1 : i64
      %12 = vm.trunc.i64.i32 %11 : i64 -> i32
      vm.br_table %12 {
        default: ^bb5,
        0: ^bb4
      }
    ^bb3(%13: i32):  // 3 preds: ^bb0, ^bb1, ^bb5
      vm.fail %13, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb2
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_14, %_pipeline_layout_0, %zero, [(%zero, %zero, %3, %zero_0, %8), (%c1_3, %zero, %7, %zero_0, %9), (%c2, %zero, %ref_13, %zero_0, %c128000)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_14, %_executable__main_dispatch_0, %zero, %c32000, %c1_3, %c1_3) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.br ^bb5
    ^bb5:  // 2 preds: ^bb2, ^bb4
      vm.call @hal.command_buffer.fill_buffer(%ref_14, %ref_13, %c64000, %c64000, %c27648, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_14, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_14) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_15 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %14 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_15, [%ref_14]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %15 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_15]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %15, ^bb3(%15 : i32), ^bb6
    ^bb6:  // pred: ^bb5
      %ref_16 = vm.call.variadic @hal.buffer_view.create(%ref_13, %c64000, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      %ref_17 = vm.call.variadic @hal.buffer_view.create(%ref_13, %zero_0, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      %has_check_expect_eq = vm.import.resolved @check.expect_eq : i32
      vm.cond_br %has_check_expect_eq, ^bb7, ^bb8
    ^bb7:  // pred: ^bb6
      vm.call @check.expect_eq(%ref, %ref_17, %ref_16) : (!vm.ref<!hal.device>, !vm.ref<!hal.buffer_view>, !vm.ref<!hal.buffer_view>) -> ()
      vm.br ^bb8
    ^bb8:  // 2 preds: ^bb6, ^bb7
      vm.return
    }
    vm.export @main attributes {iree.abi.stub}
    vm.import private optional @check.expect_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
    vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.export @__init
    vm.func private @__init() {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 = vm.const.ref.rodata @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_2 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_3 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_2]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      %4 = vm.select.i64 %3, %zero_1, %c-1 : i64
      %5 = vm.trunc.i64.i32 %4 : i64 -> i32
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_3, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.br_table %5 {
        default: ^bb2,
        0: ^bb1
      }
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_main_dispatch_0_vulkan_spirv_fb = vm.const.ref.rodata @_main_dispatch_0_vulkan_spirv_fb : !vm.buffer
      %ref_4 = vm.call.variadic @hal.executable.create(%ref, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0, %_main_dispatch_0_vulkan_spirv_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_4 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%6: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %6, @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.export @__deinit
    vm.func private @__deinit() {
      vm.return
    }
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private mutable @_device_query_0 : i32
    vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private mutable @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @_main_dispatch_0_vulkan_spirv_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<5020xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 {alignment = 1 : i64} "vulkan-spirv-fb"
    vm.func private @main() {
      %c32000 = vm.const.i32 32000
      %c3 = vm.const.i32 3
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %c17 = vm.const.i32 17
      %c3075 = vm.const.i32 3075
      %c48 = vm.const.i32 48
      %zero = vm.const.i32.zero
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c8192 = vm.const.i64 8192
      %c15360 = vm.const.i32 15360
      %c262144000 = vm.const.i64 262144000
      %c27648 = vm.const.i32 27648
      %c64000 = vm.const.i64 64000
      %zero_0 = vm.const.i64.zero
      %c128000 = vm.const.i64 128000
      %c-1 = vm.const.i64 -1
      %c-1_1 = vm.const.i32 -1
      %c1 = vm.const.i64 1
      %c32000_2 = vm.const.i64 32000
      %c553648144 = vm.const.i32 553648144
      %c1_3 = vm.const.i32 1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable__main_dispatch_0 = vm.global.load.ref @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_4 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_5 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_4, %zero, %c48, %c3075, %c8192) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_6 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.call @hal.command_buffer.fill_buffer(%ref_6, %ref_5, %zero_0, %c8192, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_6, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_6) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_7 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %1 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_7, [%ref_6]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %2 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %2, ^bb3(%2 : i32), ^bb1
    ^bb1:  // pred: ^bb0
      %3 = util.optimization_barrier %ref_5 : !vm.ref<!hal.buffer>
      %ref_8 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_9 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c262144000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %4 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_10 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.call @hal.command_buffer.fill_buffer(%ref_10, %ref_9, %zero_0, %c262144000, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_11 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %5 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %6 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %6, ^bb3(%6 : i32), ^bb2
    ^bb2:  // pred: ^bb1
      %7 = util.optimization_barrier %ref_9 : !vm.ref<!hal.buffer>
      %8 = vm.call @hal.buffer.length(%3) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
      %9 = vm.call @hal.buffer.length(%7) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
      %ref_12 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_13 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_12, %zero, %c48, %c3075, %c128000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %10 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_14 = vm.call @hal.command_buffer.create(%ref, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      %11 = vm.select.i64 %_device_query_0, %zero_0, %c-1 : i64
      %12 = vm.trunc.i64.i32 %11 : i64 -> i32
      vm.br_table %12 {
        default: ^bb5,
        0: ^bb4
      }
    ^bb3(%13: i32):  // 3 preds: ^bb0, ^bb1, ^bb5
      vm.fail %13, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb2
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_14, %_pipeline_layout_0, %zero, [(%zero, %zero, %3, %zero_0, %8), (%c1_3, %zero, %7, %zero_0, %9), (%c2, %zero, %ref_13, %zero_0, %c128000)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_14, %_executable__main_dispatch_0, %zero, %c32000, %c1_3, %c1_3) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.br ^bb5
    ^bb5:  // 2 preds: ^bb2, ^bb4
      vm.call @hal.command_buffer.fill_buffer(%ref_14, %ref_13, %c64000, %c64000, %c27648, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_14, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_14) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_15 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %14 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_15, [%ref_14]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %15 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_15]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %15, ^bb3(%15 : i32), ^bb6
    ^bb6:  // pred: ^bb5
      %ref_16 = vm.call.variadic @hal.buffer_view.create(%ref_13, %c64000, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      %ref_17 = vm.call.variadic @hal.buffer_view.create(%ref_13, %zero_0, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      %has_check_expect_eq = vm.import.resolved @check.expect_eq : i32
      vm.cond_br %has_check_expect_eq, ^bb7, ^bb8
    ^bb7:  // pred: ^bb6
      vm.call @check.expect_eq(%ref, %ref_17, %ref_16) : (!vm.ref<!hal.device>, !vm.ref<!hal.buffer_view>, !vm.ref<!hal.buffer_view>) -> ()
      vm.br ^bb8
    ^bb8:  // 2 preds: ^bb6, ^bb7
      vm.return
    }
    vm.export @main attributes {iree.abi.stub}
    vm.import private optional @check.expect_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
    vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.export @__init
    vm.func private @__init() {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 = vm.const.ref.rodata @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_2 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_3 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_2]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      %4 = vm.select.i64 %3, %zero_1, %c-1 : i64
      %5 = vm.trunc.i64.i32 %4 : i64 -> i32
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_3, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.br_table %5 {
        default: ^bb2,
        0: ^bb1
      }
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_main_dispatch_0_vulkan_spirv_fb = vm.const.ref.rodata @_main_dispatch_0_vulkan_spirv_fb : !vm.buffer
      %ref_4 = vm.call.variadic @hal.executable.create(%ref, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0, %_main_dispatch_0_vulkan_spirv_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_4 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%6: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %6, @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.export @__deinit
    vm.func private @__deinit() {
      vm.return
    }
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private mutable @_device_query_0 : i32
    vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private mutable @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @_main_dispatch_0_vulkan_spirv_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<5020xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 {alignment = 1 : i64} "vulkan-spirv-fb"
    vm.func private @main() {
      %c32000 = vm.const.i32 32000
      %c3 = vm.const.i32 3
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %c17 = vm.const.i32 17
      %c3075 = vm.const.i32 3075
      %c48 = vm.const.i32 48
      %zero = vm.const.i32.zero
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c8192 = vm.const.i64 8192
      %c15360 = vm.const.i32 15360
      %c262144000 = vm.const.i64 262144000
      %c27648 = vm.const.i32 27648
      %c64000 = vm.const.i64 64000
      %zero_0 = vm.const.i64.zero
      %c128000 = vm.const.i64 128000
      %c-1 = vm.const.i64 -1
      %c-1_1 = vm.const.i32 -1
      %c1 = vm.const.i64 1
      %c32000_2 = vm.const.i64 32000
      %c553648144 = vm.const.i32 553648144
      %c1_3 = vm.const.i32 1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable__main_dispatch_0 = vm.global.load.ref @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_4 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_5 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_4, %zero, %c48, %c3075, %c8192) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_6 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.call @hal.command_buffer.fill_buffer(%ref_6, %ref_5, %zero_0, %c8192, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_6, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_6) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_7 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %1 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_7, [%ref_6]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %2 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %2, ^bb3(%2 : i32), ^bb1
    ^bb1:  // pred: ^bb0
      %3 = util.optimization_barrier %ref_5 : !vm.ref<!hal.buffer>
      %ref_8 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_9 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c262144000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %4 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_10 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.call @hal.command_buffer.fill_buffer(%ref_10, %ref_9, %zero_0, %c262144000, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_11 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %5 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %6 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %6, ^bb3(%6 : i32), ^bb2
    ^bb2:  // pred: ^bb1
      %7 = util.optimization_barrier %ref_9 : !vm.ref<!hal.buffer>
      %8 = vm.call @hal.buffer.length(%3) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
      %9 = vm.call @hal.buffer.length(%7) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
      %ref_12 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_13 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_12, %zero, %c48, %c3075, %c128000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %10 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_14 = vm.call @hal.command_buffer.create(%ref, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      %11 = vm.select.i64 %_device_query_0, %zero_0, %c-1 : i64
      %12 = vm.trunc.i64.i32 %11 : i64 -> i32
      vm.br_table %12 {
        default: ^bb5,
        0: ^bb4
      }
    ^bb3(%13: i32):  // 3 preds: ^bb0, ^bb1, ^bb5
      vm.fail %13, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb2
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_14, %_pipeline_layout_0, %zero, [(%zero, %zero, %3, %zero_0, %8), (%c1_3, %zero, %7, %zero_0, %9), (%c2, %zero, %ref_13, %zero_0, %c128000)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_14, %_executable__main_dispatch_0, %zero, %c32000, %c1_3, %c1_3) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.br ^bb5
    ^bb5:  // 2 preds: ^bb2, ^bb4
      vm.call @hal.command_buffer.fill_buffer(%ref_14, %ref_13, %c64000, %c64000, %c27648, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_14, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_14) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_15 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %14 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_15, [%ref_14]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %15 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_15]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %15, ^bb3(%15 : i32), ^bb6
    ^bb6:  // pred: ^bb5
      %ref_16 = vm.call.variadic @hal.buffer_view.create(%ref_13, %c64000, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      %ref_17 = vm.call.variadic @hal.buffer_view.create(%ref_13, %zero_0, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      %has_check_expect_eq = vm.import.resolved @check.expect_eq : i32
      vm.cond_br %has_check_expect_eq, ^bb7, ^bb8
    ^bb7:  // pred: ^bb6
      vm.call @check.expect_eq(%ref, %ref_17, %ref_16) : (!vm.ref<!hal.device>, !vm.ref<!hal.buffer_view>, !vm.ref<!hal.buffer_view>) -> ()
      vm.br ^bb8
    ^bb8:  // 2 preds: ^bb6, ^bb7
      vm.return
    }
    vm.export @main attributes {iree.abi.stub}
    vm.import private optional @check.expect_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
    vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.export @__init
    vm.func private @__init() {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 = vm.const.ref.rodata @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_2 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_3 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_2]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      %4 = vm.select.i64 %3, %zero_1, %c-1 : i64
      %5 = vm.trunc.i64.i32 %4 : i64 -> i32
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_3, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.br_table %5 {
        default: ^bb2,
        0: ^bb1
      }
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_main_dispatch_0_vulkan_spirv_fb = vm.const.ref.rodata @_main_dispatch_0_vulkan_spirv_fb : !vm.buffer
      %ref_4 = vm.call.variadic @hal.executable.create(%ref, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0, %_main_dispatch_0_vulkan_spirv_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_4 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%6: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %6, @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.export @__deinit
    vm.func private @__deinit() {
      vm.return
    }
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::DropEmptyModuleInitializersPass (iree-vm-drop-empty-module-initializers) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @_main_dispatch_0_vulkan_spirv_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<5020xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 {alignment = 1 : i64} "vulkan-spirv-fb"
  vm.func private @main() {
    %c32000 = vm.const.i32 32000
    %c3 = vm.const.i32 3
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c2 = vm.const.i32 2
    %c17 = vm.const.i32 17
    %c3075 = vm.const.i32 3075
    %c48 = vm.const.i32 48
    %zero = vm.const.i32.zero
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c8192 = vm.const.i64 8192
    %c15360 = vm.const.i32 15360
    %c262144000 = vm.const.i64 262144000
    %c27648 = vm.const.i32 27648
    %c64000 = vm.const.i64 64000
    %zero_0 = vm.const.i64.zero
    %c128000 = vm.const.i64 128000
    %c-1 = vm.const.i64 -1
    %c-1_1 = vm.const.i32 -1
    %c1 = vm.const.i64 1
    %c32000_2 = vm.const.i64 32000
    %c553648144 = vm.const.i32 553648144
    %c1_3 = vm.const.i32 1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable__main_dispatch_0 = vm.global.load.ref @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_4 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_5 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_4, %zero, %c48, %c3075, %c8192) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    %ref_6 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.call @hal.command_buffer.fill_buffer(%ref_6, %ref_5, %zero_0, %c8192, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_6, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_6) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_7 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %1 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_7, [%ref_6]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %2 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %2, ^bb3(%2 : i32), ^bb1
  ^bb1:  // pred: ^bb0
    %3 = util.optimization_barrier %ref_5 : !vm.ref<!hal.buffer>
    %ref_8 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_9 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c262144000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %4 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    %ref_10 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.call @hal.command_buffer.fill_buffer(%ref_10, %ref_9, %zero_0, %c262144000, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_11 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %5 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %6 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %6, ^bb3(%6 : i32), ^bb2
  ^bb2:  // pred: ^bb1
    %7 = util.optimization_barrier %ref_9 : !vm.ref<!hal.buffer>
    %8 = vm.call @hal.buffer.length(%3) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
    %9 = vm.call @hal.buffer.length(%7) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
    %ref_12 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_13 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_12, %zero, %c48, %c3075, %c128000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %10 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    %ref_14 = vm.call @hal.command_buffer.create(%ref, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    %11 = vm.select.i64 %_device_query_0, %zero_0, %c-1 : i64
    %12 = vm.trunc.i64.i32 %11 : i64 -> i32
    vm.br_table %12 {
      default: ^bb5,
      0: ^bb4
    }
  ^bb3(%13: i32):  // 3 preds: ^bb0, ^bb1, ^bb5
    vm.fail %13, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb2
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_14, %_pipeline_layout_0, %zero, [(%zero, %zero, %3, %zero_0, %8), (%c1_3, %zero, %7, %zero_0, %9), (%c2, %zero, %ref_13, %zero_0, %c128000)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_14, %_executable__main_dispatch_0, %zero, %c32000, %c1_3, %c1_3) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.br ^bb5
  ^bb5:  // 2 preds: ^bb2, ^bb4
    vm.call @hal.command_buffer.fill_buffer(%ref_14, %ref_13, %c64000, %c64000, %c27648, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_14, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_14) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_15 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %14 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_15, [%ref_14]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %15 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_15]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %15, ^bb3(%15 : i32), ^bb6
  ^bb6:  // pred: ^bb5
    %ref_16 = vm.call.variadic @hal.buffer_view.create(%ref_13, %c64000, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    %ref_17 = vm.call.variadic @hal.buffer_view.create(%ref_13, %zero_0, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    %has_check_expect_eq = vm.import.resolved @check.expect_eq : i32
    vm.cond_br %has_check_expect_eq, ^bb7, ^bb8
  ^bb7:  // pred: ^bb6
    vm.call @check.expect_eq(%ref, %ref_17, %ref_16) : (!vm.ref<!hal.device>, !vm.ref<!hal.buffer_view>, !vm.ref<!hal.buffer_view>) -> ()
    vm.br ^bb8
  ^bb8:  // 2 preds: ^bb6, ^bb7
    vm.return
  }
  vm.export @main attributes {iree.abi.stub}
  vm.import private optional @check.expect_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
  vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
  vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %zero_1 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 = vm.const.ref.rodata @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_2 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_3 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_2]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    %4 = vm.select.i64 %3, %zero_1, %c-1 : i64
    %5 = vm.trunc.i64.i32 %4 : i64 -> i32
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_3, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.br_table %5 {
      default: ^bb2,
      0: ^bb1
    }
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_main_dispatch_0_vulkan_spirv_fb = vm.const.ref.rodata @_main_dispatch_0_vulkan_spirv_fb : !vm.buffer
    %ref_4 = vm.call.variadic @hal.executable.create(%ref, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0, %_main_dispatch_0_vulkan_spirv_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_4 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%6: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %6, @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  }
}

// -----// IR Dump After DropCompilerHints (iree-util-drop-compiler-hints) //----- //
#executable_target_vulkan_spirv_fb = #hal.executable.target<"vulkan", "vulkan-spirv-fb", {spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader, Float64, Float16, Int64, Int16, Int8, StorageBuffer16BitAccess, StorageUniform16, StoragePushConstant16, StorageBuffer8BitAccess, UniformAndStorageBuffer8BitAccess, StoragePushConstant8, GroupNonUniform, GroupNonUniformVote, GroupNonUniformArithmetic, GroupNonUniformBallot, GroupNonUniformShuffle, GroupNonUniformShuffleRelative, GroupNonUniformClustered, GroupNonUniformQuad, VariablePointers, VariablePointersStorageBuffer, DotProduct, DotProductInputAll, DotProductInput4x8BitPacked, DotProductInput4x8Bit, CooperativeMatrixKHR], [SPV_KHR_16bit_storage, SPV_KHR_8bit_storage, SPV_KHR_integer_dot_product, SPV_KHR_storage_buffer_storage_class, SPV_KHR_variable_pointers, SPV_KHR_cooperative_matrix]>, api=Vulkan, AMD:DiscreteGPU, #spirv.resource_limits<max_compute_shared_memory_size = 65536, max_compute_workgroup_invocations = 1024, max_compute_workgroup_size = [1024, 1024, 1024], subgroup_size = 64, min_subgroup_size = 32, max_subgroup_size = 64, cooperative_matrix_properties_khr = [#spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = i8, b_type = i8, c_type = i32, result_type = i32, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f16, result_type = f16, acc_sat = false, scope = <Subgroup>>, #spirv.coop_matrix_props_khr<m_size = 16, n_size = 16, k_size = 16, a_type = f16, b_type = f16, c_type = f32, result_type = f32, acc_sat = false, scope = <Subgroup>>]>>}>
#device_target_vulkan = #hal.device.target<"vulkan", {executable_targets = [#executable_target_vulkan_spirv_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_vulkan], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private mutable @_device_query_0 : i32
    vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private mutable @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @_main_dispatch_0_vulkan_spirv_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<5020xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 {alignment = 1 : i64} "vulkan-spirv-fb"
    vm.func private @main() {
      %c32000 = vm.const.i32 32000
      %c3 = vm.const.i32 3
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %c17 = vm.const.i32 17
      %c3075 = vm.const.i32 3075
      %c48 = vm.const.i32 48
      %zero = vm.const.i32.zero
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c8192 = vm.const.i64 8192
      %c15360 = vm.const.i32 15360
      %c262144000 = vm.const.i64 262144000
      %c27648 = vm.const.i32 27648
      %c64000 = vm.const.i64 64000
      %zero_0 = vm.const.i64.zero
      %c128000 = vm.const.i64 128000
      %c-1 = vm.const.i64 -1
      %c-1_1 = vm.const.i32 -1
      %c1 = vm.const.i64 1
      %c32000_2 = vm.const.i64 32000
      %c553648144 = vm.const.i32 553648144
      %c1_3 = vm.const.i32 1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable__main_dispatch_0 = vm.global.load.ref @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_4 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_5 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_4, %zero, %c48, %c3075, %c8192) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_6 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.call @hal.command_buffer.fill_buffer(%ref_6, %ref_5, %zero_0, %c8192, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_6, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_6) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_7 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %1 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_7, [%ref_6]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %2 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %2, ^bb3(%2 : i32), ^bb1
    ^bb1:  // pred: ^bb0
      %ref_8 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_9 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c262144000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %3 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_10 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.call @hal.command_buffer.fill_buffer(%ref_10, %ref_9, %zero_0, %c262144000, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_11 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %4 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %5 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %5, ^bb3(%5 : i32), ^bb2
    ^bb2:  // pred: ^bb1
      %6 = vm.call @hal.buffer.length(%ref_5) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
      %7 = vm.call @hal.buffer.length(%ref_9) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
      %ref_12 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_13 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_12, %zero, %c48, %c3075, %c128000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %8 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      %ref_14 = vm.call @hal.command_buffer.create(%ref, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      %9 = vm.select.i64 %_device_query_0, %zero_0, %c-1 : i64
      %10 = vm.trunc.i64.i32 %9 : i64 -> i32
      vm.br_table %10 {
        default: ^bb5,
        0: ^bb4
      }
    ^bb3(%11: i32):  // 3 preds: ^bb0, ^bb1, ^bb5
      vm.fail %11, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb2
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_14, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_5, %zero_0, %6), (%c1_3, %zero, %ref_9, %zero_0, %7), (%c2, %zero, %ref_13, %zero_0, %c128000)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_14, %_executable__main_dispatch_0, %zero, %c32000, %c1_3, %c1_3) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.br ^bb5
    ^bb5:  // 2 preds: ^bb2, ^bb4
      vm.call @hal.command_buffer.fill_buffer(%ref_14, %ref_13, %c64000, %c64000, %c27648, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_14, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_14) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_15 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %12 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_15, [%ref_14]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %13 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_15]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %13, ^bb3(%13 : i32), ^bb6
    ^bb6:  // pred: ^bb5
      %ref_16 = vm.call.variadic @hal.buffer_view.create(%ref_13, %c64000, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      %ref_17 = vm.call.variadic @hal.buffer_view.create(%ref_13, %zero_0, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      %has_check_expect_eq = vm.import.resolved @check.expect_eq : i32
      vm.cond_br %has_check_expect_eq, ^bb7, ^bb8
    ^bb7:  // pred: ^bb6
      vm.call @check.expect_eq(%ref, %ref_17, %ref_16) : (!vm.ref<!hal.device>, !vm.ref<!hal.buffer_view>, !vm.ref<!hal.buffer_view>) -> ()
      vm.br ^bb8
    ^bb8:  // 2 preds: ^bb6, ^bb7
      vm.return
    }
    vm.export @main attributes {iree.abi.stub}
    vm.import private optional @check.expect_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
    vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.export @__init
    vm.func private @__init() {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 = vm.const.ref.rodata @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_2 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_3 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_2]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      %4 = vm.select.i64 %3, %zero_1, %c-1 : i64
      %5 = vm.trunc.i64.i32 %4 : i64 -> i32
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_3, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.br_table %5 {
        default: ^bb2,
        0: ^bb1
      }
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_main_dispatch_0_vulkan_spirv_fb = vm.const.ref.rodata @_main_dispatch_0_vulkan_spirv_fb : !vm.buffer
      %ref_4 = vm.call.variadic @hal.executable.create(%ref, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0, %_main_dispatch_0_vulkan_spirv_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_4 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%6: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %6, @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::GlobalInitializationPass (iree-vm-global-initialization) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @_main_dispatch_0_vulkan_spirv_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<5020xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 {alignment = 1 : i64} "vulkan-spirv-fb"
  vm.func private @main() {
    %c32000 = vm.const.i32 32000
    %c3 = vm.const.i32 3
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c2 = vm.const.i32 2
    %c17 = vm.const.i32 17
    %c3075 = vm.const.i32 3075
    %c48 = vm.const.i32 48
    %zero = vm.const.i32.zero
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c8192 = vm.const.i64 8192
    %c15360 = vm.const.i32 15360
    %c262144000 = vm.const.i64 262144000
    %c27648 = vm.const.i32 27648
    %c64000 = vm.const.i64 64000
    %zero_0 = vm.const.i64.zero
    %c128000 = vm.const.i64 128000
    %c-1 = vm.const.i64 -1
    %c-1_1 = vm.const.i32 -1
    %c1 = vm.const.i64 1
    %c32000_2 = vm.const.i64 32000
    %c553648144 = vm.const.i32 553648144
    %c1_3 = vm.const.i32 1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable__main_dispatch_0 = vm.global.load.ref @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_4 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_5 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_4, %zero, %c48, %c3075, %c8192) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    %ref_6 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.call @hal.command_buffer.fill_buffer(%ref_6, %ref_5, %zero_0, %c8192, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_6, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_6) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_7 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %1 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_7, [%ref_6]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %2 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %2, ^bb3(%2 : i32), ^bb1
  ^bb1:  // pred: ^bb0
    %ref_8 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_9 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c262144000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %3 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    %ref_10 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.call @hal.command_buffer.fill_buffer(%ref_10, %ref_9, %zero_0, %c262144000, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_11 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %4 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %5 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %5, ^bb3(%5 : i32), ^bb2
  ^bb2:  // pred: ^bb1
    %6 = vm.call @hal.buffer.length(%ref_5) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
    %7 = vm.call @hal.buffer.length(%ref_9) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
    %ref_12 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_13 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_12, %zero, %c48, %c3075, %c128000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %8 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    %ref_14 = vm.call @hal.command_buffer.create(%ref, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    %9 = vm.select.i64 %_device_query_0, %zero_0, %c-1 : i64
    %10 = vm.trunc.i64.i32 %9 : i64 -> i32
    vm.br_table %10 {
      default: ^bb5,
      0: ^bb4
    }
  ^bb3(%11: i32):  // 3 preds: ^bb0, ^bb1, ^bb5
    vm.fail %11, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb2
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_14, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_5, %zero_0, %6), (%c1_3, %zero, %ref_9, %zero_0, %7), (%c2, %zero, %ref_13, %zero_0, %c128000)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_14, %_executable__main_dispatch_0, %zero, %c32000, %c1_3, %c1_3) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.br ^bb5
  ^bb5:  // 2 preds: ^bb2, ^bb4
    vm.call @hal.command_buffer.fill_buffer(%ref_14, %ref_13, %c64000, %c64000, %c27648, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_14, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_14) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_15 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %12 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_15, [%ref_14]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %13 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_15]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %13, ^bb3(%13 : i32), ^bb6
  ^bb6:  // pred: ^bb5
    %ref_16 = vm.call.variadic @hal.buffer_view.create(%ref_13, %c64000, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    %ref_17 = vm.call.variadic @hal.buffer_view.create(%ref_13, %zero_0, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    %has_check_expect_eq = vm.import.resolved @check.expect_eq : i32
    vm.cond_br %has_check_expect_eq, ^bb7, ^bb8
  ^bb7:  // pred: ^bb6
    vm.call @check.expect_eq(%ref, %ref_17, %ref_16) : (!vm.ref<!hal.device>, !vm.ref<!hal.buffer_view>, !vm.ref<!hal.buffer_view>) -> ()
    vm.br ^bb8
  ^bb8:  // 2 preds: ^bb6, ^bb7
    vm.return
  }
  vm.export @main attributes {iree.abi.stub}
  vm.import private optional @check.expect_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
  vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
  vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %zero_1 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 = vm.const.ref.rodata @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_2 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_3 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_2]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    %4 = vm.select.i64 %3, %zero_1, %c-1 : i64
    %5 = vm.trunc.i64.i32 %4 : i64 -> i32
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_3, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.br_table %5 {
      default: ^bb2,
      0: ^bb1
    }
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_main_dispatch_0_vulkan_spirv_fb = vm.const.ref.rodata @_main_dispatch_0_vulkan_spirv_fb : !vm.buffer
    %ref_4 = vm.call.variadic @hal.executable.create(%ref, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0, %_main_dispatch_0_vulkan_spirv_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_4 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%6: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %6, @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    vm.br ^bb4
  ^bb4:  // pred: ^bb3
    vm.return
  }
  vm.export @__deinit
  vm.func private @__deinit() {
    vm.return
  }
}

// -----// IR Dump After mlir::iree_compiler::IREE::VM::DropEmptyModuleInitializersPass (iree-vm-drop-empty-module-initializers) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @_main_dispatch_0_vulkan_spirv_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<5020xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 {alignment = 1 : i64} "vulkan-spirv-fb"
  vm.func private @main() {
    %c32000 = vm.const.i32 32000
    %c3 = vm.const.i32 3
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c2 = vm.const.i32 2
    %c17 = vm.const.i32 17
    %c3075 = vm.const.i32 3075
    %c48 = vm.const.i32 48
    %zero = vm.const.i32.zero
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c8192 = vm.const.i64 8192
    %c15360 = vm.const.i32 15360
    %c262144000 = vm.const.i64 262144000
    %c27648 = vm.const.i32 27648
    %c64000 = vm.const.i64 64000
    %zero_0 = vm.const.i64.zero
    %c128000 = vm.const.i64 128000
    %c-1 = vm.const.i64 -1
    %c-1_1 = vm.const.i32 -1
    %c1 = vm.const.i64 1
    %c32000_2 = vm.const.i64 32000
    %c553648144 = vm.const.i32 553648144
    %c1_3 = vm.const.i32 1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable__main_dispatch_0 = vm.global.load.ref @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_4 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_5 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_4, %zero, %c48, %c3075, %c8192) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    %ref_6 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.call @hal.command_buffer.fill_buffer(%ref_6, %ref_5, %zero_0, %c8192, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_6, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_6) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_7 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %1 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_7, [%ref_6]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %2 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %2, ^bb3(%2 : i32), ^bb1
  ^bb1:  // pred: ^bb0
    %ref_8 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_9 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c262144000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %3 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    %ref_10 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.call @hal.command_buffer.fill_buffer(%ref_10, %ref_9, %zero_0, %c262144000, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_11 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %4 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %5 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %5, ^bb3(%5 : i32), ^bb2
  ^bb2:  // pred: ^bb1
    %6 = vm.call @hal.buffer.length(%ref_5) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
    %7 = vm.call @hal.buffer.length(%ref_9) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
    %ref_12 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_13 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_12, %zero, %c48, %c3075, %c128000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %8 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    %ref_14 = vm.call @hal.command_buffer.create(%ref, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    %9 = vm.select.i64 %_device_query_0, %zero_0, %c-1 : i64
    %10 = vm.trunc.i64.i32 %9 : i64 -> i32
    vm.br_table %10 {
      default: ^bb5,
      0: ^bb4
    }
  ^bb3(%11: i32):  // 3 preds: ^bb0, ^bb1, ^bb5
    vm.fail %11, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb2
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_14, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_5, %zero_0, %6), (%c1_3, %zero, %ref_9, %zero_0, %7), (%c2, %zero, %ref_13, %zero_0, %c128000)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_14, %_executable__main_dispatch_0, %zero, %c32000, %c1_3, %c1_3) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.br ^bb5
  ^bb5:  // 2 preds: ^bb2, ^bb4
    vm.call @hal.command_buffer.fill_buffer(%ref_14, %ref_13, %c64000, %c64000, %c27648, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_14, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_14) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_15 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %12 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_15, [%ref_14]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %13 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_15]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %13, ^bb3(%13 : i32), ^bb6
  ^bb6:  // pred: ^bb5
    %ref_16 = vm.call.variadic @hal.buffer_view.create(%ref_13, %c64000, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    %ref_17 = vm.call.variadic @hal.buffer_view.create(%ref_13, %zero_0, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    %has_check_expect_eq = vm.import.resolved @check.expect_eq : i32
    vm.cond_br %has_check_expect_eq, ^bb7, ^bb8
  ^bb7:  // pred: ^bb6
    vm.call @check.expect_eq(%ref, %ref_17, %ref_16) : (!vm.ref<!hal.device>, !vm.ref<!hal.buffer_view>, !vm.ref<!hal.buffer_view>) -> ()
    vm.br ^bb8
  ^bb8:  // 2 preds: ^bb6, ^bb7
    vm.return
  }
  vm.export @main attributes {iree.abi.stub}
  vm.import private optional @check.expect_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
  vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
  vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %zero_1 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 = vm.const.ref.rodata @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_2 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_3 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_2]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    %4 = vm.select.i64 %3, %zero_1, %c-1 : i64
    %5 = vm.trunc.i64.i32 %4 : i64 -> i32
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_3, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.br_table %5 {
      default: ^bb2,
      0: ^bb1
    }
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_main_dispatch_0_vulkan_spirv_fb = vm.const.ref.rodata @_main_dispatch_0_vulkan_spirv_fb : !vm.buffer
    %ref_4 = vm.call.variadic @hal.executable.create(%ref, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0, %_main_dispatch_0_vulkan_spirv_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_4 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%6: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %6, @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    vm.br ^bb4
  ^bb4:  // pred: ^bb3
    vm.return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
vm.func private @main() {
  %c32000 = vm.const.i32 32000
  %c3 = vm.const.i32 3
  %c13 = vm.const.i32 13
  %c28 = vm.const.i32 28
  %c2 = vm.const.i32 2
  %c17 = vm.const.i32 17
  %c3075 = vm.const.i32 3075
  %c48 = vm.const.i32 48
  %zero = vm.const.i32.zero
  %null = vm.const.ref.zero : !vm.ref<!hal.fence>
  %c8192 = vm.const.i64 8192
  %c15360 = vm.const.i32 15360
  %c262144000 = vm.const.i64 262144000
  %c27648 = vm.const.i32 27648
  %c64000 = vm.const.i64 64000
  %zero_0 = vm.const.i64.zero
  %c128000 = vm.const.i64 128000
  %c-1 = vm.const.i64 -1
  %c-1_1 = vm.const.i32 -1
  %c1 = vm.const.i64 1
  %c32000_2 = vm.const.i64 32000
  %c553648144 = vm.const.i32 553648144
  %c1_3 = vm.const.i32 1
  %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
  %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  %_executable__main_dispatch_0 = vm.global.load.ref @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
  %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
  %ref_4 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
  %ref_5 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_4, %zero, %c48, %c3075, %c8192) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
  %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  %ref_6 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
  vm.call @hal.command_buffer.fill_buffer(%ref_6, %ref_5, %zero_0, %c8192, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
  vm.call @hal.command_buffer.execution_barrier(%ref_6, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
  vm.call @hal.command_buffer.finalize(%ref_6) : (!vm.ref<!hal.command_buffer>) -> ()
  %ref_7 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
  %1 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_7, [%ref_6]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
  %2 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  vm.cond_br %2, ^bb3(%2 : i32), ^bb1
^bb1:  // pred: ^bb0
  %ref_8 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
  %ref_9 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c262144000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
  %3 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  %ref_10 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
  vm.call @hal.command_buffer.fill_buffer(%ref_10, %ref_9, %zero_0, %c262144000, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
  vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
  vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
  %ref_11 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
  %4 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
  %5 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  vm.cond_br %5, ^bb3(%5 : i32), ^bb2
^bb2:  // pred: ^bb1
  %6 = vm.call @hal.buffer.length(%ref_5) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
  %7 = vm.call @hal.buffer.length(%ref_9) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
  %ref_12 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
  %ref_13 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_12, %zero, %c48, %c3075, %c128000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
  %8 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  %ref_14 = vm.call @hal.command_buffer.create(%ref, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
  %9 = vm.select.i64 %_device_query_0, %zero_0, %c-1 : i64
  %10 = vm.trunc.i64.i32 %9 : i64 -> i32
  vm.br_table %10 {
    default: ^bb5,
    0: ^bb4
  }
^bb3(%11: i32):  // 3 preds: ^bb0, ^bb1, ^bb5
  vm.fail %11, "failed to wait on timepoint"
^bb4:  // pred: ^bb2
  vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_14, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_5, %zero_0, %6), (%c1_3, %zero, %ref_9, %zero_0, %7), (%c2, %zero, %ref_13, %zero_0, %c128000)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.call @hal.command_buffer.dispatch(%ref_14, %_executable__main_dispatch_0, %zero, %c32000, %c1_3, %c1_3) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
  vm.br ^bb5
^bb5:  // 2 preds: ^bb2, ^bb4
  vm.call @hal.command_buffer.fill_buffer(%ref_14, %ref_13, %c64000, %c64000, %c27648, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
  vm.call @hal.command_buffer.execution_barrier(%ref_14, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
  vm.call @hal.command_buffer.finalize(%ref_14) : (!vm.ref<!hal.command_buffer>) -> ()
  %ref_15 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
  %12 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_15, [%ref_14]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
  %13 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_15]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  vm.cond_br %13, ^bb3(%13 : i32), ^bb6
^bb6:  // pred: ^bb5
  %ref_16 = vm.call.variadic @hal.buffer_view.create(%ref_13, %c64000, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
  %ref_17 = vm.call.variadic @hal.buffer_view.create(%ref_13, %zero_0, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
  %has_check_expect_eq = vm.import.resolved @check.expect_eq : i32
  vm.cond_br %has_check_expect_eq, ^bb7, ^bb8
^bb7:  // pred: ^bb6
  vm.call @check.expect_eq(%ref, %ref_17, %ref_16) : (!vm.ref<!hal.device>, !vm.ref<!hal.buffer_view>, !vm.ref<!hal.buffer_view>) -> ()
  vm.br ^bb8
^bb8:  // 2 preds: ^bb6, ^bb7
  vm.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
vm.func private @__init() {
  %null = vm.const.ref.zero : !vm.ref<!hal.executable>
  %null_0 = vm.const.ref.zero : !vm.buffer
  %c2 = vm.const.i32 2
  %c7 = vm.const.i32 7
  %zero = vm.const.i32.zero
  %c1 = vm.const.i32 1
  %zero_1 = vm.const.i64.zero
  %c-1 = vm.const.i64 -1
  %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
  %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
  %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 = vm.const.ref.rodata @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 : !vm.buffer
  %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
  %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
  %2 = vm.and.i32 %1, %c1 : i32
  %3 = vm.select.i32 %0#0, %2, %zero : i32
  %ref_2 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
  %ref_3 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_2]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
  %4 = vm.select.i64 %3, %zero_1, %c-1 : i64
  %5 = vm.trunc.i64.i32 %4 : i64 -> i32
  vm.global.store.i32 %3, @_device_query_0 : i32
  vm.global.store.ref %ref_3, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.br_table %5 {
    default: ^bb2,
    0: ^bb1
  }
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  %_main_dispatch_0_vulkan_spirv_fb = vm.const.ref.rodata @_main_dispatch_0_vulkan_spirv_fb : !vm.buffer
  %ref_4 = vm.call.variadic @hal.executable.create(%ref, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0, %_main_dispatch_0_vulkan_spirv_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
  vm.br ^bb3(%ref_4 : !vm.ref<!hal.executable>)
^bb2:  // pred: ^bb0
  vm.br ^bb3(%null : !vm.ref<!hal.executable>)
^bb3(%6: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
  vm.global.store.ref %6, @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
  vm.return
}

// -----// IR Dump After Inliner (inline) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @_main_dispatch_0_vulkan_spirv_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<5020xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 {alignment = 1 : i64} "vulkan-spirv-fb"
  vm.func private @main() {
    %c32000 = vm.const.i32 32000
    %c3 = vm.const.i32 3
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c2 = vm.const.i32 2
    %c17 = vm.const.i32 17
    %c3075 = vm.const.i32 3075
    %c48 = vm.const.i32 48
    %zero = vm.const.i32.zero
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c8192 = vm.const.i64 8192
    %c15360 = vm.const.i32 15360
    %c262144000 = vm.const.i64 262144000
    %c27648 = vm.const.i32 27648
    %c64000 = vm.const.i64 64000
    %zero_0 = vm.const.i64.zero
    %c128000 = vm.const.i64 128000
    %c-1 = vm.const.i64 -1
    %c-1_1 = vm.const.i32 -1
    %c1 = vm.const.i64 1
    %c32000_2 = vm.const.i64 32000
    %c553648144 = vm.const.i32 553648144
    %c1_3 = vm.const.i32 1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable__main_dispatch_0 = vm.global.load.ref @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_4 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_5 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_4, %zero, %c48, %c3075, %c8192) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    %ref_6 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.call @hal.command_buffer.fill_buffer(%ref_6, %ref_5, %zero_0, %c8192, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_6, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_6) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_7 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %1 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_7, [%ref_6]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %2 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %2, ^bb3(%2 : i32), ^bb1
  ^bb1:  // pred: ^bb0
    %ref_8 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_9 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c262144000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %3 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    %ref_10 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.call @hal.command_buffer.fill_buffer(%ref_10, %ref_9, %zero_0, %c262144000, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_11 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %4 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %5 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %5, ^bb3(%5 : i32), ^bb2
  ^bb2:  // pred: ^bb1
    %6 = vm.call @hal.buffer.length(%ref_5) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
    %7 = vm.call @hal.buffer.length(%ref_9) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
    %ref_12 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_13 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_12, %zero, %c48, %c3075, %c128000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %8 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    %ref_14 = vm.call @hal.command_buffer.create(%ref, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    %9 = vm.select.i64 %_device_query_0, %zero_0, %c-1 : i64
    %10 = vm.trunc.i64.i32 %9 : i64 -> i32
    vm.br_table %10 {
      default: ^bb5,
      0: ^bb4
    }
  ^bb3(%11: i32):  // 3 preds: ^bb0, ^bb1, ^bb5
    vm.fail %11, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb2
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_14, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_5, %zero_0, %6), (%c1_3, %zero, %ref_9, %zero_0, %7), (%c2, %zero, %ref_13, %zero_0, %c128000)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_14, %_executable__main_dispatch_0, %zero, %c32000, %c1_3, %c1_3) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.br ^bb5
  ^bb5:  // 2 preds: ^bb2, ^bb4
    vm.call @hal.command_buffer.fill_buffer(%ref_14, %ref_13, %c64000, %c64000, %c27648, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_14, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_14) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_15 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %12 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_15, [%ref_14]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %13 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_15]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %13, ^bb3(%13 : i32), ^bb6
  ^bb6:  // pred: ^bb5
    %ref_16 = vm.call.variadic @hal.buffer_view.create(%ref_13, %c64000, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    %ref_17 = vm.call.variadic @hal.buffer_view.create(%ref_13, %zero_0, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    %has_check_expect_eq = vm.import.resolved @check.expect_eq : i32
    vm.cond_br %has_check_expect_eq, ^bb7, ^bb8
  ^bb7:  // pred: ^bb6
    vm.call @check.expect_eq(%ref, %ref_17, %ref_16) : (!vm.ref<!hal.device>, !vm.ref<!hal.buffer_view>, !vm.ref<!hal.buffer_view>) -> ()
    vm.br ^bb8
  ^bb8:  // 2 preds: ^bb6, ^bb7
    vm.return
  }
  vm.export @main attributes {iree.abi.stub}
  vm.import private optional @check.expect_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
  vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
  vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %zero_1 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 = vm.const.ref.rodata @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_2 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_3 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_2]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    %4 = vm.select.i64 %3, %zero_1, %c-1 : i64
    %5 = vm.trunc.i64.i32 %4 : i64 -> i32
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_3, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.br_table %5 {
      default: ^bb2,
      0: ^bb1
    }
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_main_dispatch_0_vulkan_spirv_fb = vm.const.ref.rodata @_main_dispatch_0_vulkan_spirv_fb : !vm.buffer
    %ref_4 = vm.call.variadic @hal.executable.create(%ref, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0, %_main_dispatch_0_vulkan_spirv_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_4 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%6: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %6, @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  }
}

// -----// IR Dump After CSE (cse) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @_main_dispatch_0_vulkan_spirv_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"0x080000005350564578ECFFFF14000000940000000400000001000000480000000100000004000000340000005F6D61696E5F64697370617463685F305F6D61746D756C5F7472616E73706F73655F625F3178333230303078343039365F66313600000000DCECFFFF08000000090000002D00000064756D70732F636F6E666967757265645F6D6F64756C655F5F6D61696E5F64697370617463685F302E6D6C6972000000B8040000030223070003010016000000D200000000000000110002000100000011000200511100001100020009000000110002003F0000000A000B005350565F4B48525F73746F726167655F6275666665725F73746F726167655F636C617373000000000A0007005350565F4B48525F31366269745F73746F726167650000000E00030000000000010000000F00130005000000140000005F6D61696E5F64697370617463685F305F6D61746D756C5F7472616E73706F73655F625F3178333230303078343039365F66313600000000050000000400000010000600140000001100000040000000010000000100000005000900040000005F5F6275696C74696E5F5F576F726B67726F757049645F5F0000000005000A00050000005F5F6275696C74696E5F5F4C6F63616C496E766F636174696F6E49645F5F0000050007000B0000005F5F7265736F757263655F7661725F305F305F00050007000C0000005F5F7265736F757263655F7661725F305F315F0005000700110000005F5F7265736F757263655F7661725F305F325F0005001000140000005F6D61696E5F64697370617463685F305F6D61746D756C5F7472616E73706F73655F625F3178333230303078343039365F6631360000000047000400040000000B0000001A00000047000400050000000B0000001B000000470004000800000006000000100000004800050007000000000000002300000000000000470003000700000002000000470004000B0000002100000000000000470004000B0000002200000000000000470004000C0000002100000001000000470004000C0000002200000000000000470004000F0000000600000002000000480005000E000000000000002300000000000000470003000E0000000200000047000400110000002100000002000000470004001100000022000000000000001500040003000000200000000000000017000400020000000300000003000000200004000100000001000000020000003B0004000100000004000000010000003B000400010000000500000001000000160003000A0000002000000017000400090000000A000000040000001D00030008000000090000001E000300070000000800000020000400060000000C000000070000003B000400060000000B0000000C0000003B000400060000000C0000000C0000001600030010000000100000001D0003000F000000100000001E0003000E0000000F000000200004000D0000000C0000000E0000003B0004000D000000110000000C00000013000200130000002100030012000000130000002B0004000300000016000000400000002B0004000300000017000000800000002B0004000300000018000000C00000002B0004000300000019000000000100002B000400030000001A000000400100002B000400030000001B000000800100002B000400030000001C000000C00100002B000400030000001D00000000020000170004001E00000010000000040000002B0004001000000020000000000000002C0007001E0000001F000000200000002000000020000000200000002B00040003000000210000000000000020000400290000000C0000000900000017000400580000000A000000020000002B00040003000000C90000000300000014000200CB00000020000400D00000000C000000100000003600050013000000140000000000000012000000F8000200150000003D00040002000000220000000500000051000500030000002300000022000000000000003D0004000200000024000000040000005100050003000000250000002400000000000000840005000300000026000000250000001D0000008000050003000000270000002300000026000000800005000300000028000000270000001C00000041000600290000002A0000000C00000021000000280000003D000400090000002B0000002A00000080000500030000002C000000230000001C00000041000600290000002D0000000B000000210000002C0000003D000400090000002E0000002D00000080000500030000002F000000270000001B0000004100060029000000300000000C000000210000002F0000003D000400090000003100000030000000800005000300000032000000230000001B0000004100060029000000330000000B00000021000000320000003D000400090000003400000033000000800005000300000035000000270000001A0000004100060029000000360000000C00000021000000350000003D000400090000003700000036000000800005000300000038000000230000001A0000004100060029000000390000000B00000021000000380000003D000400090000003A0000003900000080000500030000003B000000270000001900000041000600290000003C0000000C000000210000003B0000003D000400090000003D0000003C00000080000500030000003E000000230000001900000041000600290000003F0000000B000000210000003E0000003D00040009000000400000003F00000080000500030000004100000027000000180000004100060029000000420000000C00000021000000410000003D00040009000000430000004200000080000500030000004400000023000000180000004100060029000000450000000B00000021000000440000003D00040009000000460000004500000080000500030000004700000027000000170000004100060029000000480000000C00000021000000470000003D00040009000000490000004800000080000500030000004A000000230000001700000041000600290000004B0000000B000000210000004A0000003D000400090000004C0000004B00000080000500030000004D000000270000001600000041000600290000004E0000000C000000210000004D0000003D000400090000004F0000004E00000080000500030000005000000023000000160000004100060029000000510000000B00000021000000500000003D00040009000000520000005100000080000500030000005300000026000000230000004100060029000000540000000C00000021000000530000003D0004000900000055000000540000004100060029000000560000000B00000021000000230000003D0004000900000057000000560000004F00070058000000590000002E0000002E00000000000000010000007C0004001E0000005A000000590000004F000700580000005B0000002B0000002B00000000000000010000007C0004001E0000005C0000005B000000850005001E0000005D0000005A0000005C0000004F000700580000005E0000002E0000002E00000002000000030000007C0004001E0000005F0000005E0000004F00070058000000600000002B0000002B00000002000000030000007C0004001E0000006100000060000000850005001E000000620000005F00000061000000810005001E000000630000005D0000001F000000810005001E00000064000000620000001F0000004F0007005800000065000000340000003400000000000000010000007C0004001E00000066000000650000004F0007005800000067000000310000003100000000000000010000007C0004001E0000006800000067000000850005001E0000006900000066000000680000004F000700580000006A000000340000003400000002000000030000007C0004001E0000006B0000006A0000004F000700580000006C000000310000003100000002000000030000007C0004001E0000006D0000006C000000850005001E0000006E0000006B0000006D000000810005001E0000006F0000006900000063000000810005001E000000700000006E000000640000004F00070058000000710000003A0000003A00000000000000010000007C0004001E00000072000000710000004F0007005800000073000000370000003700000000000000010000007C0004001E0000007400000073000000850005001E0000007500000072000000740000004F00070058000000760000003A0000003A00000002000000030000007C0004001E00000077000000760000004F0007005800000078000000370000003700000002000000030000007C0004001E0000007900000078000000850005001E0000007A0000007700000079000000810005001E0000007B000000750000006F000000810005001E0000007C0000007A000000700000004F000700580000007D000000400000004000000000000000010000007C0004001E0000007E0000007D0000004F000700580000007F0000003D0000003D00000000000000010000007C0004001E000000800000007F000000850005001E000000810000007E000000800000004F0007005800000082000000400000004000000002000000030000007C0004001E00000083000000820000004F00070058000000840000003D0000003D00000002000000030000007C0004001E0000008500000084000000850005001E000000860000008300000085000000810005001E00000087000000810000007B000000810005001E00000088000000860000007C0000004F0007005800000089000000460000004600000000000000010000007C0004001E0000008A000000890000004F000700580000008B000000430000004300000000000000010000007C0004001E0000008C0000008B000000850005001E0000008D0000008A0000008C0000004F000700580000008E000000460000004600000002000000030000007C0004001E0000008F0000008E0000004F0007005800000090000000430000004300000002000000030000007C0004001E0000009100000090000000850005001E000000920000008F00000091000000810005001E000000930000008D00000087000000810005001E0000009400000092000000880000004F00070058000000950000004C0000004C00000000000000010000007C0004001E00000096000000950000004F0007005800000097000000490000004900000000000000010000007C0004001E0000009800000097000000850005001E0000009900000096000000980000004F000700580000009A0000004C0000004C00000002000000030000007C0004001E0000009B0000009A0000004F000700580000009C000000490000004900000002000000030000007C0004001E0000009D0000009C000000850005001E0000009E0000009B0000009D000000810005001E0000009F0000009900000093000000810005001E000000A00000009E000000940000004F00070058000000A1000000520000005200000000000000010000007C0004001E000000A2000000A10000004F00070058000000A30000004F0000004F00000000000000010000007C0004001E000000A4000000A3000000850005001E000000A5000000A2000000A40000004F00070058000000A6000000520000005200000002000000030000007C0004001E000000A7000000A60000004F00070058000000A80000004F0000004F00000002000000030000007C0004001E000000A9000000A8000000850005001E000000AA000000A7000000A9000000810005001E000000AB000000A50000009F000000810005001E000000AC000000AA000000A00000004F00070058000000AD000000570000005700000000000000010000007C0004001E000000AE000000AD0000004F00070058000000AF000000550000005500000000000000010000007C0004001E000000B0000000AF000000850005001E000000B1000000AE000000B00000004F00070058000000B2000000570000005700000002000000030000007C0004001E000000B3000000B20000004F00070058000000B4000000550000005500000002000000030000007C0004001E000000B5000000B4000000850005001E000000B6000000B3000000B5000000810005001E000000B7000000B1000000AB000000810005001E000000B8000000B6000000AC0000005100050010000000B9000000B7000000000000005100050010000000BA000000B7000000010000005100050010000000BB000000B7000000020000005100050010000000BC000000B7000000030000008100050010000000BD000000B9000000BA0000008100050010000000BE000000BD000000BB0000008100050010000000BF000000BE000000BC0000005100050010000000C0000000B8000000000000005100050010000000C1000000B8000000010000005100050010000000C2000000B8000000020000005100050010000000C3000000B8000000030000008100050010000000C4000000C0000000C10000008100050010000000C5000000C4000000C20000008100050010000000C6000000C5000000C30000008100050010000000C7000000BF000000C60000005E01060010000000C8000000C900000000000000C70000008100050010000000CA000000C800000020000000AA000500CB000000CC0000002300000021000000F9000200CD000000F8000200CD000000F7000300CF00000000000000FA000400CC000000CE000000CF000000F8000200CE00000041000600D0000000D10000001100000021000000250000003E000300D1000000CA000000F9000200CF000000F8000200CF000000FD0001003800010008000C00040008000C0010000400000008000C00"> : vector<5020xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 {alignment = 1 : i64} "vulkan-spirv-fb"
  vm.func private @main() {
    %c32000 = vm.const.i32 32000
    %c3 = vm.const.i32 3
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c2 = vm.const.i32 2
    %c17 = vm.const.i32 17
    %c3075 = vm.const.i32 3075
    %c48 = vm.const.i32 48
    %zero = vm.const.i32.zero
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c8192 = vm.const.i64 8192
    %c15360 = vm.const.i32 15360
    %c262144000 = vm.const.i64 262144000
    %c27648 = vm.const.i32 27648
    %c64000 = vm.const.i64 64000
    %zero_0 = vm.const.i64.zero
    %c128000 = vm.const.i64 128000
    %c-1 = vm.const.i64 -1
    %c-1_1 = vm.const.i32 -1
    %c1 = vm.const.i64 1
    %c32000_2 = vm.const.i64 32000
    %c553648144 = vm.const.i32 553648144
    %c1_3 = vm.const.i32 1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable__main_dispatch_0 = vm.global.load.ref @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_4 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_5 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_4, %zero, %c48, %c3075, %c8192) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    %ref_6 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.call @hal.command_buffer.fill_buffer(%ref_6, %ref_5, %zero_0, %c8192, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_6, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_6) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_7 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %1 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_7, [%ref_6]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %2 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %2, ^bb3(%2 : i32), ^bb1
  ^bb1:  // pred: ^bb0
    %ref_8 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_9 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c262144000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %3 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    %ref_10 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.call @hal.command_buffer.fill_buffer(%ref_10, %ref_9, %zero_0, %c262144000, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_11 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %4 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %5 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %5, ^bb3(%5 : i32), ^bb2
  ^bb2:  // pred: ^bb1
    %6 = vm.call @hal.buffer.length(%ref_5) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
    %7 = vm.call @hal.buffer.length(%ref_9) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
    %ref_12 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_13 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_12, %zero, %c48, %c3075, %c128000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %8 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    %ref_14 = vm.call @hal.command_buffer.create(%ref, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    %9 = vm.select.i64 %_device_query_0, %zero_0, %c-1 : i64
    %10 = vm.trunc.i64.i32 %9 : i64 -> i32
    vm.br_table %10 {
      default: ^bb5,
      0: ^bb4
    }
  ^bb3(%11: i32):  // 3 preds: ^bb0, ^bb1, ^bb5
    vm.fail %11, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb2
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_14, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_5, %zero_0, %6), (%c1_3, %zero, %ref_9, %zero_0, %7), (%c2, %zero, %ref_13, %zero_0, %c128000)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_14, %_executable__main_dispatch_0, %zero, %c32000, %c1_3, %c1_3) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.br ^bb5
  ^bb5:  // 2 preds: ^bb2, ^bb4
    vm.call @hal.command_buffer.fill_buffer(%ref_14, %ref_13, %c64000, %c64000, %c27648, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_14, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_14) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_15 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %12 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_15, [%ref_14]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %13 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_15]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %13, ^bb3(%13 : i32), ^bb6
  ^bb6:  // pred: ^bb5
    %ref_16 = vm.call.variadic @hal.buffer_view.create(%ref_13, %c64000, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    %ref_17 = vm.call.variadic @hal.buffer_view.create(%ref_13, %zero_0, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    %has_check_expect_eq = vm.import.resolved @check.expect_eq : i32
    vm.cond_br %has_check_expect_eq, ^bb7, ^bb8
  ^bb7:  // pred: ^bb6
    vm.call @check.expect_eq(%ref, %ref_17, %ref_16) : (!vm.ref<!hal.device>, !vm.ref<!hal.buffer_view>, !vm.ref<!hal.buffer_view>) -> ()
    vm.br ^bb8
  ^bb8:  // 2 preds: ^bb6, ^bb7
    vm.return
  }
  vm.export @main attributes {iree.abi.stub}
  vm.import private optional @check.expect_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
  vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
  vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %zero_1 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 = vm.const.ref.rodata @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_2 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_3 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_2]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    %4 = vm.select.i64 %3, %zero_1, %c-1 : i64
    %5 = vm.trunc.i64.i32 %4 : i64 -> i32
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_3, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.br_table %5 {
      default: ^bb2,
      0: ^bb1
    }
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_main_dispatch_0_vulkan_spirv_fb = vm.const.ref.rodata @_main_dispatch_0_vulkan_spirv_fb : !vm.buffer
    %ref_4 = vm.call.variadic @hal.executable.create(%ref, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0, %_main_dispatch_0_vulkan_spirv_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_4 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%6: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %6, @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @_main_dispatch_0_vulkan_spirv_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"0x080000005350564578ECFFFF14000000940000000400000001000000480000000100000004000000340000005F6D61696E5F64697370617463685F305F6D61746D756C5F7472616E73706F73655F625F3178333230303078343039365F66313600000000DCECFFFF08000000090000002D00000064756D70732F636F6E666967757265645F6D6F64756C655F5F6D61696E5F64697370617463685F302E6D6C6972000000B8040000030223070003010016000000D200000000000000110002000100000011000200511100001100020009000000110002003F0000000A000B005350565F4B48525F73746F726167655F6275666665725F73746F726167655F636C617373000000000A0007005350565F4B48525F31366269745F73746F726167650000000E00030000000000010000000F00130005000000140000005F6D61696E5F64697370617463685F305F6D61746D756C5F7472616E73706F73655F625F3178333230303078343039365F66313600000000050000000400000010000600140000001100000040000000010000000100000005000900040000005F5F6275696C74696E5F5F576F726B67726F757049645F5F0000000005000A00050000005F5F6275696C74696E5F5F4C6F63616C496E766F636174696F6E49645F5F0000050007000B0000005F5F7265736F757263655F7661725F305F305F00050007000C0000005F5F7265736F757263655F7661725F305F315F0005000700110000005F5F7265736F757263655F7661725F305F325F0005001000140000005F6D61696E5F64697370617463685F305F6D61746D756C5F7472616E73706F73655F625F3178333230303078343039365F6631360000000047000400040000000B0000001A00000047000400050000000B0000001B000000470004000800000006000000100000004800050007000000000000002300000000000000470003000700000002000000470004000B0000002100000000000000470004000B0000002200000000000000470004000C0000002100000001000000470004000C0000002200000000000000470004000F0000000600000002000000480005000E000000000000002300000000000000470003000E0000000200000047000400110000002100000002000000470004001100000022000000000000001500040003000000200000000000000017000400020000000300000003000000200004000100000001000000020000003B0004000100000004000000010000003B000400010000000500000001000000160003000A0000002000000017000400090000000A000000040000001D00030008000000090000001E000300070000000800000020000400060000000C000000070000003B000400060000000B0000000C0000003B000400060000000C0000000C0000001600030010000000100000001D0003000F000000100000001E0003000E0000000F000000200004000D0000000C0000000E0000003B0004000D000000110000000C00000013000200130000002100030012000000130000002B0004000300000016000000400000002B0004000300000017000000800000002B0004000300000018000000C00000002B0004000300000019000000000100002B000400030000001A000000400100002B000400030000001B000000800100002B000400030000001C000000C00100002B000400030000001D00000000020000170004001E00000010000000040000002B0004001000000020000000000000002C0007001E0000001F000000200000002000000020000000200000002B00040003000000210000000000000020000400290000000C0000000900000017000400580000000A000000020000002B00040003000000C90000000300000014000200CB00000020000400D00000000C000000100000003600050013000000140000000000000012000000F8000200150000003D00040002000000220000000500000051000500030000002300000022000000000000003D0004000200000024000000040000005100050003000000250000002400000000000000840005000300000026000000250000001D0000008000050003000000270000002300000026000000800005000300000028000000270000001C00000041000600290000002A0000000C00000021000000280000003D000400090000002B0000002A00000080000500030000002C000000230000001C00000041000600290000002D0000000B000000210000002C0000003D000400090000002E0000002D00000080000500030000002F000000270000001B0000004100060029000000300000000C000000210000002F0000003D000400090000003100000030000000800005000300000032000000230000001B0000004100060029000000330000000B00000021000000320000003D000400090000003400000033000000800005000300000035000000270000001A0000004100060029000000360000000C00000021000000350000003D000400090000003700000036000000800005000300000038000000230000001A0000004100060029000000390000000B00000021000000380000003D000400090000003A0000003900000080000500030000003B000000270000001900000041000600290000003C0000000C000000210000003B0000003D000400090000003D0000003C00000080000500030000003E000000230000001900000041000600290000003F0000000B000000210000003E0000003D00040009000000400000003F00000080000500030000004100000027000000180000004100060029000000420000000C00000021000000410000003D00040009000000430000004200000080000500030000004400000023000000180000004100060029000000450000000B00000021000000440000003D00040009000000460000004500000080000500030000004700000027000000170000004100060029000000480000000C00000021000000470000003D00040009000000490000004800000080000500030000004A000000230000001700000041000600290000004B0000000B000000210000004A0000003D000400090000004C0000004B00000080000500030000004D000000270000001600000041000600290000004E0000000C000000210000004D0000003D000400090000004F0000004E00000080000500030000005000000023000000160000004100060029000000510000000B00000021000000500000003D00040009000000520000005100000080000500030000005300000026000000230000004100060029000000540000000C00000021000000530000003D0004000900000055000000540000004100060029000000560000000B00000021000000230000003D0004000900000057000000560000004F00070058000000590000002E0000002E00000000000000010000007C0004001E0000005A000000590000004F000700580000005B0000002B0000002B00000000000000010000007C0004001E0000005C0000005B000000850005001E0000005D0000005A0000005C0000004F000700580000005E0000002E0000002E00000002000000030000007C0004001E0000005F0000005E0000004F00070058000000600000002B0000002B00000002000000030000007C0004001E0000006100000060000000850005001E000000620000005F00000061000000810005001E000000630000005D0000001F000000810005001E00000064000000620000001F0000004F0007005800000065000000340000003400000000000000010000007C0004001E00000066000000650000004F0007005800000067000000310000003100000000000000010000007C0004001E0000006800000067000000850005001E0000006900000066000000680000004F000700580000006A000000340000003400000002000000030000007C0004001E0000006B0000006A0000004F000700580000006C000000310000003100000002000000030000007C0004001E0000006D0000006C000000850005001E0000006E0000006B0000006D000000810005001E0000006F0000006900000063000000810005001E000000700000006E000000640000004F00070058000000710000003A0000003A00000000000000010000007C0004001E00000072000000710000004F0007005800000073000000370000003700000000000000010000007C0004001E0000007400000073000000850005001E0000007500000072000000740000004F00070058000000760000003A0000003A00000002000000030000007C0004001E00000077000000760000004F0007005800000078000000370000003700000002000000030000007C0004001E0000007900000078000000850005001E0000007A0000007700000079000000810005001E0000007B000000750000006F000000810005001E0000007C0000007A000000700000004F000700580000007D000000400000004000000000000000010000007C0004001E0000007E0000007D0000004F000700580000007F0000003D0000003D00000000000000010000007C0004001E000000800000007F000000850005001E000000810000007E000000800000004F0007005800000082000000400000004000000002000000030000007C0004001E00000083000000820000004F00070058000000840000003D0000003D00000002000000030000007C0004001E0000008500000084000000850005001E000000860000008300000085000000810005001E00000087000000810000007B000000810005001E00000088000000860000007C0000004F0007005800000089000000460000004600000000000000010000007C0004001E0000008A000000890000004F000700580000008B000000430000004300000000000000010000007C0004001E0000008C0000008B000000850005001E0000008D0000008A0000008C0000004F000700580000008E000000460000004600000002000000030000007C0004001E0000008F0000008E0000004F0007005800000090000000430000004300000002000000030000007C0004001E0000009100000090000000850005001E000000920000008F00000091000000810005001E000000930000008D00000087000000810005001E0000009400000092000000880000004F00070058000000950000004C0000004C00000000000000010000007C0004001E00000096000000950000004F0007005800000097000000490000004900000000000000010000007C0004001E0000009800000097000000850005001E0000009900000096000000980000004F000700580000009A0000004C0000004C00000002000000030000007C0004001E0000009B0000009A0000004F000700580000009C000000490000004900000002000000030000007C0004001E0000009D0000009C000000850005001E0000009E0000009B0000009D000000810005001E0000009F0000009900000093000000810005001E000000A00000009E000000940000004F00070058000000A1000000520000005200000000000000010000007C0004001E000000A2000000A10000004F00070058000000A30000004F0000004F00000000000000010000007C0004001E000000A4000000A3000000850005001E000000A5000000A2000000A40000004F00070058000000A6000000520000005200000002000000030000007C0004001E000000A7000000A60000004F00070058000000A80000004F0000004F00000002000000030000007C0004001E000000A9000000A8000000850005001E000000AA000000A7000000A9000000810005001E000000AB000000A50000009F000000810005001E000000AC000000AA000000A00000004F00070058000000AD000000570000005700000000000000010000007C0004001E000000AE000000AD0000004F00070058000000AF000000550000005500000000000000010000007C0004001E000000B0000000AF000000850005001E000000B1000000AE000000B00000004F00070058000000B2000000570000005700000002000000030000007C0004001E000000B3000000B20000004F00070058000000B4000000550000005500000002000000030000007C0004001E000000B5000000B4000000850005001E000000B6000000B3000000B5000000810005001E000000B7000000B1000000AB000000810005001E000000B8000000B6000000AC0000005100050010000000B9000000B7000000000000005100050010000000BA000000B7000000010000005100050010000000BB000000B7000000020000005100050010000000BC000000B7000000030000008100050010000000BD000000B9000000BA0000008100050010000000BE000000BD000000BB0000008100050010000000BF000000BE000000BC0000005100050010000000C0000000B8000000000000005100050010000000C1000000B8000000010000005100050010000000C2000000B8000000020000005100050010000000C3000000B8000000030000008100050010000000C4000000C0000000C10000008100050010000000C5000000C4000000C20000008100050010000000C6000000C5000000C30000008100050010000000C7000000BF000000C60000005E01060010000000C8000000C900000000000000C70000008100050010000000CA000000C800000020000000AA000500CB000000CC0000002300000021000000F9000200CD000000F8000200CD000000F7000300CF00000000000000FA000400CC000000CE000000CF000000F8000200CE00000041000600D0000000D10000001100000021000000250000003E000300D1000000CA000000F9000200CF000000F8000200CF000000FD0001003800010008000C00040008000C0010000400000008000C00"> : vector<5020xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 {alignment = 1 : i64} "vulkan-spirv-fb"
  vm.func private @main() {
    %c32000 = vm.const.i32 32000
    %c3 = vm.const.i32 3
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c2 = vm.const.i32 2
    %c17 = vm.const.i32 17
    %c3075 = vm.const.i32 3075
    %c48 = vm.const.i32 48
    %zero = vm.const.i32.zero
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c8192 = vm.const.i64 8192
    %c15360 = vm.const.i32 15360
    %c262144000 = vm.const.i64 262144000
    %c27648 = vm.const.i32 27648
    %c64000 = vm.const.i64 64000
    %zero_0 = vm.const.i64.zero
    %c128000 = vm.const.i64 128000
    %c-1 = vm.const.i64 -1
    %c-1_1 = vm.const.i32 -1
    %c1 = vm.const.i64 1
    %c32000_2 = vm.const.i64 32000
    %c553648144 = vm.const.i32 553648144
    %c1_3 = vm.const.i32 1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable__main_dispatch_0 = vm.global.load.ref @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_4 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_5 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_4, %zero, %c48, %c3075, %c8192) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    %ref_6 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.call @hal.command_buffer.fill_buffer(%ref_6, %ref_5, %zero_0, %c8192, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_6, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_6) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_7 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %1 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_7, [%ref_6]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %2 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %2, ^bb3(%2 : i32), ^bb1
  ^bb1:  // pred: ^bb0
    %ref_8 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_9 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c262144000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %3 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    %ref_10 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.call @hal.command_buffer.fill_buffer(%ref_10, %ref_9, %zero_0, %c262144000, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_11 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %4 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %5 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %5, ^bb3(%5 : i32), ^bb2
  ^bb2:  // pred: ^bb1
    %6 = vm.call @hal.buffer.length(%ref_5) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
    %7 = vm.call @hal.buffer.length(%ref_9) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
    %ref_12 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_13 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_12, %zero, %c48, %c3075, %c128000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %8 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    %ref_14 = vm.call @hal.command_buffer.create(%ref, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    %9 = vm.select.i64 %_device_query_0, %zero_0, %c-1 : i64
    %10 = vm.trunc.i64.i32 %9 : i64 -> i32
    vm.br_table %10 {
      default: ^bb5,
      0: ^bb4
    }
  ^bb3(%11: i32):  // 3 preds: ^bb0, ^bb1, ^bb5
    vm.fail %11, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb2
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_14, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_5, %zero_0, %6), (%c1_3, %zero, %ref_9, %zero_0, %7), (%c2, %zero, %ref_13, %zero_0, %c128000)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_14, %_executable__main_dispatch_0, %zero, %c32000, %c1_3, %c1_3) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.br ^bb5
  ^bb5:  // 2 preds: ^bb2, ^bb4
    vm.call @hal.command_buffer.fill_buffer(%ref_14, %ref_13, %c64000, %c64000, %c27648, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_14, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_14) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_15 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %12 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_15, [%ref_14]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %13 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_15]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %13, ^bb3(%13 : i32), ^bb6
  ^bb6:  // pred: ^bb5
    %ref_16 = vm.call.variadic @hal.buffer_view.create(%ref_13, %c64000, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    %ref_17 = vm.call.variadic @hal.buffer_view.create(%ref_13, %zero_0, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    %has_check_expect_eq = vm.import.resolved @check.expect_eq : i32
    vm.cond_br %has_check_expect_eq, ^bb7, ^bb8
  ^bb7:  // pred: ^bb6
    vm.call @check.expect_eq(%ref, %ref_17, %ref_16) : (!vm.ref<!hal.device>, !vm.ref<!hal.buffer_view>, !vm.ref<!hal.buffer_view>) -> ()
    vm.br ^bb8
  ^bb8:  // 2 preds: ^bb6, ^bb7
    vm.return
  }
  vm.export @main attributes {iree.abi.stub}
  vm.import private optional @check.expect_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
  vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
  vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %zero_1 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 = vm.const.ref.rodata @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_2 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_3 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_2]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    %4 = vm.select.i64 %3, %zero_1, %c-1 : i64
    %5 = vm.trunc.i64.i32 %4 : i64 -> i32
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_3, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.br_table %5 {
      default: ^bb2,
      0: ^bb1
    }
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_main_dispatch_0_vulkan_spirv_fb = vm.const.ref.rodata @_main_dispatch_0_vulkan_spirv_fb : !vm.buffer
    %ref_4 = vm.call.variadic @hal.executable.create(%ref, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0, %_main_dispatch_0_vulkan_spirv_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_4 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%6: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %6, @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  }
}

// -----// IR Dump After DropCompilerHints (iree-util-drop-compiler-hints) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @_main_dispatch_0_vulkan_spirv_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<5020xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 {alignment = 1 : i64} "vulkan-spirv-fb"
  vm.func private @main() {
    %c32000 = vm.const.i32 32000
    %c3 = vm.const.i32 3
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c2 = vm.const.i32 2
    %c17 = vm.const.i32 17
    %c3075 = vm.const.i32 3075
    %c48 = vm.const.i32 48
    %zero = vm.const.i32.zero
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c8192 = vm.const.i64 8192
    %c15360 = vm.const.i32 15360
    %c262144000 = vm.const.i64 262144000
    %c27648 = vm.const.i32 27648
    %c64000 = vm.const.i64 64000
    %zero_0 = vm.const.i64.zero
    %c128000 = vm.const.i64 128000
    %c-1 = vm.const.i64 -1
    %c-1_1 = vm.const.i32 -1
    %c1 = vm.const.i64 1
    %c32000_2 = vm.const.i64 32000
    %c553648144 = vm.const.i32 553648144
    %c1_3 = vm.const.i32 1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable__main_dispatch_0 = vm.global.load.ref @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_4 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_5 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_4, %zero, %c48, %c3075, %c8192) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    %ref_6 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.call @hal.command_buffer.fill_buffer(%ref_6, %ref_5, %zero_0, %c8192, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_6, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_6) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_7 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %1 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_7, [%ref_6]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %2 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %2, ^bb3(%2 : i32), ^bb1
  ^bb1:  // pred: ^bb0
    %ref_8 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_9 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c262144000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %3 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    %ref_10 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.call @hal.command_buffer.fill_buffer(%ref_10, %ref_9, %zero_0, %c262144000, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_11 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %4 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %5 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %5, ^bb3(%5 : i32), ^bb2
  ^bb2:  // pred: ^bb1
    %6 = vm.call @hal.buffer.length(%ref_5) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
    %7 = vm.call @hal.buffer.length(%ref_9) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
    %ref_12 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_13 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_12, %zero, %c48, %c3075, %c128000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %8 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    %ref_14 = vm.call @hal.command_buffer.create(%ref, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    %9 = vm.select.i64 %_device_query_0, %zero_0, %c-1 : i64
    %10 = vm.trunc.i64.i32 %9 : i64 -> i32
    vm.br_table %10 {
      default: ^bb5,
      0: ^bb4
    }
  ^bb3(%11: i32):  // 3 preds: ^bb0, ^bb1, ^bb5
    vm.fail %11, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb2
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_14, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_5, %zero_0, %6), (%c1_3, %zero, %ref_9, %zero_0, %7), (%c2, %zero, %ref_13, %zero_0, %c128000)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_14, %_executable__main_dispatch_0, %zero, %c32000, %c1_3, %c1_3) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.br ^bb5
  ^bb5:  // 2 preds: ^bb2, ^bb4
    vm.call @hal.command_buffer.fill_buffer(%ref_14, %ref_13, %c64000, %c64000, %c27648, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_14, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_14) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_15 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %12 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_15, [%ref_14]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %13 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_15]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %13, ^bb3(%13 : i32), ^bb6
  ^bb6:  // pred: ^bb5
    %ref_16 = vm.call.variadic @hal.buffer_view.create(%ref_13, %c64000, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    %ref_17 = vm.call.variadic @hal.buffer_view.create(%ref_13, %zero_0, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    %has_check_expect_eq = vm.import.resolved @check.expect_eq : i32
    vm.cond_br %has_check_expect_eq, ^bb7, ^bb8
  ^bb7:  // pred: ^bb6
    vm.call @check.expect_eq(%ref, %ref_17, %ref_16) : (!vm.ref<!hal.device>, !vm.ref<!hal.buffer_view>, !vm.ref<!hal.buffer_view>) -> ()
    vm.br ^bb8
  ^bb8:  // 2 preds: ^bb6, ^bb7
    vm.return
  }
  vm.export @main attributes {iree.abi.stub}
  vm.import private optional @check.expect_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>)
  vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
  vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %zero_1 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 = vm.const.ref.rodata @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_2 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_3 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_2]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    %4 = vm.select.i64 %3, %zero_1, %c-1 : i64
    %5 = vm.trunc.i64.i32 %4 : i64 -> i32
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_3, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.br_table %5 {
      default: ^bb2,
      0: ^bb1
    }
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_main_dispatch_0_vulkan_spirv_fb = vm.const.ref.rodata @_main_dispatch_0_vulkan_spirv_fb : !vm.buffer
    %ref_4 = vm.call.variadic @hal.executable.create(%ref, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0, %_main_dispatch_0_vulkan_spirv_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_4 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%6: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %6, @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  }
}

// -----// IR Dump After mlir::iree_compiler::IREE::VM::OrdinalAllocationPass (iree-vm-ordinal-allocation) //----- //
vm.module public @module attributes {ordinal_counts = #vm.ordinal_counts<import_funcs = 18, export_funcs = 2, internal_funcs = 2, global_bytes = 4, global_refs = 2, rodatas = 3, rwdatas = 0>} {
  vm.global.i32 private mutable @_device_query_0 {ordinal = 0 : i32} : i32
  vm.global.ref private mutable @_pipeline_layout_0 {ordinal = 0 : i32} : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable__main_dispatch_0 {ordinal = 1 : i32} : !vm.ref<!hal.executable>
  vm.rodata private @_main_dispatch_0_vulkan_spirv_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers", ordinal = 0 : i32} dense<"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vector<5020xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64, ordinal = 1 : i32} "hal.executable.format"
  vm.rodata private @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 {alignment = 1 : i64, ordinal = 2 : i32} "vulkan-spirv-fb"
  vm.func private @main() attributes {ordinal = 0 : i32} {
    %c32000 = vm.const.i32 32000
    %c3 = vm.const.i32 3
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c2 = vm.const.i32 2
    %c17 = vm.const.i32 17
    %c3075 = vm.const.i32 3075
    %c48 = vm.const.i32 48
    %zero = vm.const.i32.zero
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c8192 = vm.const.i64 8192
    %c15360 = vm.const.i32 15360
    %c262144000 = vm.const.i64 262144000
    %c27648 = vm.const.i32 27648
    %c64000 = vm.const.i64 64000
    %zero_0 = vm.const.i64.zero
    %c128000 = vm.const.i64 128000
    %c-1 = vm.const.i64 -1
    %c-1_1 = vm.const.i32 -1
    %c1 = vm.const.i64 1
    %c32000_2 = vm.const.i64 32000
    %c553648144 = vm.const.i32 553648144
    %c1_3 = vm.const.i32 1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable__main_dispatch_0 = vm.global.load.ref @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_4 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_5 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_4, %zero, %c48, %c3075, %c8192) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    %ref_6 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.call @hal.command_buffer.fill_buffer(%ref_6, %ref_5, %zero_0, %c8192, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_6, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_6) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_7 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %1 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_4]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_7, [%ref_6]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %2 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_7]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %2, ^bb3(%2 : i32), ^bb1
  ^bb1:  // pred: ^bb0
    %ref_8 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_9 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c262144000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %3 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    %ref_10 = vm.call @hal.command_buffer.create(%ref, %c17, %c1_3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.call @hal.command_buffer.fill_buffer(%ref_10, %ref_9, %zero_0, %c262144000, %c15360, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_11 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %4 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_8]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %5 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %5, ^bb3(%5 : i32), ^bb2
  ^bb2:  // pred: ^bb1
    %6 = vm.call @hal.buffer.length(%ref_5) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
    %7 = vm.call @hal.buffer.length(%ref_9) {nosideeffects} : (!vm.ref<!hal.buffer>) -> i64
    %ref_12 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_13 = vm.call @hal.device.queue.alloca(%ref, %c-1, %null, %ref_12, %zero, %c48, %c3075, %c128000) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %8 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    %ref_14 = vm.call @hal.command_buffer.create(%ref, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    %9 = vm.select.i64 %_device_query_0, %zero_0, %c-1 : i64
    %10 = vm.trunc.i64.i32 %9 : i64 -> i32
    vm.br_table %10 {
      default: ^bb5,
      0: ^bb4
    }
  ^bb3(%11: i32):  // 3 preds: ^bb0, ^bb1, ^bb5
    vm.fail %11, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb2
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_14, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_5, %zero_0, %6), (%c1_3, %zero, %ref_9, %zero_0, %7), (%c2, %zero, %ref_13, %zero_0, %c128000)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_14, %_executable__main_dispatch_0, %zero, %c32000, %c1_3, %c1_3) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.br ^bb5
  ^bb5:  // 2 preds: ^bb2, ^bb4
    vm.call @hal.command_buffer.fill_buffer(%ref_14, %ref_13, %c64000, %c64000, %c27648, %c2) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.buffer>, i64, i64, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_14, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_14) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_15 = vm.call @hal.fence.create(%ref, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %12 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.call.variadic @hal.device.queue.execute(%ref, %c-1, %null, %ref_15, [%ref_14]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %13 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_15]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %13, ^bb3(%13 : i32), ^bb6
  ^bb6:  // pred: ^bb5
    %ref_16 = vm.call.variadic @hal.buffer_view.create(%ref_13, %c64000, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    %ref_17 = vm.call.variadic @hal.buffer_view.create(%ref_13, %zero_0, %c64000, %c553648144, %c1_3, [%c1, %c32000_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    %has_check_expect_eq = vm.import.resolved @check.expect_eq : i32
    vm.cond_br %has_check_expect_eq, ^bb7, ^bb8
  ^bb7:  // pred: ^bb6
    vm.call @check.expect_eq(%ref, %ref_17, %ref_16) : (!vm.ref<!hal.device>, !vm.ref<!hal.buffer_view>, !vm.ref<!hal.buffer_view>) -> ()
    vm.br ^bb8
  ^bb8:  // 2 preds: ^bb6, ^bb7
    vm.return
  }
  vm.export @main attributes {iree.abi.stub, ordinal = 0 : i32}
  vm.import private optional @check.expect_eq(%device : !vm.ref<!hal.device>, %lhs : !vm.ref<!hal.buffer_view>, %rhs : !vm.ref<!hal.buffer_view>) attributes {ordinal = 0 : i32}
  vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, ordinal = 1 : i32}
  vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects, ordinal = 2 : i32}
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, ordinal = 3 : i32}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {ordinal = 4 : i32}
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {ordinal = 5 : i32}
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {ordinal = 6 : i32}
  vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32) attributes {ordinal = 7 : i32}
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {ordinal = 8 : i32}
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {ordinal = 9 : i32}
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, ordinal = 10 : i32}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, ordinal = 11 : i32}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {ordinal = 12 : i32}
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {ordinal = 13 : i32}
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, ordinal = 14 : i32}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {ordinal = 15 : i32}
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {ordinal = 16 : i32, vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, ordinal = 17 : i32}
  vm.export @__init attributes {ordinal = 1 : i32}
  vm.func private @__init() attributes {ordinal = 1 : i32} {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %zero_1 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 = vm.const.ref.rodata @_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0 : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_2 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_3 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_2]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    %4 = vm.select.i64 %3, %zero_1, %c-1 : i64
    %5 = vm.trunc.i64.i32 %4 : i64 -> i32
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_3, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.br_table %5 {
      default: ^bb2,
      0: ^bb1
    }
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_main_dispatch_0_vulkan_spirv_fb = vm.const.ref.rodata @_main_dispatch_0_vulkan_spirv_fb : !vm.buffer
    %ref_4 = vm.call.variadic @hal.executable.create(%ref, %_utf8_vulkan_spirv_fb_6BCD18EEFCFDB5E0, %_main_dispatch_0_vulkan_spirv_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_4 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%6: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %6, @_executable__main_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  }
}

